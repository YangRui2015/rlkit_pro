2020-07-08 19:58:46.554165 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 0 finished
----------------------------------------------  --------------
replay_buffer/size                              2000
trainer/QF1 Loss                                  14.2855
trainer/QF2 Loss                                  14.2431
trainer/Policy Loss                               -4.05487
trainer/Q1 Predictions Mean                        0.000159003
trainer/Q1 Predictions Std                         0.00451315
trainer/Q1 Predictions Max                         0.0118113
trainer/Q1 Predictions Min                        -0.0145004
trainer/Q2 Predictions Mean                        0.0057865
trainer/Q2 Predictions Std                         0.00428939
trainer/Q2 Predictions Max                         0.0202817
trainer/Q2 Predictions Min                        -0.00657601
trainer/Q Targets Mean                             3.68975
trainer/Q Targets Std                              0.819969
trainer/Q Targets Max                              7.03229
trainer/Q Targets Min                              1.6204
trainer/Log Pis Mean                              -4.05496
trainer/Log Pis Std                                0.58149
trainer/Log Pis Max                               -2.63255
trainer/Log Pis Min                               -5.45192
trainer/Policy mu Mean                            -0.000606934
trainer/Policy mu Std                              0.00281677
trainer/Policy mu Max                              0.00574692
trainer/Policy mu Min                             -0.00985818
trainer/Policy log std Mean                        0.000280087
trainer/Policy log std Std                         0.00265667
trainer/Policy log std Max                         0.00774485
trainer/Policy log std Min                        -0.00797199
trainer/Alpha                                      0.9997
trainer/Alpha Loss                                -0
exploration/num steps total                     2000
exploration/num paths total                        2
exploration/path length Mean                    1000
exploration/path length Std                        0
exploration/path length Max                     1000
exploration/path length Min                     1000
exploration/Rewards Mean                          -0.218349
exploration/Rewards Std                            0.714125
exploration/Rewards Max                            1.71071
exploration/Rewards Min                           -2.35344
exploration/Returns Mean                        -218.349
exploration/Returns Std                            0
exploration/Returns Max                         -218.349
exploration/Returns Min                         -218.349
exploration/Actions Mean                          -0.00830904
exploration/Actions Std                            0.627136
exploration/Actions Max                            0.999426
exploration/Actions Min                           -0.998721
exploration/Num Paths                              1
exploration/Average Returns                     -218.349
exploration/env_infos/final/reward_run Mean        0.184925
exploration/env_infos/final/reward_run Std         0
exploration/env_infos/final/reward_run Max         0.184925
exploration/env_infos/final/reward_run Min         0.184925
exploration/env_infos/initial/reward_run Mean     -0.0601567
exploration/env_infos/initial/reward_run Std       0
exploration/env_infos/initial/reward_run Max      -0.0601567
exploration/env_infos/initial/reward_run Min      -0.0601567
exploration/env_infos/reward_run Mean              0.0176725
exploration/env_infos/reward_run Std               0.707249
exploration/env_infos/reward_run Max               2.092
exploration/env_infos/reward_run Min              -2.13755
exploration/env_infos/final/reward_ctrl Mean      -0.278341
exploration/env_infos/final/reward_ctrl Std        0
exploration/env_infos/final/reward_ctrl Max       -0.278341
exploration/env_infos/final/reward_ctrl Min       -0.278341
exploration/env_infos/initial/reward_ctrl Mean    -0.276045
exploration/env_infos/initial/reward_ctrl Std      0
exploration/env_infos/initial/reward_ctrl Max     -0.276045
exploration/env_infos/initial/reward_ctrl Min     -0.276045
exploration/env_infos/reward_ctrl Mean            -0.236022
exploration/env_infos/reward_ctrl Std              0.0772382
exploration/env_infos/reward_ctrl Max             -0.0332546
exploration/env_infos/reward_ctrl Min             -0.487867
evaluation/num steps total                      5000
evaluation/num paths total                         5
evaluation/path length Mean                     1000
evaluation/path length Std                         0
evaluation/path length Max                      1000
evaluation/path length Min                      1000
evaluation/Rewards Mean                           -0.000731365
evaluation/Rewards Std                             0.017849
evaluation/Rewards Max                             0.230964
evaluation/Rewards Min                            -0.418896
evaluation/Returns Mean                           -0.731365
evaluation/Returns Std                             0.779769
evaluation/Returns Max                             0.436557
evaluation/Returns Min                            -1.94504
evaluation/Actions Mean                           -0.00049622
evaluation/Actions Std                             0.00121234
evaluation/Actions Max                             0.00108095
evaluation/Actions Min                            -0.00443033
evaluation/Num Paths                               5
evaluation/Average Returns                        -0.731365
evaluation/env_infos/final/reward_run Mean        -1.73472e-16
evaluation/env_infos/final/reward_run Std          8.77708e-17
evaluation/env_infos/final/reward_run Max         -6.93889e-17
evaluation/env_infos/final/reward_run Min         -2.77556e-16
evaluation/env_infos/initial/reward_run Mean      -0.0888204
evaluation/env_infos/initial/reward_run Std        0.132768
evaluation/env_infos/initial/reward_run Max        0.100819
evaluation/env_infos/initial/reward_run Min       -0.237799
evaluation/env_infos/reward_run Mean              -0.000730335
evaluation/env_infos/reward_run Std                0.017849
evaluation/env_infos/reward_run Max                0.230965
evaluation/env_infos/reward_run Min               -0.418894
evaluation/env_infos/final/reward_ctrl Mean       -1.02747e-06
evaluation/env_infos/final/reward_ctrl Std         0
evaluation/env_infos/final/reward_ctrl Max        -1.02747e-06
evaluation/env_infos/final/reward_ctrl Min        -1.02747e-06
evaluation/env_infos/initial/reward_ctrl Mean     -1.06503e-06
evaluation/env_infos/initial/reward_ctrl Std       2.31565e-08
evaluation/env_infos/initial/reward_ctrl Max      -1.0348e-06
evaluation/env_infos/initial/reward_ctrl Min      -1.1023e-06
evaluation/env_infos/reward_ctrl Mean             -1.02961e-06
evaluation/env_infos/reward_ctrl Std               4.14209e-08
evaluation/env_infos/reward_ctrl Max              -9.29945e-07
evaluation/env_infos/reward_ctrl Min              -2.98401e-06
time/data storing (s)                              0.00679693
time/evaluation sampling (s)                       2.36228
time/exploration sampling (s)                      0.594386
time/logging (s)                                   0.0379452
time/saving (s)                                    0.0304198
time/training (s)                                 31.0598
time/epoch (s)                                    34.0916
time/total (s)                                    34.7817
Epoch                                              0
----------------------------------------------  --------------
2020-07-08 19:59:24.034047 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 1 finished
----------------------------------------------  ---------------
replay_buffer/size                               3000
trainer/QF1 Loss                                    0.481405
trainer/QF2 Loss                                    0.510209
trainer/Policy Loss                               -17.4285
trainer/Q1 Predictions Mean                        13.4148
trainer/Q1 Predictions Std                          1.21005
trainer/Q1 Predictions Max                         17.2362
trainer/Q1 Predictions Min                         10.3966
trainer/Q2 Predictions Mean                        13.402
trainer/Q2 Predictions Std                          1.20935
trainer/Q2 Predictions Max                         16.8408
trainer/Q2 Predictions Min                         10.2743
trainer/Q Targets Mean                             13.3064
trainer/Q Targets Std                               1.33131
trainer/Q Targets Max                              17.2526
trainer/Q Targets Min                               8.98412
trainer/Log Pis Mean                               -4.0211
trainer/Log Pis Std                                 0.49514
trainer/Log Pis Max                                -2.4858
trainer/Log Pis Min                                -5.12379
trainer/Policy mu Mean                              0.00820418
trainer/Policy mu Std                               0.15074
trainer/Policy mu Max                               0.557704
trainer/Policy mu Min                              -0.486318
trainer/Policy log std Mean                        -0.107653
trainer/Policy log std Std                          0.0167359
trainer/Policy log std Max                         -0.0628482
trainer/Policy log std Min                         -0.168389
trainer/Alpha                                       0.741283
trainer/Alpha Loss                                 -2.99705
exploration/num steps total                      3000
exploration/num paths total                         3
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.281721
exploration/Rewards Std                             0.631406
exploration/Rewards Max                             1.76867
exploration/Rewards Min                            -2.46662
exploration/Returns Mean                         -281.721
exploration/Returns Std                             0
exploration/Returns Max                          -281.721
exploration/Returns Min                          -281.721
exploration/Actions Mean                           -0.0281902
exploration/Actions Std                             0.60116
exploration/Actions Max                             0.997343
exploration/Actions Min                            -0.995612
exploration/Num Paths                               1
exploration/Average Returns                      -281.721
exploration/env_infos/final/reward_run Mean         0.428496
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          0.428496
exploration/env_infos/final/reward_run Min          0.428496
exploration/env_infos/initial/reward_run Mean       0.731784
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.731784
exploration/env_infos/initial/reward_run Min        0.731784
exploration/env_infos/reward_run Mean              -0.0644081
exploration/env_infos/reward_run Std                0.630519
exploration/env_infos/reward_run Max                1.9808
exploration/env_infos/reward_run Min               -2.16397
exploration/env_infos/final/reward_ctrl Mean       -0.24782
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.24782
exploration/env_infos/final/reward_ctrl Min        -0.24782
exploration/env_infos/initial/reward_ctrl Mean     -0.222123
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.222123
exploration/env_infos/initial/reward_ctrl Min      -0.222123
exploration/env_infos/reward_ctrl Mean             -0.217313
exploration/env_infos/reward_ctrl Std               0.0715127
exploration/env_infos/reward_ctrl Max              -0.0294576
exploration/env_infos/reward_ctrl Min              -0.439247
evaluation/num steps total                      10000
evaluation/num paths total                         10
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                            -0.00997832
evaluation/Rewards Std                              0.0227835
evaluation/Rewards Max                              0.60449
evaluation/Rewards Min                             -0.323308
evaluation/Returns Mean                            -9.97832
evaluation/Returns Std                              0.267402
evaluation/Returns Max                             -9.6277
evaluation/Returns Min                            -10.3876
evaluation/Actions Mean                             0.0807011
evaluation/Actions Std                              0.0996213
evaluation/Actions Max                              0.341587
evaluation/Actions Min                             -0.0574772
evaluation/Num Paths                                5
evaluation/Average Returns                         -9.97832
evaluation/env_infos/final/reward_run Mean          8.32667e-17
evaluation/env_infos/final/reward_run Std           1.11022e-16
evaluation/env_infos/final/reward_run Max           2.77556e-16
evaluation/env_infos/final/reward_run Min           0
evaluation/env_infos/initial/reward_run Mean        0.248069
evaluation/env_infos/initial/reward_run Std         0.142777
evaluation/env_infos/initial/reward_run Max         0.377877
evaluation/env_infos/initial/reward_run Min        -0.0268382
evaluation/env_infos/reward_run Mean               -0.00011607
evaluation/env_infos/reward_run Std                 0.023045
evaluation/env_infos/reward_run Max                 0.621971
evaluation/env_infos/reward_run Min                -0.316897
evaluation/env_infos/final/reward_ctrl Mean        -0.00987156
evaluation/env_infos/final/reward_ctrl Std          0
evaluation/env_infos/final/reward_ctrl Max         -0.00987156
evaluation/env_infos/final/reward_ctrl Min         -0.00987156
evaluation/env_infos/initial/reward_ctrl Mean      -0.0167175
evaluation/env_infos/initial/reward_ctrl Std        0.00201605
evaluation/env_infos/initial/reward_ctrl Max       -0.0129506
evaluation/env_infos/initial/reward_ctrl Min       -0.0182692
evaluation/env_infos/reward_ctrl Mean              -0.00986225
evaluation/env_infos/reward_ctrl Std                0.000369409
evaluation/env_infos/reward_ctrl Max               -0.0034609
evaluation/env_infos/reward_ctrl Min               -0.0182692
time/data storing (s)                               0.00705918
time/evaluation sampling (s)                        2.36886
time/exploration sampling (s)                       0.635565
time/logging (s)                                    0.0466553
time/saving (s)                                     0.0156572
time/training (s)                                  34.4002
time/epoch (s)                                     37.474
time/total (s)                                     72.268
Epoch                                               1
----------------------------------------------  ---------------
2020-07-08 20:00:03.301119 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 2 finished
----------------------------------------------  ---------------
replay_buffer/size                               4000
trainer/QF1 Loss                                    0.632179
trainer/QF2 Loss                                    0.655874
trainer/Policy Loss                               -26.7622
trainer/Q1 Predictions Mean                        22.8199
trainer/Q1 Predictions Std                          2.24012
trainer/Q1 Predictions Max                         31.3032
trainer/Q1 Predictions Min                         15.8177
trainer/Q2 Predictions Mean                        22.7879
trainer/Q2 Predictions Std                          2.2135
trainer/Q2 Predictions Max                         31.257
trainer/Q2 Predictions Min                         16.4093
trainer/Q Targets Mean                             22.716
trainer/Q Targets Std                               2.39104
trainer/Q Targets Max                              31.8286
trainer/Q Targets Min                              15.4378
trainer/Log Pis Mean                               -4.05796
trainer/Log Pis Std                                 0.659291
trainer/Log Pis Max                                -1.30974
trainer/Log Pis Min                                -6.53288
trainer/Policy mu Mean                              0.105285
trainer/Policy mu Std                               0.164134
trainer/Policy mu Max                               0.969161
trainer/Policy mu Min                              -0.388763
trainer/Policy log std Mean                        -0.0845048
trainer/Policy log std Std                          0.0262807
trainer/Policy log std Max                         -0.0268157
trainer/Policy log std Min                         -0.206149
trainer/Alpha                                       0.549182
trainer/Alpha Loss                                 -6.025
exploration/num steps total                      4000
exploration/num paths total                         4
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.213266
exploration/Rewards Std                             0.627333
exploration/Rewards Max                             1.90054
exploration/Rewards Min                            -2.26032
exploration/Returns Mean                         -213.266
exploration/Returns Std                             0
exploration/Returns Max                          -213.266
exploration/Returns Min                          -213.266
exploration/Actions Mean                            0.0688739
exploration/Actions Std                             0.60346
exploration/Actions Max                             0.999435
exploration/Actions Min                            -0.997655
exploration/Num Paths                               1
exploration/Average Returns                      -213.266
exploration/env_infos/final/reward_run Mean         0.14784
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          0.14784
exploration/env_infos/final/reward_run Min          0.14784
exploration/env_infos/initial/reward_run Mean       0.505619
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.505619
exploration/env_infos/initial/reward_run Min        0.505619
exploration/env_infos/reward_run Mean               0.00807884
exploration/env_infos/reward_run Std                0.620771
exploration/env_infos/reward_run Max                2.09324
exploration/env_infos/reward_run Min               -1.89216
exploration/env_infos/final/reward_ctrl Mean       -0.252466
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.252466
exploration/env_infos/final/reward_ctrl Min        -0.252466
exploration/env_infos/initial/reward_ctrl Mean     -0.275951
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.275951
exploration/env_infos/initial/reward_ctrl Min      -0.275951
exploration/env_infos/reward_ctrl Mean             -0.221345
exploration/env_infos/reward_ctrl Std               0.0751829
exploration/env_infos/reward_ctrl Max              -0.0270879
exploration/env_infos/reward_ctrl Min              -0.460688
evaluation/num steps total                      15000
evaluation/num paths total                         15
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                            -0.0141941
evaluation/Rewards Std                              0.04707
evaluation/Rewards Max                              0.764413
evaluation/Rewards Min                             -0.528841
evaluation/Returns Mean                           -14.1941
evaluation/Returns Std                              1.32442
evaluation/Returns Max                            -11.874
evaluation/Returns Min                            -15.6298
evaluation/Actions Mean                             0.107219
evaluation/Actions Std                              0.123729
evaluation/Actions Max                              0.529642
evaluation/Actions Min                             -0.135631
evaluation/Num Paths                                5
evaluation/Average Returns                        -14.1941
evaluation/env_infos/final/reward_run Mean          8.05735e-06
evaluation/env_infos/final/reward_run Std           3.52631e-06
evaluation/env_infos/final/reward_run Max           1.21965e-05
evaluation/env_infos/final/reward_run Min           2.34402e-06
evaluation/env_infos/initial/reward_run Mean        0.24394
evaluation/env_infos/initial/reward_run Std         0.179858
evaluation/env_infos/initial/reward_run Max         0.52551
evaluation/env_infos/initial/reward_run Min        -0.00595886
evaluation/env_infos/reward_run Mean                0.00188869
evaluation/env_infos/reward_run Std                 0.0483498
evaluation/env_infos/reward_run Max                 0.795659
evaluation/env_infos/reward_run Min                -0.526438
evaluation/env_infos/final/reward_ctrl Mean        -0.0161007
evaluation/env_infos/final/reward_ctrl Std          1.17103e-07
evaluation/env_infos/final/reward_ctrl Max         -0.0161005
evaluation/env_infos/final/reward_ctrl Min         -0.0161008
evaluation/env_infos/initial/reward_ctrl Mean      -0.0183545
evaluation/env_infos/initial/reward_ctrl Std        0.00334085
evaluation/env_infos/initial/reward_ctrl Max       -0.0142927
evaluation/env_infos/initial/reward_ctrl Min       -0.0237721
evaluation/env_infos/reward_ctrl Mean              -0.0160828
evaluation/env_infos/reward_ctrl Std                0.00149731
evaluation/env_infos/reward_ctrl Max               -0.00240326
evaluation/env_infos/reward_ctrl Min               -0.0372627
time/data storing (s)                               0.00712782
time/evaluation sampling (s)                        2.51042
time/exploration sampling (s)                       0.648404
time/logging (s)                                    0.045562
time/saving (s)                                     0.0158664
time/training (s)                                  36.0158
time/epoch (s)                                     39.2432
time/total (s)                                    111.532
Epoch                                               2
----------------------------------------------  ---------------
2020-07-08 20:00:48.835499 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 3 finished
----------------------------------------------  ---------------
replay_buffer/size                               5000
trainer/QF1 Loss                                    0.691975
trainer/QF2 Loss                                    0.671689
trainer/Policy Loss                               -32.3161
trainer/Q1 Predictions Mean                        28.4766
trainer/Q1 Predictions Std                          2.48449
trainer/Q1 Predictions Max                         38.3923
trainer/Q1 Predictions Min                         20.6907
trainer/Q2 Predictions Mean                        28.5112
trainer/Q2 Predictions Std                          2.50516
trainer/Q2 Predictions Max                         38.3122
trainer/Q2 Predictions Min                         21.2022
trainer/Q Targets Mean                             28.5679
trainer/Q Targets Std                               2.66333
trainer/Q Targets Max                              38.0251
trainer/Q Targets Min                              19.9594
trainer/Log Pis Mean                               -3.74947
trainer/Log Pis Std                                 0.895347
trainer/Log Pis Max                                -1.06244
trainer/Log Pis Min                                -6.60408
trainer/Policy mu Mean                              0.0973976
trainer/Policy mu Std                               0.291006
trainer/Policy mu Max                               1.42603
trainer/Policy mu Min                              -1.12359
trainer/Policy log std Mean                        -0.0606859
trainer/Policy log std Std                          0.0371185
trainer/Policy log std Max                          0.0460648
trainer/Policy log std Min                         -0.320995
trainer/Alpha                                       0.408088
trainer/Alpha Loss                                 -8.73532
exploration/num steps total                      5000
exploration/num paths total                         5
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.219809
exploration/Rewards Std                             0.667128
exploration/Rewards Max                             2.27107
exploration/Rewards Min                            -2.13878
exploration/Returns Mean                         -219.809
exploration/Returns Std                             0
exploration/Returns Max                          -219.809
exploration/Returns Min                          -219.809
exploration/Actions Mean                            0.0389537
exploration/Actions Std                             0.617232
exploration/Actions Max                             0.999593
exploration/Actions Min                            -0.997701
exploration/Num Paths                               1
exploration/Average Returns                      -219.809
exploration/env_infos/final/reward_run Mean         0.621466
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          0.621466
exploration/env_infos/final/reward_run Min          0.621466
exploration/env_infos/initial/reward_run Mean      -0.125556
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max       -0.125556
exploration/env_infos/initial/reward_run Min       -0.125556
exploration/env_infos/reward_run Mean               0.00968713
exploration/env_infos/reward_run Std                0.66552
exploration/env_infos/reward_run Max                2.50187
exploration/env_infos/reward_run Min               -2.00344
exploration/env_infos/final/reward_ctrl Mean       -0.0808133
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.0808133
exploration/env_infos/final/reward_ctrl Min        -0.0808133
exploration/env_infos/initial/reward_ctrl Mean     -0.106099
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.106099
exploration/env_infos/initial/reward_ctrl Min      -0.106099
exploration/env_infos/reward_ctrl Mean             -0.229496
exploration/env_infos/reward_ctrl Std               0.076907
exploration/env_infos/reward_ctrl Max              -0.03824
exploration/env_infos/reward_ctrl Min              -0.441208
evaluation/num steps total                      20000
evaluation/num paths total                         20
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.00449899
evaluation/Rewards Std                              0.153302
evaluation/Rewards Max                              1.38982
evaluation/Rewards Min                             -1.01289
evaluation/Returns Mean                             4.49899
evaluation/Returns Std                             23.6636
evaluation/Returns Max                             51.7183
evaluation/Returns Min                             -9.5721
evaluation/Actions Mean                             0.00443068
evaluation/Actions Std                              0.179358
evaluation/Actions Max                              0.896292
evaluation/Actions Min                             -0.60942
evaluation/Num Paths                                5
evaluation/Average Returns                          4.49899
evaluation/env_infos/final/reward_run Mean          4.29741e-08
evaluation/env_infos/final/reward_run Std           5.04894e-08
evaluation/env_infos/final/reward_run Max           1.32491e-07
evaluation/env_infos/final/reward_run Min          -1.21447e-08
evaluation/env_infos/initial/reward_run Mean        0.394817
evaluation/env_infos/initial/reward_run Std         0.116146
evaluation/env_infos/initial/reward_run Max         0.565152
evaluation/env_infos/initial/reward_run Min         0.245357
evaluation/env_infos/reward_run Mean                0.0238123
evaluation/env_infos/reward_run Std                 0.160362
evaluation/env_infos/reward_run Max                 1.51264
evaluation/env_infos/reward_run Min                -0.977364
evaluation/env_infos/final/reward_ctrl Mean        -0.0168048
evaluation/env_infos/final/reward_ctrl Std          2.49344e-09
evaluation/env_infos/final/reward_ctrl Max         -0.0168048
evaluation/env_infos/final/reward_ctrl Min         -0.0168048
evaluation/env_infos/initial/reward_ctrl Mean      -0.0510434
evaluation/env_infos/initial/reward_ctrl Std        0.00896993
evaluation/env_infos/initial/reward_ctrl Max       -0.0367712
evaluation/env_infos/initial/reward_ctrl Min       -0.061584
evaluation/env_infos/reward_ctrl Mean              -0.0193133
evaluation/env_infos/reward_ctrl Std                0.0109299
evaluation/env_infos/reward_ctrl Max               -0.00583055
evaluation/env_infos/reward_ctrl Min               -0.123513
time/data storing (s)                               0.010892
time/evaluation sampling (s)                        3.03916
time/exploration sampling (s)                       1.05343
time/logging (s)                                    0.0527135
time/saving (s)                                     0.0246775
time/training (s)                                  41.3452
time/epoch (s)                                     45.5261
time/total (s)                                    157.07
Epoch                                               3
----------------------------------------------  ---------------
2020-07-08 20:01:28.753221 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 4 finished
----------------------------------------------  --------------
replay_buffer/size                               6000
trainer/QF1 Loss                                    0.827223
trainer/QF2 Loss                                    0.893846
trainer/Policy Loss                               -36.0163
trainer/Q1 Predictions Mean                        32.6226
trainer/Q1 Predictions Std                          2.50641
trainer/Q1 Predictions Max                         42.7586
trainer/Q1 Predictions Min                         21.4206
trainer/Q2 Predictions Mean                        32.7355
trainer/Q2 Predictions Std                          2.47668
trainer/Q2 Predictions Max                         43.3301
trainer/Q2 Predictions Min                         22.3855
trainer/Q Targets Mean                             32.4881
trainer/Q Targets Std                               2.67291
trainer/Q Targets Max                              43.2577
trainer/Q Targets Min                              22.4305
trainer/Log Pis Mean                               -3.18442
trainer/Log Pis Std                                 1.28163
trainer/Log Pis Max                                 1.18419
trainer/Log Pis Min                                -6.32946
trainer/Policy mu Mean                              0.0968225
trainer/Policy mu Std                               0.430073
trainer/Policy mu Max                               1.85462
trainer/Policy mu Min                              -1.50094
trainer/Policy log std Mean                        -0.0627523
trainer/Policy log std Std                          0.0711649
trainer/Policy log std Max                          0.136125
trainer/Policy log std Min                         -0.500721
trainer/Alpha                                       0.304515
trainer/Alpha Loss                                -10.918
exploration/num steps total                      6000
exploration/num paths total                         6
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.210413
exploration/Rewards Std                             0.69328
exploration/Rewards Max                             1.81516
exploration/Rewards Min                            -2.33348
exploration/Returns Mean                         -210.413
exploration/Returns Std                             0
exploration/Returns Max                          -210.413
exploration/Returns Min                          -210.413
exploration/Actions Mean                            0.0242781
exploration/Actions Std                             0.633688
exploration/Actions Max                             0.999137
exploration/Actions Min                            -0.999276
exploration/Num Paths                               1
exploration/Average Returns                      -210.413
exploration/env_infos/final/reward_run Mean        -1.84652
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max         -1.84652
exploration/env_infos/final/reward_run Min         -1.84652
exploration/env_infos/initial/reward_run Mean       0.563263
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.563263
exploration/env_infos/initial/reward_run Min        0.563263
exploration/env_infos/reward_run Mean               0.0308772
exploration/env_infos/reward_run Std                0.68744
exploration/env_infos/reward_run Max                2.06598
exploration/env_infos/reward_run Min               -2.12519
exploration/env_infos/final/reward_ctrl Mean       -0.217236
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.217236
exploration/env_infos/final/reward_ctrl Min        -0.217236
exploration/env_infos/initial/reward_ctrl Mean     -0.342612
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.342612
exploration/env_infos/initial/reward_ctrl Min      -0.342612
exploration/env_infos/reward_ctrl Mean             -0.24129
exploration/env_infos/reward_ctrl Std               0.0719955
exploration/env_infos/reward_ctrl Max              -0.0513327
exploration/env_infos/reward_ctrl Min              -0.480346
evaluation/num steps total                      25000
evaluation/num paths total                         25
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                            -0.01216
evaluation/Rewards Std                              0.229291
evaluation/Rewards Max                              1.35071
evaluation/Rewards Min                             -0.598169
evaluation/Returns Mean                           -12.16
evaluation/Returns Std                             21.072
evaluation/Returns Max                             -0.257168
evaluation/Returns Min                            -54.2784
evaluation/Actions Mean                            -0.00695767
evaluation/Actions Std                              0.250757
evaluation/Actions Max                              0.863371
evaluation/Actions Min                             -0.812007
evaluation/Num Paths                                5
evaluation/Average Returns                        -12.16
evaluation/env_infos/final/reward_run Mean         -0.088244
evaluation/env_infos/final/reward_run Std           0.160946
evaluation/env_infos/final/reward_run Max           0.0848354
evaluation/env_infos/final/reward_run Min          -0.3723
evaluation/env_infos/initial/reward_run Mean        0.474867
evaluation/env_infos/initial/reward_run Std         0.145538
evaluation/env_infos/initial/reward_run Max         0.681433
evaluation/env_infos/initial/reward_run Min         0.272537
evaluation/env_infos/reward_run Mean                0.0255966
evaluation/env_infos/reward_run Std                 0.236743
evaluation/env_infos/reward_run Max                 1.37013
evaluation/env_infos/reward_run Min                -0.576976
evaluation/env_infos/final/reward_ctrl Mean        -0.0398522
evaluation/env_infos/final/reward_ctrl Std          0.0069218
evaluation/env_infos/final/reward_ctrl Max         -0.0290505
evaluation/env_infos/final/reward_ctrl Min         -0.0505495
evaluation/env_infos/initial/reward_ctrl Mean      -0.11571
evaluation/env_infos/initial/reward_ctrl Std        0.0064833
evaluation/env_infos/initial/reward_ctrl Max       -0.107002
evaluation/env_infos/initial/reward_ctrl Min       -0.125076
evaluation/env_infos/reward_ctrl Mean              -0.0377566
evaluation/env_infos/reward_ctrl Std                0.0199248
evaluation/env_infos/reward_ctrl Max               -0.00282238
evaluation/env_infos/reward_ctrl Min               -0.172909
time/data storing (s)                               0.00754205
time/evaluation sampling (s)                        4.76209
time/exploration sampling (s)                       0.807159
time/logging (s)                                    0.0471901
time/saving (s)                                     0.0176342
time/training (s)                                  34.2516
time/epoch (s)                                     39.8932
time/total (s)                                    196.98
Epoch                                               4
----------------------------------------------  --------------
2020-07-08 20:02:11.766926 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 5 finished
----------------------------------------------  --------------
replay_buffer/size                               7000
trainer/QF1 Loss                                    0.784669
trainer/QF2 Loss                                    0.789093
trainer/Policy Loss                               -37.4212
trainer/Q1 Predictions Mean                        34.3375
trainer/Q1 Predictions Std                          2.41813
trainer/Q1 Predictions Max                         47.3409
trainer/Q1 Predictions Min                         27.1085
trainer/Q2 Predictions Mean                        34.3248
trainer/Q2 Predictions Std                          2.43997
trainer/Q2 Predictions Max                         47.9135
trainer/Q2 Predictions Min                         27.8749
trainer/Q Targets Mean                             34.285
trainer/Q Targets Std                               2.63869
trainer/Q Targets Max                              47.1514
trainer/Q Targets Min                              27.6268
trainer/Log Pis Mean                               -2.84324
trainer/Log Pis Std                                 2.08404
trainer/Log Pis Max                                 5.49345
trainer/Log Pis Min                                -7.45947
trainer/Policy mu Mean                              0.156389
trainer/Policy mu Std                               0.57112
trainer/Policy mu Max                               1.97145
trainer/Policy mu Min                              -1.70021
trainer/Policy log std Mean                        -0.0796879
trainer/Policy log std Std                          0.0950316
trainer/Policy log std Max                          0.143559
trainer/Policy log std Min                         -0.792767
trainer/Alpha                                       0.228132
trainer/Alpha Loss                                -13.0663
exploration/num steps total                      7000
exploration/num paths total                         7
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.213214
exploration/Rewards Std                             0.647752
exploration/Rewards Max                             1.60593
exploration/Rewards Min                            -2.2466
exploration/Returns Mean                         -213.214
exploration/Returns Std                             0
exploration/Returns Max                          -213.214
exploration/Returns Min                          -213.214
exploration/Actions Mean                            0.0335239
exploration/Actions Std                             0.640831
exploration/Actions Max                             0.997086
exploration/Actions Min                            -0.998638
exploration/Num Paths                               1
exploration/Average Returns                      -213.214
exploration/env_infos/final/reward_run Mean        -1.13822
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max         -1.13822
exploration/env_infos/final/reward_run Min         -1.13822
exploration/env_infos/initial/reward_run Mean       0.653426
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.653426
exploration/env_infos/initial/reward_run Min        0.653426
exploration/env_infos/reward_run Mean               0.0338585
exploration/env_infos/reward_run Std                0.642974
exploration/env_infos/reward_run Max                1.9389
exploration/env_infos/reward_run Min               -1.91421
exploration/env_infos/final/reward_ctrl Mean       -0.261327
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.261327
exploration/env_infos/final/reward_ctrl Min        -0.261327
exploration/env_infos/initial/reward_ctrl Mean     -0.254319
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.254319
exploration/env_infos/initial/reward_ctrl Min      -0.254319
exploration/env_infos/reward_ctrl Mean             -0.247073
exploration/env_infos/reward_ctrl Std               0.0770433
exploration/env_infos/reward_ctrl Max              -0.0574572
exploration/env_infos/reward_ctrl Min              -0.488313
evaluation/num steps total                      30000
evaluation/num paths total                         30
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                            -0.00197586
evaluation/Rewards Std                              0.403655
evaluation/Rewards Max                              1.65133
evaluation/Rewards Min                             -2.99154
evaluation/Returns Mean                            -1.97586
evaluation/Returns Std                             41.422
evaluation/Returns Max                             50.7095
evaluation/Returns Min                            -47.8512
evaluation/Actions Mean                             0.0553881
evaluation/Actions Std                              0.408673
evaluation/Actions Max                              0.959798
evaluation/Actions Min                             -0.969071
evaluation/Num Paths                                5
evaluation/Average Returns                         -1.97586
evaluation/env_infos/final/reward_run Mean         -0.0797684
evaluation/env_infos/final/reward_run Std           0.0998351
evaluation/env_infos/final/reward_run Max           0.0812236
evaluation/env_infos/final/reward_run Min          -0.209222
evaluation/env_infos/initial/reward_run Mean        0.503537
evaluation/env_infos/initial/reward_run Std         0.122161
evaluation/env_infos/initial/reward_run Max         0.677048
evaluation/env_infos/initial/reward_run Min         0.346701
evaluation/env_infos/reward_run Mean                0.100073
evaluation/env_infos/reward_run Std                 0.38785
evaluation/env_infos/reward_run Max                 1.79105
evaluation/env_infos/reward_run Min                -2.65968
evaluation/env_infos/final/reward_ctrl Mean        -0.106966
evaluation/env_infos/final/reward_ctrl Std          0.0221978
evaluation/env_infos/final/reward_ctrl Max         -0.0827436
evaluation/env_infos/final/reward_ctrl Min         -0.144447
evaluation/env_infos/initial/reward_ctrl Mean      -0.175767
evaluation/env_infos/initial/reward_ctrl Std        0.0154895
evaluation/env_infos/initial/reward_ctrl Max       -0.151863
evaluation/env_infos/initial/reward_ctrl Min       -0.195964
evaluation/env_infos/reward_ctrl Mean              -0.102049
evaluation/env_infos/reward_ctrl Std                0.0441874
evaluation/env_infos/reward_ctrl Max               -0.00862956
evaluation/env_infos/reward_ctrl Min               -0.391074
time/data storing (s)                               0.0065392
time/evaluation sampling (s)                        2.59329
time/exploration sampling (s)                       0.602723
time/logging (s)                                    0.0999603
time/saving (s)                                     0.022068
time/training (s)                                  39.7253
time/epoch (s)                                     43.0498
time/total (s)                                    240.044
Epoch                                               5
----------------------------------------------  --------------
2020-07-08 20:02:58.595024 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 6 finished
----------------------------------------------  --------------
replay_buffer/size                               8000
trainer/QF1 Loss                                    0.990678
trainer/QF2 Loss                                    1.03371
trainer/Policy Loss                               -37.5725
trainer/Q1 Predictions Mean                        34.816
trainer/Q1 Predictions Std                          2.52612
trainer/Q1 Predictions Max                         46.7931
trainer/Q1 Predictions Min                         29.373
trainer/Q2 Predictions Mean                        34.7841
trainer/Q2 Predictions Std                          2.55075
trainer/Q2 Predictions Max                         46.2255
trainer/Q2 Predictions Min                         28.9453
trainer/Q Targets Mean                             34.7957
trainer/Q Targets Std                               2.89243
trainer/Q Targets Max                              46.709
trainer/Q Targets Min                              27.0159
trainer/Log Pis Mean                               -2.60174
trainer/Log Pis Std                                 2.09327
trainer/Log Pis Max                                 5.52569
trainer/Log Pis Min                                -6.66072
trainer/Policy mu Mean                              0.136182
trainer/Policy mu Std                               0.611583
trainer/Policy mu Max                               2.14129
trainer/Policy mu Min                              -2.0246
trainer/Policy log std Mean                        -0.0467498
trainer/Policy log std Std                          0.0942811
trainer/Policy log std Max                          0.254253
trainer/Policy log std Min                         -0.772157
trainer/Alpha                                       0.17127
trainer/Alpha Loss                                -15.1754
exploration/num steps total                      8000
exploration/num paths total                         8
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.169584
exploration/Rewards Std                             0.656397
exploration/Rewards Max                             2.35623
exploration/Rewards Min                            -2.68716
exploration/Returns Mean                         -169.584
exploration/Returns Std                             0
exploration/Returns Max                          -169.584
exploration/Returns Min                          -169.584
exploration/Actions Mean                            0.053257
exploration/Actions Std                             0.653432
exploration/Actions Max                             0.99923
exploration/Actions Min                            -0.999938
exploration/Num Paths                               1
exploration/Average Returns                      -169.584
exploration/env_infos/final/reward_run Mean        -0.0784858
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max         -0.0784858
exploration/env_infos/final/reward_run Min         -0.0784858
exploration/env_infos/initial/reward_run Mean       0.603567
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.603567
exploration/env_infos/initial/reward_run Min        0.603567
exploration/env_infos/reward_run Mean               0.0883018
exploration/env_infos/reward_run Std                0.647084
exploration/env_infos/reward_run Max                2.54265
exploration/env_infos/reward_run Min               -2.30716
exploration/env_infos/final/reward_ctrl Mean       -0.312679
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.312679
exploration/env_infos/final/reward_ctrl Min        -0.312679
exploration/env_infos/initial/reward_ctrl Mean     -0.190684
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.190684
exploration/env_infos/initial/reward_ctrl Min      -0.190684
exploration/env_infos/reward_ctrl Mean             -0.257886
exploration/env_infos/reward_ctrl Std               0.0791084
exploration/env_infos/reward_ctrl Max              -0.0504849
exploration/env_infos/reward_ctrl Min              -0.481789
evaluation/num steps total                      35000
evaluation/num paths total                         35
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.0116409
evaluation/Rewards Std                              0.420709
evaluation/Rewards Max                              1.67909
evaluation/Rewards Min                             -2.68668
evaluation/Returns Mean                            11.6409
evaluation/Returns Std                             33.5465
evaluation/Returns Max                             35.3731
evaluation/Returns Min                            -54.7654
evaluation/Actions Mean                             0.0453408
evaluation/Actions Std                              0.422637
evaluation/Actions Max                              0.988252
evaluation/Actions Min                             -0.979074
evaluation/Num Paths                                5
evaluation/Average Returns                         11.6409
evaluation/env_infos/final/reward_run Mean          0.0652503
evaluation/env_infos/final/reward_run Std           0.432669
evaluation/env_infos/final/reward_run Max           0.566834
evaluation/env_infos/final/reward_run Min          -0.569226
evaluation/env_infos/initial/reward_run Mean        0.533047
evaluation/env_infos/initial/reward_run Std         0.128604
evaluation/env_infos/initial/reward_run Max         0.646366
evaluation/env_infos/initial/reward_run Min         0.287557
evaluation/env_infos/reward_run Mean                0.120047
evaluation/env_infos/reward_run Std                 0.411878
evaluation/env_infos/reward_run Max                 1.84348
evaluation/env_infos/reward_run Min                -2.37106
evaluation/env_infos/final/reward_ctrl Mean        -0.111098
evaluation/env_infos/final/reward_ctrl Std          0.0619592
evaluation/env_infos/final/reward_ctrl Max         -0.0294239
evaluation/env_infos/final/reward_ctrl Min         -0.198149
evaluation/env_infos/initial/reward_ctrl Mean      -0.175099
evaluation/env_infos/initial/reward_ctrl Std        0.0188983
evaluation/env_infos/initial/reward_ctrl Max       -0.143562
evaluation/env_infos/initial/reward_ctrl Min       -0.199672
evaluation/env_infos/reward_ctrl Mean              -0.108406
evaluation/env_infos/reward_ctrl Std                0.0532802
evaluation/env_infos/reward_ctrl Max               -0.0146112
evaluation/env_infos/reward_ctrl Min               -0.46692
time/data storing (s)                               0.00696558
time/evaluation sampling (s)                        4.30305
time/exploration sampling (s)                       0.61716
time/logging (s)                                    0.0465719
time/saving (s)                                     0.0197279
time/training (s)                                  41.7596
time/epoch (s)                                     46.7531
time/total (s)                                    286.816
Epoch                                               6
----------------------------------------------  --------------
2020-07-08 20:03:44.698602 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 7 finished
----------------------------------------------  --------------
replay_buffer/size                               9000
trainer/QF1 Loss                                    0.72357
trainer/QF2 Loss                                    0.869526
trainer/Policy Loss                               -37.4185
trainer/Q1 Predictions Mean                        35.4578
trainer/Q1 Predictions Std                          4.03785
trainer/Q1 Predictions Max                         50.6414
trainer/Q1 Predictions Min                         29.9498
trainer/Q2 Predictions Mean                        35.4389
trainer/Q2 Predictions Std                          4.06632
trainer/Q2 Predictions Max                         50.9478
trainer/Q2 Predictions Min                         29.6286
trainer/Q Targets Mean                             35.4472
trainer/Q Targets Std                               4.16452
trainer/Q Targets Max                              53.3156
trainer/Q Targets Min                              29.7189
trainer/Log Pis Mean                               -1.70727
trainer/Log Pis Std                                 2.82049
trainer/Log Pis Max                                 9.56655
trainer/Log Pis Min                                -7.32119
trainer/Policy mu Mean                              0.0793376
trainer/Policy mu Std                               0.793457
trainer/Policy mu Max                               2.74333
trainer/Policy mu Min                              -2.33185
trainer/Policy log std Mean                        -0.0867682
trainer/Policy log std Std                          0.126124
trainer/Policy log std Max                          0.274175
trainer/Policy log std Min                         -0.734273
trainer/Alpha                                       0.129023
trainer/Alpha Loss                                -15.7805
exploration/num steps total                      9000
exploration/num paths total                         9
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.157283
exploration/Rewards Std                             0.677629
exploration/Rewards Max                             1.90267
exploration/Rewards Min                            -2.20011
exploration/Returns Mean                         -157.283
exploration/Returns Std                             0
exploration/Returns Max                          -157.283
exploration/Returns Min                          -157.283
exploration/Actions Mean                           -0.0359843
exploration/Actions Std                             0.664864
exploration/Actions Max                             0.99946
exploration/Actions Min                            -0.999802
exploration/Num Paths                               1
exploration/Average Returns                      -157.283
exploration/env_infos/final/reward_run Mean         1.16244
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          1.16244
exploration/env_infos/final/reward_run Min          1.16244
exploration/env_infos/initial/reward_run Mean       0.891084
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.891084
exploration/env_infos/initial/reward_run Min        0.891084
exploration/env_infos/reward_run Mean               0.10872
exploration/env_infos/reward_run Std                0.675012
exploration/env_infos/reward_run Max                2.3301
exploration/env_infos/reward_run Min               -1.90362
exploration/env_infos/final/reward_ctrl Mean       -0.323459
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.323459
exploration/env_infos/final/reward_ctrl Min        -0.323459
exploration/env_infos/initial/reward_ctrl Mean     -0.384791
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.384791
exploration/env_infos/initial/reward_ctrl Min      -0.384791
exploration/env_infos/reward_ctrl Mean             -0.266004
exploration/env_infos/reward_ctrl Std               0.0803177
exploration/env_infos/reward_ctrl Max              -0.0379846
exploration/env_infos/reward_ctrl Min              -0.490736
evaluation/num steps total                      40000
evaluation/num paths total                         40
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.080395
evaluation/Rewards Std                              0.454063
evaluation/Rewards Max                              2.03812
evaluation/Rewards Min                             -1.23737
evaluation/Returns Mean                            80.395
evaluation/Returns Std                             11.1845
evaluation/Returns Max                             97.2736
evaluation/Returns Min                             63.4754
evaluation/Actions Mean                            -0.057615
evaluation/Actions Std                              0.522417
evaluation/Actions Max                              0.956886
evaluation/Actions Min                             -0.965471
evaluation/Num Paths                                5
evaluation/Average Returns                         80.395
evaluation/env_infos/final/reward_run Mean          0.361065
evaluation/env_infos/final/reward_run Std           0.129202
evaluation/env_infos/final/reward_run Max           0.543522
evaluation/env_infos/final/reward_run Min           0.206744
evaluation/env_infos/initial/reward_run Mean        0.800325
evaluation/env_infos/initial/reward_run Std         0.114232
evaluation/env_infos/initial/reward_run Max         0.90655
evaluation/env_infos/initial/reward_run Min         0.583574
evaluation/env_infos/reward_run Mean                0.246138
evaluation/env_infos/reward_run Std                 0.443669
evaluation/env_infos/reward_run Max                 2.12623
evaluation/env_infos/reward_run Min                -0.949339
evaluation/env_infos/final/reward_ctrl Mean        -0.19929
evaluation/env_infos/final/reward_ctrl Std          0.0648022
evaluation/env_infos/final/reward_ctrl Max         -0.0975947
evaluation/env_infos/final/reward_ctrl Min         -0.279887
evaluation/env_infos/initial/reward_ctrl Mean      -0.255984
evaluation/env_infos/initial/reward_ctrl Std        0.0140054
evaluation/env_infos/initial/reward_ctrl Max       -0.230804
evaluation/env_infos/initial/reward_ctrl Min       -0.271814
evaluation/env_infos/reward_ctrl Mean              -0.165743
evaluation/env_infos/reward_ctrl Std                0.0697573
evaluation/env_infos/reward_ctrl Max               -0.0100634
evaluation/env_infos/reward_ctrl Min               -0.393676
time/data storing (s)                               0.00716057
time/evaluation sampling (s)                        3.03781
time/exploration sampling (s)                       1.16316
time/logging (s)                                    0.0438514
time/saving (s)                                     0.0208479
time/training (s)                                  41.8078
time/epoch (s)                                     46.0807
time/total (s)                                    332.914
Epoch                                               7
----------------------------------------------  --------------
2020-07-08 20:04:30.665394 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 8 finished
----------------------------------------------  --------------
replay_buffer/size                              10000
trainer/QF1 Loss                                    0.627208
trainer/QF2 Loss                                    0.703255
trainer/Policy Loss                               -35.5684
trainer/Q1 Predictions Mean                        34.0502
trainer/Q1 Predictions Std                          4.44374
trainer/Q1 Predictions Max                         51.8219
trainer/Q1 Predictions Min                         28.2689
trainer/Q2 Predictions Mean                        34.1121
trainer/Q2 Predictions Std                          4.52521
trainer/Q2 Predictions Max                         52.5575
trainer/Q2 Predictions Min                         27.9695
trainer/Q Targets Mean                             34.0408
trainer/Q Targets Std                               4.57081
trainer/Q Targets Max                              52.3799
trainer/Q Targets Min                              27.7116
trainer/Log Pis Mean                               -1.19033
trainer/Log Pis Std                                 2.9357
trainer/Log Pis Max                                 9.3012
trainer/Log Pis Min                                -7.17997
trainer/Policy mu Mean                              0.0628534
trainer/Policy mu Std                               0.885675
trainer/Policy mu Max                               2.57625
trainer/Policy mu Min                              -2.94057
trainer/Policy log std Mean                        -0.138452
trainer/Policy log std Std                          0.135783
trainer/Policy log std Max                          0.224583
trainer/Policy log std Min                         -0.733048
trainer/Alpha                                       0.0977499
trainer/Alpha Loss                                -16.718
exploration/num steps total                     10000
exploration/num paths total                        10
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.103957
exploration/Rewards Std                             0.762466
exploration/Rewards Max                             2.25581
exploration/Rewards Min                            -3.14692
exploration/Returns Mean                         -103.957
exploration/Returns Std                             0
exploration/Returns Max                          -103.957
exploration/Returns Min                          -103.957
exploration/Actions Mean                            0.00620505
exploration/Actions Std                             0.700627
exploration/Actions Max                             0.999597
exploration/Actions Min                            -0.999719
exploration/Num Paths                               1
exploration/Average Returns                      -103.957
exploration/env_infos/final/reward_run Mean         1.51139
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          1.51139
exploration/env_infos/final/reward_run Min          1.51139
exploration/env_infos/initial/reward_run Mean       0.24446
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.24446
exploration/env_infos/initial/reward_run Min        0.24446
exploration/env_infos/reward_run Mean               0.190594
exploration/env_infos/reward_run Std                0.748179
exploration/env_infos/reward_run Max                2.60776
exploration/env_infos/reward_run Min               -2.82122
exploration/env_infos/final/reward_ctrl Mean       -0.300182
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.300182
exploration/env_infos/final/reward_ctrl Min        -0.300182
exploration/env_infos/initial/reward_ctrl Mean     -0.337967
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.337967
exploration/env_infos/initial/reward_ctrl Min      -0.337967
exploration/env_infos/reward_ctrl Mean             -0.29455
exploration/env_infos/reward_ctrl Std               0.0888244
exploration/env_infos/reward_ctrl Max              -0.0393013
exploration/env_infos/reward_ctrl Min              -0.590855
evaluation/num steps total                      45000
evaluation/num paths total                         45
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.341671
evaluation/Rewards Std                              0.79457
evaluation/Rewards Max                              2.68177
evaluation/Rewards Min                             -2.52154
evaluation/Returns Mean                           341.671
evaluation/Returns Std                             11.3541
evaluation/Returns Max                            364.22
evaluation/Returns Min                            333.656
evaluation/Actions Mean                             0.00453453
evaluation/Actions Std                              0.584434
evaluation/Actions Max                              0.995384
evaluation/Actions Min                             -0.998797
evaluation/Num Paths                                5
evaluation/Average Returns                        341.671
evaluation/env_infos/final/reward_run Mean          0.66815
evaluation/env_infos/final/reward_run Std           0.511968
evaluation/env_infos/final/reward_run Max           1.60351
evaluation/env_infos/final/reward_run Min           0.0838643
evaluation/env_infos/initial/reward_run Mean        0.555087
evaluation/env_infos/initial/reward_run Std         0.223241
evaluation/env_infos/initial/reward_run Max         0.755735
evaluation/env_infos/initial/reward_run Min         0.130241
evaluation/env_infos/reward_run Mean                0.546621
evaluation/env_infos/reward_run Std                 0.788232
evaluation/env_infos/reward_run Max                 2.90101
evaluation/env_infos/reward_run Min                -2.11308
evaluation/env_infos/final/reward_ctrl Mean        -0.221361
evaluation/env_infos/final/reward_ctrl Std          0.0590767
evaluation/env_infos/final/reward_ctrl Max         -0.116315
evaluation/env_infos/final/reward_ctrl Min         -0.277972
evaluation/env_infos/initial/reward_ctrl Mean      -0.215207
evaluation/env_infos/initial/reward_ctrl Std        0.037827
evaluation/env_infos/initial/reward_ctrl Max       -0.150721
evaluation/env_infos/initial/reward_ctrl Min       -0.252584
evaluation/env_infos/reward_ctrl Mean              -0.20495
evaluation/env_infos/reward_ctrl Std                0.0716918
evaluation/env_infos/reward_ctrl Max               -0.0214639
evaluation/env_infos/reward_ctrl Min               -0.525265
time/data storing (s)                               0.00878023
time/evaluation sampling (s)                        3.52129
time/exploration sampling (s)                       0.73565
time/logging (s)                                    0.0405029
time/saving (s)                                     0.0166902
time/training (s)                                  41.6257
time/epoch (s)                                     45.9486
time/total (s)                                    378.875
Epoch                                               8
----------------------------------------------  --------------
2020-07-08 20:05:11.435896 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 9 finished
----------------------------------------------  --------------
replay_buffer/size                              11000
trainer/QF1 Loss                                    0.926339
trainer/QF2 Loss                                    0.999112
trainer/Policy Loss                               -33.6952
trainer/Q1 Predictions Mean                        32.6702
trainer/Q1 Predictions Std                          4.93131
trainer/Q1 Predictions Max                         48.633
trainer/Q1 Predictions Min                         26.568
trainer/Q2 Predictions Mean                        32.7288
trainer/Q2 Predictions Std                          4.87126
trainer/Q2 Predictions Max                         48.3762
trainer/Q2 Predictions Min                         26.5114
trainer/Q Targets Mean                             32.6424
trainer/Q Targets Std                               4.90153
trainer/Q Targets Max                              47.6989
trainer/Q Targets Min                              26.0755
trainer/Log Pis Mean                               -0.81015
trainer/Log Pis Std                                 3.31747
trainer/Log Pis Max                                13.3427
trainer/Log Pis Min                                -7.61621
trainer/Policy mu Mean                              0.041422
trainer/Policy mu Std                               0.953334
trainer/Policy mu Max                               2.93812
trainer/Policy mu Min                              -3.08645
trainer/Policy log std Mean                        -0.157076
trainer/Policy log std Std                          0.154424
trainer/Policy log std Max                          0.363351
trainer/Policy log std Min                         -0.814671
trainer/Alpha                                       0.0739175
trainer/Alpha Loss                                -17.7372
exploration/num steps total                     11000
exploration/num paths total                        11
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                           -0.0430878
exploration/Rewards Std                             0.771084
exploration/Rewards Max                             2.27494
exploration/Rewards Min                            -2.93053
exploration/Returns Mean                          -43.0878
exploration/Returns Std                             0
exploration/Returns Max                           -43.0878
exploration/Returns Min                           -43.0878
exploration/Actions Mean                            0.0458396
exploration/Actions Std                             0.717477
exploration/Actions Max                             0.999872
exploration/Actions Min                            -0.99992
exploration/Num Paths                               1
exploration/Average Returns                       -43.0878
exploration/env_infos/final/reward_run Mean        -0.115843
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max         -0.115843
exploration/env_infos/final/reward_run Min         -0.115843
exploration/env_infos/initial/reward_run Mean       0.669257
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.669257
exploration/env_infos/initial/reward_run Min        0.669257
exploration/env_infos/reward_run Mean               0.267036
exploration/env_infos/reward_run Std                0.762627
exploration/env_infos/reward_run Max                2.69592
exploration/env_infos/reward_run Min               -2.55442
exploration/env_infos/final/reward_ctrl Mean       -0.127856
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.127856
exploration/env_infos/final/reward_ctrl Min        -0.127856
exploration/env_infos/initial/reward_ctrl Mean     -0.371993
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.371993
exploration/env_infos/initial/reward_ctrl Min      -0.371993
exploration/env_infos/reward_ctrl Mean             -0.310124
exploration/env_infos/reward_ctrl Std               0.0948951
exploration/env_infos/reward_ctrl Max              -0.0271339
exploration/env_infos/reward_ctrl Min              -0.570554
evaluation/num steps total                      50000
evaluation/num paths total                         50
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                            -0.121599
evaluation/Rewards Std                              0.944048
evaluation/Rewards Max                              2.74528
evaluation/Rewards Min                             -5.29289
evaluation/Returns Mean                          -121.599
evaluation/Returns Std                             77.1072
evaluation/Returns Max                            -12.3644
evaluation/Returns Min                           -243.547
evaluation/Actions Mean                             0.130288
evaluation/Actions Std                              0.712555
evaluation/Actions Max                              0.999366
evaluation/Actions Min                             -0.999814
evaluation/Num Paths                                5
evaluation/Average Returns                       -121.599
evaluation/env_infos/final/reward_run Mean          0.777473
evaluation/env_infos/final/reward_run Std           0.511347
evaluation/env_infos/final/reward_run Max           1.50448
evaluation/env_infos/final/reward_run Min           0.206371
evaluation/env_infos/initial/reward_run Mean        0.598115
evaluation/env_infos/initial/reward_run Std         0.130098
evaluation/env_infos/initial/reward_run Max         0.754793
evaluation/env_infos/initial/reward_run Min         0.376864
evaluation/env_infos/reward_run Mean                0.193226
evaluation/env_infos/reward_run Std                 0.920735
evaluation/env_infos/reward_run Max                 3.19577
evaluation/env_infos/reward_run Min                -4.78305
evaluation/env_infos/final/reward_ctrl Mean        -0.294295
evaluation/env_infos/final/reward_ctrl Std          0.081455
evaluation/env_infos/final/reward_ctrl Max         -0.172825
evaluation/env_infos/final/reward_ctrl Min         -0.416594
evaluation/env_infos/initial/reward_ctrl Mean      -0.268675
evaluation/env_infos/initial/reward_ctrl Std        0.0206737
evaluation/env_infos/initial/reward_ctrl Max       -0.23998
evaluation/env_infos/initial/reward_ctrl Min       -0.297669
evaluation/env_infos/reward_ctrl Mean              -0.314826
evaluation/env_infos/reward_ctrl Std                0.100351
evaluation/env_infos/reward_ctrl Max               -0.0310192
evaluation/env_infos/reward_ctrl Min               -0.573004
time/data storing (s)                               0.00791319
time/evaluation sampling (s)                        3.59544
time/exploration sampling (s)                       0.737072
time/logging (s)                                    0.0443162
time/saving (s)                                     0.0169355
time/training (s)                                  36.3108
time/epoch (s)                                     40.7125
time/total (s)                                    419.646
Epoch                                               9
----------------------------------------------  --------------
2020-07-08 20:05:46.471035 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 10 finished
----------------------------------------------  --------------
replay_buffer/size                              12000
trainer/QF1 Loss                                    0.893047
trainer/QF2 Loss                                    0.836027
trainer/Policy Loss                               -31.7341
trainer/Q1 Predictions Mean                        31.6334
trainer/Q1 Predictions Std                          5.61332
trainer/Q1 Predictions Max                         53.5575
trainer/Q1 Predictions Min                         25.8357
trainer/Q2 Predictions Mean                        31.6426
trainer/Q2 Predictions Std                          5.58602
trainer/Q2 Predictions Max                         53.2799
trainer/Q2 Predictions Min                         25.7137
trainer/Q Targets Mean                             31.529
trainer/Q Targets Std                               5.65283
trainer/Q Targets Max                              53.7039
trainer/Q Targets Min                              24.4929
trainer/Log Pis Mean                                0.0678208
trainer/Log Pis Std                                 3.32744
trainer/Log Pis Max                                16.4392
trainer/Log Pis Min                                -6.32427
trainer/Policy mu Mean                              0.183057
trainer/Policy mu Std                               1.02851
trainer/Policy mu Max                               3.53527
trainer/Policy mu Min                              -2.95814
trainer/Policy log std Mean                        -0.26073
trainer/Policy log std Std                          0.173382
trainer/Policy log std Max                          0.230067
trainer/Policy log std Min                         -0.873637
trainer/Alpha                                       0.0562429
trainer/Alpha Loss                                -17.0717
exploration/num steps total                     12000
exploration/num paths total                        12
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                            0.0555198
exploration/Rewards Std                             0.796526
exploration/Rewards Max                             2.56427
exploration/Rewards Min                            -1.84119
exploration/Returns Mean                           55.5198
exploration/Returns Std                             0
exploration/Returns Max                            55.5198
exploration/Returns Min                            55.5198
exploration/Actions Mean                            0.115363
exploration/Actions Std                             0.713446
exploration/Actions Max                             0.999726
exploration/Actions Min                            -0.999679
exploration/Num Paths                               1
exploration/Average Returns                        55.5198
exploration/env_infos/final/reward_run Mean        -0.33935
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max         -0.33935
exploration/env_infos/final/reward_run Min         -0.33935
exploration/env_infos/initial/reward_run Mean       0.0511113
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.0511113
exploration/env_infos/initial/reward_run Min        0.0511113
exploration/env_infos/reward_run Mean               0.368908
exploration/env_infos/reward_run Std                0.795286
exploration/env_infos/reward_run Max                2.99778
exploration/env_infos/reward_run Min               -1.47251
exploration/env_infos/final/reward_ctrl Mean       -0.235968
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.235968
exploration/env_infos/final/reward_ctrl Min        -0.235968
exploration/env_infos/initial/reward_ctrl Mean     -0.316782
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.316782
exploration/env_infos/initial/reward_ctrl Min      -0.316782
exploration/env_infos/reward_ctrl Mean             -0.313388
exploration/env_infos/reward_ctrl Std               0.0835942
exploration/env_infos/reward_ctrl Max              -0.0544785
exploration/env_infos/reward_ctrl Min              -0.539896
evaluation/num steps total                      55000
evaluation/num paths total                         55
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.221253
evaluation/Rewards Std                              0.890154
evaluation/Rewards Max                              2.96845
evaluation/Rewards Min                             -2.84685
evaluation/Returns Mean                           221.253
evaluation/Returns Std                            166.844
evaluation/Returns Max                            424.202
evaluation/Returns Min                            -69.0283
evaluation/Actions Mean                             0.0999419
evaluation/Actions Std                              0.6869
evaluation/Actions Max                              0.99982
evaluation/Actions Min                             -0.999783
evaluation/Num Paths                                5
evaluation/Average Returns                        221.253
evaluation/env_infos/final/reward_run Mean          0.889501
evaluation/env_infos/final/reward_run Std           0.716107
evaluation/env_infos/final/reward_run Max           2.18944
evaluation/env_infos/final/reward_run Min           0.219853
evaluation/env_infos/initial/reward_run Mean        0.298276
evaluation/env_infos/initial/reward_run Std         0.0908239
evaluation/env_infos/initial/reward_run Max         0.39027
evaluation/env_infos/initial/reward_run Min         0.17629
evaluation/env_infos/reward_run Mean                0.510345
evaluation/env_infos/reward_run Std                 0.878744
evaluation/env_infos/reward_run Max                 3.28687
evaluation/env_infos/reward_run Min                -2.38711
evaluation/env_infos/final/reward_ctrl Mean        -0.259528
evaluation/env_infos/final/reward_ctrl Std          0.0163994
evaluation/env_infos/final/reward_ctrl Max         -0.234788
evaluation/env_infos/final/reward_ctrl Min         -0.277652
evaluation/env_infos/initial/reward_ctrl Mean      -0.218171
evaluation/env_infos/initial/reward_ctrl Std        0.0709683
evaluation/env_infos/initial/reward_ctrl Max       -0.131387
evaluation/env_infos/initial/reward_ctrl Min       -0.338901
evaluation/env_infos/reward_ctrl Mean              -0.289092
evaluation/env_infos/reward_ctrl Std                0.0740858
evaluation/env_infos/reward_ctrl Max               -0.0606233
evaluation/env_infos/reward_ctrl Min               -0.569838
time/data storing (s)                               0.00680859
time/evaluation sampling (s)                        3.30055
time/exploration sampling (s)                       0.649871
time/logging (s)                                    0.04015
time/saving (s)                                     0.0158067
time/training (s)                                  30.9889
time/epoch (s)                                     35.0021
time/total (s)                                    454.675
Epoch                                              10
----------------------------------------------  --------------
2020-07-08 20:06:19.678449 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 11 finished
----------------------------------------------  --------------
replay_buffer/size                              13000
trainer/QF1 Loss                                    1.32465
trainer/QF2 Loss                                    1.22466
trainer/Policy Loss                               -30.6214
trainer/Q1 Predictions Mean                        31.4988
trainer/Q1 Predictions Std                          6.71309
trainer/Q1 Predictions Max                         53.2194
trainer/Q1 Predictions Min                         23.8056
trainer/Q2 Predictions Mean                        31.4602
trainer/Q2 Predictions Std                          6.70398
trainer/Q2 Predictions Max                         52.3723
trainer/Q2 Predictions Min                         23.5237
trainer/Q Targets Mean                             31.4135
trainer/Q Targets Std                               6.82723
trainer/Q Targets Max                              56.6692
trainer/Q Targets Min                              23.5807
trainer/Log Pis Mean                                1.10264
trainer/Log Pis Std                                 3.81478
trainer/Log Pis Max                                16.5083
trainer/Log Pis Min                                -7.70147
trainer/Policy mu Mean                              0.173901
trainer/Policy mu Std                               1.16262
trainer/Policy mu Max                               4.28326
trainer/Policy mu Min                              -3.42857
trainer/Policy log std Mean                        -0.329509
trainer/Policy log std Std                          0.176788
trainer/Policy log std Max                          0.211831
trainer/Policy log std Min                         -0.989858
trainer/Alpha                                       0.0431192
trainer/Alpha Loss                                -15.395
exploration/num steps total                     13000
exploration/num paths total                        13
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                            0.0225642
exploration/Rewards Std                             0.852859
exploration/Rewards Max                             2.80889
exploration/Rewards Min                            -2.38891
exploration/Returns Mean                           22.5642
exploration/Returns Std                             0
exploration/Returns Max                            22.5642
exploration/Returns Min                            22.5642
exploration/Actions Mean                            0.0577291
exploration/Actions Std                             0.753191
exploration/Actions Max                             0.999988
exploration/Actions Min                            -0.999976
exploration/Num Paths                               1
exploration/Average Returns                        22.5642
exploration/env_infos/final/reward_run Mean        -0.020149
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max         -0.020149
exploration/env_infos/final/reward_run Min         -0.020149
exploration/env_infos/initial/reward_run Mean       0.0942392
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.0942392
exploration/env_infos/initial/reward_run Min        0.0942392
exploration/env_infos/reward_run Mean               0.364942
exploration/env_infos/reward_run Std                0.855673
exploration/env_infos/reward_run Max                3.17874
exploration/env_infos/reward_run Min               -2.12286
exploration/env_infos/final/reward_ctrl Mean       -0.273068
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.273068
exploration/env_infos/final/reward_ctrl Min        -0.273068
exploration/env_infos/initial/reward_ctrl Mean     -0.137003
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.137003
exploration/env_infos/initial/reward_ctrl Min      -0.137003
exploration/env_infos/reward_ctrl Mean             -0.342378
exploration/env_infos/reward_ctrl Std               0.0917291
exploration/env_infos/reward_ctrl Max              -0.048564
exploration/env_infos/reward_ctrl Min              -0.563276
evaluation/num steps total                      60000
evaluation/num paths total                         60
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.188141
evaluation/Rewards Std                              0.840343
evaluation/Rewards Max                              2.94094
evaluation/Rewards Min                             -2.87792
evaluation/Returns Mean                           188.141
evaluation/Returns Std                             42.9425
evaluation/Returns Max                            264.425
evaluation/Returns Min                            144.183
evaluation/Actions Mean                             0.054625
evaluation/Actions Std                              0.742071
evaluation/Actions Max                              0.999912
evaluation/Actions Min                             -0.999953
evaluation/Num Paths                                5
evaluation/Average Returns                        188.141
evaluation/env_infos/final/reward_run Mean          0.69346
evaluation/env_infos/final/reward_run Std           0.854391
evaluation/env_infos/final/reward_run Max           1.9501
evaluation/env_infos/final/reward_run Min          -0.7294
evaluation/env_infos/initial/reward_run Mean        0.365556
evaluation/env_infos/initial/reward_run Std         0.297404
evaluation/env_infos/initial/reward_run Max         0.920277
evaluation/env_infos/initial/reward_run Min         0.0969694
evaluation/env_infos/reward_run Mean                0.520333
evaluation/env_infos/reward_run Std                 0.840923
evaluation/env_infos/reward_run Max                 3.30457
evaluation/env_infos/reward_run Min                -2.52395
evaluation/env_infos/final/reward_ctrl Mean        -0.306137
evaluation/env_infos/final/reward_ctrl Std          0.0436023
evaluation/env_infos/final/reward_ctrl Max         -0.261067
evaluation/env_infos/final/reward_ctrl Min         -0.389322
evaluation/env_infos/initial/reward_ctrl Mean      -0.194713
evaluation/env_infos/initial/reward_ctrl Std        0.0342265
evaluation/env_infos/initial/reward_ctrl Max       -0.151485
evaluation/env_infos/initial/reward_ctrl Min       -0.236584
evaluation/env_infos/reward_ctrl Mean              -0.332192
evaluation/env_infos/reward_ctrl Std                0.0833523
evaluation/env_infos/reward_ctrl Max               -0.0647422
evaluation/env_infos/reward_ctrl Min               -0.577798
time/data storing (s)                               0.00668459
time/evaluation sampling (s)                        2.30509
time/exploration sampling (s)                       0.59975
time/logging (s)                                    0.0408183
time/saving (s)                                     0.0171264
time/training (s)                                  30.169
time/epoch (s)                                     33.1385
time/total (s)                                    487.881
Epoch                                              11
----------------------------------------------  --------------
2020-07-08 20:06:53.468570 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 12 finished
----------------------------------------------  --------------
replay_buffer/size                              14000
trainer/QF1 Loss                                    0.833843
trainer/QF2 Loss                                    0.952904
trainer/Policy Loss                               -27.8783
trainer/Q1 Predictions Mean                        29.2061
trainer/Q1 Predictions Std                          6.42089
trainer/Q1 Predictions Max                         49.7881
trainer/Q1 Predictions Min                         21.6774
trainer/Q2 Predictions Mean                        29.2751
trainer/Q2 Predictions Std                          6.42213
trainer/Q2 Predictions Max                         49.5894
trainer/Q2 Predictions Min                         21.1119
trainer/Q Targets Mean                             29.3299
trainer/Q Targets Std                               6.52021
trainer/Q Targets Max                              48.9657
trainer/Q Targets Min                              21.3765
trainer/Log Pis Mean                                1.54333
trainer/Log Pis Std                                 3.24837
trainer/Log Pis Max                                12.3006
trainer/Log Pis Min                                -6.76112
trainer/Policy mu Mean                              0.0699006
trainer/Policy mu Std                               1.22341
trainer/Policy mu Max                               4.53606
trainer/Policy mu Min                              -3.75447
trainer/Policy log std Mean                        -0.443025
trainer/Policy log std Std                          0.195013
trainer/Policy log std Max                          0.0920647
trainer/Policy log std Min                         -1.50346
trainer/Alpha                                       0.033504
trainer/Alpha Loss                                -15.1341
exploration/num steps total                     14000
exploration/num paths total                        14
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                            0.265349
exploration/Rewards Std                             0.773243
exploration/Rewards Max                             2.99324
exploration/Rewards Min                            -1.89367
exploration/Returns Mean                          265.349
exploration/Returns Std                             0
exploration/Returns Max                           265.349
exploration/Returns Min                           265.349
exploration/Actions Mean                           -0.0366763
exploration/Actions Std                             0.744624
exploration/Actions Max                             0.999927
exploration/Actions Min                            -0.999947
exploration/Num Paths                               1
exploration/Average Returns                       265.349
exploration/env_infos/final/reward_run Mean         0.648902
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          0.648902
exploration/env_infos/final/reward_run Min          0.648902
exploration/env_infos/initial/reward_run Mean       0.350327
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.350327
exploration/env_infos/initial/reward_run Min        0.350327
exploration/env_infos/reward_run Mean               0.598835
exploration/env_infos/reward_run Std                0.775541
exploration/env_infos/reward_run Max                3.35724
exploration/env_infos/reward_run Min               -1.62827
exploration/env_infos/final/reward_ctrl Mean       -0.27066
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.27066
exploration/env_infos/final/reward_ctrl Min        -0.27066
exploration/env_infos/initial/reward_ctrl Mean     -0.187461
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.187461
exploration/env_infos/initial/reward_ctrl Min      -0.187461
exploration/env_infos/reward_ctrl Mean             -0.333486
exploration/env_infos/reward_ctrl Std               0.0922892
exploration/env_infos/reward_ctrl Max              -0.0751305
exploration/env_infos/reward_ctrl Min              -0.581274
evaluation/num steps total                      65000
evaluation/num paths total                         65
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.297499
evaluation/Rewards Std                              0.765034
evaluation/Rewards Max                              2.87089
evaluation/Rewards Min                             -2.26318
evaluation/Returns Mean                           297.499
evaluation/Returns Std                            117.202
evaluation/Returns Max                            392.932
evaluation/Returns Min                            114.521
evaluation/Actions Mean                            -0.0250995
evaluation/Actions Std                              0.734911
evaluation/Actions Max                              0.999797
evaluation/Actions Min                             -0.999855
evaluation/Num Paths                                5
evaluation/Average Returns                        297.499
evaluation/env_infos/final/reward_run Mean          0.689569
evaluation/env_infos/final/reward_run Std           0.909719
evaluation/env_infos/final/reward_run Max           1.80683
evaluation/env_infos/final/reward_run Min          -0.665414
evaluation/env_infos/initial/reward_run Mean        0.356071
evaluation/env_infos/initial/reward_run Std         0.161267
evaluation/env_infos/initial/reward_run Max         0.54745
evaluation/env_infos/initial/reward_run Min         0.0847444
evaluation/env_infos/reward_run Mean                0.621934
evaluation/env_infos/reward_run Std                 0.781073
evaluation/env_infos/reward_run Max                 3.1594
evaluation/env_infos/reward_run Min                -1.9456
evaluation/env_infos/final/reward_ctrl Mean        -0.365462
evaluation/env_infos/final/reward_ctrl Std          0.10145
evaluation/env_infos/final/reward_ctrl Max         -0.239557
evaluation/env_infos/final/reward_ctrl Min         -0.498667
evaluation/env_infos/initial/reward_ctrl Mean      -0.177919
evaluation/env_infos/initial/reward_ctrl Std        0.0372578
evaluation/env_infos/initial/reward_ctrl Max       -0.140175
evaluation/env_infos/initial/reward_ctrl Min       -0.234414
evaluation/env_infos/reward_ctrl Mean              -0.324435
evaluation/env_infos/reward_ctrl Std                0.0956227
evaluation/env_infos/reward_ctrl Max               -0.0446761
evaluation/env_infos/reward_ctrl Min               -0.573997
time/data storing (s)                               0.00669677
time/evaluation sampling (s)                        2.29207
time/exploration sampling (s)                       0.599833
time/logging (s)                                    0.0457177
time/saving (s)                                     0.0174383
time/training (s)                                  30.6867
time/epoch (s)                                     33.6485
time/total (s)                                    521.673
Epoch                                              12
----------------------------------------------  --------------
2020-07-08 20:07:27.843064 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 13 finished
----------------------------------------------  --------------
replay_buffer/size                              15000
trainer/QF1 Loss                                    1.19241
trainer/QF2 Loss                                    1.47489
trainer/Policy Loss                               -26.308
trainer/Q1 Predictions Mean                        28.9226
trainer/Q1 Predictions Std                          7.36058
trainer/Q1 Predictions Max                         52.7317
trainer/Q1 Predictions Min                         19.2582
trainer/Q2 Predictions Mean                        28.8854
trainer/Q2 Predictions Std                          7.31438
trainer/Q2 Predictions Max                         52.1681
trainer/Q2 Predictions Min                         19.8838
trainer/Q Targets Mean                             28.7633
trainer/Q Targets Std                               7.44566
trainer/Q Targets Max                              52.9222
trainer/Q Targets Min                              18.6372
trainer/Log Pis Mean                                2.83955
trainer/Log Pis Std                                 4.51117
trainer/Log Pis Max                                26.0815
trainer/Log Pis Min                                -7.94584
trainer/Policy mu Mean                              0.170038
trainer/Policy mu Std                               1.33552
trainer/Policy mu Max                               4.22927
trainer/Policy mu Min                              -5.54992
trainer/Policy log std Mean                        -0.498579
trainer/Policy log std Std                          0.18285
trainer/Policy log std Max                          0.10905
trainer/Policy log std Min                         -1.48302
trainer/Alpha                                       0.0264506
trainer/Alpha Loss                                -11.4795
exploration/num steps total                     15000
exploration/num paths total                        15
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                            0.233105
exploration/Rewards Std                             0.765817
exploration/Rewards Max                             2.45552
exploration/Rewards Min                            -2.38264
exploration/Returns Mean                          233.105
exploration/Returns Std                             0
exploration/Returns Max                           233.105
exploration/Returns Min                           233.105
exploration/Actions Mean                            0.134071
exploration/Actions Std                             0.747506
exploration/Actions Max                             0.999982
exploration/Actions Min                            -0.999963
exploration/Num Paths                               1
exploration/Average Returns                       233.105
exploration/env_infos/final/reward_run Mean         1.84851
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          1.84851
exploration/env_infos/final/reward_run Min          1.84851
exploration/env_infos/initial/reward_run Mean       0.153397
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.153397
exploration/env_infos/initial/reward_run Min        0.153397
exploration/env_infos/reward_run Mean               0.579149
exploration/env_infos/reward_run Std                0.766176
exploration/env_infos/reward_run Max                2.93343
exploration/env_infos/reward_run Min               -2.02284
exploration/env_infos/final/reward_ctrl Mean       -0.460909
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.460909
exploration/env_infos/final/reward_ctrl Min        -0.460909
exploration/env_infos/initial/reward_ctrl Mean     -0.077817
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.077817
exploration/env_infos/initial/reward_ctrl Min      -0.077817
exploration/env_infos/reward_ctrl Mean             -0.346045
exploration/env_infos/reward_ctrl Std               0.0928134
exploration/env_infos/reward_ctrl Max              -0.0319639
exploration/env_infos/reward_ctrl Min              -0.576492
evaluation/num steps total                      70000
evaluation/num paths total                         70
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.467171
evaluation/Rewards Std                              0.777014
evaluation/Rewards Max                              2.67364
evaluation/Rewards Min                             -2.14949
evaluation/Returns Mean                           467.171
evaluation/Returns Std                             33.0512
evaluation/Returns Max                            527.679
evaluation/Returns Min                            428.36
evaluation/Actions Mean                             0.147134
evaluation/Actions Std                              0.742069
evaluation/Actions Max                              0.999886
evaluation/Actions Min                             -0.999968
evaluation/Num Paths                                5
evaluation/Average Returns                        467.171
evaluation/env_infos/final/reward_run Mean          1.13996
evaluation/env_infos/final/reward_run Std           0.432974
evaluation/env_infos/final/reward_run Max           1.80218
evaluation/env_infos/final/reward_run Min           0.635107
evaluation/env_infos/initial/reward_run Mean       -0.0900876
evaluation/env_infos/initial/reward_run Std         0.300742
evaluation/env_infos/initial/reward_run Max         0.252532
evaluation/env_infos/initial/reward_run Min        -0.648492
evaluation/env_infos/reward_run Mean                0.81056
evaluation/env_infos/reward_run Std                 0.780545
evaluation/env_infos/reward_run Max                 3.11434
evaluation/env_infos/reward_run Min                -1.84263
evaluation/env_infos/final/reward_ctrl Mean        -0.33302
evaluation/env_infos/final/reward_ctrl Std          0.115711
evaluation/env_infos/final/reward_ctrl Max         -0.206709
evaluation/env_infos/final/reward_ctrl Min         -0.478136
evaluation/env_infos/initial/reward_ctrl Mean      -0.118157
evaluation/env_infos/initial/reward_ctrl Std        0.0307555
evaluation/env_infos/initial/reward_ctrl Max       -0.0853342
evaluation/env_infos/initial/reward_ctrl Min       -0.158717
evaluation/env_infos/reward_ctrl Mean              -0.343389
evaluation/env_infos/reward_ctrl Std                0.0964863
evaluation/env_infos/reward_ctrl Max               -0.0338432
evaluation/env_infos/reward_ctrl Min               -0.579897
time/data storing (s)                               0.00664273
time/evaluation sampling (s)                        2.36371
time/exploration sampling (s)                       0.61346
time/logging (s)                                    0.0531473
time/saving (s)                                     0.0208654
time/training (s)                                  31.2698
time/epoch (s)                                     34.3277
time/total (s)                                    556.053
Epoch                                              13
----------------------------------------------  --------------
2020-07-08 20:08:06.028983 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 14 finished
----------------------------------------------  --------------
replay_buffer/size                              16000
trainer/QF1 Loss                                    1.33208
trainer/QF2 Loss                                    1.42945
trainer/Policy Loss                               -24.1122
trainer/Q1 Predictions Mean                        28.2936
trainer/Q1 Predictions Std                          8.24836
trainer/Q1 Predictions Max                         54.7736
trainer/Q1 Predictions Min                         18.5723
trainer/Q2 Predictions Mean                        28.1899
trainer/Q2 Predictions Std                          8.17926
trainer/Q2 Predictions Max                         53.4001
trainer/Q2 Predictions Min                         19.388
trainer/Q Targets Mean                             28.2467
trainer/Q Targets Std                               8.165
trainer/Q Targets Max                              54.4261
trainer/Q Targets Min                              18.9069
trainer/Log Pis Mean                                4.31935
trainer/Log Pis Std                                 5.20392
trainer/Log Pis Max                                29.2573
trainer/Log Pis Min                                -5.04664
trainer/Policy mu Mean                              0.212743
trainer/Policy mu Std                               1.46128
trainer/Policy mu Max                               5.14503
trainer/Policy mu Min                              -4.794
trainer/Policy log std Mean                        -0.598883
trainer/Policy log std Std                          0.190595
trainer/Policy log std Max                         -0.097163
trainer/Policy log std Min                         -1.49006
trainer/Alpha                                       0.0214284
trainer/Alpha Loss                                 -6.45855
exploration/num steps total                     16000
exploration/num paths total                        16
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                            0.397347
exploration/Rewards Std                             0.901332
exploration/Rewards Max                             3.22217
exploration/Rewards Min                            -2.64047
exploration/Returns Mean                          397.347
exploration/Returns Std                             0
exploration/Returns Max                           397.347
exploration/Returns Min                           397.347
exploration/Actions Mean                            0.113291
exploration/Actions Std                             0.810202
exploration/Actions Max                             0.999995
exploration/Actions Min                            -0.999998
exploration/Num Paths                               1
exploration/Average Returns                       397.347
exploration/env_infos/final/reward_run Mean         1.14441
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          1.14441
exploration/env_infos/final/reward_run Min          1.14441
exploration/env_infos/initial/reward_run Mean       0.43773
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.43773
exploration/env_infos/initial/reward_run Min        0.43773
exploration/env_infos/reward_run Mean               0.798904
exploration/env_infos/reward_run Std                0.883347
exploration/env_infos/reward_run Max                3.71591
exploration/env_infos/reward_run Min               -2.12206
exploration/env_infos/final/reward_ctrl Mean       -0.457643
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.457643
exploration/env_infos/final/reward_ctrl Min        -0.457643
exploration/env_infos/initial/reward_ctrl Mean     -0.343788
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.343788
exploration/env_infos/initial/reward_ctrl Min      -0.343788
exploration/env_infos/reward_ctrl Mean             -0.401557
exploration/env_infos/reward_ctrl Std               0.093914
exploration/env_infos/reward_ctrl Max              -0.0283969
exploration/env_infos/reward_ctrl Min              -0.595359
evaluation/num steps total                      75000
evaluation/num paths total                         75
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.408933
evaluation/Rewards Std                              0.883957
evaluation/Rewards Max                              2.78942
evaluation/Rewards Min                             -3.69076
evaluation/Returns Mean                           408.933
evaluation/Returns Std                            175.738
evaluation/Returns Max                            625.204
evaluation/Returns Min                             91.9713
evaluation/Actions Mean                             0.135459
evaluation/Actions Std                              0.801104
evaluation/Actions Max                              0.999997
evaluation/Actions Min                             -0.999997
evaluation/Num Paths                                5
evaluation/Average Returns                        408.933
evaluation/env_infos/final/reward_run Mean          1.11612
evaluation/env_infos/final/reward_run Std           0.345514
evaluation/env_infos/final/reward_run Max           1.75048
evaluation/env_infos/final/reward_run Min           0.741375
evaluation/env_infos/initial/reward_run Mean       -0.00178484
evaluation/env_infos/initial/reward_run Std         0.187587
evaluation/env_infos/initial/reward_run Max         0.25304
evaluation/env_infos/initial/reward_run Min        -0.164035
evaluation/env_infos/reward_run Mean                0.805003
evaluation/env_infos/reward_run Std                 0.863042
evaluation/env_infos/reward_run Max                 3.22123
evaluation/env_infos/reward_run Min                -3.18483
evaluation/env_infos/final/reward_ctrl Mean        -0.371591
evaluation/env_infos/final/reward_ctrl Std          0.0802342
evaluation/env_infos/final/reward_ctrl Max         -0.218218
evaluation/env_infos/final/reward_ctrl Min         -0.453877
evaluation/env_infos/initial/reward_ctrl Mean      -0.120362
evaluation/env_infos/initial/reward_ctrl Std        0.0265539
evaluation/env_infos/initial/reward_ctrl Max       -0.102401
evaluation/env_infos/initial/reward_ctrl Min       -0.172775
evaluation/env_infos/reward_ctrl Mean              -0.39607
evaluation/env_infos/reward_ctrl Std                0.0921679
evaluation/env_infos/reward_ctrl Max               -0.0812444
evaluation/env_infos/reward_ctrl Min               -0.59406
time/data storing (s)                               0.00663695
time/evaluation sampling (s)                        4.6106
time/exploration sampling (s)                       0.666486
time/logging (s)                                    0.043255
time/saving (s)                                     0.0175457
time/training (s)                                  32.7494
time/epoch (s)                                     38.0939
time/total (s)                                    594.223
Epoch                                              14
----------------------------------------------  --------------
2020-07-08 20:08:49.364110 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 15 finished
----------------------------------------------  --------------
replay_buffer/size                              17000
trainer/QF1 Loss                                    1.27657
trainer/QF2 Loss                                    1.39156
trainer/Policy Loss                               -23.5424
trainer/Q1 Predictions Mean                        28.8959
trainer/Q1 Predictions Std                          8.96381
trainer/Q1 Predictions Max                         56.8952
trainer/Q1 Predictions Min                         17.5282
trainer/Q2 Predictions Mean                        29.0567
trainer/Q2 Predictions Std                          8.90539
trainer/Q2 Predictions Max                         55.9851
trainer/Q2 Predictions Min                         18.679
trainer/Q Targets Mean                             28.7771
trainer/Q Targets Std                               9.00888
trainer/Q Targets Max                              55.3198
trainer/Q Targets Min                              17.3066
trainer/Log Pis Mean                                5.77341
trainer/Log Pis Std                                 4.72617
trainer/Log Pis Max                                25.323
trainer/Log Pis Min                                -9.20337
trainer/Policy mu Mean                              0.407505
trainer/Policy mu Std                               1.53984
trainer/Policy mu Max                               4.72466
trainer/Policy mu Min                              -4.68746
trainer/Policy log std Mean                        -0.673179
trainer/Policy log std Std                          0.18155
trainer/Policy log std Max                         -0.135216
trainer/Policy log std Min                         -1.64021
trainer/Alpha                                       0.0185983
trainer/Alpha Loss                                 -0.902857
exploration/num steps total                     17000
exploration/num paths total                        17
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                            1.20681
exploration/Rewards Std                             0.937365
exploration/Rewards Max                             4.05712
exploration/Rewards Min                            -1.81515
exploration/Returns Mean                         1206.81
exploration/Returns Std                             0
exploration/Returns Max                          1206.81
exploration/Returns Min                          1206.81
exploration/Actions Mean                            0.155466
exploration/Actions Std                             0.796144
exploration/Actions Max                             0.999988
exploration/Actions Min                            -0.999948
exploration/Num Paths                               1
exploration/Average Returns                      1206.81
exploration/env_infos/final/reward_run Mean         1.58896
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          1.58896
exploration/env_infos/final/reward_run Min          1.58896
exploration/env_infos/initial/reward_run Mean       0.472281
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.472281
exploration/env_infos/initial/reward_run Min        0.472281
exploration/env_infos/reward_run Mean               1.60161
exploration/env_infos/reward_run Std                0.925608
exploration/env_infos/reward_run Max                4.49402
exploration/env_infos/reward_run Min               -1.37092
exploration/env_infos/final/reward_ctrl Mean       -0.334589
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.334589
exploration/env_infos/final/reward_ctrl Min        -0.334589
exploration/env_infos/initial/reward_ctrl Mean     -0.220729
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.220729
exploration/env_infos/initial/reward_ctrl Min      -0.220729
exploration/env_infos/reward_ctrl Mean             -0.394809
exploration/env_infos/reward_ctrl Std               0.0841231
exploration/env_infos/reward_ctrl Max              -0.099169
exploration/env_infos/reward_ctrl Min              -0.582613
evaluation/num steps total                      80000
evaluation/num paths total                         80
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             1.41975
evaluation/Rewards Std                              0.938075
evaluation/Rewards Max                              4.01494
evaluation/Rewards Min                             -2.60666
evaluation/Returns Mean                          1419.75
evaluation/Returns Std                             69.6574
evaluation/Returns Max                           1482.61
evaluation/Returns Min                           1301.18
evaluation/Actions Mean                             0.132597
evaluation/Actions Std                              0.794638
evaluation/Actions Max                              0.999966
evaluation/Actions Min                             -0.999969
evaluation/Num Paths                                5
evaluation/Average Returns                       1419.75
evaluation/env_infos/final/reward_run Mean          1.9835
evaluation/env_infos/final/reward_run Std           0.681872
evaluation/env_infos/final/reward_run Max           3.14729
evaluation/env_infos/final/reward_run Min           1.07574
evaluation/env_infos/initial/reward_run Mean        0.521828
evaluation/env_infos/initial/reward_run Std         0.28864
evaluation/env_infos/initial/reward_run Max         0.840921
evaluation/env_infos/initial/reward_run Min         0.0452548
evaluation/env_infos/reward_run Mean                1.80917
evaluation/env_infos/reward_run Std                 0.918532
evaluation/env_infos/reward_run Max                 4.41143
evaluation/env_infos/reward_run Min                -2.32722
evaluation/env_infos/final/reward_ctrl Mean        -0.398748
evaluation/env_infos/final/reward_ctrl Std          0.101573
evaluation/env_infos/final/reward_ctrl Max         -0.198596
evaluation/env_infos/final/reward_ctrl Min         -0.47061
evaluation/env_infos/initial/reward_ctrl Mean      -0.204297
evaluation/env_infos/initial/reward_ctrl Std        0.0390788
evaluation/env_infos/initial/reward_ctrl Max       -0.16732
evaluation/env_infos/initial/reward_ctrl Min       -0.272033
evaluation/env_infos/reward_ctrl Mean              -0.389419
evaluation/env_infos/reward_ctrl Std                0.0803549
evaluation/env_infos/reward_ctrl Max               -0.111014
evaluation/env_infos/reward_ctrl Min               -0.587977
time/data storing (s)                               0.00663239
time/evaluation sampling (s)                        2.42012
time/exploration sampling (s)                       0.624855
time/logging (s)                                    0.0608418
time/saving (s)                                     0.0193966
time/training (s)                                  40.2018
time/epoch (s)                                     43.3337
time/total (s)                                    637.573
Epoch                                              15
----------------------------------------------  --------------
2020-07-08 20:09:38.087285 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 16 finished
----------------------------------------------  --------------
replay_buffer/size                              18000
trainer/QF1 Loss                                    1.36897
trainer/QF2 Loss                                    1.45056
trainer/Policy Loss                               -23.7536
trainer/Q1 Predictions Mean                        29.9439
trainer/Q1 Predictions Std                         10.7803
trainer/Q1 Predictions Max                         53.9879
trainer/Q1 Predictions Min                         17.2941
trainer/Q2 Predictions Mean                        29.9521
trainer/Q2 Predictions Std                         10.8023
trainer/Q2 Predictions Max                         53.5265
trainer/Q2 Predictions Min                         17.3382
trainer/Q Targets Mean                             29.8425
trainer/Q Targets Std                              10.7682
trainer/Q Targets Max                              53.1768
trainer/Q Targets Min                              17.1946
trainer/Log Pis Mean                                6.49728
trainer/Log Pis Std                                 5.69956
trainer/Log Pis Max                                25.3508
trainer/Log Pis Min                                -7.66427
trainer/Policy mu Mean                              0.241718
trainer/Policy mu Std                               1.66501
trainer/Policy mu Max                               5.68041
trainer/Policy mu Min                              -5.42074
trainer/Policy log std Mean                        -0.692497
trainer/Policy log std Std                          0.188542
trainer/Policy log std Max                          0.00622791
trainer/Policy log std Min                         -1.41911
trainer/Alpha                                       0.0186817
trainer/Alpha Loss                                  1.97934
exploration/num steps total                     18000
exploration/num paths total                        18
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                            0.815801
exploration/Rewards Std                             1.18438
exploration/Rewards Max                             4.03956
exploration/Rewards Min                            -3.5305
exploration/Returns Mean                          815.801
exploration/Returns Std                             0
exploration/Returns Max                           815.801
exploration/Returns Min                           815.801
exploration/Actions Mean                            0.11503
exploration/Actions Std                             0.829351
exploration/Actions Max                             0.999999
exploration/Actions Min                            -0.999998
exploration/Num Paths                               1
exploration/Average Returns                       815.801
exploration/env_infos/final/reward_run Mean         2.05994
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          2.05994
exploration/env_infos/final/reward_run Min          2.05994
exploration/env_infos/initial/reward_run Mean       0.845899
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.845899
exploration/env_infos/initial/reward_run Min        0.845899
exploration/env_infos/reward_run Mean               1.23643
exploration/env_infos/reward_run Std                1.13703
exploration/env_infos/reward_run Max                4.40835
exploration/env_infos/reward_run Min               -3.02716
exploration/env_infos/final/reward_ctrl Mean       -0.497466
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.497466
exploration/env_infos/final/reward_ctrl Min        -0.497466
exploration/env_infos/initial/reward_ctrl Mean     -0.234497
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.234497
exploration/env_infos/initial/reward_ctrl Min      -0.234497
exploration/env_infos/reward_ctrl Mean             -0.420633
exploration/env_infos/reward_ctrl Std               0.101228
exploration/env_infos/reward_ctrl Max              -0.0593396
exploration/env_infos/reward_ctrl Min              -0.596647
evaluation/num steps total                      85000
evaluation/num paths total                         85
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             1.06246
evaluation/Rewards Std                              1.23542
evaluation/Rewards Max                              4.36376
evaluation/Rewards Min                             -3.45075
evaluation/Returns Mean                          1062.46
evaluation/Returns Std                             58.589
evaluation/Returns Max                           1129.27
evaluation/Returns Min                            975.263
evaluation/Actions Mean                             0.0986271
evaluation/Actions Std                              0.839236
evaluation/Actions Max                              1
evaluation/Actions Min                             -0.999997
evaluation/Num Paths                                5
evaluation/Average Returns                       1062.46
evaluation/env_infos/final/reward_run Mean          1.91697
evaluation/env_infos/final/reward_run Std           1.00323
evaluation/env_infos/final/reward_run Max           3.27457
evaluation/env_infos/final/reward_run Min           0.824321
evaluation/env_infos/initial/reward_run Mean        0.171261
evaluation/env_infos/initial/reward_run Std         0.162249
evaluation/env_infos/initial/reward_run Max         0.330012
evaluation/env_infos/initial/reward_run Min        -0.028772
evaluation/env_infos/reward_run Mean                1.49089
evaluation/env_infos/reward_run Std                 1.19047
evaluation/env_infos/reward_run Max                 4.80358
evaluation/env_infos/reward_run Min                -2.88812
evaluation/env_infos/final/reward_ctrl Mean        -0.372173
evaluation/env_infos/final/reward_ctrl Std          0.0750638
evaluation/env_infos/final/reward_ctrl Max         -0.244327
evaluation/env_infos/final/reward_ctrl Min         -0.460719
evaluation/env_infos/initial/reward_ctrl Mean      -0.153569
evaluation/env_infos/initial/reward_ctrl Std        0.0121253
evaluation/env_infos/initial/reward_ctrl Max       -0.132433
evaluation/env_infos/initial/reward_ctrl Min       -0.167584
evaluation/env_infos/reward_ctrl Mean              -0.428427
evaluation/env_infos/reward_ctrl Std                0.101469
evaluation/env_infos/reward_ctrl Max               -0.0247378
evaluation/env_infos/reward_ctrl Min               -0.596979
time/data storing (s)                               0.00707397
time/evaluation sampling (s)                        3.18901
time/exploration sampling (s)                       0.838212
time/logging (s)                                    0.0626231
time/saving (s)                                     0.0348342
time/training (s)                                  44.5663
time/epoch (s)                                     48.698
time/total (s)                                    686.295
Epoch                                              16
----------------------------------------------  --------------
2020-07-08 20:10:23.847013 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 17 finished
----------------------------------------------  --------------
replay_buffer/size                              19000
trainer/QF1 Loss                                    1.74629
trainer/QF2 Loss                                    1.84746
trainer/Policy Loss                               -25.0964
trainer/Q1 Predictions Mean                        30.8823
trainer/Q1 Predictions Std                         11.9486
trainer/Q1 Predictions Max                         61.7863
trainer/Q1 Predictions Min                         17.0013
trainer/Q2 Predictions Mean                        30.9659
trainer/Q2 Predictions Std                         12.015
trainer/Q2 Predictions Max                         62.4472
trainer/Q2 Predictions Min                         17.236
trainer/Q Targets Mean                             30.9974
trainer/Q Targets Std                              11.978
trainer/Q Targets Max                              64.5448
trainer/Q Targets Min                              16.2048
trainer/Log Pis Mean                                6.2132
trainer/Log Pis Std                                 5.44518
trainer/Log Pis Max                                23.3956
trainer/Log Pis Min                                -5.85038
trainer/Policy mu Mean                             -0.0107944
trainer/Policy mu Std                               1.67941
trainer/Policy mu Max                               4.41347
trainer/Policy mu Min                              -4.88261
trainer/Policy log std Mean                        -0.632781
trainer/Policy log std Std                          0.167288
trainer/Policy log std Max                         -0.132729
trainer/Policy log std Min                         -1.28584
trainer/Alpha                                       0.0212358
trainer/Alpha Loss                                  0.821285
exploration/num steps total                     19000
exploration/num paths total                        19
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                            0.992971
exploration/Rewards Std                             1.11059
exploration/Rewards Max                             3.70096
exploration/Rewards Min                            -2.60765
exploration/Returns Mean                          992.971
exploration/Returns Std                             0
exploration/Returns Max                           992.971
exploration/Returns Min                           992.971
exploration/Actions Mean                            0.0850599
exploration/Actions Std                             0.814463
exploration/Actions Max                             0.99995
exploration/Actions Min                            -0.999959
exploration/Num Paths                               1
exploration/Average Returns                       992.971
exploration/env_infos/final/reward_run Mean         1.57404
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          1.57404
exploration/env_infos/final/reward_run Min          1.57404
exploration/env_infos/initial/reward_run Mean       0.528202
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.528202
exploration/env_infos/initial/reward_run Min        0.528202
exploration/env_infos/reward_run Mean               1.39532
exploration/env_infos/reward_run Std                1.09658
exploration/env_infos/reward_run Max                4.07397
exploration/env_infos/reward_run Min               -2.1237
exploration/env_infos/final/reward_ctrl Mean       -0.472256
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.472256
exploration/env_infos/final/reward_ctrl Min        -0.472256
exploration/env_infos/initial/reward_ctrl Mean     -0.135506
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.135506
exploration/env_infos/initial/reward_ctrl Min      -0.135506
exploration/env_infos/reward_ctrl Mean             -0.402351
exploration/env_infos/reward_ctrl Std               0.0880428
exploration/env_infos/reward_ctrl Max              -0.110504
exploration/env_infos/reward_ctrl Min              -0.592391
evaluation/num steps total                      90000
evaluation/num paths total                         90
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             0.87791
evaluation/Rewards Std                              1.10301
evaluation/Rewards Max                              4.22087
evaluation/Rewards Min                             -2.87898
evaluation/Returns Mean                           877.91
evaluation/Returns Std                            138.314
evaluation/Returns Max                           1021.12
evaluation/Returns Min                            691.53
evaluation/Actions Mean                             0.0811309
evaluation/Actions Std                              0.824267
evaluation/Actions Max                              0.999979
evaluation/Actions Min                             -0.99998
evaluation/Num Paths                                5
evaluation/Average Returns                        877.91
evaluation/env_infos/final/reward_run Mean          1.28724
evaluation/env_infos/final/reward_run Std           0.888237
evaluation/env_infos/final/reward_run Max           2.21805
evaluation/env_infos/final/reward_run Min           0.128055
evaluation/env_infos/initial/reward_run Mean        0.579715
evaluation/env_infos/initial/reward_run Std         0.0687904
evaluation/env_infos/initial/reward_run Max         0.681021
evaluation/env_infos/initial/reward_run Min         0.483837
evaluation/env_infos/reward_run Mean                1.28951
evaluation/env_infos/reward_run Std                 1.08025
evaluation/env_infos/reward_run Max                 4.55673
evaluation/env_infos/reward_run Min                -2.4003
evaluation/env_infos/final/reward_ctrl Mean        -0.425692
evaluation/env_infos/final/reward_ctrl Std          0.065604
evaluation/env_infos/final/reward_ctrl Max         -0.313294
evaluation/env_infos/final/reward_ctrl Min         -0.496861
evaluation/env_infos/initial/reward_ctrl Mean      -0.139247
evaluation/env_infos/initial/reward_ctrl Std        0.0227513
evaluation/env_infos/initial/reward_ctrl Max       -0.0978939
evaluation/env_infos/initial/reward_ctrl Min       -0.162301
evaluation/env_infos/reward_ctrl Mean              -0.411599
evaluation/env_infos/reward_ctrl Std                0.0893921
evaluation/env_infos/reward_ctrl Max               -0.0700658
evaluation/env_infos/reward_ctrl Min               -0.593385
time/data storing (s)                               0.00716197
time/evaluation sampling (s)                        3.46207
time/exploration sampling (s)                       1.1333
time/logging (s)                                    0.050656
time/saving (s)                                     0.0200247
time/training (s)                                  41.0463
time/epoch (s)                                     45.7195
time/total (s)                                    732.038
Epoch                                              17
----------------------------------------------  --------------
2020-07-08 20:11:12.559561 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 18 finished
----------------------------------------------  --------------
replay_buffer/size                              20000
trainer/QF1 Loss                                    1.62942
trainer/QF2 Loss                                    1.81692
trainer/Policy Loss                               -26.1108
trainer/Q1 Predictions Mean                        31.5238
trainer/Q1 Predictions Std                         12.9301
trainer/Q1 Predictions Max                         61.6033
trainer/Q1 Predictions Min                         16.6023
trainer/Q2 Predictions Mean                        31.3883
trainer/Q2 Predictions Std                         12.8121
trainer/Q2 Predictions Max                         61.9107
trainer/Q2 Predictions Min                         16.9532
trainer/Q Targets Mean                             31.3112
trainer/Q Targets Std                              12.8883
trainer/Q Targets Max                              63.9637
trainer/Q Targets Min                              16.5815
trainer/Log Pis Mean                                5.84396
trainer/Log Pis Std                                 4.51432
trainer/Log Pis Max                                21.931
trainer/Log Pis Min                                -9.27663
trainer/Policy mu Mean                              0.187305
trainer/Policy mu Std                               1.59136
trainer/Policy mu Max                               4.08323
trainer/Policy mu Min                              -4.9724
trainer/Policy log std Mean                        -0.664446
trainer/Policy log std Std                          0.16715
trainer/Policy log std Max                         -0.159977
trainer/Policy log std Min                         -1.29308
trainer/Alpha                                       0.0237483
trainer/Alpha Loss                                 -0.583615
exploration/num steps total                     20000
exploration/num paths total                        20
exploration/path length Mean                     1000
exploration/path length Std                         0
exploration/path length Max                      1000
exploration/path length Min                      1000
exploration/Rewards Mean                            1.3353
exploration/Rewards Std                             0.671461
exploration/Rewards Max                             3.51115
exploration/Rewards Min                            -1.08857
exploration/Returns Mean                         1335.3
exploration/Returns Std                             0
exploration/Returns Max                          1335.3
exploration/Returns Min                          1335.3
exploration/Actions Mean                            0.0399795
exploration/Actions Std                             0.819439
exploration/Actions Max                             0.999901
exploration/Actions Min                            -0.999997
exploration/Num Paths                               1
exploration/Average Returns                      1335.3
exploration/env_infos/final/reward_run Mean         2.04188
exploration/env_infos/final/reward_run Std          0
exploration/env_infos/final/reward_run Max          2.04188
exploration/env_infos/final/reward_run Min          2.04188
exploration/env_infos/initial/reward_run Mean       0.630914
exploration/env_infos/initial/reward_run Std        0
exploration/env_infos/initial/reward_run Max        0.630914
exploration/env_infos/initial/reward_run Min        0.630914
exploration/env_infos/reward_run Mean               1.73914
exploration/env_infos/reward_run Std                0.653887
exploration/env_infos/reward_run Max                3.788
exploration/env_infos/reward_run Min               -0.596856
exploration/env_infos/final/reward_ctrl Mean       -0.35708
exploration/env_infos/final/reward_ctrl Std         0
exploration/env_infos/final/reward_ctrl Max        -0.35708
exploration/env_infos/final/reward_ctrl Min        -0.35708
exploration/env_infos/initial/reward_ctrl Mean     -0.206639
exploration/env_infos/initial/reward_ctrl Std       0
exploration/env_infos/initial/reward_ctrl Max      -0.206639
exploration/env_infos/initial/reward_ctrl Min      -0.206639
exploration/env_infos/reward_ctrl Mean             -0.403847
exploration/env_infos/reward_ctrl Std               0.0793771
exploration/env_infos/reward_ctrl Max              -0.145417
exploration/env_infos/reward_ctrl Min              -0.592438
evaluation/num steps total                      95000
evaluation/num paths total                         95
evaluation/path length Mean                      1000
evaluation/path length Std                          0
evaluation/path length Max                       1000
evaluation/path length Min                       1000
evaluation/Rewards Mean                             1.37883
evaluation/Rewards Std                              0.717575
evaluation/Rewards Max                              3.834
evaluation/Rewards Min                             -2.50187
evaluation/Returns Mean                          1378.83
evaluation/Returns Std                             34.8461
evaluation/Returns Max                           1422.46
evaluation/Returns Min                           1331.54
evaluation/Actions Mean                             0.060892
evaluation/Actions Std                              0.814408
evaluation/Actions Max                              0.999628
evaluation/Actions Min                             -0.999999
evaluation/Num Paths                                5
evaluation/Average Returns                       1378.83
evaluation/env_infos/final/reward_run Mean          2.13368
evaluation/env_infos/final/reward_run Std           0.583584
evaluation/env_infos/final/reward_run Max           2.819
evaluation/env_infos/final/reward_run Min           1.41336
evaluation/env_infos/initial/reward_run Mean        0.531661
evaluation/env_infos/initial/reward_run Std         0.103596
evaluation/env_infos/initial/reward_run Max         0.632198
evaluation/env_infos/initial/reward_run Min         0.336838
evaluation/env_infos/reward_run Mean                1.77901
evaluation/env_infos/reward_run Std                 0.698478
evaluation/env_infos/reward_run Max                 4.23375
evaluation/env_infos/reward_run Min                -2.17785
evaluation/env_infos/final/reward_ctrl Mean        -0.425481
evaluation/env_infos/final/reward_ctrl Std          0.0715695
evaluation/env_infos/final/reward_ctrl Max         -0.360352
evaluation/env_infos/final/reward_ctrl Min         -0.537863
evaluation/env_infos/initial/reward_ctrl Mean      -0.15477
evaluation/env_infos/initial/reward_ctrl Std        0.0250507
evaluation/env_infos/initial/reward_ctrl Max       -0.133613
evaluation/env_infos/initial/reward_ctrl Min       -0.200436
evaluation/env_infos/reward_ctrl Mean              -0.400181
evaluation/env_infos/reward_ctrl Std                0.0789719
evaluation/env_infos/reward_ctrl Max               -0.122515
evaluation/env_infos/reward_ctrl Min               -0.590309
time/data storing (s)                               0.00986588
time/evaluation sampling (s)                        3.7621
time/exploration sampling (s)                       0.906349
time/logging (s)                                    0.0445027
time/saving (s)                                     0.0265497
time/training (s)                                  43.7752
time/epoch (s)                                     48.5246
time/total (s)                                    780.739
Epoch                                              18
----------------------------------------------  --------------
2020-07-08 20:11:51.537600 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 19 finished
----------------------------------------------  ---------------
replay_buffer/size                               21000
trainer/QF1 Loss                                     2.23046
trainer/QF2 Loss                                     2.24816
trainer/Policy Loss                                -29.627
trainer/Q1 Predictions Mean                         35.1044
trainer/Q1 Predictions Std                          14.0531
trainer/Q1 Predictions Max                          68.0989
trainer/Q1 Predictions Min                          16.1274
trainer/Q2 Predictions Mean                         35.0348
trainer/Q2 Predictions Std                          13.9412
trainer/Q2 Predictions Max                          66.9108
trainer/Q2 Predictions Min                          14.4037
trainer/Q Targets Mean                              35.0048
trainer/Q Targets Std                               13.952
trainer/Q Targets Max                               66.8262
trainer/Q Targets Min                               12.9413
trainer/Log Pis Mean                                 5.83639
trainer/Log Pis Std                                  4.48516
trainer/Log Pis Max                                 19.5088
trainer/Log Pis Min                                 -6.49148
trainer/Policy mu Mean                               0.305737
trainer/Policy mu Std                                1.57484
trainer/Policy mu Max                                4.90138
trainer/Policy mu Min                               -4.71471
trainer/Policy log std Mean                         -0.675948
trainer/Policy log std Std                           0.182394
trainer/Policy log std Max                          -0.1768
trainer/Policy log std Min                          -1.256
trainer/Alpha                                        0.0262961
trainer/Alpha Loss                                  -0.595271
exploration/num steps total                      21000
exploration/num paths total                         21
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.982789
exploration/Rewards Std                              0.653579
exploration/Rewards Max                              3.09298
exploration/Rewards Min                             -1.80441
exploration/Returns Mean                           982.789
exploration/Returns Std                              0
exploration/Returns Max                            982.789
exploration/Returns Min                            982.789
exploration/Actions Mean                             0.0507055
exploration/Actions Std                              0.821073
exploration/Actions Max                              0.999843
exploration/Actions Min                             -0.999982
exploration/Num Paths                                1
exploration/Average Returns                        982.789
exploration/env_infos/final/reward_run Mean          0.867119
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.867119
exploration/env_infos/final/reward_run Min           0.867119
exploration/env_infos/initial/reward_run Mean       -0.306617
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.306617
exploration/env_infos/initial/reward_run Min        -0.306617
exploration/env_infos/reward_run Mean                1.38883
exploration/env_infos/reward_run Std                 0.648165
exploration/env_infos/reward_run Max                 3.47171
exploration/env_infos/reward_run Min                -1.34793
exploration/env_infos/final/reward_ctrl Mean        -0.56519
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.56519
exploration/env_infos/final/reward_ctrl Min         -0.56519
exploration/env_infos/initial/reward_ctrl Mean      -0.195941
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.195941
exploration/env_infos/initial/reward_ctrl Min       -0.195941
exploration/env_infos/reward_ctrl Mean              -0.406039
exploration/env_infos/reward_ctrl Std                0.0773054
exploration/env_infos/reward_ctrl Max               -0.149878
exploration/env_infos/reward_ctrl Min               -0.584837
evaluation/num steps total                      100000
evaluation/num paths total                         100
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.11811
evaluation/Rewards Std                               0.651464
evaluation/Rewards Max                               3.23887
evaluation/Rewards Min                              -2.93368
evaluation/Returns Mean                           1118.11
evaluation/Returns Std                              78.8946
evaluation/Returns Max                            1193.94
evaluation/Returns Min                             965.513
evaluation/Actions Mean                              0.0406685
evaluation/Actions Std                               0.828551
evaluation/Actions Max                               0.999199
evaluation/Actions Min                              -0.99999
evaluation/Num Paths                                 5
evaluation/Average Returns                        1118.11
evaluation/env_infos/final/reward_run Mean           1.4046
evaluation/env_infos/final/reward_run Std            0.374116
evaluation/env_infos/final/reward_run Max            2.1
evaluation/env_infos/final/reward_run Min            1.04742
evaluation/env_infos/initial/reward_run Mean         0.279839
evaluation/env_infos/initial/reward_run Std          0.231793
evaluation/env_infos/initial/reward_run Max          0.56371
evaluation/env_infos/initial/reward_run Min         -0.0508006
evaluation/env_infos/reward_run Mean                 1.531
evaluation/env_infos/reward_run Std                  0.645613
evaluation/env_infos/reward_run Max                  3.65117
evaluation/env_infos/reward_run Min                 -2.62679
evaluation/env_infos/final/reward_ctrl Mean         -0.409849
evaluation/env_infos/final/reward_ctrl Std           0.0713508
evaluation/env_infos/final/reward_ctrl Max          -0.281485
evaluation/env_infos/final/reward_ctrl Min          -0.477743
evaluation/env_infos/initial/reward_ctrl Mean       -0.160305
evaluation/env_infos/initial/reward_ctrl Std         0.0406017
evaluation/env_infos/initial/reward_ctrl Max        -0.111859
evaluation/env_infos/initial/reward_ctrl Min        -0.203495
evaluation/env_infos/reward_ctrl Mean               -0.412891
evaluation/env_infos/reward_ctrl Std                 0.0693238
evaluation/env_infos/reward_ctrl Max                -0.111859
evaluation/env_infos/reward_ctrl Min                -0.583416
time/data storing (s)                                0.00779159
time/evaluation sampling (s)                         3.31307
time/exploration sampling (s)                        0.802484
time/logging (s)                                     0.0546463
time/saving (s)                                      0.0191845
time/training (s)                                   34.7759
time/epoch (s)                                      38.9731
time/total (s)                                     819.724
Epoch                                               19
----------------------------------------------  ---------------
2020-07-08 20:12:34.135059 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 20 finished
----------------------------------------------  ---------------
replay_buffer/size                               22000
trainer/QF1 Loss                                     2.5921
trainer/QF2 Loss                                     3.02054
trainer/Policy Loss                                -27.13
trainer/Q1 Predictions Mean                         32.7986
trainer/Q1 Predictions Std                          14.0796
trainer/Q1 Predictions Max                          65.9445
trainer/Q1 Predictions Min                          16.1844
trainer/Q2 Predictions Mean                         32.8002
trainer/Q2 Predictions Std                          14.1123
trainer/Q2 Predictions Max                          65.6482
trainer/Q2 Predictions Min                          16.0371
trainer/Q Targets Mean                              32.8849
trainer/Q Targets Std                               14.3038
trainer/Q Targets Max                               67.6909
trainer/Q Targets Min                               16.1503
trainer/Log Pis Mean                                 6.16085
trainer/Log Pis Std                                  4.59753
trainer/Log Pis Max                                 24.5465
trainer/Log Pis Min                                 -4.1221
trainer/Policy mu Mean                               0.189895
trainer/Policy mu Std                                1.61284
trainer/Policy mu Max                                3.60164
trainer/Policy mu Min                               -8.20066
trainer/Policy log std Mean                         -0.647822
trainer/Policy log std Std                           0.179054
trainer/Policy log std Max                          -0.125393
trainer/Policy log std Min                          -1.37564
trainer/Alpha                                        0.0285058
trainer/Alpha Loss                                   0.572285
exploration/num steps total                      22000
exploration/num paths total                         22
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.331434
exploration/Rewards Std                              0.979528
exploration/Rewards Max                              3.32244
exploration/Rewards Min                             -1.79477
exploration/Returns Mean                           331.434
exploration/Returns Std                              0
exploration/Returns Max                            331.434
exploration/Returns Min                            331.434
exploration/Actions Mean                             0.00695384
exploration/Actions Std                              0.811877
exploration/Actions Max                              0.999958
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                        331.434
exploration/env_infos/final/reward_run Mean          1.81451
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.81451
exploration/env_infos/final/reward_run Min           1.81451
exploration/env_infos/initial/reward_run Mean       -0.125054
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.125054
exploration/env_infos/initial/reward_run Min        -0.125054
exploration/env_infos/reward_run Mean                0.72695
exploration/env_infos/reward_run Std                 0.986633
exploration/env_infos/reward_run Max                 3.89742
exploration/env_infos/reward_run Min                -1.49145
exploration/env_infos/final/reward_ctrl Mean        -0.239632
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.239632
exploration/env_infos/final/reward_ctrl Min         -0.239632
exploration/env_infos/initial/reward_ctrl Mean      -0.122154
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.122154
exploration/env_infos/initial/reward_ctrl Min       -0.122154
exploration/env_infos/reward_ctrl Mean              -0.395516
exploration/env_infos/reward_ctrl Std                0.0769211
exploration/env_infos/reward_ctrl Max               -0.122154
exploration/env_infos/reward_ctrl Min               -0.584395
evaluation/num steps total                      105000
evaluation/num paths total                         105
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.263842
evaluation/Rewards Std                               1.01564
evaluation/Rewards Max                               3.04928
evaluation/Rewards Min                              -2.56543
evaluation/Returns Mean                            263.842
evaluation/Returns Std                             204.634
evaluation/Returns Max                             640.929
evaluation/Returns Min                              44.1673
evaluation/Actions Mean                              0.00469757
evaluation/Actions Std                               0.816297
evaluation/Actions Max                               0.999498
evaluation/Actions Min                              -0.999992
evaluation/Num Paths                                 5
evaluation/Average Returns                         263.842
evaluation/env_infos/final/reward_run Mean           0.0351911
evaluation/env_infos/final/reward_run Std            0.610987
evaluation/env_infos/final/reward_run Max            1.07355
evaluation/env_infos/final/reward_run Min           -0.520614
evaluation/env_infos/initial/reward_run Mean         0.260275
evaluation/env_infos/initial/reward_run Std          0.0825544
evaluation/env_infos/initial/reward_run Max          0.422424
evaluation/env_infos/initial/reward_run Min          0.200516
evaluation/env_infos/reward_run Mean                 0.663659
evaluation/env_infos/reward_run Std                  1.02512
evaluation/env_infos/reward_run Max                  3.56799
evaluation/env_infos/reward_run Min                 -2.17146
evaluation/env_infos/final/reward_ctrl Mean         -0.341585
evaluation/env_infos/final/reward_ctrl Std           0.0865796
evaluation/env_infos/final/reward_ctrl Max          -0.192021
evaluation/env_infos/final/reward_ctrl Min          -0.461063
evaluation/env_infos/initial/reward_ctrl Mean       -0.10093
evaluation/env_infos/initial/reward_ctrl Std         0.0262639
evaluation/env_infos/initial/reward_ctrl Max        -0.0682009
evaluation/env_infos/initial/reward_ctrl Min        -0.147936
evaluation/env_infos/reward_ctrl Mean               -0.399817
evaluation/env_infos/reward_ctrl Std                 0.0786053
evaluation/env_infos/reward_ctrl Max                -0.0682009
evaluation/env_infos/reward_ctrl Min                -0.584895
time/data storing (s)                                0.00717669
time/evaluation sampling (s)                         3.67996
time/exploration sampling (s)                        0.976212
time/logging (s)                                     0.0469776
time/saving (s)                                      0.0172862
time/training (s)                                   37.8204
time/epoch (s)                                      42.5481
time/total (s)                                     862.311
Epoch                                               20
----------------------------------------------  ---------------
2020-07-08 20:13:16.720980 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 21 finished
----------------------------------------------  ---------------
replay_buffer/size                               23000
trainer/QF1 Loss                                     2.23197
trainer/QF2 Loss                                     2.3476
trainer/Policy Loss                                -29.7779
trainer/Q1 Predictions Mean                         35.1537
trainer/Q1 Predictions Std                          14.4785
trainer/Q1 Predictions Max                          70.9038
trainer/Q1 Predictions Min                          15.1271
trainer/Q2 Predictions Mean                         35.142
trainer/Q2 Predictions Std                          14.3806
trainer/Q2 Predictions Max                          72.1192
trainer/Q2 Predictions Min                          14.7927
trainer/Q Targets Mean                              35.2394
trainer/Q Targets Std                               14.6958
trainer/Q Targets Max                               75.1388
trainer/Q Targets Min                               15.8402
trainer/Log Pis Mean                                 5.8286
trainer/Log Pis Std                                  4.7526
trainer/Log Pis Max                                 18.8719
trainer/Log Pis Min                                 -4.25072
trainer/Policy mu Mean                               0.154691
trainer/Policy mu Std                                1.59451
trainer/Policy mu Max                                4.22987
trainer/Policy mu Min                               -4.28688
trainer/Policy log std Mean                         -0.655116
trainer/Policy log std Std                           0.174892
trainer/Policy log std Max                           0.00990701
trainer/Policy log std Min                          -1.2723
trainer/Alpha                                        0.0296682
trainer/Alpha Loss                                  -0.602923
exploration/num steps total                      23000
exploration/num paths total                         23
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.318351
exploration/Rewards Std                              1.07233
exploration/Rewards Max                              3.95411
exploration/Rewards Min                             -2.06665
exploration/Returns Mean                           318.351
exploration/Returns Std                              0
exploration/Returns Max                            318.351
exploration/Returns Min                            318.351
exploration/Actions Mean                             0.0781727
exploration/Actions Std                              0.793359
exploration/Actions Max                              0.99985
exploration/Actions Min                             -0.99999
exploration/Num Paths                                1
exploration/Average Returns                        318.351
exploration/env_infos/final/reward_run Mean         -1.47861
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max          -1.47861
exploration/env_infos/final/reward_run Min          -1.47861
exploration/env_infos/initial/reward_run Mean        0.549401
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.549401
exploration/env_infos/initial/reward_run Min         0.549401
exploration/env_infos/reward_run Mean                0.699669
exploration/env_infos/reward_run Std                 1.07143
exploration/env_infos/reward_run Max                 4.37189
exploration/env_infos/reward_run Min                -1.80126
exploration/env_infos/final/reward_ctrl Mean        -0.399852
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.399852
exploration/env_infos/final/reward_ctrl Min         -0.399852
exploration/env_infos/initial/reward_ctrl Mean      -0.113589
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.113589
exploration/env_infos/initial/reward_ctrl Min       -0.113589
exploration/env_infos/reward_ctrl Mean              -0.381318
exploration/env_infos/reward_ctrl Std                0.076345
exploration/env_infos/reward_ctrl Max               -0.101911
exploration/env_infos/reward_ctrl Min               -0.568141
evaluation/num steps total                      110000
evaluation/num paths total                         110
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.510701
evaluation/Rewards Std                               1.10949
evaluation/Rewards Max                               3.55001
evaluation/Rewards Min                              -2.64843
evaluation/Returns Mean                            510.701
evaluation/Returns Std                              96.6396
evaluation/Returns Max                             669.156
evaluation/Returns Min                             383.229
evaluation/Actions Mean                              0.0769832
evaluation/Actions Std                               0.794098
evaluation/Actions Max                               0.99994
evaluation/Actions Min                              -0.99999
evaluation/Num Paths                                 5
evaluation/Average Returns                         510.701
evaluation/env_infos/final/reward_run Mean           0.227488
evaluation/env_infos/final/reward_run Std            0.886479
evaluation/env_infos/final/reward_run Max            1.31045
evaluation/env_infos/final/reward_run Min           -1.0529
evaluation/env_infos/initial/reward_run Mean        -0.0348832
evaluation/env_infos/initial/reward_run Std          0.344124
evaluation/env_infos/initial/reward_run Max          0.261381
evaluation/env_infos/initial/reward_run Min         -0.709238
evaluation/env_infos/reward_run Mean                 0.892612
evaluation/env_infos/reward_run Std                  1.10551
evaluation/env_infos/reward_run Max                  3.96141
evaluation/env_infos/reward_run Min                 -2.26548
evaluation/env_infos/final/reward_ctrl Mean         -0.418343
evaluation/env_infos/final/reward_ctrl Std           0.0790945
evaluation/env_infos/final/reward_ctrl Max          -0.339764
evaluation/env_infos/final/reward_ctrl Min          -0.568453
evaluation/env_infos/initial/reward_ctrl Mean       -0.0737571
evaluation/env_infos/initial/reward_ctrl Std         0.0422646
evaluation/env_infos/initial/reward_ctrl Max        -0.0315338
evaluation/env_infos/initial/reward_ctrl Min        -0.153521
evaluation/env_infos/reward_ctrl Mean               -0.381911
evaluation/env_infos/reward_ctrl Std                 0.0754057
evaluation/env_infos/reward_ctrl Max                -0.0315338
evaluation/env_infos/reward_ctrl Min                -0.575687
time/data storing (s)                                0.00749924
time/evaluation sampling (s)                         2.95694
time/exploration sampling (s)                        0.736041
time/logging (s)                                     0.0473714
time/saving (s)                                      0.0188011
time/training (s)                                   38.8034
time/epoch (s)                                      42.5701
time/total (s)                                     904.895
Epoch                                               21
----------------------------------------------  ---------------
2020-07-08 20:14:00.683176 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 22 finished
----------------------------------------------  ---------------
replay_buffer/size                               24000
trainer/QF1 Loss                                     1.96411
trainer/QF2 Loss                                     2.18459
trainer/Policy Loss                                -28.2749
trainer/Q1 Predictions Mean                         33.6215
trainer/Q1 Predictions Std                          15.7199
trainer/Q1 Predictions Max                          74.9096
trainer/Q1 Predictions Min                          16.0754
trainer/Q2 Predictions Mean                         33.7443
trainer/Q2 Predictions Std                          15.5998
trainer/Q2 Predictions Max                          75.0912
trainer/Q2 Predictions Min                          15.3746
trainer/Q Targets Mean                              33.7802
trainer/Q Targets Std                               15.6703
trainer/Q Targets Max                               74.085
trainer/Q Targets Min                               13.883
trainer/Log Pis Mean                                 5.71226
trainer/Log Pis Std                                  5.26138
trainer/Log Pis Max                                 31.4657
trainer/Log Pis Min                                 -7.65592
trainer/Policy mu Mean                               0.199235
trainer/Policy mu Std                                1.56546
trainer/Policy mu Max                                4.54926
trainer/Policy mu Min                               -5.47282
trainer/Policy log std Mean                         -0.700015
trainer/Policy log std Std                           0.187603
trainer/Policy log std Max                          -0.206702
trainer/Policy log std Min                          -1.53368
trainer/Alpha                                        0.027675
trainer/Alpha Loss                                  -1.03214
exploration/num steps total                      24000
exploration/num paths total                         24
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.466068
exploration/Rewards Std                              1.0579
exploration/Rewards Max                              3.73467
exploration/Rewards Min                             -2.06808
exploration/Returns Mean                           466.068
exploration/Returns Std                              0
exploration/Returns Max                            466.068
exploration/Returns Min                            466.068
exploration/Actions Mean                             0.0303612
exploration/Actions Std                              0.779286
exploration/Actions Max                              0.999958
exploration/Actions Min                             -0.999984
exploration/Num Paths                                1
exploration/Average Returns                        466.068
exploration/env_infos/final/reward_run Mean          0.695632
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.695632
exploration/env_infos/final/reward_run Min           0.695632
exploration/env_infos/initial/reward_run Mean       -0.0943157
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0943157
exploration/env_infos/initial/reward_run Min        -0.0943157
exploration/env_infos/reward_run Mean                0.830994
exploration/env_infos/reward_run Std                 1.07134
exploration/env_infos/reward_run Max                 4.23223
exploration/env_infos/reward_run Min                -1.65273
exploration/env_infos/final/reward_ctrl Mean        -0.231095
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.231095
exploration/env_infos/final/reward_ctrl Min         -0.231095
exploration/env_infos/initial/reward_ctrl Mean      -0.0395242
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0395242
exploration/env_infos/initial/reward_ctrl Min       -0.0395242
exploration/env_infos/reward_ctrl Mean              -0.364925
exploration/env_infos/reward_ctrl Std                0.0898359
exploration/env_infos/reward_ctrl Max               -0.0395242
exploration/env_infos/reward_ctrl Min               -0.579607
evaluation/num steps total                      115000
evaluation/num paths total                         115
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.49482
evaluation/Rewards Std                               0.995823
evaluation/Rewards Max                               4.45765
evaluation/Rewards Min                              -1.87534
evaluation/Returns Mean                            494.82
evaluation/Returns Std                             159.745
evaluation/Returns Max                             770.334
evaluation/Returns Min                             339.197
evaluation/Actions Mean                              0.0226084
evaluation/Actions Std                               0.772571
evaluation/Actions Max                               0.999891
evaluation/Actions Min                              -0.999996
evaluation/Num Paths                                 5
evaluation/Average Returns                         494.82
evaluation/env_infos/final/reward_run Mean           0.854285
evaluation/env_infos/final/reward_run Std            0.435959
evaluation/env_infos/final/reward_run Max            1.29437
evaluation/env_infos/final/reward_run Min            0.207533
evaluation/env_infos/initial/reward_run Mean         0.0137954
evaluation/env_infos/initial/reward_run Std          0.251481
evaluation/env_infos/initial/reward_run Max          0.441844
evaluation/env_infos/initial/reward_run Min         -0.335684
evaluation/env_infos/reward_run Mean                 0.853246
evaluation/env_infos/reward_run Std                  1.00694
evaluation/env_infos/reward_run Max                  4.90292
evaluation/env_infos/reward_run Min                 -1.42663
evaluation/env_infos/final/reward_ctrl Mean         -0.296778
evaluation/env_infos/final/reward_ctrl Std           0.0334252
evaluation/env_infos/final/reward_ctrl Max          -0.24617
evaluation/env_infos/final/reward_ctrl Min          -0.32926
evaluation/env_infos/initial/reward_ctrl Mean       -0.0642847
evaluation/env_infos/initial/reward_ctrl Std         0.0175011
evaluation/env_infos/initial/reward_ctrl Max        -0.0390012
evaluation/env_infos/initial/reward_ctrl Min        -0.0919578
evaluation/env_infos/reward_ctrl Mean               -0.358426
evaluation/env_infos/reward_ctrl Std                 0.0881311
evaluation/env_infos/reward_ctrl Max                -0.0390012
evaluation/env_infos/reward_ctrl Min                -0.589071
time/data storing (s)                                0.00760323
time/evaluation sampling (s)                         3.65179
time/exploration sampling (s)                        0.6451
time/logging (s)                                     0.0394718
time/saving (s)                                      0.0161562
time/training (s)                                   39.5759
time/epoch (s)                                      43.936
time/total (s)                                     948.846
Epoch                                               22
----------------------------------------------  ---------------
2020-07-08 20:14:48.812853 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 23 finished
----------------------------------------------  ---------------
replay_buffer/size                               25000
trainer/QF1 Loss                                     2.38017
trainer/QF2 Loss                                     2.96007
trainer/Policy Loss                                -31.2424
trainer/Q1 Predictions Mean                         36.8735
trainer/Q1 Predictions Std                          17.2736
trainer/Q1 Predictions Max                          75.2374
trainer/Q1 Predictions Min                          14.8057
trainer/Q2 Predictions Mean                         36.7154
trainer/Q2 Predictions Std                          17.2702
trainer/Q2 Predictions Max                          75.7001
trainer/Q2 Predictions Min                          14.9722
trainer/Q Targets Mean                              36.5983
trainer/Q Targets Std                               17.3067
trainer/Q Targets Max                               74.3697
trainer/Q Targets Min                               13.5234
trainer/Log Pis Mean                                 6.02215
trainer/Log Pis Std                                  5.01456
trainer/Log Pis Max                                 23.4499
trainer/Log Pis Min                                 -6.47734
trainer/Policy mu Mean                              -0.014981
trainer/Policy mu Std                                1.61063
trainer/Policy mu Max                                4.51618
trainer/Policy mu Min                               -5.84364
trainer/Policy log std Mean                         -0.689771
trainer/Policy log std Std                           0.220761
trainer/Policy log std Max                          -0.0299919
trainer/Policy log std Min                          -1.4502
trainer/Alpha                                        0.0263545
trainer/Alpha Loss                                   0.080529
exploration/num steps total                      25000
exploration/num paths total                         25
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.225637
exploration/Rewards Std                              0.914986
exploration/Rewards Max                              3.67314
exploration/Rewards Min                             -1.98675
exploration/Returns Mean                           225.637
exploration/Returns Std                              0
exploration/Returns Max                            225.637
exploration/Returns Min                            225.637
exploration/Actions Mean                            -0.0371616
exploration/Actions Std                              0.772576
exploration/Actions Max                              0.999992
exploration/Actions Min                             -0.999993
exploration/Num Paths                                1
exploration/Average Returns                        225.637
exploration/env_infos/final/reward_run Mean          0.575258
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.575258
exploration/env_infos/final/reward_run Min           0.575258
exploration/env_infos/initial/reward_run Mean        0.0472531
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0472531
exploration/env_infos/initial/reward_run Min         0.0472531
exploration/env_infos/reward_run Mean                0.58459
exploration/env_infos/reward_run Std                 0.924568
exploration/env_infos/reward_run Max                 4.14633
exploration/env_infos/reward_run Min                -1.47362
exploration/env_infos/final/reward_ctrl Mean        -0.328977
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.328977
exploration/env_infos/final/reward_ctrl Min         -0.328977
exploration/env_infos/initial/reward_ctrl Mean      -0.0610583
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0610583
exploration/env_infos/initial/reward_ctrl Min       -0.0610583
exploration/env_infos/reward_ctrl Mean              -0.358953
exploration/env_infos/reward_ctrl Std                0.0906866
exploration/env_infos/reward_ctrl Max               -0.0610583
exploration/env_infos/reward_ctrl Min               -0.589255
evaluation/num steps total                      120000
evaluation/num paths total                         120
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.305661
evaluation/Rewards Std                               0.931641
evaluation/Rewards Max                               3.57754
evaluation/Rewards Min                              -2.03172
evaluation/Returns Mean                            305.661
evaluation/Returns Std                             161.479
evaluation/Returns Max                             462.533
evaluation/Returns Min                              14.6238
evaluation/Actions Mean                             -0.0734716
evaluation/Actions Std                               0.793806
evaluation/Actions Max                               0.999998
evaluation/Actions Min                              -1
evaluation/Num Paths                                 5
evaluation/Average Returns                         305.661
evaluation/env_infos/final/reward_run Mean           0.399728
evaluation/env_infos/final/reward_run Std            0.667429
evaluation/env_infos/final/reward_run Max            1.43823
evaluation/env_infos/final/reward_run Min           -0.178257
evaluation/env_infos/initial/reward_run Mean        -0.129284
evaluation/env_infos/initial/reward_run Std          0.314243
evaluation/env_infos/initial/reward_run Max          0.305919
evaluation/env_infos/initial/reward_run Min         -0.612704
evaluation/env_infos/reward_run Mean                 0.686977
evaluation/env_infos/reward_run Std                  0.927895
evaluation/env_infos/reward_run Max                  3.91403
evaluation/env_infos/reward_run Min                 -1.70432
evaluation/env_infos/final/reward_ctrl Mean         -0.350363
evaluation/env_infos/final/reward_ctrl Std           0.0967845
evaluation/env_infos/final/reward_ctrl Max          -0.242391
evaluation/env_infos/final/reward_ctrl Min          -0.50899
evaluation/env_infos/initial/reward_ctrl Mean       -0.0818411
evaluation/env_infos/initial/reward_ctrl Std         0.0337799
evaluation/env_infos/initial/reward_ctrl Max        -0.0332422
evaluation/env_infos/initial/reward_ctrl Min        -0.122831
evaluation/env_infos/reward_ctrl Mean               -0.381316
evaluation/env_infos/reward_ctrl Std                 0.100616
evaluation/env_infos/reward_ctrl Max                -0.0274707
evaluation/env_infos/reward_ctrl Min                -0.597018
time/data storing (s)                                0.00678625
time/evaluation sampling (s)                         3.30558
time/exploration sampling (s)                        0.707925
time/logging (s)                                     0.0402824
time/saving (s)                                      0.0159288
time/training (s)                                   43.984
time/epoch (s)                                      48.0605
time/total (s)                                     996.973
Epoch                                               23
----------------------------------------------  ---------------
2020-07-08 20:15:34.212143 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 24 finished
----------------------------------------------  ---------------
replay_buffer/size                               26000
trainer/QF1 Loss                                     2.27285
trainer/QF2 Loss                                     2.3539
trainer/Policy Loss                                -31.2252
trainer/Q1 Predictions Mean                         36.9743
trainer/Q1 Predictions Std                          18.747
trainer/Q1 Predictions Max                          78.7871
trainer/Q1 Predictions Min                          14.6831
trainer/Q2 Predictions Mean                         36.9185
trainer/Q2 Predictions Std                          18.6922
trainer/Q2 Predictions Max                          79.294
trainer/Q2 Predictions Min                          14.4695
trainer/Q Targets Mean                              36.9627
trainer/Q Targets Std                               18.891
trainer/Q Targets Max                               77.8847
trainer/Q Targets Min                               13.854
trainer/Log Pis Mean                                 6.15925
trainer/Log Pis Std                                  5.46005
trainer/Log Pis Max                                 23.6431
trainer/Log Pis Min                                 -4.77384
trainer/Policy mu Mean                              -0.00454591
trainer/Policy mu Std                                1.59798
trainer/Policy mu Max                                4.53959
trainer/Policy mu Min                               -5.81013
trainer/Policy log std Mean                         -0.700959
trainer/Policy log std Std                           0.23851
trainer/Policy log std Max                           0.132248
trainer/Policy log std Min                          -1.82335
trainer/Alpha                                        0.0259119
trainer/Alpha Loss                                   0.581759
exploration/num steps total                      26000
exploration/num paths total                         26
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.335292
exploration/Rewards Std                              1.02976
exploration/Rewards Max                              3.55324
exploration/Rewards Min                             -2.08497
exploration/Returns Mean                           335.292
exploration/Returns Std                              0
exploration/Returns Max                            335.292
exploration/Returns Min                            335.292
exploration/Actions Mean                             0.0401233
exploration/Actions Std                              0.769126
exploration/Actions Max                              0.99998
exploration/Actions Min                             -0.999955
exploration/Num Paths                                1
exploration/Average Returns                        335.292
exploration/env_infos/final/reward_run Mean         -0.51779
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max          -0.51779
exploration/env_infos/final/reward_run Min          -0.51779
exploration/env_infos/initial/reward_run Mean       -0.199016
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.199016
exploration/env_infos/initial/reward_run Min        -0.199016
exploration/env_infos/reward_run Mean                0.69119
exploration/env_infos/reward_run Std                 1.03213
exploration/env_infos/reward_run Max                 3.96766
exploration/env_infos/reward_run Min                -1.66695
exploration/env_infos/final/reward_ctrl Mean        -0.182323
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.182323
exploration/env_infos/final/reward_ctrl Min         -0.182323
exploration/env_infos/initial/reward_ctrl Mean      -0.0600098
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0600098
exploration/env_infos/initial/reward_ctrl Min       -0.0600098
exploration/env_infos/reward_ctrl Mean              -0.355899
exploration/env_infos/reward_ctrl Std                0.0910435
exploration/env_infos/reward_ctrl Max               -0.0600098
exploration/env_infos/reward_ctrl Min               -0.592179
evaluation/num steps total                      125000
evaluation/num paths total                         125
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.640691
evaluation/Rewards Std                               1.16966
evaluation/Rewards Max                               3.811
evaluation/Rewards Min                              -2.91029
evaluation/Returns Mean                            640.691
evaluation/Returns Std                             460.261
evaluation/Returns Max                            1493.88
evaluation/Returns Min                             147.926
evaluation/Actions Mean                              0.0220116
evaluation/Actions Std                               0.777989
evaluation/Actions Max                               0.999997
evaluation/Actions Min                              -0.999995
evaluation/Num Paths                                 5
evaluation/Average Returns                         640.691
evaluation/env_infos/final/reward_run Mean           1.33072
evaluation/env_infos/final/reward_run Std            1.44883
evaluation/env_infos/final/reward_run Max            2.90337
evaluation/env_infos/final/reward_run Min           -0.835768
evaluation/env_infos/initial/reward_run Mean        -0.233229
evaluation/env_infos/initial/reward_run Std          0.220552
evaluation/env_infos/initial/reward_run Max          0.0522596
evaluation/env_infos/initial/reward_run Min         -0.516254
evaluation/env_infos/reward_run Mean                 1.00414
evaluation/env_infos/reward_run Std                  1.17788
evaluation/env_infos/reward_run Max                  4.28742
evaluation/env_infos/reward_run Min                 -2.46248
evaluation/env_infos/final/reward_ctrl Mean         -0.346075
evaluation/env_infos/final/reward_ctrl Std           0.0805989
evaluation/env_infos/final/reward_ctrl Max          -0.263476
evaluation/env_infos/final/reward_ctrl Min          -0.457793
evaluation/env_infos/initial/reward_ctrl Mean       -0.122423
evaluation/env_infos/initial/reward_ctrl Std         0.0696882
evaluation/env_infos/initial/reward_ctrl Max        -0.0741006
evaluation/env_infos/initial/reward_ctrl Min        -0.259817
evaluation/env_infos/reward_ctrl Mean               -0.363451
evaluation/env_infos/reward_ctrl Std                 0.0932562
evaluation/env_infos/reward_ctrl Max                -0.036604
evaluation/env_infos/reward_ctrl Min                -0.589713
time/data storing (s)                                0.00717977
time/evaluation sampling (s)                         2.7516
time/exploration sampling (s)                        0.84219
time/logging (s)                                     0.0435958
time/saving (s)                                      0.0169854
time/training (s)                                   41.6495
time/epoch (s)                                      45.3111
time/total (s)                                    1042.37
Epoch                                               24
----------------------------------------------  ---------------
2020-07-08 20:16:10.362562 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 25 finished
----------------------------------------------  ---------------
replay_buffer/size                               27000
trainer/QF1 Loss                                     2.33526
trainer/QF2 Loss                                     2.7815
trainer/Policy Loss                                -30.3311
trainer/Q1 Predictions Mean                         35.278
trainer/Q1 Predictions Std                          19.8761
trainer/Q1 Predictions Max                          77.884
trainer/Q1 Predictions Min                          13.9603
trainer/Q2 Predictions Mean                         35.2319
trainer/Q2 Predictions Std                          19.915
trainer/Q2 Predictions Max                          78.7271
trainer/Q2 Predictions Min                          13.9815
trainer/Q Targets Mean                              35.2005
trainer/Q Targets Std                               20.0753
trainer/Q Targets Max                               78.7406
trainer/Q Targets Min                               12.395
trainer/Log Pis Mean                                 5.32405
trainer/Log Pis Std                                  4.96062
trainer/Log Pis Max                                 22.7228
trainer/Log Pis Min                                 -7.3594
trainer/Policy mu Mean                               0.0863718
trainer/Policy mu Std                                1.54961
trainer/Policy mu Max                                3.77397
trainer/Policy mu Min                               -4.52959
trainer/Policy log std Mean                         -0.684954
trainer/Policy log std Std                           0.240933
trainer/Policy log std Max                          -0.0498279
trainer/Policy log std Min                          -1.5802
trainer/Alpha                                        0.0253309
trainer/Alpha Loss                                  -2.48449
exploration/num steps total                      27000
exploration/num paths total                         27
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.103455
exploration/Rewards Std                              0.940683
exploration/Rewards Max                              2.67934
exploration/Rewards Min                             -2.64373
exploration/Returns Mean                           103.455
exploration/Returns Std                              0
exploration/Returns Max                            103.455
exploration/Returns Min                            103.455
exploration/Actions Mean                             0.0685523
exploration/Actions Std                              0.776977
exploration/Actions Max                              0.999932
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                        103.455
exploration/env_infos/final/reward_run Mean          2.16845
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.16845
exploration/env_infos/final/reward_run Min           2.16845
exploration/env_infos/initial/reward_run Mean        0.056672
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.056672
exploration/env_infos/initial/reward_run Min         0.056672
exploration/env_infos/reward_run Mean                0.46849
exploration/env_infos/reward_run Std                 0.938646
exploration/env_infos/reward_run Max                 3.1087
exploration/env_infos/reward_run Min                -2.2986
exploration/env_infos/final/reward_ctrl Mean        -0.214721
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.214721
exploration/env_infos/final/reward_ctrl Min         -0.214721
exploration/env_infos/initial/reward_ctrl Mean      -0.019678
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.019678
exploration/env_infos/initial/reward_ctrl Min       -0.019678
exploration/env_infos/reward_ctrl Mean              -0.365036
exploration/env_infos/reward_ctrl Std                0.0873016
exploration/env_infos/reward_ctrl Max               -0.019678
exploration/env_infos/reward_ctrl Min               -0.584978
evaluation/num steps total                      130000
evaluation/num paths total                         130
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.353482
evaluation/Rewards Std                               1.03747
evaluation/Rewards Max                               3.90541
evaluation/Rewards Min                              -2.50418
evaluation/Returns Mean                            353.482
evaluation/Returns Std                             131.61
evaluation/Returns Max                             502.675
evaluation/Returns Min                             176.292
evaluation/Actions Mean                              0.0655241
evaluation/Actions Std                               0.772775
evaluation/Actions Max                               0.999935
evaluation/Actions Min                              -0.999994
evaluation/Num Paths                                 5
evaluation/Average Returns                         353.482
evaluation/env_infos/final/reward_run Mean           0.566882
evaluation/env_infos/final/reward_run Std            0.554242
evaluation/env_infos/final/reward_run Max            1.06357
evaluation/env_infos/final/reward_run Min           -0.296268
evaluation/env_infos/initial/reward_run Mean        -0.0999722
evaluation/env_infos/initial/reward_run Std          0.305842
evaluation/env_infos/initial/reward_run Max          0.390138
evaluation/env_infos/initial/reward_run Min         -0.394835
evaluation/env_infos/reward_run Mean                 0.714366
evaluation/env_infos/reward_run Std                  1.03721
evaluation/env_infos/reward_run Max                  4.3966
evaluation/env_infos/reward_run Min                 -2.00454
evaluation/env_infos/final/reward_ctrl Mean         -0.411291
evaluation/env_infos/final/reward_ctrl Std           0.0932653
evaluation/env_infos/final/reward_ctrl Max          -0.282775
evaluation/env_infos/final/reward_ctrl Min          -0.526887
evaluation/env_infos/initial/reward_ctrl Mean       -0.0448741
evaluation/env_infos/initial/reward_ctrl Std         0.0214441
evaluation/env_infos/initial/reward_ctrl Max        -0.0207879
evaluation/env_infos/initial/reward_ctrl Min        -0.0783342
evaluation/env_infos/reward_ctrl Mean               -0.360884
evaluation/env_infos/reward_ctrl Std                 0.0903522
evaluation/env_infos/reward_ctrl Max                -0.0207879
evaluation/env_infos/reward_ctrl Min                -0.594308
time/data storing (s)                                0.00720515
time/evaluation sampling (s)                         2.59227
time/exploration sampling (s)                        0.649142
time/logging (s)                                     0.0514278
time/saving (s)                                      0.0173562
time/training (s)                                   32.7503
time/epoch (s)                                      36.0678
time/total (s)                                    1078.53
Epoch                                               25
----------------------------------------------  ---------------
2020-07-08 20:16:50.163639 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 26 finished
----------------------------------------------  ---------------
replay_buffer/size                               28000
trainer/QF1 Loss                                     2.44195
trainer/QF2 Loss                                     2.84997
trainer/Policy Loss                                -28.41
trainer/Q1 Predictions Mean                         34.3064
trainer/Q1 Predictions Std                          20.9907
trainer/Q1 Predictions Max                          77.1563
trainer/Q1 Predictions Min                          10.6769
trainer/Q2 Predictions Mean                         34.1853
trainer/Q2 Predictions Std                          20.9776
trainer/Q2 Predictions Max                          77.4324
trainer/Q2 Predictions Min                          11.2175
trainer/Q Targets Mean                              34.4555
trainer/Q Targets Std                               21.1628
trainer/Q Targets Max                               79.8352
trainer/Q Targets Min                               10.4191
trainer/Log Pis Mean                                 6.26682
trainer/Log Pis Std                                  5.66585
trainer/Log Pis Max                                 23.8965
trainer/Log Pis Min                                 -6.52456
trainer/Policy mu Mean                               0.055249
trainer/Policy mu Std                                1.63873
trainer/Policy mu Max                                5.2915
trainer/Policy mu Min                               -4.61536
trainer/Policy log std Mean                         -0.715453
trainer/Policy log std Std                           0.249524
trainer/Policy log std Max                           0.0670003
trainer/Policy log std Min                          -2.14032
trainer/Alpha                                        0.0247082
trainer/Alpha Loss                                   0.987447
exploration/num steps total                      28000
exploration/num paths total                         28
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.490421
exploration/Rewards Std                              1.11052
exploration/Rewards Max                              4.04362
exploration/Rewards Min                             -1.93111
exploration/Returns Mean                           490.421
exploration/Returns Std                              0
exploration/Returns Max                            490.421
exploration/Returns Min                            490.421
exploration/Actions Mean                             0.0222428
exploration/Actions Std                              0.773946
exploration/Actions Max                              0.999895
exploration/Actions Min                             -0.999989
exploration/Num Paths                                1
exploration/Average Returns                        490.421
exploration/env_infos/final/reward_run Mean          1.71288
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.71288
exploration/env_infos/final/reward_run Min           1.71288
exploration/env_infos/initial/reward_run Mean       -0.501793
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.501793
exploration/env_infos/initial/reward_run Min        -0.501793
exploration/env_infos/reward_run Mean                0.850113
exploration/env_infos/reward_run Std                 1.12362
exploration/env_infos/reward_run Max                 4.43508
exploration/env_infos/reward_run Min                -1.518
exploration/env_infos/final/reward_ctrl Mean        -0.359113
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.359113
exploration/env_infos/final/reward_ctrl Min         -0.359113
exploration/env_infos/initial/reward_ctrl Mean      -0.13827
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.13827
exploration/env_infos/initial/reward_ctrl Min       -0.13827
exploration/env_infos/reward_ctrl Mean              -0.359692
exploration/env_infos/reward_ctrl Std                0.0852259
exploration/env_infos/reward_ctrl Max               -0.102495
exploration/env_infos/reward_ctrl Min               -0.567762
evaluation/num steps total                      135000
evaluation/num paths total                         135
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.889487
evaluation/Rewards Std                               1.23498
evaluation/Rewards Max                               4.2061
evaluation/Rewards Min                              -2.44098
evaluation/Returns Mean                            889.487
evaluation/Returns Std                             548.281
evaluation/Returns Max                            1799.96
evaluation/Returns Min                             303.137
evaluation/Actions Mean                              0.0354541
evaluation/Actions Std                               0.78655
evaluation/Actions Max                               0.999925
evaluation/Actions Min                              -0.999994
evaluation/Num Paths                                 5
evaluation/Average Returns                         889.487
evaluation/env_infos/final/reward_run Mean           1.01497
evaluation/env_infos/final/reward_run Std            0.915131
evaluation/env_infos/final/reward_run Max            2.2166
evaluation/env_infos/final/reward_run Min           -0.470769
evaluation/env_infos/initial/reward_run Mean        -0.215244
evaluation/env_infos/initial/reward_run Std          0.376822
evaluation/env_infos/initial/reward_run Max          0.294124
evaluation/env_infos/initial/reward_run Min         -0.656918
evaluation/env_infos/reward_run Mean                 1.26144
evaluation/env_infos/reward_run Std                  1.25692
evaluation/env_infos/reward_run Max                  4.66649
evaluation/env_infos/reward_run Min                 -2.09732
evaluation/env_infos/final/reward_ctrl Mean         -0.347091
evaluation/env_infos/final/reward_ctrl Std           0.0501429
evaluation/env_infos/final/reward_ctrl Max          -0.292871
evaluation/env_infos/final/reward_ctrl Min          -0.414278
evaluation/env_infos/initial/reward_ctrl Mean       -0.100708
evaluation/env_infos/initial/reward_ctrl Std         0.040523
evaluation/env_infos/initial/reward_ctrl Max        -0.0537191
evaluation/env_infos/initial/reward_ctrl Min        -0.165478
evaluation/env_infos/reward_ctrl Mean               -0.371951
evaluation/env_infos/reward_ctrl Std                 0.093311
evaluation/env_infos/reward_ctrl Max                -0.0537191
evaluation/env_infos/reward_ctrl Min                -0.590038
time/data storing (s)                                0.00673842
time/evaluation sampling (s)                         2.54913
time/exploration sampling (s)                        0.616954
time/logging (s)                                     0.0430291
time/saving (s)                                      0.0160328
time/training (s)                                   36.5241
time/epoch (s)                                      39.756
time/total (s)                                    1118.32
Epoch                                               26
----------------------------------------------  ---------------
2020-07-08 20:17:29.181294 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 27 finished
----------------------------------------------  ---------------
replay_buffer/size                               29000
trainer/QF1 Loss                                     2.44226
trainer/QF2 Loss                                     2.80154
trainer/Policy Loss                                -27.6773
trainer/Q1 Predictions Mean                         33.8234
trainer/Q1 Predictions Std                          22.2703
trainer/Q1 Predictions Max                          82.7165
trainer/Q1 Predictions Min                          11.3627
trainer/Q2 Predictions Mean                         33.8392
trainer/Q2 Predictions Std                          22.184
trainer/Q2 Predictions Max                          84.0223
trainer/Q2 Predictions Min                          11.7922
trainer/Q Targets Mean                              33.7678
trainer/Q Targets Std                               22.5214
trainer/Q Targets Max                               83.0353
trainer/Q Targets Min                               11.7283
trainer/Log Pis Mean                                 6.50818
trainer/Log Pis Std                                  5.5405
trainer/Log Pis Max                                 27.0943
trainer/Log Pis Min                                 -4.08451
trainer/Policy mu Mean                               0.243253
trainer/Policy mu Std                                1.59825
trainer/Policy mu Max                                4.90778
trainer/Policy mu Min                               -4.26217
trainer/Policy log std Mean                         -0.734916
trainer/Policy log std Std                           0.277249
trainer/Policy log std Max                           0.260413
trainer/Policy log std Min                          -2.21063
trainer/Alpha                                        0.0243704
trainer/Alpha Loss                                   1.88761
exploration/num steps total                      29000
exploration/num paths total                         29
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.417834
exploration/Rewards Std                              1.04527
exploration/Rewards Max                              3.44623
exploration/Rewards Min                             -2.59873
exploration/Returns Mean                           417.834
exploration/Returns Std                              0
exploration/Returns Max                            417.834
exploration/Returns Min                            417.834
exploration/Actions Mean                             0.0795388
exploration/Actions Std                              0.759576
exploration/Actions Max                              0.999973
exploration/Actions Min                             -0.999984
exploration/Num Paths                                1
exploration/Average Returns                        417.834
exploration/env_infos/final/reward_run Mean          0.227314
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.227314
exploration/env_infos/final/reward_run Min           0.227314
exploration/env_infos/initial/reward_run Mean       -0.179706
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.179706
exploration/env_infos/initial/reward_run Min        -0.179706
exploration/env_infos/reward_run Mean                0.767804
exploration/env_infos/reward_run Std                 1.05702
exploration/env_infos/reward_run Max                 4.02246
exploration/env_infos/reward_run Min                -2.0947
exploration/env_infos/final/reward_ctrl Mean        -0.234345
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.234345
exploration/env_infos/final/reward_ctrl Min         -0.234345
exploration/env_infos/initial/reward_ctrl Mean      -0.0871439
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0871439
exploration/env_infos/initial/reward_ctrl Min       -0.0871439
exploration/env_infos/reward_ctrl Mean              -0.349969
exploration/env_infos/reward_ctrl Std                0.0837671
exploration/env_infos/reward_ctrl Max               -0.0871439
exploration/env_infos/reward_ctrl Min               -0.580439
evaluation/num steps total                      140000
evaluation/num paths total                         140
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.646565
evaluation/Rewards Std                               1.15686
evaluation/Rewards Max                               4.10951
evaluation/Rewards Min                              -2.50307
evaluation/Returns Mean                            646.565
evaluation/Returns Std                             396.786
evaluation/Returns Max                            1168.05
evaluation/Returns Min                             289.984
evaluation/Actions Mean                              0.0695073
evaluation/Actions Std                               0.776364
evaluation/Actions Max                               0.999941
evaluation/Actions Min                              -0.999975
evaluation/Num Paths                                 5
evaluation/Average Returns                         646.565
evaluation/env_infos/final/reward_run Mean           0.526492
evaluation/env_infos/final/reward_run Std            0.879372
evaluation/env_infos/final/reward_run Max            2.13306
evaluation/env_infos/final/reward_run Min           -0.49327
evaluation/env_infos/initial/reward_run Mean         0.137869
evaluation/env_infos/initial/reward_run Std          0.171255
evaluation/env_infos/initial/reward_run Max          0.390987
evaluation/env_infos/initial/reward_run Min         -0.114458
evaluation/env_infos/reward_run Mean                 1.01111
evaluation/env_infos/reward_run Std                  1.17655
evaluation/env_infos/reward_run Max                  4.57006
evaluation/env_infos/reward_run Min                 -1.95968
evaluation/env_infos/final/reward_ctrl Mean         -0.305712
evaluation/env_infos/final/reward_ctrl Std           0.052989
evaluation/env_infos/final/reward_ctrl Max          -0.247592
evaluation/env_infos/final/reward_ctrl Min          -0.402556
evaluation/env_infos/initial/reward_ctrl Mean       -0.0339148
evaluation/env_infos/initial/reward_ctrl Std         0.00809205
evaluation/env_infos/initial/reward_ctrl Max        -0.025811
evaluation/env_infos/initial/reward_ctrl Min        -0.0443226
evaluation/env_infos/reward_ctrl Mean               -0.364544
evaluation/env_infos/reward_ctrl Std                 0.0887409
evaluation/env_infos/reward_ctrl Max                -0.025811
evaluation/env_infos/reward_ctrl Min                -0.589827
time/data storing (s)                                0.00754874
time/evaluation sampling (s)                         2.61555
time/exploration sampling (s)                        0.677062
time/logging (s)                                     0.0419171
time/saving (s)                                      0.0167659
time/training (s)                                   35.6375
time/epoch (s)                                      38.9963
time/total (s)                                    1157.33
Epoch                                               27
----------------------------------------------  ---------------
2020-07-08 20:18:12.361344 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 28 finished
----------------------------------------------  ---------------
replay_buffer/size                               30000
trainer/QF1 Loss                                     2.37147
trainer/QF2 Loss                                     2.61406
trainer/Policy Loss                                -28.8327
trainer/Q1 Predictions Mean                         34.5603
trainer/Q1 Predictions Std                          24.9934
trainer/Q1 Predictions Max                          88.7767
trainer/Q1 Predictions Min                           9.55771
trainer/Q2 Predictions Mean                         34.4153
trainer/Q2 Predictions Std                          24.9886
trainer/Q2 Predictions Max                          89.4512
trainer/Q2 Predictions Min                           8.47288
trainer/Q Targets Mean                              34.5664
trainer/Q Targets Std                               25.0312
trainer/Q Targets Max                               88.1899
trainer/Q Targets Min                                7.54202
trainer/Log Pis Mean                                 6.10846
trainer/Log Pis Std                                  5.18242
trainer/Log Pis Max                                 22.3533
trainer/Log Pis Min                                 -5.8644
trainer/Policy mu Mean                               0.0719062
trainer/Policy mu Std                                1.58896
trainer/Policy mu Max                                4.46034
trainer/Policy mu Min                               -6.3391
trainer/Policy log std Mean                         -0.724998
trainer/Policy log std Std                           0.275023
trainer/Policy log std Max                           0.00427933
trainer/Policy log std Min                          -1.65779
trainer/Alpha                                        0.0241202
trainer/Alpha Loss                                   0.404001
exploration/num steps total                      30000
exploration/num paths total                         30
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.694026
exploration/Rewards Std                              1.20911
exploration/Rewards Max                              4.08751
exploration/Rewards Min                             -2.00155
exploration/Returns Mean                           694.026
exploration/Returns Std                              0
exploration/Returns Max                            694.026
exploration/Returns Min                            694.026
exploration/Actions Mean                             0.0284669
exploration/Actions Std                              0.78363
exploration/Actions Max                              0.999996
exploration/Actions Min                             -0.999968
exploration/Num Paths                                1
exploration/Average Returns                        694.026
exploration/env_infos/final/reward_run Mean         -0.712672
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max          -0.712672
exploration/env_infos/final/reward_run Min          -0.712672
exploration/env_infos/initial/reward_run Mean       -0.124773
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.124773
exploration/env_infos/initial/reward_run Min        -0.124773
exploration/env_infos/reward_run Mean                1.06296
exploration/env_infos/reward_run Std                 1.21525
exploration/env_infos/reward_run Max                 4.45834
exploration/env_infos/reward_run Min                -1.63204
exploration/env_infos/final/reward_ctrl Mean        -0.497107
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.497107
exploration/env_infos/final/reward_ctrl Min         -0.497107
exploration/env_infos/initial/reward_ctrl Mean      -0.0637243
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0637243
exploration/env_infos/initial/reward_ctrl Min       -0.0637243
exploration/env_infos/reward_ctrl Mean              -0.368932
exploration/env_infos/reward_ctrl Std                0.0852128
exploration/env_infos/reward_ctrl Max               -0.0637243
exploration/env_infos/reward_ctrl Min               -0.586348
evaluation/num steps total                      145000
evaluation/num paths total                         145
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.749025
evaluation/Rewards Std                               1.19319
evaluation/Rewards Max                               4.41006
evaluation/Rewards Min                              -2.01428
evaluation/Returns Mean                            749.025
evaluation/Returns Std                             539.453
evaluation/Returns Max                            1798.29
evaluation/Returns Min                             275.705
evaluation/Actions Mean                              0.0425095
evaluation/Actions Std                               0.785777
evaluation/Actions Max                               0.999991
evaluation/Actions Min                              -0.999966
evaluation/Num Paths                                 5
evaluation/Average Returns                         749.025
evaluation/env_infos/final/reward_run Mean           0.695739
evaluation/env_infos/final/reward_run Std            1.0423
evaluation/env_infos/final/reward_run Max            2.74022
evaluation/env_infos/final/reward_run Min           -0.0582784
evaluation/env_infos/initial/reward_run Mean         0.0345907
evaluation/env_infos/initial/reward_run Std          0.139216
evaluation/env_infos/initial/reward_run Max          0.174108
evaluation/env_infos/initial/reward_run Min         -0.229444
evaluation/env_infos/reward_run Mean                 1.12058
evaluation/env_infos/reward_run Std                  1.2006
evaluation/env_infos/reward_run Max                  4.83624
evaluation/env_infos/reward_run Min                 -1.56134
evaluation/env_infos/final/reward_ctrl Mean         -0.361663
evaluation/env_infos/final/reward_ctrl Std           0.0653428
evaluation/env_infos/final/reward_ctrl Max          -0.249939
evaluation/env_infos/final/reward_ctrl Min          -0.418217
evaluation/env_infos/initial/reward_ctrl Mean       -0.0400658
evaluation/env_infos/initial/reward_ctrl Std         0.0252439
evaluation/env_infos/initial/reward_ctrl Max        -0.00978912
evaluation/env_infos/initial/reward_ctrl Min        -0.072961
evaluation/env_infos/reward_ctrl Mean               -0.371551
evaluation/env_infos/reward_ctrl Std                 0.0873601
evaluation/env_infos/reward_ctrl Max                -0.00978912
evaluation/env_infos/reward_ctrl Min                -0.593282
time/data storing (s)                                0.00960144
time/evaluation sampling (s)                         2.80657
time/exploration sampling (s)                        1.37261
time/logging (s)                                     0.0433656
time/saving (s)                                      0.0162077
time/training (s)                                   38.9171
time/epoch (s)                                      43.1655
time/total (s)                                    1200.51
Epoch                                               28
----------------------------------------------  ---------------
2020-07-08 20:19:02.915086 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 29 finished
----------------------------------------------  --------------
replay_buffer/size                               31000
trainer/QF1 Loss                                     2.58244
trainer/QF2 Loss                                     2.76139
trainer/Policy Loss                                -28.0709
trainer/Q1 Predictions Mean                         33.374
trainer/Q1 Predictions Std                          25.9526
trainer/Q1 Predictions Max                          91.5524
trainer/Q1 Predictions Min                           9.19033
trainer/Q2 Predictions Mean                         33.4496
trainer/Q2 Predictions Std                          25.9613
trainer/Q2 Predictions Max                          92.5356
trainer/Q2 Predictions Min                           9.21967
trainer/Q Targets Mean                              33.4147
trainer/Q Targets Std                               25.7415
trainer/Q Targets Max                               88.7378
trainer/Q Targets Min                                8.53543
trainer/Log Pis Mean                                 5.62929
trainer/Log Pis Std                                  5.78205
trainer/Log Pis Max                                 26.2145
trainer/Log Pis Min                                 -5.5267
trainer/Policy mu Mean                               0.1054
trainer/Policy mu Std                                1.59655
trainer/Policy mu Max                                4.72525
trainer/Policy mu Min                               -6.82075
trainer/Policy log std Mean                         -0.69727
trainer/Policy log std Std                           0.298604
trainer/Policy log std Max                           0.078404
trainer/Policy log std Min                          -1.90649
trainer/Alpha                                        0.0247193
trainer/Alpha Loss                                  -1.37172
exploration/num steps total                      31000
exploration/num paths total                         31
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.338296
exploration/Rewards Std                              0.923633
exploration/Rewards Max                              3.0525
exploration/Rewards Min                             -2.16464
exploration/Returns Mean                           338.296
exploration/Returns Std                              0
exploration/Returns Max                            338.296
exploration/Returns Min                            338.296
exploration/Actions Mean                             0.0672231
exploration/Actions Std                              0.757618
exploration/Actions Max                              0.999943
exploration/Actions Min                             -0.999984
exploration/Num Paths                                1
exploration/Average Returns                        338.296
exploration/env_infos/final/reward_run Mean          0.815298
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.815298
exploration/env_infos/final/reward_run Min           0.815298
exploration/env_infos/initial/reward_run Mean       -0.050682
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.050682
exploration/env_infos/initial/reward_run Min        -0.050682
exploration/env_infos/reward_run Mean                0.685398
exploration/env_infos/reward_run Std                 0.925343
exploration/env_infos/reward_run Max                 3.46852
exploration/env_infos/reward_run Min                -1.77405
exploration/env_infos/final/reward_ctrl Mean        -0.281402
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.281402
exploration/env_infos/final/reward_ctrl Min         -0.281402
exploration/env_infos/initial/reward_ctrl Mean      -0.0573963
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0573963
exploration/env_infos/initial/reward_ctrl Min       -0.0573963
exploration/env_infos/reward_ctrl Mean              -0.347102
exploration/env_infos/reward_ctrl Std                0.0899222
exploration/env_infos/reward_ctrl Max               -0.0573963
exploration/env_infos/reward_ctrl Min               -0.58845
evaluation/num steps total                      150000
evaluation/num paths total                         150
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.18163
evaluation/Rewards Std                               1.12265
evaluation/Rewards Max                               4.49674
evaluation/Rewards Min                              -1.89555
evaluation/Returns Mean                           1181.63
evaluation/Returns Std                             561.094
evaluation/Returns Max                            1908.61
evaluation/Returns Min                             440.778
evaluation/Actions Mean                              0.0884066
evaluation/Actions Std                               0.776061
evaluation/Actions Max                               0.999909
evaluation/Actions Min                              -0.999999
evaluation/Num Paths                                 5
evaluation/Average Returns                        1181.63
evaluation/env_infos/final/reward_run Mean           0.533245
evaluation/env_infos/final/reward_run Std            0.75026
evaluation/env_infos/final/reward_run Max            1.63409
evaluation/env_infos/final/reward_run Min           -0.634642
evaluation/env_infos/initial/reward_run Mean        -0.204556
evaluation/env_infos/initial/reward_run Std          0.105452
evaluation/env_infos/initial/reward_run Max         -0.0483118
evaluation/env_infos/initial/reward_run Min         -0.342875
evaluation/env_infos/reward_run Mean                 1.54769
evaluation/env_infos/reward_run Std                  1.1327
evaluation/env_infos/reward_run Max                  4.99378
evaluation/env_infos/reward_run Min                 -1.42557
evaluation/env_infos/final/reward_ctrl Mean         -0.344484
evaluation/env_infos/final/reward_ctrl Std           0.0991313
evaluation/env_infos/final/reward_ctrl Max          -0.200315
evaluation/env_infos/final/reward_ctrl Min          -0.511252
evaluation/env_infos/initial/reward_ctrl Mean       -0.0705791
evaluation/env_infos/initial/reward_ctrl Std         0.0420641
evaluation/env_infos/initial/reward_ctrl Max        -0.0278123
evaluation/env_infos/initial/reward_ctrl Min        -0.131825
evaluation/env_infos/reward_ctrl Mean               -0.366052
evaluation/env_infos/reward_ctrl Std                 0.0958577
evaluation/env_infos/reward_ctrl Max                -0.0278123
evaluation/env_infos/reward_ctrl Min                -0.59635
time/data storing (s)                                0.0070172
time/evaluation sampling (s)                         2.68298
time/exploration sampling (s)                        0.671606
time/logging (s)                                     0.0442976
time/saving (s)                                      0.0170825
time/training (s)                                   47.1161
time/epoch (s)                                      50.5391
time/total (s)                                    1251.06
Epoch                                               29
----------------------------------------------  --------------
2020-07-08 20:19:42.874976 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 30 finished
----------------------------------------------  --------------
replay_buffer/size                               32000
trainer/QF1 Loss                                     2.46436
trainer/QF2 Loss                                     2.83282
trainer/Policy Loss                                -31.2104
trainer/Q1 Predictions Mean                         36.6164
trainer/Q1 Predictions Std                          27.6453
trainer/Q1 Predictions Max                          93.768
trainer/Q1 Predictions Min                           7.78542
trainer/Q2 Predictions Mean                         36.9215
trainer/Q2 Predictions Std                          27.735
trainer/Q2 Predictions Max                          93.3686
trainer/Q2 Predictions Min                           8.34581
trainer/Q Targets Mean                              36.5159
trainer/Q Targets Std                               27.8367
trainer/Q Targets Max                               94.025
trainer/Q Targets Min                                7.55383
trainer/Log Pis Mean                                 5.94073
trainer/Log Pis Std                                  5.54222
trainer/Log Pis Max                                 22.6748
trainer/Log Pis Min                                 -5.76128
trainer/Policy mu Mean                               0.0396943
trainer/Policy mu Std                                1.56252
trainer/Policy mu Max                                4.7566
trainer/Policy mu Min                               -5.2268
trainer/Policy log std Mean                         -0.748209
trainer/Policy log std Std                           0.316501
trainer/Policy log std Max                          -0.0342978
trainer/Policy log std Min                          -2.0233
trainer/Alpha                                        0.0248142
trainer/Alpha Loss                                  -0.219095
exploration/num steps total                      32000
exploration/num paths total                         32
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             1.15187
exploration/Rewards Std                              1.06954
exploration/Rewards Max                              3.65432
exploration/Rewards Min                             -1.73337
exploration/Returns Mean                          1151.87
exploration/Returns Std                              0
exploration/Returns Max                           1151.87
exploration/Returns Min                           1151.87
exploration/Actions Mean                             0.0194839
exploration/Actions Std                              0.798759
exploration/Actions Max                              0.999974
exploration/Actions Min                             -0.999992
exploration/Num Paths                                1
exploration/Average Returns                       1151.87
exploration/env_infos/final/reward_run Mean          0.964279
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.964279
exploration/env_infos/final/reward_run Min           0.964279
exploration/env_infos/initial/reward_run Mean       -0.220067
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.220067
exploration/env_infos/initial/reward_run Min        -0.220067
exploration/env_infos/reward_run Mean                1.53491
exploration/env_infos/reward_run Std                 1.06988
exploration/env_infos/reward_run Max                 3.93578
exploration/env_infos/reward_run Min                -1.29715
exploration/env_infos/final/reward_ctrl Mean        -0.447157
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.447157
exploration/env_infos/final/reward_ctrl Min         -0.447157
exploration/env_infos/initial/reward_ctrl Mean      -0.0263784
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0263784
exploration/env_infos/initial/reward_ctrl Min       -0.0263784
exploration/env_infos/reward_ctrl Mean              -0.383038
exploration/env_infos/reward_ctrl Std                0.0889267
exploration/env_infos/reward_ctrl Max               -0.0263784
exploration/env_infos/reward_ctrl Min               -0.588769
evaluation/num steps total                      155000
evaluation/num paths total                         155
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.931033
evaluation/Rewards Std                               1.21499
evaluation/Rewards Max                               4.05242
evaluation/Rewards Min                              -1.99027
evaluation/Returns Mean                            931.033
evaluation/Returns Std                             526.854
evaluation/Returns Max                            1854.82
evaluation/Returns Min                             406.729
evaluation/Actions Mean                              0.0222017
evaluation/Actions Std                               0.79174
evaluation/Actions Max                               0.999857
evaluation/Actions Min                              -0.999985
evaluation/Num Paths                                 5
evaluation/Average Returns                         931.033
evaluation/env_infos/final/reward_run Mean           0.793477
evaluation/env_infos/final/reward_run Std            0.885671
evaluation/env_infos/final/reward_run Max            2.35387
evaluation/env_infos/final/reward_run Min           -0.146602
evaluation/env_infos/initial/reward_run Mean        -0.229174
evaluation/env_infos/initial/reward_run Std          0.194504
evaluation/env_infos/initial/reward_run Max         -0.0140767
evaluation/env_infos/initial/reward_run Min         -0.513885
evaluation/env_infos/reward_run Mean                 1.30744
evaluation/env_infos/reward_run Std                  1.22347
evaluation/env_infos/reward_run Max                  4.56639
evaluation/env_infos/reward_run Min                 -1.63419
evaluation/env_infos/final/reward_ctrl Mean         -0.402788
evaluation/env_infos/final/reward_ctrl Std           0.0952302
evaluation/env_infos/final/reward_ctrl Max          -0.223863
evaluation/env_infos/final/reward_ctrl Min          -0.504614
evaluation/env_infos/initial/reward_ctrl Mean       -0.0770863
evaluation/env_infos/initial/reward_ctrl Std         0.0278516
evaluation/env_infos/initial/reward_ctrl Max        -0.030142
evaluation/env_infos/initial/reward_ctrl Min        -0.115442
evaluation/env_infos/reward_ctrl Mean               -0.376407
evaluation/env_infos/reward_ctrl Std                 0.0870613
evaluation/env_infos/reward_ctrl Max                -0.030142
evaluation/env_infos/reward_ctrl Min                -0.592603
time/data storing (s)                                0.0078402
time/evaluation sampling (s)                         3.22891
time/exploration sampling (s)                        0.61992
time/logging (s)                                     0.0395929
time/saving (s)                                      0.0160186
time/training (s)                                   35.8986
time/epoch (s)                                      39.8108
time/total (s)                                    1291.01
Epoch                                               30
----------------------------------------------  --------------
2020-07-08 20:20:20.888952 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 31 finished
----------------------------------------------  ---------------
replay_buffer/size                               33000
trainer/QF1 Loss                                     2.69384
trainer/QF2 Loss                                     2.75451
trainer/Policy Loss                                -30.5089
trainer/Q1 Predictions Mean                         35.5408
trainer/Q1 Predictions Std                          30.4043
trainer/Q1 Predictions Max                         100.228
trainer/Q1 Predictions Min                           8.14259
trainer/Q2 Predictions Mean                         35.7845
trainer/Q2 Predictions Std                          30.447
trainer/Q2 Predictions Max                          98.5448
trainer/Q2 Predictions Min                           7.51906
trainer/Q Targets Mean                              35.5126
trainer/Q Targets Std                               30.2549
trainer/Q Targets Max                               98.6151
trainer/Q Targets Min                                6.67958
trainer/Log Pis Mean                                 5.4756
trainer/Log Pis Std                                  4.97553
trainer/Log Pis Max                                 22.5546
trainer/Log Pis Min                                 -5.11172
trainer/Policy mu Mean                               0.0736995
trainer/Policy mu Std                                1.55268
trainer/Policy mu Max                                4.88885
trainer/Policy mu Min                               -4.75981
trainer/Policy log std Mean                         -0.724269
trainer/Policy log std Std                           0.315472
trainer/Policy log std Max                          -0.00662663
trainer/Policy log std Min                          -2.18003
trainer/Alpha                                        0.0248446
trainer/Alpha Loss                                  -1.93773
exploration/num steps total                      33000
exploration/num paths total                         33
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             1.20946
exploration/Rewards Std                              1.20753
exploration/Rewards Max                              4.05894
exploration/Rewards Min                             -2.05375
exploration/Returns Mean                          1209.46
exploration/Returns Std                              0
exploration/Returns Max                           1209.46
exploration/Returns Min                           1209.46
exploration/Actions Mean                             0.0480411
exploration/Actions Std                              0.795872
exploration/Actions Max                              0.999958
exploration/Actions Min                             -0.99999
exploration/Num Paths                                1
exploration/Average Returns                       1209.46
exploration/env_infos/final/reward_run Mean          0.0923936
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.0923936
exploration/env_infos/final/reward_run Min           0.0923936
exploration/env_infos/initial/reward_run Mean       -0.428672
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.428672
exploration/env_infos/initial/reward_run Min        -0.428672
exploration/env_infos/reward_run Mean                1.59089
exploration/env_infos/reward_run Std                 1.21088
exploration/env_infos/reward_run Max                 4.44777
exploration/env_infos/reward_run Min                -1.57403
exploration/env_infos/final/reward_ctrl Mean        -0.431306
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.431306
exploration/env_infos/final/reward_ctrl Min         -0.431306
exploration/env_infos/initial/reward_ctrl Mean      -0.126112
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.126112
exploration/env_infos/initial/reward_ctrl Min       -0.126112
exploration/env_infos/reward_ctrl Mean              -0.381432
exploration/env_infos/reward_ctrl Std                0.0899179
exploration/env_infos/reward_ctrl Max               -0.126112
exploration/env_infos/reward_ctrl Min               -0.593111
evaluation/num steps total                      160000
evaluation/num paths total                         160
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.81574
evaluation/Rewards Std                               0.980459
evaluation/Rewards Max                               4.68797
evaluation/Rewards Min                              -1.98883
evaluation/Returns Mean                           1815.74
evaluation/Returns Std                             225.78
evaluation/Returns Max                            2051.34
evaluation/Returns Min                            1387.38
evaluation/Actions Mean                              0.0776995
evaluation/Actions Std                               0.796046
evaluation/Actions Max                               0.999936
evaluation/Actions Min                              -0.999978
evaluation/Num Paths                                 5
evaluation/Average Returns                        1815.74
evaluation/env_infos/final/reward_run Mean           2.85317
evaluation/env_infos/final/reward_run Std            0.722667
evaluation/env_infos/final/reward_run Max            3.9564
evaluation/env_infos/final/reward_run Min            1.99342
evaluation/env_infos/initial/reward_run Mean        -0.03013
evaluation/env_infos/initial/reward_run Std          0.269473
evaluation/env_infos/initial/reward_run Max          0.474393
evaluation/env_infos/initial/reward_run Min         -0.319812
evaluation/env_infos/reward_run Mean                 2.19957
evaluation/env_infos/reward_run Std                  0.978389
evaluation/env_infos/reward_run Max                  5.01208
evaluation/env_infos/reward_run Min                 -1.61388
evaluation/env_infos/final/reward_ctrl Mean         -0.344046
evaluation/env_infos/final/reward_ctrl Std           0.0950324
evaluation/env_infos/final/reward_ctrl Max          -0.265648
evaluation/env_infos/final/reward_ctrl Min          -0.516422
evaluation/env_infos/initial/reward_ctrl Mean       -0.0903309
evaluation/env_infos/initial/reward_ctrl Std         0.0244378
evaluation/env_infos/initial/reward_ctrl Max        -0.0547739
evaluation/env_infos/initial/reward_ctrl Min        -0.117885
evaluation/env_infos/reward_ctrl Mean               -0.383835
evaluation/env_infos/reward_ctrl Std                 0.0956946
evaluation/env_infos/reward_ctrl Max                -0.0547739
evaluation/env_infos/reward_ctrl Min                -0.593556
time/data storing (s)                                0.00671436
time/evaluation sampling (s)                         2.42027
time/exploration sampling (s)                        0.611084
time/logging (s)                                     0.0401504
time/saving (s)                                      0.0171824
time/training (s)                                   34.8385
time/epoch (s)                                      37.9339
time/total (s)                                    1329.03
Epoch                                               31
----------------------------------------------  ---------------
2020-07-08 20:20:57.158297 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 32 finished
----------------------------------------------  ---------------
replay_buffer/size                               34000
trainer/QF1 Loss                                     3.44124
trainer/QF2 Loss                                     3.80963
trainer/Policy Loss                                -35.2242
trainer/Q1 Predictions Mean                         40.5559
trainer/Q1 Predictions Std                          32.3919
trainer/Q1 Predictions Max                         108.169
trainer/Q1 Predictions Min                           6.51939
trainer/Q2 Predictions Mean                         40.9758
trainer/Q2 Predictions Std                          32.5566
trainer/Q2 Predictions Max                         107.463
trainer/Q2 Predictions Min                           7.45654
trainer/Q Targets Mean                              40.7685
trainer/Q Targets Std                               32.3169
trainer/Q Targets Max                              106.149
trainer/Q Targets Min                                5.86813
trainer/Log Pis Mean                                 5.848
trainer/Log Pis Std                                  5.31202
trainer/Log Pis Max                                 26.2367
trainer/Log Pis Min                                 -7.3309
trainer/Policy mu Mean                               0.182985
trainer/Policy mu Std                                1.61652
trainer/Policy mu Max                                5.22953
trainer/Policy mu Min                               -7.3139
trainer/Policy log std Mean                         -0.69404
trainer/Policy log std Std                           0.320728
trainer/Policy log std Max                           0.123192
trainer/Policy log std Min                          -2.41584
trainer/Alpha                                        0.0259841
trainer/Alpha Loss                                  -0.554856
exploration/num steps total                      34000
exploration/num paths total                         34
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             1.73137
exploration/Rewards Std                              0.823953
exploration/Rewards Max                              4.29407
exploration/Rewards Min                             -1.05643
exploration/Returns Mean                          1731.37
exploration/Returns Std                              0
exploration/Returns Max                           1731.37
exploration/Returns Min                           1731.37
exploration/Actions Mean                             0.0989687
exploration/Actions Std                              0.766406
exploration/Actions Max                              0.999913
exploration/Actions Min                             -0.999997
exploration/Num Paths                                1
exploration/Average Returns                       1731.37
exploration/env_infos/final/reward_run Mean          1.91152
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.91152
exploration/env_infos/final/reward_run Min           1.91152
exploration/env_infos/initial/reward_run Mean        0.205004
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.205004
exploration/env_infos/initial/reward_run Min         0.205004
exploration/env_infos/reward_run Mean                2.08967
exploration/env_infos/reward_run Std                 0.817988
exploration/env_infos/reward_run Max                 4.5955
exploration/env_infos/reward_run Min                -0.624351
exploration/env_infos/final/reward_ctrl Mean        -0.265599
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.265599
exploration/env_infos/final/reward_ctrl Min         -0.265599
exploration/env_infos/initial/reward_ctrl Mean      -0.0567202
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0567202
exploration/env_infos/initial/reward_ctrl Min       -0.0567202
exploration/env_infos/reward_ctrl Mean              -0.358304
exploration/env_infos/reward_ctrl Std                0.108934
exploration/env_infos/reward_ctrl Max               -0.0567202
exploration/env_infos/reward_ctrl Min               -0.594625
evaluation/num steps total                      165000
evaluation/num paths total                         165
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.30636
evaluation/Rewards Std                               1.12906
evaluation/Rewards Max                               4.27588
evaluation/Rewards Min                              -2.35585
evaluation/Returns Mean                           1306.36
evaluation/Returns Std                             427.597
evaluation/Returns Max                            1771.96
evaluation/Returns Min                             645.504
evaluation/Actions Mean                              0.0919427
evaluation/Actions Std                               0.780017
evaluation/Actions Max                               0.999866
evaluation/Actions Min                              -0.999995
evaluation/Num Paths                                 5
evaluation/Average Returns                        1306.36
evaluation/env_infos/final/reward_run Mean           0.733863
evaluation/env_infos/final/reward_run Std            0.877537
evaluation/env_infos/final/reward_run Max            1.77377
evaluation/env_infos/final/reward_run Min           -0.270223
evaluation/env_infos/initial/reward_run Mean        -0.15946
evaluation/env_infos/initial/reward_run Std          0.121129
evaluation/env_infos/initial/reward_run Max         -0.0523288
evaluation/env_infos/initial/reward_run Min         -0.395378
evaluation/env_infos/reward_run Mean                 1.67648
evaluation/env_infos/reward_run Std                  1.11793
evaluation/env_infos/reward_run Max                  4.48803
evaluation/env_infos/reward_run Min                 -1.81409
evaluation/env_infos/final/reward_ctrl Mean         -0.45692
evaluation/env_infos/final/reward_ctrl Std           0.0543743
evaluation/env_infos/final/reward_ctrl Max          -0.390567
evaluation/env_infos/final/reward_ctrl Min          -0.541842
evaluation/env_infos/initial/reward_ctrl Mean       -0.0805452
evaluation/env_infos/initial/reward_ctrl Std         0.03124
evaluation/env_infos/initial/reward_ctrl Max        -0.0367544
evaluation/env_infos/initial/reward_ctrl Min        -0.114677
evaluation/env_infos/reward_ctrl Mean               -0.370127
evaluation/env_infos/reward_ctrl Std                 0.104257
evaluation/env_infos/reward_ctrl Max                -0.0367544
evaluation/env_infos/reward_ctrl Min                -0.5928
time/data storing (s)                                0.00692966
time/evaluation sampling (s)                         3.42971
time/exploration sampling (s)                        0.642084
time/logging (s)                                     0.0424384
time/saving (s)                                      0.0162252
time/training (s)                                   32.1193
time/epoch (s)                                      36.2567
time/total (s)                                    1365.29
Epoch                                               32
----------------------------------------------  ---------------
2020-07-08 20:21:36.905798 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 33 finished
----------------------------------------------  ---------------
replay_buffer/size                               35000
trainer/QF1 Loss                                     2.12167
trainer/QF2 Loss                                     2.46309
trainer/Policy Loss                                -34.8118
trainer/Q1 Predictions Mean                         40.5992
trainer/Q1 Predictions Std                          33.6878
trainer/Q1 Predictions Max                         105.045
trainer/Q1 Predictions Min                           6.25817
trainer/Q2 Predictions Mean                         40.6716
trainer/Q2 Predictions Std                          33.66
trainer/Q2 Predictions Max                         104.707
trainer/Q2 Predictions Min                           6.55987
trainer/Q Targets Mean                              40.4271
trainer/Q Targets Std                               33.6557
trainer/Q Targets Max                              103.374
trainer/Q Targets Min                                5.89123
trainer/Log Pis Mean                                 6.22437
trainer/Log Pis Std                                  4.75822
trainer/Log Pis Max                                 19.3541
trainer/Log Pis Min                                 -5.26967
trainer/Policy mu Mean                               0.115251
trainer/Policy mu Std                                1.55293
trainer/Policy mu Max                                4.13173
trainer/Policy mu Min                               -4.60268
trainer/Policy log std Mean                         -0.742087
trainer/Policy log std Std                           0.334361
trainer/Policy log std Max                           0.0777475
trainer/Policy log std Min                          -2.05609
trainer/Alpha                                        0.0260104
trainer/Alpha Loss                                   0.818742
exploration/num steps total                      35000
exploration/num paths total                         35
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.640107
exploration/Rewards Std                              1.08991
exploration/Rewards Max                              3.30726
exploration/Rewards Min                             -2.05848
exploration/Returns Mean                           640.107
exploration/Returns Std                              0
exploration/Returns Max                            640.107
exploration/Returns Min                            640.107
exploration/Actions Mean                             0.0253428
exploration/Actions Std                              0.791875
exploration/Actions Max                              0.999949
exploration/Actions Min                             -0.999999
exploration/Num Paths                                1
exploration/Average Returns                        640.107
exploration/env_infos/final/reward_run Mean          0.709057
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.709057
exploration/env_infos/final/reward_run Min           0.709057
exploration/env_infos/initial/reward_run Mean       -0.0351147
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0351147
exploration/env_infos/initial/reward_run Min        -0.0351147
exploration/env_infos/reward_run Mean                1.01673
exploration/env_infos/reward_run Std                 1.08663
exploration/env_infos/reward_run Max                 3.63001
exploration/env_infos/reward_run Min                -1.61729
exploration/env_infos/final/reward_ctrl Mean        -0.282235
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.282235
exploration/env_infos/final/reward_ctrl Min         -0.282235
exploration/env_infos/initial/reward_ctrl Mean      -0.0950588
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0950588
exploration/env_infos/initial/reward_ctrl Min       -0.0950588
exploration/env_infos/reward_ctrl Mean              -0.376625
exploration/env_infos/reward_ctrl Std                0.0859415
exploration/env_infos/reward_ctrl Max               -0.0460962
exploration/env_infos/reward_ctrl Min               -0.590789
evaluation/num steps total                      170000
evaluation/num paths total                         170
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.5167
evaluation/Rewards Std                               1.16744
evaluation/Rewards Max                               4.56172
evaluation/Rewards Min                              -2.62901
evaluation/Returns Mean                           1516.7
evaluation/Returns Std                             549.836
evaluation/Returns Max                            2079.57
evaluation/Returns Min                             709.557
evaluation/Actions Mean                              0.0589786
evaluation/Actions Std                               0.786749
evaluation/Actions Max                               0.999949
evaluation/Actions Min                              -0.999988
evaluation/Num Paths                                 5
evaluation/Average Returns                        1516.7
evaluation/env_infos/final/reward_run Mean           1.94626
evaluation/env_infos/final/reward_run Std            0.722395
evaluation/env_infos/final/reward_run Max            3.28942
evaluation/env_infos/final/reward_run Min            1.14345
evaluation/env_infos/initial/reward_run Mean         0.201217
evaluation/env_infos/initial/reward_run Std          0.297087
evaluation/env_infos/initial/reward_run Max          0.629424
evaluation/env_infos/initial/reward_run Min         -0.146743
evaluation/env_infos/reward_run Mean                 1.89017
evaluation/env_infos/reward_run Std                  1.16158
evaluation/env_infos/reward_run Max                  4.88574
evaluation/env_infos/reward_run Min                 -2.23347
evaluation/env_infos/final/reward_ctrl Mean         -0.356971
evaluation/env_infos/final/reward_ctrl Std           0.117676
evaluation/env_infos/final/reward_ctrl Max          -0.166961
evaluation/env_infos/final/reward_ctrl Min          -0.512721
evaluation/env_infos/initial/reward_ctrl Mean       -0.0721391
evaluation/env_infos/initial/reward_ctrl Std         0.0297886
evaluation/env_infos/initial/reward_ctrl Max        -0.0315503
evaluation/env_infos/initial/reward_ctrl Min        -0.100306
evaluation/env_infos/reward_ctrl Mean               -0.373472
evaluation/env_infos/reward_ctrl Std                 0.0969985
evaluation/env_infos/reward_ctrl Max                -0.0315503
evaluation/env_infos/reward_ctrl Min                -0.592617
time/data storing (s)                                0.00720196
time/evaluation sampling (s)                         2.64209
time/exploration sampling (s)                        0.630765
time/logging (s)                                     0.0639636
time/saving (s)                                      0.0209845
time/training (s)                                   36.3848
time/epoch (s)                                      39.7498
time/total (s)                                    1405.06
Epoch                                               33
----------------------------------------------  ---------------
2020-07-08 20:22:34.642499 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 34 finished
----------------------------------------------  ---------------
replay_buffer/size                               36000
trainer/QF1 Loss                                     2.78926
trainer/QF2 Loss                                     3.53553
trainer/Policy Loss                                -34.7908
trainer/Q1 Predictions Mean                         40.5717
trainer/Q1 Predictions Std                          34.3767
trainer/Q1 Predictions Max                         107.927
trainer/Q1 Predictions Min                           4.94837
trainer/Q2 Predictions Mean                         40.4919
trainer/Q2 Predictions Std                          34.2429
trainer/Q2 Predictions Max                         108.031
trainer/Q2 Predictions Min                           3.96301
trainer/Q Targets Mean                              40.6263
trainer/Q Targets Std                               34.2916
trainer/Q Targets Max                              107.921
trainer/Q Targets Min                                2.7676
trainer/Log Pis Mean                                 6.12794
trainer/Log Pis Std                                  6.06223
trainer/Log Pis Max                                 35.1462
trainer/Log Pis Min                                 -5.18439
trainer/Policy mu Mean                               0.0941086
trainer/Policy mu Std                                1.62576
trainer/Policy mu Max                                5.93
trainer/Policy mu Min                               -7.70211
trainer/Policy log std Mean                         -0.709756
trainer/Policy log std Std                           0.327677
trainer/Policy log std Max                           0.404951
trainer/Policy log std Min                          -2.22195
trainer/Alpha                                        0.0262133
trainer/Alpha Loss                                   0.465893
exploration/num steps total                      36000
exploration/num paths total                         36
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.12521
exploration/Rewards Std                              0.712009
exploration/Rewards Max                              4.02345
exploration/Rewards Min                             -1.382
exploration/Returns Mean                          2125.21
exploration/Returns Std                              0
exploration/Returns Max                           2125.21
exploration/Returns Min                           2125.21
exploration/Actions Mean                             0.103157
exploration/Actions Std                              0.78006
exploration/Actions Max                              0.999989
exploration/Actions Min                             -0.999999
exploration/Num Paths                                1
exploration/Average Returns                       2125.21
exploration/env_infos/final/reward_run Mean          2.40605
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.40605
exploration/env_infos/final/reward_run Min           2.40605
exploration/env_infos/initial/reward_run Mean       -0.00927822
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.00927822
exploration/env_infos/initial/reward_run Min        -0.00927822
exploration/env_infos/reward_run Mean                2.49669
exploration/env_infos/reward_run Std                 0.694186
exploration/env_infos/reward_run Max                 4.34536
exploration/env_infos/reward_run Min                -0.86871
exploration/env_infos/final/reward_ctrl Mean        -0.518421
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.518421
exploration/env_infos/final/reward_ctrl Min         -0.518421
exploration/env_infos/initial/reward_ctrl Mean      -0.120352
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.120352
exploration/env_infos/initial/reward_ctrl Min       -0.120352
exploration/env_infos/reward_ctrl Mean              -0.371481
exploration/env_infos/reward_ctrl Std                0.110496
exploration/env_infos/reward_ctrl Max               -0.0571663
exploration/env_infos/reward_ctrl Min               -0.592891
evaluation/num steps total                      175000
evaluation/num paths total                         175
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.72447
evaluation/Rewards Std                               1.10953
evaluation/Rewards Max                               4.6204
evaluation/Rewards Min                              -2.28742
evaluation/Returns Mean                           1724.47
evaluation/Returns Std                             315.698
evaluation/Returns Max                            2071.26
evaluation/Returns Min                            1220.24
evaluation/Actions Mean                              0.0973639
evaluation/Actions Std                               0.782395
evaluation/Actions Max                               0.999948
evaluation/Actions Min                              -0.999996
evaluation/Num Paths                                 5
evaluation/Average Returns                        1724.47
evaluation/env_infos/final/reward_run Mean           1.22947
evaluation/env_infos/final/reward_run Std            0.978084
evaluation/env_infos/final/reward_run Max            2.98363
evaluation/env_infos/final/reward_run Min            0.395187
evaluation/env_infos/initial/reward_run Mean         0.0282976
evaluation/env_infos/initial/reward_run Std          0.114324
evaluation/env_infos/initial/reward_run Max          0.157997
evaluation/env_infos/initial/reward_run Min         -0.153806
evaluation/env_infos/reward_run Mean                 2.09744
evaluation/env_infos/reward_run Std                  1.09891
evaluation/env_infos/reward_run Max                  4.95297
evaluation/env_infos/reward_run Min                 -1.92522
evaluation/env_infos/final/reward_ctrl Mean         -0.387949
evaluation/env_infos/final/reward_ctrl Std           0.0740974
evaluation/env_infos/final/reward_ctrl Max          -0.289696
evaluation/env_infos/final/reward_ctrl Min          -0.516393
evaluation/env_infos/initial/reward_ctrl Mean       -0.05448
evaluation/env_infos/initial/reward_ctrl Std         0.0270024
evaluation/env_infos/initial/reward_ctrl Max        -0.0362924
evaluation/env_infos/initial/reward_ctrl Min        -0.107839
evaluation/env_infos/reward_ctrl Mean               -0.372973
evaluation/env_infos/reward_ctrl Std                 0.107156
evaluation/env_infos/reward_ctrl Max                -0.0362924
evaluation/env_infos/reward_ctrl Min                -0.592994
time/data storing (s)                                0.00757776
time/evaluation sampling (s)                         4.17743
time/exploration sampling (s)                        0.843112
time/logging (s)                                     0.0421002
time/saving (s)                                      0.0217852
time/training (s)                                   52.5987
time/epoch (s)                                      57.6907
time/total (s)                                    1462.77
Epoch                                               34
----------------------------------------------  ---------------
2020-07-08 20:23:25.131359 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 35 finished
----------------------------------------------  --------------
replay_buffer/size                               37000
trainer/QF1 Loss                                     2.42565
trainer/QF2 Loss                                     2.62245
trainer/Policy Loss                                -38.9856
trainer/Q1 Predictions Mean                         44.7562
trainer/Q1 Predictions Std                          36.6473
trainer/Q1 Predictions Max                         109.467
trainer/Q1 Predictions Min                           4.62208
trainer/Q2 Predictions Mean                         44.6333
trainer/Q2 Predictions Std                          36.6514
trainer/Q2 Predictions Max                         108.043
trainer/Q2 Predictions Min                           5.62375
trainer/Q Targets Mean                              44.4852
trainer/Q Targets Std                               36.72
trainer/Q Targets Max                              108.858
trainer/Q Targets Min                                4.98835
trainer/Log Pis Mean                                 5.99178
trainer/Log Pis Std                                  5.30106
trainer/Log Pis Max                                 29.6977
trainer/Log Pis Min                                 -5.98871
trainer/Policy mu Mean                               0.0281428
trainer/Policy mu Std                                1.56758
trainer/Policy mu Max                                4.1755
trainer/Policy mu Min                               -5.48808
trainer/Policy log std Mean                         -0.749577
trainer/Policy log std Std                           0.33156
trainer/Policy log std Max                           0.217936
trainer/Policy log std Min                          -1.98797
trainer/Alpha                                        0.0265098
trainer/Alpha Loss                                  -0.0298444
exploration/num steps total                      37000
exploration/num paths total                         37
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             1.7066
exploration/Rewards Std                              1.1311
exploration/Rewards Max                              4.41113
exploration/Rewards Min                             -1.83152
exploration/Returns Mean                          1706.6
exploration/Returns Std                              0
exploration/Returns Max                           1706.6
exploration/Returns Min                           1706.6
exploration/Actions Mean                             0.0504695
exploration/Actions Std                              0.788935
exploration/Actions Max                              0.999878
exploration/Actions Min                             -0.999991
exploration/Num Paths                                1
exploration/Average Returns                       1706.6
exploration/env_infos/final/reward_run Mean         -1.03626
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max          -1.03626
exploration/env_infos/final/reward_run Min          -1.03626
exploration/env_infos/initial/reward_run Mean        0.0480072
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0480072
exploration/env_infos/initial/reward_run Min         0.0480072
exploration/env_infos/reward_run Mean                2.08158
exploration/env_infos/reward_run Std                 1.12849
exploration/env_infos/reward_run Max                 4.76747
exploration/env_infos/reward_run Min                -1.46133
exploration/env_infos/final/reward_ctrl Mean        -0.274833
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.274833
exploration/env_infos/final/reward_ctrl Min         -0.274833
exploration/env_infos/initial/reward_ctrl Mean      -0.0417708
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0417708
exploration/env_infos/initial/reward_ctrl Min       -0.0417708
exploration/env_infos/reward_ctrl Mean              -0.37498
exploration/env_infos/reward_ctrl Std                0.0988226
exploration/env_infos/reward_ctrl Max               -0.0417708
exploration/env_infos/reward_ctrl Min               -0.591748
evaluation/num steps total                      180000
evaluation/num paths total                         180
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.94941
evaluation/Rewards Std                               1.04522
evaluation/Rewards Max                               4.72036
evaluation/Rewards Min                              -1.57435
evaluation/Returns Mean                           1949.41
evaluation/Returns Std                             613.541
evaluation/Returns Max                            2470.59
evaluation/Returns Min                             747.87
evaluation/Actions Mean                              0.0589425
evaluation/Actions Std                               0.780296
evaluation/Actions Max                               0.999917
evaluation/Actions Min                              -0.999993
evaluation/Num Paths                                 5
evaluation/Average Returns                        1949.41
evaluation/env_infos/final/reward_run Mean           2.04642
evaluation/env_infos/final/reward_run Std            1.12164
evaluation/env_infos/final/reward_run Max            3.28112
evaluation/env_infos/final/reward_run Min           -0.0630802
evaluation/env_infos/initial/reward_run Mean        -0.112816
evaluation/env_infos/initial/reward_run Std          0.170254
evaluation/env_infos/initial/reward_run Max          0.1481
evaluation/env_infos/initial/reward_run Min         -0.280506
evaluation/env_infos/reward_run Mean                 2.31682
evaluation/env_infos/reward_run Std                  1.0419
evaluation/env_infos/reward_run Max                  5.00079
evaluation/env_infos/reward_run Min                 -1.12962
evaluation/env_infos/final/reward_ctrl Mean         -0.35156
evaluation/env_infos/final/reward_ctrl Std           0.0895844
evaluation/env_infos/final/reward_ctrl Max          -0.22584
evaluation/env_infos/final/reward_ctrl Min          -0.459002
evaluation/env_infos/initial/reward_ctrl Mean       -0.0458172
evaluation/env_infos/initial/reward_ctrl Std         0.0151135
evaluation/env_infos/initial/reward_ctrl Max        -0.032252
evaluation/env_infos/initial/reward_ctrl Min        -0.0744342
evaluation/env_infos/reward_ctrl Mean               -0.367401
evaluation/env_infos/reward_ctrl Std                 0.10213
evaluation/env_infos/reward_ctrl Max                -0.032252
evaluation/env_infos/reward_ctrl Min                -0.594507
time/data storing (s)                                0.0154386
time/evaluation sampling (s)                         3.4787
time/exploration sampling (s)                        1.69119
time/logging (s)                                     0.0526716
time/saving (s)                                      0.0176156
time/training (s)                                   45.2284
time/epoch (s)                                      50.484
time/total (s)                                    1513.27
Epoch                                               35
----------------------------------------------  --------------
2020-07-08 20:24:05.117074 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 36 finished
----------------------------------------------  ---------------
replay_buffer/size                               38000
trainer/QF1 Loss                                     3.24579
trainer/QF2 Loss                                     4.11096
trainer/Policy Loss                                -39.3716
trainer/Q1 Predictions Mean                         44.8752
trainer/Q1 Predictions Std                          38.2114
trainer/Q1 Predictions Max                         110.268
trainer/Q1 Predictions Min                           5.67589
trainer/Q2 Predictions Mean                         44.7786
trainer/Q2 Predictions Std                          38.1883
trainer/Q2 Predictions Max                         111.599
trainer/Q2 Predictions Min                           5.69989
trainer/Q Targets Mean                              44.6453
trainer/Q Targets Std                               38.0696
trainer/Q Targets Max                              111.378
trainer/Q Targets Min                                4.51418
trainer/Log Pis Mean                                 5.8129
trainer/Log Pis Std                                  5.39172
trainer/Log Pis Max                                 26.4482
trainer/Log Pis Min                                 -5.90171
trainer/Policy mu Mean                               0.0449451
trainer/Policy mu Std                                1.60763
trainer/Policy mu Max                                4.29365
trainer/Policy mu Min                               -4.6539
trainer/Policy log std Mean                         -0.718573
trainer/Policy log std Std                           0.321827
trainer/Policy log std Max                           0.0408869
trainer/Policy log std Min                          -1.97848
trainer/Alpha                                        0.0277312
trainer/Alpha Loss                                  -0.670796
exploration/num steps total                      38000
exploration/num paths total                         38
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.83191
exploration/Rewards Std                              1.2705
exploration/Rewards Max                              4.15665
exploration/Rewards Min                             -1.82412
exploration/Returns Mean                           831.91
exploration/Returns Std                              0
exploration/Returns Max                            831.91
exploration/Returns Min                            831.91
exploration/Actions Mean                            -0.00291483
exploration/Actions Std                              0.80364
exploration/Actions Max                              0.999955
exploration/Actions Min                             -0.999998
exploration/Num Paths                                1
exploration/Average Returns                        831.91
exploration/env_infos/final/reward_run Mean          1.38289
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.38289
exploration/env_infos/final/reward_run Min           1.38289
exploration/env_infos/initial/reward_run Mean       -0.192887
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.192887
exploration/env_infos/initial/reward_run Min        -0.192887
exploration/env_infos/reward_run Mean                1.21942
exploration/env_infos/reward_run Std                 1.29085
exploration/env_infos/reward_run Max                 4.49326
exploration/env_infos/reward_run Min                -1.44951
exploration/env_infos/final/reward_ctrl Mean        -0.420739
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.420739
exploration/env_infos/final/reward_ctrl Min         -0.420739
exploration/env_infos/initial/reward_ctrl Mean      -0.0735401
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0735401
exploration/env_infos/initial/reward_ctrl Min       -0.0735401
exploration/env_infos/reward_ctrl Mean              -0.387507
exploration/env_infos/reward_ctrl Std                0.0937404
exploration/env_infos/reward_ctrl Max               -0.0735401
exploration/env_infos/reward_ctrl Min               -0.593748
evaluation/num steps total                      185000
evaluation/num paths total                         185
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.41127
evaluation/Rewards Std                               1.26678
evaluation/Rewards Max                               4.5572
evaluation/Rewards Min                              -2.80899
evaluation/Returns Mean                           1411.27
evaluation/Returns Std                             701.473
evaluation/Returns Max                            2093.54
evaluation/Returns Min                             394.134
evaluation/Actions Mean                              0.0434718
evaluation/Actions Std                               0.7961
evaluation/Actions Max                               0.999981
evaluation/Actions Min                              -0.999995
evaluation/Num Paths                                 5
evaluation/Average Returns                        1411.27
evaluation/env_infos/final/reward_run Mean           1.42782
evaluation/env_infos/final/reward_run Std            1.33639
evaluation/env_infos/final/reward_run Max            3.45565
evaluation/env_infos/final/reward_run Min           -0.0177932
evaluation/env_infos/initial/reward_run Mean        -0.0835262
evaluation/env_infos/initial/reward_run Std          0.209159
evaluation/env_infos/initial/reward_run Max          0.126473
evaluation/env_infos/initial/reward_run Min         -0.347835
evaluation/env_infos/reward_run Mean                 1.79267
evaluation/env_infos/reward_run Std                  1.26993
evaluation/env_infos/reward_run Max                  5.10508
evaluation/env_infos/reward_run Min                 -2.3934
evaluation/env_infos/final/reward_ctrl Mean         -0.438794
evaluation/env_infos/final/reward_ctrl Std           0.114813
evaluation/env_infos/final/reward_ctrl Max          -0.241318
evaluation/env_infos/final/reward_ctrl Min          -0.584071
evaluation/env_infos/initial/reward_ctrl Mean       -0.0770796
evaluation/env_infos/initial/reward_ctrl Std         0.0422983
evaluation/env_infos/initial/reward_ctrl Max        -0.0282929
evaluation/env_infos/initial/reward_ctrl Min        -0.148138
evaluation/env_infos/reward_ctrl Mean               -0.381399
evaluation/env_infos/reward_ctrl Std                 0.101272
evaluation/env_infos/reward_ctrl Max                -0.0282929
evaluation/env_infos/reward_ctrl Min                -0.59633
time/data storing (s)                                0.0070177
time/evaluation sampling (s)                         4.22905
time/exploration sampling (s)                        0.824628
time/logging (s)                                     0.0396199
time/saving (s)                                      0.0161284
time/training (s)                                   34.8336
time/epoch (s)                                      39.9501
time/total (s)                                    1553.24
Epoch                                               36
----------------------------------------------  ---------------
2020-07-08 20:24:43.250181 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 37 finished
----------------------------------------------  ---------------
replay_buffer/size                               39000
trainer/QF1 Loss                                     2.19896
trainer/QF2 Loss                                     3.14498
trainer/Policy Loss                                -40.7237
trainer/Q1 Predictions Mean                         47.1306
trainer/Q1 Predictions Std                          40.2088
trainer/Q1 Predictions Max                         111.633
trainer/Q1 Predictions Min                           5.20591
trainer/Q2 Predictions Mean                         46.9363
trainer/Q2 Predictions Std                          40.2199
trainer/Q2 Predictions Max                         110.263
trainer/Q2 Predictions Min                           4.53677
trainer/Q Targets Mean                              47.0566
trainer/Q Targets Std                               40.3068
trainer/Q Targets Max                              110.447
trainer/Q Targets Min                                4.60033
trainer/Log Pis Mean                                 6.63912
trainer/Log Pis Std                                  5.30696
trainer/Log Pis Max                                 29.0617
trainer/Log Pis Min                                 -7.6078
trainer/Policy mu Mean                               0.0662814
trainer/Policy mu Std                                1.59193
trainer/Policy mu Max                                5.02687
trainer/Policy mu Min                               -4.98272
trainer/Policy log std Mean                         -0.749895
trainer/Policy log std Std                           0.360328
trainer/Policy log std Max                           0.0368437
trainer/Policy log std Min                          -2.27124
trainer/Alpha                                        0.0284153
trainer/Alpha Loss                                   2.27581
exploration/num steps total                      39000
exploration/num paths total                         39
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.769475
exploration/Rewards Std                              1.15145
exploration/Rewards Max                              3.96379
exploration/Rewards Min                             -1.85246
exploration/Returns Mean                           769.475
exploration/Returns Std                              0
exploration/Returns Max                            769.475
exploration/Returns Min                            769.475
exploration/Actions Mean                             0.00980903
exploration/Actions Std                              0.801799
exploration/Actions Max                              0.999981
exploration/Actions Min                             -0.999997
exploration/Num Paths                                1
exploration/Average Returns                        769.475
exploration/env_infos/final/reward_run Mean         -0.179759
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max          -0.179759
exploration/env_infos/final/reward_run Min          -0.179759
exploration/env_infos/initial/reward_run Mean       -0.128898
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.128898
exploration/env_infos/initial/reward_run Min        -0.128898
exploration/env_infos/reward_run Mean                1.15526
exploration/env_infos/reward_run Std                 1.16823
exploration/env_infos/reward_run Max                 4.40298
exploration/env_infos/reward_run Min                -1.40407
exploration/env_infos/final/reward_ctrl Mean        -0.381828
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.381828
exploration/env_infos/final/reward_ctrl Min         -0.381828
exploration/env_infos/initial/reward_ctrl Mean      -0.0951721
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0951721
exploration/env_infos/initial/reward_ctrl Min       -0.0951721
exploration/env_infos/reward_ctrl Mean              -0.385787
exploration/env_infos/reward_ctrl Std                0.0918764
exploration/env_infos/reward_ctrl Max               -0.0951721
exploration/env_infos/reward_ctrl Min               -0.596363
evaluation/num steps total                      190000
evaluation/num paths total                         190
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.983073
evaluation/Rewards Std                               1.20462
evaluation/Rewards Max                               5.28358
evaluation/Rewards Min                              -1.95976
evaluation/Returns Mean                            983.073
evaluation/Returns Std                             468.697
evaluation/Returns Max                            1749.36
evaluation/Returns Min                             363.678
evaluation/Actions Mean                              0.0118087
evaluation/Actions Std                               0.811272
evaluation/Actions Max                               0.999946
evaluation/Actions Min                              -0.999992
evaluation/Num Paths                                 5
evaluation/Average Returns                         983.073
evaluation/env_infos/final/reward_run Mean           1.32368
evaluation/env_infos/final/reward_run Std            1.14794
evaluation/env_infos/final/reward_run Max            2.60465
evaluation/env_infos/final/reward_run Min           -0.497091
evaluation/env_infos/initial/reward_run Mean         0.305228
evaluation/env_infos/initial/reward_run Std          0.170937
evaluation/env_infos/initial/reward_run Max          0.474119
evaluation/env_infos/initial/reward_run Min          0.0849496
evaluation/env_infos/reward_run Mean                 1.37805
evaluation/env_infos/reward_run Std                  1.22224
evaluation/env_infos/reward_run Max                  5.55525
evaluation/env_infos/reward_run Min                 -1.53144
evaluation/env_infos/final/reward_ctrl Mean         -0.386926
evaluation/env_infos/final/reward_ctrl Std           0.0478551
evaluation/env_infos/final/reward_ctrl Max          -0.332933
evaluation/env_infos/final/reward_ctrl Min          -0.466892
evaluation/env_infos/initial/reward_ctrl Mean       -0.0612239
evaluation/env_infos/initial/reward_ctrl Std         0.0160989
evaluation/env_infos/initial/reward_ctrl Max        -0.0376695
evaluation/env_infos/initial/reward_ctrl Min        -0.0848702
evaluation/env_infos/reward_ctrl Mean               -0.394982
evaluation/env_infos/reward_ctrl Std                 0.0937784
evaluation/env_infos/reward_ctrl Max                -0.0376695
evaluation/env_infos/reward_ctrl Min                -0.596664
time/data storing (s)                                0.0075224
time/evaluation sampling (s)                         2.80739
time/exploration sampling (s)                        1.02359
time/logging (s)                                     0.0404279
time/saving (s)                                      0.0163926
time/training (s)                                   34.1341
time/epoch (s)                                      38.0294
time/total (s)                                    1591.37
Epoch                                               37
----------------------------------------------  ---------------
2020-07-08 20:25:21.989487 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 38 finished
----------------------------------------------  ---------------
replay_buffer/size                               40000
trainer/QF1 Loss                                     2.69135
trainer/QF2 Loss                                     3.06048
trainer/Policy Loss                                -37.8857
trainer/Q1 Predictions Mean                         43.6285
trainer/Q1 Predictions Std                          39.6131
trainer/Q1 Predictions Max                         114.47
trainer/Q1 Predictions Min                           5.23293
trainer/Q2 Predictions Mean                         43.7128
trainer/Q2 Predictions Std                          39.6368
trainer/Q2 Predictions Max                         116.435
trainer/Q2 Predictions Min                           5.12443
trainer/Q Targets Mean                              43.6202
trainer/Q Targets Std                               39.8021
trainer/Q Targets Max                              114.037
trainer/Q Targets Min                                3.77233
trainer/Log Pis Mean                                 6.13214
trainer/Log Pis Std                                  5.45964
trainer/Log Pis Max                                 27.9967
trainer/Log Pis Min                                 -4.22726
trainer/Policy mu Mean                               0.194729
trainer/Policy mu Std                                1.57406
trainer/Policy mu Max                                5.31477
trainer/Policy mu Min                               -4.42512
trainer/Policy log std Mean                         -0.731731
trainer/Policy log std Std                           0.321835
trainer/Policy log std Max                          -0.00266616
trainer/Policy log std Min                          -2.01276
trainer/Alpha                                        0.0283976
trainer/Alpha Loss                                   0.47062
exploration/num steps total                      40000
exploration/num paths total                         40
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.871084
exploration/Rewards Std                              1.18861
exploration/Rewards Max                              4.09915
exploration/Rewards Min                             -1.75327
exploration/Returns Mean                           871.084
exploration/Returns Std                              0
exploration/Returns Max                            871.084
exploration/Returns Min                            871.084
exploration/Actions Mean                             0.0615781
exploration/Actions Std                              0.808089
exploration/Actions Max                              0.999867
exploration/Actions Min                             -0.999996
exploration/Num Paths                                1
exploration/Average Returns                        871.084
exploration/env_infos/final/reward_run Mean          1.73941
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.73941
exploration/env_infos/final/reward_run Min           1.73941
exploration/env_infos/initial/reward_run Mean        0.0692594
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0692594
exploration/env_infos/initial/reward_run Min         0.0692594
exploration/env_infos/reward_run Mean                1.26516
exploration/env_infos/reward_run Std                 1.20064
exploration/env_infos/reward_run Max                 4.53481
exploration/env_infos/reward_run Min                -1.3915
exploration/env_infos/final/reward_ctrl Mean        -0.383897
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.383897
exploration/env_infos/final/reward_ctrl Min         -0.383897
exploration/env_infos/initial/reward_ctrl Mean      -0.037625
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.037625
exploration/env_infos/initial/reward_ctrl Min       -0.037625
exploration/env_infos/reward_ctrl Mean              -0.39408
exploration/env_infos/reward_ctrl Std                0.0960919
exploration/env_infos/reward_ctrl Max               -0.037625
exploration/env_infos/reward_ctrl Min               -0.59519
evaluation/num steps total                      195000
evaluation/num paths total                         195
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.900362
evaluation/Rewards Std                               1.2505
evaluation/Rewards Max                               4.4209
evaluation/Rewards Min                              -2.4157
evaluation/Returns Mean                            900.362
evaluation/Returns Std                             405.647
evaluation/Returns Max                            1490.69
evaluation/Returns Min                             357.845
evaluation/Actions Mean                              0.0526871
evaluation/Actions Std                               0.810784
evaluation/Actions Max                               0.999936
evaluation/Actions Min                              -0.99999
evaluation/Num Paths                                 5
evaluation/Average Returns                         900.362
evaluation/env_infos/final/reward_run Mean           0.220622
evaluation/env_infos/final/reward_run Std            0.838327
evaluation/env_infos/final/reward_run Max            1.50835
evaluation/env_infos/final/reward_run Min           -0.774545
evaluation/env_infos/initial/reward_run Mean         0.0677943
evaluation/env_infos/initial/reward_run Std          0.150147
evaluation/env_infos/initial/reward_run Max          0.254247
evaluation/env_infos/initial/reward_run Min         -0.100671
evaluation/env_infos/reward_run Mean                 1.29645
evaluation/env_infos/reward_run Std                  1.25802
evaluation/env_infos/reward_run Max                  4.7171
evaluation/env_infos/reward_run Min                 -1.91224
evaluation/env_infos/final/reward_ctrl Mean         -0.450605
evaluation/env_infos/final/reward_ctrl Std           0.0532703
evaluation/env_infos/final/reward_ctrl Max          -0.366274
evaluation/env_infos/final/reward_ctrl Min          -0.518742
evaluation/env_infos/initial/reward_ctrl Mean       -0.0644612
evaluation/env_infos/initial/reward_ctrl Std         0.0293933
evaluation/env_infos/initial/reward_ctrl Max        -0.0248036
evaluation/env_infos/initial/reward_ctrl Min        -0.100357
evaluation/env_infos/reward_ctrl Mean               -0.396088
evaluation/env_infos/reward_ctrl Std                 0.0890077
evaluation/env_infos/reward_ctrl Max                -0.0248036
evaluation/env_infos/reward_ctrl Min                -0.596051
time/data storing (s)                                0.00670021
time/evaluation sampling (s)                         2.55083
time/exploration sampling (s)                        0.673355
time/logging (s)                                     0.0414079
time/saving (s)                                      0.0159005
time/training (s)                                   35.3797
time/epoch (s)                                      38.6679
time/total (s)                                    1630.1
Epoch                                               38
----------------------------------------------  ---------------
2020-07-08 20:26:04.048671 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 39 finished
----------------------------------------------  ---------------
replay_buffer/size                               41000
trainer/QF1 Loss                                     4.1233
trainer/QF2 Loss                                     4.15975
trainer/Policy Loss                                -44.3154
trainer/Q1 Predictions Mean                         49.9665
trainer/Q1 Predictions Std                          41.4469
trainer/Q1 Predictions Max                         124.424
trainer/Q1 Predictions Min                           5.32668
trainer/Q2 Predictions Mean                         49.8804
trainer/Q2 Predictions Std                          41.2561
trainer/Q2 Predictions Max                         122.095
trainer/Q2 Predictions Min                           4.98664
trainer/Q Targets Mean                              49.9097
trainer/Q Targets Std                               41.3764
trainer/Q Targets Max                              122.261
trainer/Q Targets Min                                4.42414
trainer/Log Pis Mean                                 6.03863
trainer/Log Pis Std                                  5.85192
trainer/Log Pis Max                                 26.7228
trainer/Log Pis Min                                 -6.30262
trainer/Policy mu Mean                               0.0899455
trainer/Policy mu Std                                1.59842
trainer/Policy mu Max                                5.41768
trainer/Policy mu Min                               -4.71833
trainer/Policy log std Mean                         -0.748428
trainer/Policy log std Std                           0.341863
trainer/Policy log std Max                           0.0894684
trainer/Policy log std Min                          -2.08005
trainer/Alpha                                        0.0283865
trainer/Alpha Loss                                   0.137585
exploration/num steps total                      41000
exploration/num paths total                         41
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.25429
exploration/Rewards Std                              0.76988
exploration/Rewards Max                              4.38632
exploration/Rewards Min                             -0.383324
exploration/Returns Mean                          2254.29
exploration/Returns Std                              0
exploration/Returns Max                           2254.29
exploration/Returns Min                           2254.29
exploration/Actions Mean                             0.0206266
exploration/Actions Std                              0.832639
exploration/Actions Max                              0.999945
exploration/Actions Min                             -0.999998
exploration/Num Paths                                1
exploration/Average Returns                       2254.29
exploration/env_infos/final/reward_run Mean          1.7859
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.7859
exploration/env_infos/final/reward_run Min           1.7859
exploration/env_infos/initial/reward_run Mean        0.012599
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.012599
exploration/env_infos/initial/reward_run Min         0.012599
exploration/env_infos/reward_run Mean                2.67052
exploration/env_infos/reward_run Std                 0.750435
exploration/env_infos/reward_run Max                 4.83342
exploration/env_infos/reward_run Min                 0.012599
exploration/env_infos/final/reward_ctrl Mean        -0.323836
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.323836
exploration/env_infos/final/reward_ctrl Min         -0.323836
exploration/env_infos/initial/reward_ctrl Mean      -0.026506
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.026506
exploration/env_infos/initial/reward_ctrl Min       -0.026506
exploration/env_infos/reward_ctrl Mean              -0.416228
exploration/env_infos/reward_ctrl Std                0.103394
exploration/env_infos/reward_ctrl Max               -0.026506
exploration/env_infos/reward_ctrl Min               -0.595885
evaluation/num steps total                      200000
evaluation/num paths total                         200
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.39594
evaluation/Rewards Std                               1.28594
evaluation/Rewards Max                               4.63559
evaluation/Rewards Min                              -1.9631
evaluation/Returns Mean                           1395.94
evaluation/Returns Std                             589.818
evaluation/Returns Max                            2367.97
evaluation/Returns Min                             690.594
evaluation/Actions Mean                              0.0109816
evaluation/Actions Std                               0.820748
evaluation/Actions Max                               0.999918
evaluation/Actions Min                              -0.999968
evaluation/Num Paths                                 5
evaluation/Average Returns                        1395.94
evaluation/env_infos/final/reward_run Mean           0.710985
evaluation/env_infos/final/reward_run Std            0.423582
evaluation/env_infos/final/reward_run Max            1.21635
evaluation/env_infos/final/reward_run Min           -0.0157458
evaluation/env_infos/initial/reward_run Mean         0.0713549
evaluation/env_infos/initial/reward_run Std          0.173338
evaluation/env_infos/initial/reward_run Max          0.183773
evaluation/env_infos/initial/reward_run Min         -0.270908
evaluation/env_infos/reward_run Mean                 1.80019
evaluation/env_infos/reward_run Std                  1.30378
evaluation/env_infos/reward_run Max                  5.06664
evaluation/env_infos/reward_run Min                 -1.53275
evaluation/env_infos/final/reward_ctrl Mean         -0.401484
evaluation/env_infos/final/reward_ctrl Std           0.0782261
evaluation/env_infos/final/reward_ctrl Max          -0.3048
evaluation/env_infos/final/reward_ctrl Min          -0.509643
evaluation/env_infos/initial/reward_ctrl Mean       -0.0430499
evaluation/env_infos/initial/reward_ctrl Std         0.0156627
evaluation/env_infos/initial/reward_ctrl Max        -0.0292337
evaluation/env_infos/initial/reward_ctrl Min        -0.0725286
evaluation/env_infos/reward_ctrl Mean               -0.404249
evaluation/env_infos/reward_ctrl Std                 0.0988267
evaluation/env_infos/reward_ctrl Max                -0.0292337
evaluation/env_infos/reward_ctrl Min                -0.596939
time/data storing (s)                                0.00677592
time/evaluation sampling (s)                         3.29207
time/exploration sampling (s)                        0.651363
time/logging (s)                                     0.0421344
time/saving (s)                                      0.0188083
time/training (s)                                   38.0283
time/epoch (s)                                      42.0395
time/total (s)                                    1672.16
Epoch                                               39
----------------------------------------------  ---------------
2020-07-08 20:26:48.362043 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 40 finished
----------------------------------------------  --------------
replay_buffer/size                               42000
trainer/QF1 Loss                                     3.8785
trainer/QF2 Loss                                     4.30789
trainer/Policy Loss                                -44.5246
trainer/Q1 Predictions Mean                         50.1291
trainer/Q1 Predictions Std                          43.9739
trainer/Q1 Predictions Max                         121.935
trainer/Q1 Predictions Min                           4.57071
trainer/Q2 Predictions Mean                         50.463
trainer/Q2 Predictions Std                          44.04
trainer/Q2 Predictions Max                         123.245
trainer/Q2 Predictions Min                           4.29827
trainer/Q Targets Mean                              49.898
trainer/Q Targets Std                               43.8803
trainer/Q Targets Max                              120.063
trainer/Q Targets Min                                3.73572
trainer/Log Pis Mean                                 6.10524
trainer/Log Pis Std                                  5.86742
trainer/Log Pis Max                                 42.5526
trainer/Log Pis Min                                 -4.07201
trainer/Policy mu Mean                               0.161675
trainer/Policy mu Std                                1.60322
trainer/Policy mu Max                                5.7706
trainer/Policy mu Min                               -6.8518
trainer/Policy log std Mean                         -0.717827
trainer/Policy log std Std                           0.322304
trainer/Policy log std Max                           0.0121628
trainer/Policy log std Min                          -2.11099
trainer/Alpha                                        0.0296914
trainer/Alpha Loss                                   0.370131
exploration/num steps total                      42000
exploration/num paths total                         42
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             0.815004
exploration/Rewards Std                              1.08506
exploration/Rewards Max                              3.95639
exploration/Rewards Min                             -2.00477
exploration/Returns Mean                           815.004
exploration/Returns Std                              0
exploration/Returns Max                            815.004
exploration/Returns Min                            815.004
exploration/Actions Mean                             0.044907
exploration/Actions Std                              0.805139
exploration/Actions Max                              0.999973
exploration/Actions Min                             -0.999999
exploration/Num Paths                                1
exploration/Average Returns                        815.004
exploration/env_infos/final/reward_run Mean          0.909273
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.909273
exploration/env_infos/final/reward_run Min           0.909273
exploration/env_infos/initial/reward_run Mean       -0.132322
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.132322
exploration/env_infos/initial/reward_run Min        -0.132322
exploration/env_infos/reward_run Mean                1.20516
exploration/env_infos/reward_run Std                 1.09174
exploration/env_infos/reward_run Max                 4.4233
exploration/env_infos/reward_run Min                -1.62051
exploration/env_infos/final/reward_ctrl Mean        -0.491046
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.491046
exploration/env_infos/final/reward_ctrl Min         -0.491046
exploration/env_infos/initial/reward_ctrl Mean      -0.0555033
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0555033
exploration/env_infos/initial/reward_ctrl Min       -0.0555033
exploration/env_infos/reward_ctrl Mean              -0.390159
exploration/env_infos/reward_ctrl Std                0.0985726
exploration/env_infos/reward_ctrl Max               -0.0555033
exploration/env_infos/reward_ctrl Min               -0.597468
evaluation/num steps total                      205000
evaluation/num paths total                         205
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              0.826549
evaluation/Rewards Std                               1.18249
evaluation/Rewards Max                               4.5634
evaluation/Rewards Min                              -2.26817
evaluation/Returns Mean                            826.549
evaluation/Returns Std                             201.983
evaluation/Returns Max                            1093.73
evaluation/Returns Min                             555.362
evaluation/Actions Mean                              0.0282809
evaluation/Actions Std                               0.806494
evaluation/Actions Max                               0.999989
evaluation/Actions Min                              -1
evaluation/Num Paths                                 5
evaluation/Average Returns                         826.549
evaluation/env_infos/final/reward_run Mean           1.91601
evaluation/env_infos/final/reward_run Std            0.987736
evaluation/env_infos/final/reward_run Max            2.89414
evaluation/env_infos/final/reward_run Min            0.483147
evaluation/env_infos/initial/reward_run Mean        -0.0153962
evaluation/env_infos/initial/reward_run Std          0.154525
evaluation/env_infos/initial/reward_run Max          0.248077
evaluation/env_infos/initial/reward_run Min         -0.216326
evaluation/env_infos/reward_run Mean                 1.21729
evaluation/env_infos/reward_run Std                  1.19545
evaluation/env_infos/reward_run Max                  4.9328
evaluation/env_infos/reward_run Min                 -1.81526
evaluation/env_infos/final/reward_ctrl Mean         -0.348235
evaluation/env_infos/final/reward_ctrl Std           0.0781919
evaluation/env_infos/final/reward_ctrl Max          -0.203026
evaluation/env_infos/final/reward_ctrl Min          -0.419822
evaluation/env_infos/initial/reward_ctrl Mean       -0.0701171
evaluation/env_infos/initial/reward_ctrl Std         0.0293602
evaluation/env_infos/initial/reward_ctrl Max        -0.0370238
evaluation/env_infos/initial/reward_ctrl Min        -0.120963
evaluation/env_infos/reward_ctrl Mean               -0.390739
evaluation/env_infos/reward_ctrl Std                 0.0907516
evaluation/env_infos/reward_ctrl Max                -0.0370238
evaluation/env_infos/reward_ctrl Min                -0.597094
time/data storing (s)                                0.0065705
time/evaluation sampling (s)                         2.76558
time/exploration sampling (s)                        0.680318
time/logging (s)                                     0.0406248
time/saving (s)                                      0.0159238
time/training (s)                                   40.7839
time/epoch (s)                                      44.2929
time/total (s)                                    1716.47
Epoch                                               40
----------------------------------------------  --------------
2020-07-08 20:27:29.124788 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 41 finished
----------------------------------------------  ----------------
replay_buffer/size                               43000
trainer/QF1 Loss                                     3.64816
trainer/QF2 Loss                                     3.91335
trainer/Policy Loss                                -48.5238
trainer/Q1 Predictions Mean                         54.1115
trainer/Q1 Predictions Std                          46.7466
trainer/Q1 Predictions Max                         127.158
trainer/Q1 Predictions Min                           4.60672
trainer/Q2 Predictions Mean                         54.2642
trainer/Q2 Predictions Std                          46.867
trainer/Q2 Predictions Max                         124.912
trainer/Q2 Predictions Min                           3.42957
trainer/Q Targets Mean                              53.8323
trainer/Q Targets Std                               46.7278
trainer/Q Targets Max                              123.262
trainer/Q Targets Min                                3.82331
trainer/Log Pis Mean                                 6.01083
trainer/Log Pis Std                                  5.12627
trainer/Log Pis Max                                 27.494
trainer/Log Pis Min                                 -5.18124
trainer/Policy mu Mean                               0.0032625
trainer/Policy mu Std                                1.53366
trainer/Policy mu Max                                4.02425
trainer/Policy mu Min                               -4.31034
trainer/Policy log std Mean                         -0.756545
trainer/Policy log std Std                           0.337752
trainer/Policy log std Max                          -0.019672
trainer/Policy log std Min                          -2.91095
trainer/Alpha                                        0.0295143
trainer/Alpha Loss                                   0.0381523
exploration/num steps total                      43000
exploration/num paths total                         43
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.3026
exploration/Rewards Std                              0.830543
exploration/Rewards Max                              4.73517
exploration/Rewards Min                             -1.20271
exploration/Returns Mean                          2302.6
exploration/Returns Std                              0
exploration/Returns Max                           2302.6
exploration/Returns Min                           2302.6
exploration/Actions Mean                            -0.000771651
exploration/Actions Std                              0.812097
exploration/Actions Max                              0.999951
exploration/Actions Min                             -0.999992
exploration/Num Paths                                1
exploration/Average Returns                       2302.6
exploration/env_infos/final/reward_run Mean          2.84663
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.84663
exploration/env_infos/final/reward_run Min           2.84663
exploration/env_infos/initial/reward_run Mean        0.163448
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.163448
exploration/env_infos/initial/reward_run Min         0.163448
exploration/env_infos/reward_run Mean                2.6983
exploration/env_infos/reward_run Std                 0.808641
exploration/env_infos/reward_run Max                 5.1673
exploration/env_infos/reward_run Min                -0.971009
exploration/env_infos/final/reward_ctrl Mean        -0.506791
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.506791
exploration/env_infos/final/reward_ctrl Min         -0.506791
exploration/env_infos/initial/reward_ctrl Mean      -0.169046
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.169046
exploration/env_infos/initial/reward_ctrl Min       -0.169046
exploration/env_infos/reward_ctrl Mean              -0.395701
exploration/env_infos/reward_ctrl Std                0.106244
exploration/env_infos/reward_ctrl Max               -0.101777
exploration/env_infos/reward_ctrl Min               -0.595059
evaluation/num steps total                      210000
evaluation/num paths total                         210
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.49452
evaluation/Rewards Std                               1.29876
evaluation/Rewards Max                               4.58606
evaluation/Rewards Min                              -1.92029
evaluation/Returns Mean                           1494.52
evaluation/Returns Std                             564.196
evaluation/Returns Max                            2239.11
evaluation/Returns Min                             647.11
evaluation/Actions Mean                             -0.0130211
evaluation/Actions Std                               0.814695
evaluation/Actions Max                               0.999945
evaluation/Actions Min                              -0.99999
evaluation/Num Paths                                 5
evaluation/Average Returns                        1494.52
evaluation/env_infos/final/reward_run Mean           0.961601
evaluation/env_infos/final/reward_run Std            1.45818
evaluation/env_infos/final/reward_run Max            2.79109
evaluation/env_infos/final/reward_run Min           -0.763192
evaluation/env_infos/initial/reward_run Mean         0.0608482
evaluation/env_infos/initial/reward_run Std          0.0525622
evaluation/env_infos/initial/reward_run Max          0.137042
evaluation/env_infos/initial/reward_run Min         -0.00996034
evaluation/env_infos/reward_run Mean                 1.89286
evaluation/env_infos/reward_run Std                  1.31048
evaluation/env_infos/reward_run Max                  5.02157
evaluation/env_infos/reward_run Min                 -1.46971
evaluation/env_infos/final/reward_ctrl Mean         -0.379315
evaluation/env_infos/final/reward_ctrl Std           0.0679582
evaluation/env_infos/final/reward_ctrl Max          -0.286947
evaluation/env_infos/final/reward_ctrl Min          -0.492112
evaluation/env_infos/initial/reward_ctrl Mean       -0.0606657
evaluation/env_infos/initial/reward_ctrl Std         0.0361692
evaluation/env_infos/initial/reward_ctrl Max        -0.0337644
evaluation/env_infos/initial/reward_ctrl Min        -0.132258
evaluation/env_infos/reward_ctrl Mean               -0.398339
evaluation/env_infos/reward_ctrl Std                 0.0996116
evaluation/env_infos/reward_ctrl Max                -0.0337644
evaluation/env_infos/reward_ctrl Min                -0.597177
time/data storing (s)                                0.00645094
time/evaluation sampling (s)                         2.70592
time/exploration sampling (s)                        0.592865
time/logging (s)                                     0.0442388
time/saving (s)                                      0.0162506
time/training (s)                                   37.1846
time/epoch (s)                                      40.5504
time/total (s)                                    1757.23
Epoch                                               41
----------------------------------------------  ----------------
2020-07-08 20:28:16.257312 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 42 finished
----------------------------------------------  ---------------
replay_buffer/size                               44000
trainer/QF1 Loss                                     3.56769
trainer/QF2 Loss                                     3.97104
trainer/Policy Loss                                -47.7117
trainer/Q1 Predictions Mean                         53.9749
trainer/Q1 Predictions Std                          46.477
trainer/Q1 Predictions Max                         129.756
trainer/Q1 Predictions Min                           4.03
trainer/Q2 Predictions Mean                         54.1496
trainer/Q2 Predictions Std                          46.5645
trainer/Q2 Predictions Max                         125.654
trainer/Q2 Predictions Min                           4.34588
trainer/Q Targets Mean                              54.1726
trainer/Q Targets Std                               46.8633
trainer/Q Targets Max                              131.61
trainer/Q Targets Min                                3.85904
trainer/Log Pis Mean                                 6.64052
trainer/Log Pis Std                                  6.02316
trainer/Log Pis Max                                 29.8242
trainer/Log Pis Min                                 -4.71649
trainer/Policy mu Mean                              -0.0318405
trainer/Policy mu Std                                1.63084
trainer/Policy mu Max                                4.70228
trainer/Policy mu Min                               -6.34059
trainer/Policy log std Mean                         -0.72474
trainer/Policy log std Std                           0.334207
trainer/Policy log std Max                          -0.00408813
trainer/Policy log std Min                          -2.15529
trainer/Alpha                                        0.0304864
trainer/Alpha Loss                                   2.23581
exploration/num steps total                      44000
exploration/num paths total                         44
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.16147
exploration/Rewards Std                              0.907887
exploration/Rewards Max                              4.44467
exploration/Rewards Min                             -0.7455
exploration/Returns Mean                          2161.47
exploration/Returns Std                              0
exploration/Returns Max                           2161.47
exploration/Returns Min                           2161.47
exploration/Actions Mean                            -0.0293043
exploration/Actions Std                              0.804227
exploration/Actions Max                              0.999981
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       2161.47
exploration/env_infos/final/reward_run Mean          3.62849
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.62849
exploration/env_infos/final/reward_run Min           3.62849
exploration/env_infos/initial/reward_run Mean       -0.285342
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.285342
exploration/env_infos/initial/reward_run Min        -0.285342
exploration/env_infos/reward_run Mean                2.55006
exploration/env_infos/reward_run Std                 0.894691
exploration/env_infos/reward_run Max                 4.81686
exploration/env_infos/reward_run Min                -0.307595
exploration/env_infos/final/reward_ctrl Mean        -0.564726
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.564726
exploration/env_infos/final/reward_ctrl Min         -0.564726
exploration/env_infos/initial/reward_ctrl Mean      -0.0244456
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0244456
exploration/env_infos/initial/reward_ctrl Min       -0.0244456
exploration/env_infos/reward_ctrl Mean              -0.388584
exploration/env_infos/reward_ctrl Std                0.0990876
exploration/env_infos/reward_ctrl Max               -0.0244456
exploration/env_infos/reward_ctrl Min               -0.593844
evaluation/num steps total                      215000
evaluation/num paths total                         215
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.48558
evaluation/Rewards Std                               1.32741
evaluation/Rewards Max                               4.74064
evaluation/Rewards Min                              -2.02374
evaluation/Returns Mean                           1485.58
evaluation/Returns Std                             745.015
evaluation/Returns Max                            2485.04
evaluation/Returns Min                             612.702
evaluation/Actions Mean                             -0.0247131
evaluation/Actions Std                               0.80099
evaluation/Actions Max                               0.999968
evaluation/Actions Min                              -0.999999
evaluation/Num Paths                                 5
evaluation/Average Returns                        1485.58
evaluation/env_infos/final/reward_run Mean           1.93906
evaluation/env_infos/final/reward_run Std            1.24998
evaluation/env_infos/final/reward_run Max            3.86114
evaluation/env_infos/final/reward_run Min            0.751049
evaluation/env_infos/initial/reward_run Mean        -0.254796
evaluation/env_infos/initial/reward_run Std          0.145806
evaluation/env_infos/initial/reward_run Max         -0.0659675
evaluation/env_infos/initial/reward_run Min         -0.447065
evaluation/env_infos/reward_run Mean                 1.8709
evaluation/env_infos/reward_run Std                  1.32899
evaluation/env_infos/reward_run Max                  5.1603
evaluation/env_infos/reward_run Min                 -1.57444
evaluation/env_infos/final/reward_ctrl Mean         -0.362624
evaluation/env_infos/final/reward_ctrl Std           0.086984
evaluation/env_infos/final/reward_ctrl Max          -0.226782
evaluation/env_infos/final/reward_ctrl Min          -0.449916
evaluation/env_infos/initial/reward_ctrl Mean       -0.1476
evaluation/env_infos/initial/reward_ctrl Std         0.0397878
evaluation/env_infos/initial/reward_ctrl Max        -0.0978415
evaluation/env_infos/initial/reward_ctrl Min        -0.211732
evaluation/env_infos/reward_ctrl Mean               -0.385318
evaluation/env_infos/reward_ctrl Std                 0.088004
evaluation/env_infos/reward_ctrl Max                -0.0978415
evaluation/env_infos/reward_ctrl Min                -0.59698
time/data storing (s)                                0.00708808
time/evaluation sampling (s)                         2.86301
time/exploration sampling (s)                        0.794513
time/logging (s)                                     0.0561558
time/saving (s)                                      0.0328369
time/training (s)                                   43.3559
time/epoch (s)                                      47.1095
time/total (s)                                    1804.37
Epoch                                               42
----------------------------------------------  ---------------
2020-07-08 20:28:54.408753 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 43 finished
----------------------------------------------  ---------------
replay_buffer/size                               45000
trainer/QF1 Loss                                     3.15347
trainer/QF2 Loss                                     3.01842
trainer/Policy Loss                                -53.8745
trainer/Q1 Predictions Mean                         59.6447
trainer/Q1 Predictions Std                          47.1615
trainer/Q1 Predictions Max                         128.568
trainer/Q1 Predictions Min                           4.15767
trainer/Q2 Predictions Mean                         59.7875
trainer/Q2 Predictions Std                          47.2173
trainer/Q2 Predictions Max                         126.831
trainer/Q2 Predictions Min                           4.08865
trainer/Q Targets Mean                              59.6911
trainer/Q Targets Std                               47.2894
trainer/Q Targets Max                              129.684
trainer/Q Targets Min                                3.10035
trainer/Log Pis Mean                                 6.3183
trainer/Log Pis Std                                  5.64471
trainer/Log Pis Max                                 27.6799
trainer/Log Pis Min                                 -3.15779
trainer/Policy mu Mean                               0.0158858
trainer/Policy mu Std                                1.61138
trainer/Policy mu Max                                4.43853
trainer/Policy mu Min                               -6.49937
trainer/Policy log std Mean                         -0.715661
trainer/Policy log std Std                           0.303541
trainer/Policy log std Max                           0.0145391
trainer/Policy log std Min                          -2.13639
trainer/Alpha                                        0.0311185
trainer/Alpha Loss                                   1.1045
exploration/num steps total                      45000
exploration/num paths total                         45
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             1.2417
exploration/Rewards Std                              1.32739
exploration/Rewards Max                              4.4435
exploration/Rewards Min                             -2.20429
exploration/Returns Mean                          1241.7
exploration/Returns Std                              0
exploration/Returns Max                           1241.7
exploration/Returns Min                           1241.7
exploration/Actions Mean                            -0.0174505
exploration/Actions Std                              0.788975
exploration/Actions Max                              0.999997
exploration/Actions Min                             -0.999989
exploration/Num Paths                                1
exploration/Average Returns                       1241.7
exploration/env_infos/final/reward_run Mean          0.767101
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           0.767101
exploration/env_infos/final/reward_run Min           0.767101
exploration/env_infos/initial/reward_run Mean       -0.0389844
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0389844
exploration/env_infos/initial/reward_run Min        -0.0389844
exploration/env_infos/reward_run Mean                1.61537
exploration/env_infos/reward_run Std                 1.33931
exploration/env_infos/reward_run Max                 4.75257
exploration/env_infos/reward_run Min                -1.81422
exploration/env_infos/final/reward_ctrl Mean        -0.336011
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.336011
exploration/env_infos/final/reward_ctrl Min         -0.336011
exploration/env_infos/initial/reward_ctrl Mean      -0.120096
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.120096
exploration/env_infos/initial/reward_ctrl Min       -0.120096
exploration/env_infos/reward_ctrl Mean              -0.373672
exploration/env_infos/reward_ctrl Std                0.0896145
exploration/env_infos/reward_ctrl Max               -0.119134
exploration/env_infos/reward_ctrl Min               -0.595694
evaluation/num steps total                      220000
evaluation/num paths total                         220
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              1.51551
evaluation/Rewards Std                               1.26452
evaluation/Rewards Max                               4.77249
evaluation/Rewards Min                              -2.10383
evaluation/Returns Mean                           1515.51
evaluation/Returns Std                             706.644
evaluation/Returns Max                            2415.72
evaluation/Returns Min                             787.426
evaluation/Actions Mean                             -0.029126
evaluation/Actions Std                               0.790836
evaluation/Actions Max                               0.999929
evaluation/Actions Min                              -0.999998
evaluation/Num Paths                                 5
evaluation/Average Returns                        1515.51
evaluation/env_infos/final/reward_run Mean           1.75957
evaluation/env_infos/final/reward_run Std            1.07118
evaluation/env_infos/final/reward_run Max            3.01362
evaluation/env_infos/final/reward_run Min            0.353245
evaluation/env_infos/initial/reward_run Mean         0.0532913
evaluation/env_infos/initial/reward_run Std          0.153863
evaluation/env_infos/initial/reward_run Max          0.215099
evaluation/env_infos/initial/reward_run Min         -0.220374
evaluation/env_infos/reward_run Mean                 1.89127
evaluation/env_infos/reward_run Std                  1.2733
evaluation/env_infos/reward_run Max                  5.17833
evaluation/env_infos/reward_run Min                 -1.69877
evaluation/env_infos/final/reward_ctrl Mean         -0.345321
evaluation/env_infos/final/reward_ctrl Std           0.0834585
evaluation/env_infos/final/reward_ctrl Max          -0.21924
evaluation/env_infos/final/reward_ctrl Min          -0.47897
evaluation/env_infos/initial/reward_ctrl Mean       -0.0970449
evaluation/env_infos/initial/reward_ctrl Std         0.037469
evaluation/env_infos/initial/reward_ctrl Max        -0.0328144
evaluation/env_infos/initial/reward_ctrl Min        -0.142488
evaluation/env_infos/reward_ctrl Mean               -0.375762
evaluation/env_infos/reward_ctrl Std                 0.0922853
evaluation/env_infos/reward_ctrl Max                -0.0328144
evaluation/env_infos/reward_ctrl Min                -0.596602
time/data storing (s)                                0.00686522
time/evaluation sampling (s)                         3.58072
time/exploration sampling (s)                        0.944987
time/logging (s)                                     0.0420983
time/saving (s)                                      0.0167803
time/training (s)                                   33.3513
time/epoch (s)                                      37.9428
time/total (s)                                    1842.51
Epoch                                               43
----------------------------------------------  ---------------
2020-07-08 20:29:39.194319 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 44 finished
----------------------------------------------  ---------------
replay_buffer/size                               46000
trainer/QF1 Loss                                     2.89786
trainer/QF2 Loss                                     4.74985
trainer/Policy Loss                                -56.4035
trainer/Q1 Predictions Mean                         62.2791
trainer/Q1 Predictions Std                          50.8107
trainer/Q1 Predictions Max                         135.66
trainer/Q1 Predictions Min                           4.10627
trainer/Q2 Predictions Mean                         62.5594
trainer/Q2 Predictions Std                          51.0523
trainer/Q2 Predictions Max                         139.557
trainer/Q2 Predictions Min                           4.31622
trainer/Q Targets Mean                              61.9377
trainer/Q Targets Std                               50.6851
trainer/Q Targets Max                              134.128
trainer/Q Targets Min                                3.20274
trainer/Log Pis Mean                                 6.28472
trainer/Log Pis Std                                  6.06482
trainer/Log Pis Max                                 30.4055
trainer/Log Pis Min                                 -7.58456
trainer/Policy mu Mean                               0.0451764
trainer/Policy mu Std                                1.60283
trainer/Policy mu Max                                4.89732
trainer/Policy mu Min                               -5.35207
trainer/Policy log std Mean                         -0.751555
trainer/Policy log std Std                           0.331125
trainer/Policy log std Max                           0.139853
trainer/Policy log std Min                          -2.02424
trainer/Alpha                                        0.0319885
trainer/Alpha Loss                                   0.98013
exploration/num steps total                      46000
exploration/num paths total                         46
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.60087
exploration/Rewards Std                              0.8483
exploration/Rewards Max                              4.71937
exploration/Rewards Min                             -0.566637
exploration/Returns Mean                          2600.87
exploration/Returns Std                              0
exploration/Returns Max                           2600.87
exploration/Returns Min                           2600.87
exploration/Actions Mean                             0.0243045
exploration/Actions Std                              0.81202
exploration/Actions Max                              0.999968
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       2600.87
exploration/env_infos/final/reward_run Mean          3.49416
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.49416
exploration/env_infos/final/reward_run Min           3.49416
exploration/env_infos/initial/reward_run Mean        0.00144381
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.00144381
exploration/env_infos/initial/reward_run Min         0.00144381
exploration/env_infos/reward_run Mean                2.99685
exploration/env_infos/reward_run Std                 0.837691
exploration/env_infos/reward_run Max                 5.12574
exploration/env_infos/reward_run Min                -0.121277
exploration/env_infos/final/reward_ctrl Mean        -0.471168
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.471168
exploration/env_infos/final/reward_ctrl Min         -0.471168
exploration/env_infos/initial/reward_ctrl Mean      -0.0481433
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0481433
exploration/env_infos/initial/reward_ctrl Min       -0.0481433
exploration/env_infos/reward_ctrl Mean              -0.39598
exploration/env_infos/reward_ctrl Std                0.102749
exploration/env_infos/reward_ctrl Max               -0.0346731
exploration/env_infos/reward_ctrl Min               -0.595735
evaluation/num steps total                      225000
evaluation/num paths total                         225
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.45971
evaluation/Rewards Std                               1.07828
evaluation/Rewards Max                               4.95135
evaluation/Rewards Min                              -2.14393
evaluation/Returns Mean                           2459.71
evaluation/Returns Std                             375.703
evaluation/Returns Max                            2776.32
evaluation/Returns Min                            1722.18
evaluation/Actions Mean                              0.0160974
evaluation/Actions Std                               0.820141
evaluation/Actions Max                               0.999952
evaluation/Actions Min                              -0.999998
evaluation/Num Paths                                 5
evaluation/Average Returns                        2459.71
evaluation/env_infos/final/reward_run Mean           2.17935
evaluation/env_infos/final/reward_run Std            1.71013
evaluation/env_infos/final/reward_run Max            3.97098
evaluation/env_infos/final/reward_run Min           -0.869358
evaluation/env_infos/initial/reward_run Mean         0.0927268
evaluation/env_infos/initial/reward_run Std          0.216276
evaluation/env_infos/initial/reward_run Max          0.364175
evaluation/env_infos/initial/reward_run Min         -0.293026
evaluation/env_infos/reward_run Mean                 2.86344
evaluation/env_infos/reward_run Std                  1.07497
evaluation/env_infos/reward_run Max                  5.22399
evaluation/env_infos/reward_run Min                 -1.71011
evaluation/env_infos/final/reward_ctrl Mean         -0.412468
evaluation/env_infos/final/reward_ctrl Std           0.0678451
evaluation/env_infos/final/reward_ctrl Max          -0.328348
evaluation/env_infos/final/reward_ctrl Min          -0.528033
evaluation/env_infos/initial/reward_ctrl Mean       -0.0765161
evaluation/env_infos/initial/reward_ctrl Std         0.0461541
evaluation/env_infos/initial/reward_ctrl Max        -0.0273439
evaluation/env_infos/initial/reward_ctrl Min        -0.142126
evaluation/env_infos/reward_ctrl Mean               -0.403734
evaluation/env_infos/reward_ctrl Std                 0.106329
evaluation/env_infos/reward_ctrl Max                -0.0273439
evaluation/env_infos/reward_ctrl Min                -0.597022
time/data storing (s)                                0.00730297
time/evaluation sampling (s)                         2.43256
time/exploration sampling (s)                        0.621017
time/logging (s)                                     0.0500916
time/saving (s)                                      0.017496
time/training (s)                                   41.6477
time/epoch (s)                                      44.7762
time/total (s)                                    1887.3
Epoch                                               44
----------------------------------------------  ---------------
2020-07-08 20:30:17.818246 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 45 finished
----------------------------------------------  ---------------
replay_buffer/size                               47000
trainer/QF1 Loss                                     4.59081
trainer/QF2 Loss                                     5.20633
trainer/Policy Loss                                -50.62
trainer/Q1 Predictions Mean                         55.8717
trainer/Q1 Predictions Std                          49.9366
trainer/Q1 Predictions Max                         135.022
trainer/Q1 Predictions Min                           3.37181
trainer/Q2 Predictions Mean                         55.9208
trainer/Q2 Predictions Std                          50.0429
trainer/Q2 Predictions Max                         135.336
trainer/Q2 Predictions Min                           3.61857
trainer/Q Targets Mean                              56.021
trainer/Q Targets Std                               50.1594
trainer/Q Targets Max                              135.808
trainer/Q Targets Min                                2.80211
trainer/Log Pis Mean                                 5.50145
trainer/Log Pis Std                                  5.82679
trainer/Log Pis Max                                 29.7512
trainer/Log Pis Min                                 -7.59502
trainer/Policy mu Mean                               0.0466201
trainer/Policy mu Std                                1.54883
trainer/Policy mu Max                                4.79301
trainer/Policy mu Min                               -8.03525
trainer/Policy log std Mean                         -0.705587
trainer/Policy log std Std                           0.321106
trainer/Policy log std Max                           0.198755
trainer/Policy log std Min                          -2.42632
trainer/Alpha                                        0.0337656
trainer/Alpha Loss                                  -1.68923
exploration/num steps total                      47000
exploration/num paths total                         47
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.56087
exploration/Rewards Std                              0.959204
exploration/Rewards Max                              5.14913
exploration/Rewards Min                             -0.726738
exploration/Returns Mean                          2560.87
exploration/Returns Std                              0
exploration/Returns Max                           2560.87
exploration/Returns Min                           2560.87
exploration/Actions Mean                             0.0217315
exploration/Actions Std                              0.827146
exploration/Actions Max                              0.999989
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       2560.87
exploration/env_infos/final/reward_run Mean          3.04679
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.04679
exploration/env_infos/final/reward_run Min           3.04679
exploration/env_infos/initial/reward_run Mean        0.446532
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.446532
exploration/env_infos/initial/reward_run Min         0.446532
exploration/env_infos/reward_run Mean                2.97166
exploration/env_infos/reward_run Std                 0.936764
exploration/env_infos/reward_run Max                 5.46774
exploration/env_infos/reward_run Min                -0.293914
exploration/env_infos/final/reward_ctrl Mean        -0.588411
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.588411
exploration/env_infos/final/reward_ctrl Min         -0.588411
exploration/env_infos/initial/reward_ctrl Mean      -0.140567
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.140567
exploration/env_infos/initial/reward_ctrl Min       -0.140567
exploration/env_infos/reward_ctrl Mean              -0.410785
exploration/env_infos/reward_ctrl Std                0.105335
exploration/env_infos/reward_ctrl Max               -0.0720699
exploration/env_infos/reward_ctrl Min               -0.596546
evaluation/num steps total                      230000
evaluation/num paths total                         230
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.74125
evaluation/Rewards Std                               0.949769
evaluation/Rewards Max                               5.43911
evaluation/Rewards Min                              -1.34878
evaluation/Returns Mean                           2741.25
evaluation/Returns Std                              67.6439
evaluation/Returns Max                            2830.27
evaluation/Returns Min                            2644.37
evaluation/Actions Mean                              0.0330975
evaluation/Actions Std                               0.820756
evaluation/Actions Max                               0.999967
evaluation/Actions Min                              -0.999999
evaluation/Num Paths                                 5
evaluation/Average Returns                        2741.25
evaluation/env_infos/final/reward_run Mean           3.34318
evaluation/env_infos/final/reward_run Std            0.735333
evaluation/env_infos/final/reward_run Max            4.50527
evaluation/env_infos/final/reward_run Min            2.59075
evaluation/env_infos/initial/reward_run Mean         0.261415
evaluation/env_infos/initial/reward_run Std          0.0651009
evaluation/env_infos/initial/reward_run Max          0.335605
evaluation/env_infos/initial/reward_run Min          0.139647
evaluation/env_infos/reward_run Mean                 3.14609
evaluation/env_infos/reward_run Std                  0.929487
evaluation/env_infos/reward_run Max                  5.87199
evaluation/env_infos/reward_run Min                 -0.869651
evaluation/env_infos/final/reward_ctrl Mean         -0.424079
evaluation/env_infos/final/reward_ctrl Std           0.160898
evaluation/env_infos/final/reward_ctrl Max          -0.13983
evaluation/env_infos/final/reward_ctrl Min          -0.579852
evaluation/env_infos/initial/reward_ctrl Mean       -0.0852891
evaluation/env_infos/initial/reward_ctrl Std         0.0426753
evaluation/env_infos/initial/reward_ctrl Max        -0.0460067
evaluation/env_infos/initial/reward_ctrl Min        -0.164701
evaluation/env_infos/reward_ctrl Mean               -0.404841
evaluation/env_infos/reward_ctrl Std                 0.101781
evaluation/env_infos/reward_ctrl Max                -0.0460067
evaluation/env_infos/reward_ctrl Min                -0.5963
time/data storing (s)                                0.00813923
time/evaluation sampling (s)                         2.74173
time/exploration sampling (s)                        0.718992
time/logging (s)                                     0.0402539
time/saving (s)                                      0.0162832
time/training (s)                                   35.0658
time/epoch (s)                                      38.5912
time/total (s)                                    1925.91
Epoch                                               45
----------------------------------------------  ---------------
2020-07-08 20:30:59.971794 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 46 finished
----------------------------------------------  ---------------
replay_buffer/size                               48000
trainer/QF1 Loss                                     3.84968
trainer/QF2 Loss                                     5.50692
trainer/Policy Loss                                -53.1693
trainer/Q1 Predictions Mean                         59.4809
trainer/Q1 Predictions Std                          51.2438
trainer/Q1 Predictions Max                         139.411
trainer/Q1 Predictions Min                           3.55127
trainer/Q2 Predictions Mean                         59.3609
trainer/Q2 Predictions Std                          51.2406
trainer/Q2 Predictions Max                         141.549
trainer/Q2 Predictions Min                           3.84881
trainer/Q Targets Mean                              59.7782
trainer/Q Targets Std                               51.2759
trainer/Q Targets Max                              140.292
trainer/Q Targets Min                                3.56388
trainer/Log Pis Mean                                 6.47898
trainer/Log Pis Std                                  6.14813
trainer/Log Pis Max                                 31.4653
trainer/Log Pis Min                                 -5.41509
trainer/Policy mu Mean                               0.00937683
trainer/Policy mu Std                                1.64732
trainer/Policy mu Max                                4.74854
trainer/Policy mu Min                               -5.77282
trainer/Policy log std Mean                         -0.693193
trainer/Policy log std Std                           0.277679
trainer/Policy log std Max                           0.190935
trainer/Policy log std Min                          -1.91616
trainer/Alpha                                        0.0346978
trainer/Alpha Loss                                   1.60987
exploration/num steps total                      48000
exploration/num paths total                         48
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.65102
exploration/Rewards Std                              0.863068
exploration/Rewards Max                              4.74918
exploration/Rewards Min                             -0.878403
exploration/Returns Mean                          2651.02
exploration/Returns Std                              0
exploration/Returns Max                           2651.02
exploration/Returns Min                           2651.02
exploration/Actions Mean                             0.0294707
exploration/Actions Std                              0.805115
exploration/Actions Max                              0.999939
exploration/Actions Min                             -0.999998
exploration/Num Paths                                1
exploration/Average Returns                       2651.02
exploration/env_infos/final/reward_run Mean          4.12542
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.12542
exploration/env_infos/final/reward_run Min           4.12542
exploration/env_infos/initial/reward_run Mean        0.145089
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.145089
exploration/env_infos/initial/reward_run Min         0.145089
exploration/env_infos/reward_run Mean                3.04046
exploration/env_infos/reward_run Std                 0.84009
exploration/env_infos/reward_run Max                 5.15236
exploration/env_infos/reward_run Min                -0.391782
exploration/env_infos/final/reward_ctrl Mean        -0.328869
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.328869
exploration/env_infos/final/reward_ctrl Min         -0.328869
exploration/env_infos/initial/reward_ctrl Mean      -0.0687213
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0687213
exploration/env_infos/initial/reward_ctrl Min       -0.0687213
exploration/env_infos/reward_ctrl Mean              -0.389447
exploration/env_infos/reward_ctrl Std                0.100196
exploration/env_infos/reward_ctrl Max               -0.0687213
exploration/env_infos/reward_ctrl Min               -0.59627
evaluation/num steps total                      235000
evaluation/num paths total                         235
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.72048
evaluation/Rewards Std                               0.926508
evaluation/Rewards Max                               5.04688
evaluation/Rewards Min                              -1.4176
evaluation/Returns Mean                           2720.48
evaluation/Returns Std                             108.138
evaluation/Returns Max                            2889.66
evaluation/Returns Min                            2547.97
evaluation/Actions Mean                              0.0165293
evaluation/Actions Std                               0.826889
evaluation/Actions Max                               0.999964
evaluation/Actions Min                              -0.999999
evaluation/Num Paths                                 5
evaluation/Average Returns                        2720.48
evaluation/env_infos/final/reward_run Mean           3.69365
evaluation/env_infos/final/reward_run Std            0.723778
evaluation/env_infos/final/reward_run Max            4.52231
evaluation/env_infos/final/reward_run Min            2.59093
evaluation/env_infos/initial/reward_run Mean         0.236036
evaluation/env_infos/initial/reward_run Std          0.168104
evaluation/env_infos/initial/reward_run Max          0.448321
evaluation/env_infos/initial/reward_run Min         -0.0667312
evaluation/env_infos/reward_run Mean                 3.13089
evaluation/env_infos/reward_run Std                  0.914786
evaluation/env_infos/reward_run Max                  5.39492
evaluation/env_infos/reward_run Min                 -0.985116
evaluation/env_infos/final/reward_ctrl Mean         -0.390701
evaluation/env_infos/final/reward_ctrl Std           0.0592238
evaluation/env_infos/final/reward_ctrl Max          -0.321912
evaluation/env_infos/final/reward_ctrl Min          -0.486984
evaluation/env_infos/initial/reward_ctrl Mean       -0.0683941
evaluation/env_infos/initial/reward_ctrl Std         0.031086
evaluation/env_infos/initial/reward_ctrl Max        -0.032967
evaluation/env_infos/initial/reward_ctrl Min        -0.116506
evaluation/env_infos/reward_ctrl Mean               -0.410411
evaluation/env_infos/reward_ctrl Std                 0.105075
evaluation/env_infos/reward_ctrl Max                -0.032967
evaluation/env_infos/reward_ctrl Min                -0.59664
time/data storing (s)                                0.00669753
time/evaluation sampling (s)                         2.6427
time/exploration sampling (s)                        0.639338
time/logging (s)                                     0.0472104
time/saving (s)                                      0.0179483
time/training (s)                                   38.7897
time/epoch (s)                                      42.1436
time/total (s)                                    1968.06
Epoch                                               46
----------------------------------------------  ---------------
2020-07-08 20:31:42.799318 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 47 finished
----------------------------------------------  ---------------
replay_buffer/size                               49000
trainer/QF1 Loss                                     3.16125
trainer/QF2 Loss                                     3.6705
trainer/Policy Loss                                -61.6456
trainer/Q1 Predictions Mean                         67.4206
trainer/Q1 Predictions Std                          53.678
trainer/Q1 Predictions Max                         148.464
trainer/Q1 Predictions Min                           4.23014
trainer/Q2 Predictions Mean                         67.3748
trainer/Q2 Predictions Std                          53.4804
trainer/Q2 Predictions Max                         148.269
trainer/Q2 Predictions Min                           4.45053
trainer/Q Targets Mean                              67.6412
trainer/Q Targets Std                               53.8195
trainer/Q Targets Max                              146.135
trainer/Q Targets Min                                3.5001
trainer/Log Pis Mean                                 5.99603
trainer/Log Pis Std                                  6.32335
trainer/Log Pis Max                                 30.967
trainer/Log Pis Min                                 -8.69464
trainer/Policy mu Mean                               0.027957
trainer/Policy mu Std                                1.61165
trainer/Policy mu Max                                4.89316
trainer/Policy mu Min                               -5.31793
trainer/Policy log std Mean                         -0.710099
trainer/Policy log std Std                           0.279073
trainer/Policy log std Max                          -0.0546049
trainer/Policy log std Min                          -1.92523
trainer/Alpha                                        0.0352903
trainer/Alpha Loss                                  -0.0132916
exploration/num steps total                      49000
exploration/num paths total                         49
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.18153
exploration/Rewards Std                              1.07793
exploration/Rewards Max                              4.92855
exploration/Rewards Min                             -1.82755
exploration/Returns Mean                          2181.53
exploration/Returns Std                              0
exploration/Returns Max                           2181.53
exploration/Returns Min                           2181.53
exploration/Actions Mean                             0.023426
exploration/Actions Std                              0.824554
exploration/Actions Max                              0.999981
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       2181.53
exploration/env_infos/final/reward_run Mean          2.89827
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.89827
exploration/env_infos/final/reward_run Min           2.89827
exploration/env_infos/initial/reward_run Mean       -0.431255
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.431255
exploration/env_infos/initial/reward_run Min        -0.431255
exploration/env_infos/reward_run Mean                2.58979
exploration/env_infos/reward_run Std                 1.04349
exploration/env_infos/reward_run Max                 5.35538
exploration/env_infos/reward_run Min                -1.48406
exploration/env_infos/final/reward_ctrl Mean        -0.579769
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.579769
exploration/env_infos/final/reward_ctrl Min         -0.579769
exploration/env_infos/initial/reward_ctrl Mean      -0.29727
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.29727
exploration/env_infos/initial/reward_ctrl Min       -0.29727
exploration/env_infos/reward_ctrl Mean              -0.408263
exploration/env_infos/reward_ctrl Std                0.108689
exploration/env_infos/reward_ctrl Max               -0.0922702
exploration/env_infos/reward_ctrl Min               -0.597761
evaluation/num steps total                      240000
evaluation/num paths total                         240
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.21079
evaluation/Rewards Std                               1.28335
evaluation/Rewards Max                               5.16623
evaluation/Rewards Min                              -2.11453
evaluation/Returns Mean                           2210.79
evaluation/Returns Std                             700.383
evaluation/Returns Max                            2880.73
evaluation/Returns Min                             915.457
evaluation/Actions Mean                              0.00794029
evaluation/Actions Std                               0.817362
evaluation/Actions Max                               0.999961
evaluation/Actions Min                              -0.999998
evaluation/Num Paths                                 5
evaluation/Average Returns                        2210.79
evaluation/env_infos/final/reward_run Mean           1.67836
evaluation/env_infos/final/reward_run Std            1.70384
evaluation/env_infos/final/reward_run Max            3.70789
evaluation/env_infos/final/reward_run Min           -0.469049
evaluation/env_infos/initial/reward_run Mean         0.084772
evaluation/env_infos/initial/reward_run Std          0.0957871
evaluation/env_infos/initial/reward_run Max          0.217244
evaluation/env_infos/initial/reward_run Min         -0.0744662
evaluation/env_infos/reward_run Mean                 2.61168
evaluation/env_infos/reward_run Std                  1.28471
evaluation/env_infos/reward_run Max                  5.49955
evaluation/env_infos/reward_run Min                 -1.66361
evaluation/env_infos/final/reward_ctrl Mean         -0.316712
evaluation/env_infos/final/reward_ctrl Std           0.0680064
evaluation/env_infos/final/reward_ctrl Max          -0.201601
evaluation/env_infos/final/reward_ctrl Min          -0.3809
evaluation/env_infos/initial/reward_ctrl Mean       -0.108717
evaluation/env_infos/initial/reward_ctrl Std         0.0781879
evaluation/env_infos/initial/reward_ctrl Max        -0.0144774
evaluation/env_infos/initial/reward_ctrl Min        -0.233554
evaluation/env_infos/reward_ctrl Mean               -0.400886
evaluation/env_infos/reward_ctrl Std                 0.108229
evaluation/env_infos/reward_ctrl Max                -0.0144774
evaluation/env_infos/reward_ctrl Min                -0.597892
time/data storing (s)                                0.00905712
time/evaluation sampling (s)                         3.35515
time/exploration sampling (s)                        0.821018
time/logging (s)                                     0.0429519
time/saving (s)                                      0.0163055
time/training (s)                                   38.5362
time/epoch (s)                                      42.7806
time/total (s)                                    2010.88
Epoch                                               47
----------------------------------------------  ---------------
2020-07-08 20:32:23.966747 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 48 finished
----------------------------------------------  ---------------
replay_buffer/size                               50000
trainer/QF1 Loss                                     3.36308
trainer/QF2 Loss                                     4.67848
trainer/Policy Loss                                -60.9837
trainer/Q1 Predictions Mean                         66.6
trainer/Q1 Predictions Std                          54.4878
trainer/Q1 Predictions Max                         142.138
trainer/Q1 Predictions Min                           1.65333
trainer/Q2 Predictions Mean                         67.0766
trainer/Q2 Predictions Std                          54.7529
trainer/Q2 Predictions Max                         144.376
trainer/Q2 Predictions Min                           2.33291
trainer/Q Targets Mean                              66.4841
trainer/Q Targets Std                               54.255
trainer/Q Targets Max                              143.733
trainer/Q Targets Min                                1.28677
trainer/Log Pis Mean                                 6.0887
trainer/Log Pis Std                                  6.33935
trainer/Log Pis Max                                 42.0099
trainer/Log Pis Min                                 -4.16047
trainer/Policy mu Mean                               0.159272
trainer/Policy mu Std                                1.64007
trainer/Policy mu Max                                5.47224
trainer/Policy mu Min                               -7.04591
trainer/Policy log std Mean                         -0.686535
trainer/Policy log std Std                           0.279618
trainer/Policy log std Max                           0.220109
trainer/Policy log std Min                          -1.98011
trainer/Alpha                                        0.0356818
trainer/Alpha Loss                                   0.295648
exploration/num steps total                      50000
exploration/num paths total                         50
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.45825
exploration/Rewards Std                              0.87127
exploration/Rewards Max                              4.48401
exploration/Rewards Min                             -0.892123
exploration/Returns Mean                          2458.25
exploration/Returns Std                              0
exploration/Returns Max                           2458.25
exploration/Returns Min                           2458.25
exploration/Actions Mean                             0.0493473
exploration/Actions Std                              0.830986
exploration/Actions Max                              0.999992
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       2458.25
exploration/env_infos/final/reward_run Mean          4.09509
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.09509
exploration/env_infos/final/reward_run Min           4.09509
exploration/env_infos/initial/reward_run Mean        0.33751
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.33751
exploration/env_infos/initial/reward_run Min         0.33751
exploration/env_infos/reward_run Mean                2.87403
exploration/env_infos/reward_run Std                 0.860559
exploration/env_infos/reward_run Max                 4.93119
exploration/env_infos/reward_run Min                -0.484308
exploration/env_infos/final/reward_ctrl Mean        -0.42643
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.42643
exploration/env_infos/final/reward_ctrl Min         -0.42643
exploration/env_infos/initial/reward_ctrl Mean      -0.0939254
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0939254
exploration/env_infos/initial/reward_ctrl Min       -0.0939254
exploration/env_infos/reward_ctrl Mean              -0.415784
exploration/env_infos/reward_ctrl Std                0.0922924
exploration/env_infos/reward_ctrl Max               -0.0939254
exploration/env_infos/reward_ctrl Min               -0.592991
evaluation/num steps total                      245000
evaluation/num paths total                         245
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.069
evaluation/Rewards Std                               1.38054
evaluation/Rewards Max                               5.23563
evaluation/Rewards Min                              -2.1045
evaluation/Returns Mean                           2069
evaluation/Returns Std                             878.212
evaluation/Returns Max                            2801.86
evaluation/Returns Min                             457.567
evaluation/Actions Mean                              0.0135149
evaluation/Actions Std                               0.818468
evaluation/Actions Max                               0.99998
evaluation/Actions Min                              -0.999997
evaluation/Num Paths                                 5
evaluation/Average Returns                        2069
evaluation/env_infos/final/reward_run Mean           2.1373
evaluation/env_infos/final/reward_run Std            2.26667
evaluation/env_infos/final/reward_run Max            4.426
evaluation/env_infos/final/reward_run Min           -0.682196
evaluation/env_infos/initial/reward_run Mean         0.149159
evaluation/env_infos/initial/reward_run Std          0.135269
evaluation/env_infos/initial/reward_run Max          0.268032
evaluation/env_infos/initial/reward_run Min         -0.0524746
evaluation/env_infos/reward_run Mean                 2.47105
evaluation/env_infos/reward_run Std                  1.39973
evaluation/env_infos/reward_run Max                  5.64156
evaluation/env_infos/reward_run Min                 -1.7141
evaluation/env_infos/final/reward_ctrl Mean         -0.400895
evaluation/env_infos/final/reward_ctrl Std           0.0632971
evaluation/env_infos/final/reward_ctrl Max          -0.308842
evaluation/env_infos/final/reward_ctrl Min          -0.494016
evaluation/env_infos/initial/reward_ctrl Mean       -0.102498
evaluation/env_infos/initial/reward_ctrl Std         0.0805791
evaluation/env_infos/initial/reward_ctrl Max        -0.0322369
evaluation/env_infos/initial/reward_ctrl Min        -0.247476
evaluation/env_infos/reward_ctrl Mean               -0.402044
evaluation/env_infos/reward_ctrl Std                 0.0986372
evaluation/env_infos/reward_ctrl Max                -0.0322369
evaluation/env_infos/reward_ctrl Min                -0.597229
time/data storing (s)                                0.00723918
time/evaluation sampling (s)                         3.94597
time/exploration sampling (s)                        0.724931
time/logging (s)                                     0.0446735
time/saving (s)                                      0.0170402
time/training (s)                                   36.4091
time/epoch (s)                                      41.1489
time/total (s)                                    2052.05
Epoch                                               48
----------------------------------------------  ---------------
2020-07-08 20:33:08.037794 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 49 finished
----------------------------------------------  ---------------
replay_buffer/size                               51000
trainer/QF1 Loss                                     4.30093
trainer/QF2 Loss                                     5.43639
trainer/Policy Loss                                -62.3125
trainer/Q1 Predictions Mean                         68.2756
trainer/Q1 Predictions Std                          54.5359
trainer/Q1 Predictions Max                         149.276
trainer/Q1 Predictions Min                           3.39348
trainer/Q2 Predictions Mean                         68.1585
trainer/Q2 Predictions Std                          54.4809
trainer/Q2 Predictions Max                         148.889
trainer/Q2 Predictions Min                           3.42392
trainer/Q Targets Mean                              68.0198
trainer/Q Targets Std                               54.5106
trainer/Q Targets Max                              147.419
trainer/Q Targets Min                                2.68441
trainer/Log Pis Mean                                 6.28774
trainer/Log Pis Std                                  6.52966
trainer/Log Pis Max                                 28.2315
trainer/Log Pis Min                                 -7.16104
trainer/Policy mu Mean                               0.0635096
trainer/Policy mu Std                                1.64847
trainer/Policy mu Max                                5.3331
trainer/Policy mu Min                               -6.28825
trainer/Policy log std Mean                         -0.70272
trainer/Policy log std Std                           0.285514
trainer/Policy log std Max                           0.111757
trainer/Policy log std Min                          -2.2332
trainer/Alpha                                        0.0369044
trainer/Alpha Loss                                   0.949382
exploration/num steps total                      51000
exploration/num paths total                         51
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             1.40212
exploration/Rewards Std                              1.51667
exploration/Rewards Max                              5.36002
exploration/Rewards Min                             -2.00845
exploration/Returns Mean                          1402.12
exploration/Returns Std                              0
exploration/Returns Max                           1402.12
exploration/Returns Min                           1402.12
exploration/Actions Mean                             0.00438798
exploration/Actions Std                              0.799656
exploration/Actions Max                              0.999991
exploration/Actions Min                             -0.999999
exploration/Num Paths                                1
exploration/Average Returns                       1402.12
exploration/env_infos/final/reward_run Mean          1.19748
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.19748
exploration/env_infos/final/reward_run Min           1.19748
exploration/env_infos/initial/reward_run Mean        0.0481951
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0481951
exploration/env_infos/initial/reward_run Min         0.0481951
exploration/env_infos/reward_run Mean                1.7858
exploration/env_infos/reward_run Std                 1.54127
exploration/env_infos/reward_run Max                 5.69704
exploration/env_infos/reward_run Min                -1.67354
exploration/env_infos/final/reward_ctrl Mean        -0.436733
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.436733
exploration/env_infos/final/reward_ctrl Min         -0.436733
exploration/env_infos/initial/reward_ctrl Mean      -0.0518699
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0518699
exploration/env_infos/initial/reward_ctrl Min       -0.0518699
exploration/env_infos/reward_ctrl Mean              -0.383681
exploration/env_infos/reward_ctrl Std                0.0991055
exploration/env_infos/reward_ctrl Max               -0.0518699
exploration/env_infos/reward_ctrl Min               -0.595865
evaluation/num steps total                      250000
evaluation/num paths total                         250
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.52499
evaluation/Rewards Std                               1.29763
evaluation/Rewards Max                               5.41681
evaluation/Rewards Min                              -2.18781
evaluation/Returns Mean                           2524.99
evaluation/Returns Std                             622.224
evaluation/Returns Max                            3017.67
evaluation/Returns Min                            1311.96
evaluation/Actions Mean                              0.00860429
evaluation/Actions Std                               0.825402
evaluation/Actions Max                               0.999945
evaluation/Actions Min                              -0.999991
evaluation/Num Paths                                 5
evaluation/Average Returns                        2524.99
evaluation/env_infos/final/reward_run Mean           3.01404
evaluation/env_infos/final/reward_run Std            1.81888
evaluation/env_infos/final/reward_run Max            5.09523
evaluation/env_infos/final/reward_run Min            0.710437
evaluation/env_infos/initial/reward_run Mean         0.0856936
evaluation/env_infos/initial/reward_run Std          0.124835
evaluation/env_infos/initial/reward_run Max          0.213162
evaluation/env_infos/initial/reward_run Min         -0.111513
evaluation/env_infos/reward_run Mean                 2.93381
evaluation/env_infos/reward_run Std                  1.30292
evaluation/env_infos/reward_run Max                  5.76051
evaluation/env_infos/reward_run Min                 -1.80067
evaluation/env_infos/final/reward_ctrl Mean         -0.344847
evaluation/env_infos/final/reward_ctrl Std           0.0320761
evaluation/env_infos/final/reward_ctrl Max          -0.282078
evaluation/env_infos/final/reward_ctrl Min          -0.368785
evaluation/env_infos/initial/reward_ctrl Mean       -0.100274
evaluation/env_infos/initial/reward_ctrl Std         0.0596732
evaluation/env_infos/initial/reward_ctrl Max        -0.0262998
evaluation/env_infos/initial/reward_ctrl Min        -0.176696
evaluation/env_infos/reward_ctrl Mean               -0.408817
evaluation/env_infos/reward_ctrl Std                 0.100755
evaluation/env_infos/reward_ctrl Max                -0.0262998
evaluation/env_infos/reward_ctrl Min                -0.596452
time/data storing (s)                                0.0127056
time/evaluation sampling (s)                         4.11226
time/exploration sampling (s)                        0.833412
time/logging (s)                                     0.0412282
time/saving (s)                                      0.0167634
time/training (s)                                   38.8359
time/epoch (s)                                      43.8523
time/total (s)                                    2096.11
Epoch                                               49
----------------------------------------------  ---------------
2020-07-08 20:34:00.462448 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 50 finished
----------------------------------------------  ---------------
replay_buffer/size                               52000
trainer/QF1 Loss                                     4.64969
trainer/QF2 Loss                                     5.17722
trainer/Policy Loss                                -63.067
trainer/Q1 Predictions Mean                         68.685
trainer/Q1 Predictions Std                          57.1196
trainer/Q1 Predictions Max                         147.576
trainer/Q1 Predictions Min                           3.02623
trainer/Q2 Predictions Mean                         68.7802
trainer/Q2 Predictions Std                          57.1196
trainer/Q2 Predictions Max                         147.188
trainer/Q2 Predictions Min                           3.4349
trainer/Q Targets Mean                              68.8119
trainer/Q Targets Std                               57.5198
trainer/Q Targets Max                              148.643
trainer/Q Targets Min                                1.72883
trainer/Log Pis Mean                                 5.93582
trainer/Log Pis Std                                  5.84229
trainer/Log Pis Max                                 30.6883
trainer/Log Pis Min                                 -5.252
trainer/Policy mu Mean                               0.0073635
trainer/Policy mu Std                                1.57647
trainer/Policy mu Max                                5.32904
trainer/Policy mu Min                               -4.95862
trainer/Policy log std Mean                         -0.74042
trainer/Policy log std Std                           0.294296
trainer/Policy log std Max                           0.00738942
trainer/Policy log std Min                          -2.084
trainer/Alpha                                        0.0363262
trainer/Alpha Loss                                  -0.212784
exploration/num steps total                      52000
exploration/num paths total                         52
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.64818
exploration/Rewards Std                              0.874213
exploration/Rewards Max                              5.08999
exploration/Rewards Min                             -0.700906
exploration/Returns Mean                          2648.18
exploration/Returns Std                              0
exploration/Returns Max                           2648.18
exploration/Returns Min                           2648.18
exploration/Actions Mean                            -0.0233309
exploration/Actions Std                              0.833764
exploration/Actions Max                              0.999995
exploration/Actions Min                             -0.999998
exploration/Num Paths                                1
exploration/Average Returns                       2648.18
exploration/env_infos/final/reward_run Mean          3.34161
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.34161
exploration/env_infos/final/reward_run Min           3.34161
exploration/env_infos/initial/reward_run Mean        0.15155
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.15155
exploration/env_infos/initial/reward_run Min         0.15155
exploration/env_infos/reward_run Mean                3.0656
exploration/env_infos/reward_run Std                 0.858604
exploration/env_infos/reward_run Max                 5.38403
exploration/env_infos/reward_run Min                -0.131784
exploration/env_infos/final/reward_ctrl Mean        -0.453817
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.453817
exploration/env_infos/final/reward_ctrl Min         -0.453817
exploration/env_infos/initial/reward_ctrl Mean      -0.135618
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.135618
exploration/env_infos/initial/reward_ctrl Min       -0.135618
exploration/env_infos/reward_ctrl Mean              -0.417424
exploration/env_infos/reward_ctrl Std                0.101503
exploration/env_infos/reward_ctrl Max               -0.135618
exploration/env_infos/reward_ctrl Min               -0.596794
evaluation/num steps total                      255000
evaluation/num paths total                         255
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.69961
evaluation/Rewards Std                               0.913811
evaluation/Rewards Max                               5.18434
evaluation/Rewards Min                              -1.14287
evaluation/Returns Mean                           2699.61
evaluation/Returns Std                              90.8113
evaluation/Returns Max                            2829.09
evaluation/Returns Min                            2559.97
evaluation/Actions Mean                             -0.0151945
evaluation/Actions Std                               0.837017
evaluation/Actions Max                               0.999961
evaluation/Actions Min                              -0.999993
evaluation/Num Paths                                 5
evaluation/Average Returns                        2699.61
evaluation/env_infos/final/reward_run Mean           3.58582
evaluation/env_infos/final/reward_run Std            0.57465
evaluation/env_infos/final/reward_run Max            4.25379
evaluation/env_infos/final/reward_run Min            2.81019
evaluation/env_infos/initial/reward_run Mean         0.258989
evaluation/env_infos/initial/reward_run Std          0.188721
evaluation/env_infos/initial/reward_run Max          0.521666
evaluation/env_infos/initial/reward_run Min          0.0140534
evaluation/env_infos/reward_run Mean                 3.1201
evaluation/env_infos/reward_run Std                  0.901971
evaluation/env_infos/reward_run Max                  5.56484
evaluation/env_infos/reward_run Min                 -0.718569
evaluation/env_infos/final/reward_ctrl Mean         -0.454084
evaluation/env_infos/final/reward_ctrl Std           0.101279
evaluation/env_infos/final/reward_ctrl Max          -0.293338
evaluation/env_infos/final/reward_ctrl Min          -0.575726
evaluation/env_infos/initial/reward_ctrl Mean       -0.103263
evaluation/env_infos/initial/reward_ctrl Std         0.0295053
evaluation/env_infos/initial/reward_ctrl Max        -0.0492076
evaluation/env_infos/initial/reward_ctrl Min        -0.130068
evaluation/env_infos/reward_ctrl Mean               -0.420497
evaluation/env_infos/reward_ctrl Std                 0.0998003
evaluation/env_infos/reward_ctrl Max                -0.0492076
evaluation/env_infos/reward_ctrl Min                -0.598066
time/data storing (s)                                0.0104599
time/evaluation sampling (s)                         3.73598
time/exploration sampling (s)                        0.752813
time/logging (s)                                     0.0541934
time/saving (s)                                      0.0262044
time/training (s)                                   47.7164
time/epoch (s)                                      52.296
time/total (s)                                    2148.55
Epoch                                               50
----------------------------------------------  ---------------
2020-07-08 20:34:36.820765 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 51 finished
----------------------------------------------  ---------------
replay_buffer/size                               53000
trainer/QF1 Loss                                     4.7001
trainer/QF2 Loss                                     5.4755
trainer/Policy Loss                                -72.2371
trainer/Q1 Predictions Mean                         77.8914
trainer/Q1 Predictions Std                          57.5821
trainer/Q1 Predictions Max                         156.715
trainer/Q1 Predictions Min                           3.04051
trainer/Q2 Predictions Mean                         78.0283
trainer/Q2 Predictions Std                          57.768
trainer/Q2 Predictions Max                         155.66
trainer/Q2 Predictions Min                           2.37322
trainer/Q Targets Mean                              78.0239
trainer/Q Targets Std                               57.8406
trainer/Q Targets Max                              157.067
trainer/Q Targets Min                                1.88471
trainer/Log Pis Mean                                 5.91901
trainer/Log Pis Std                                  5.51279
trainer/Log Pis Max                                 24.4783
trainer/Log Pis Min                                 -5.77078
trainer/Policy mu Mean                              -0.042802
trainer/Policy mu Std                                1.57548
trainer/Policy mu Max                                5.12723
trainer/Policy mu Min                               -6.66879
trainer/Policy log std Mean                         -0.753423
trainer/Policy log std Std                           0.300279
trainer/Policy log std Max                           0.221422
trainer/Policy log std Min                          -1.7266
trainer/Alpha                                        0.0370396
trainer/Alpha Loss                                  -0.266928
exploration/num steps total                      53000
exploration/num paths total                         53
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.64241
exploration/Rewards Std                              0.839583
exploration/Rewards Max                              4.84074
exploration/Rewards Min                             -1.1738
exploration/Returns Mean                          2642.41
exploration/Returns Std                              0
exploration/Returns Max                           2642.41
exploration/Returns Min                           2642.41
exploration/Actions Mean                            -0.0383141
exploration/Actions Std                              0.813791
exploration/Actions Max                              0.999973
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       2642.41
exploration/env_infos/final/reward_run Mean          4.37075
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.37075
exploration/env_infos/final/reward_run Min           4.37075
exploration/env_infos/initial/reward_run Mean        0.146764
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.146764
exploration/env_infos/initial/reward_run Min         0.146764
exploration/env_infos/reward_run Mean                3.04065
exploration/env_infos/reward_run Std                 0.821376
exploration/env_infos/reward_run Max                 5.29477
exploration/env_infos/reward_run Min                -0.759673
exploration/env_infos/final/reward_ctrl Mean        -0.453906
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.453906
exploration/env_infos/final/reward_ctrl Min         -0.453906
exploration/env_infos/initial/reward_ctrl Mean      -0.0348414
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0348414
exploration/env_infos/initial/reward_ctrl Min       -0.0348414
exploration/env_infos/reward_ctrl Mean              -0.398234
exploration/env_infos/reward_ctrl Std                0.105391
exploration/env_infos/reward_ctrl Max               -0.0348414
exploration/env_infos/reward_ctrl Min               -0.593293
evaluation/num steps total                      260000
evaluation/num paths total                         260
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.81255
evaluation/Rewards Std                               0.889519
evaluation/Rewards Max                               5.17738
evaluation/Rewards Min                              -1.17439
evaluation/Returns Mean                           2812.55
evaluation/Returns Std                              70.9322
evaluation/Returns Max                            2905.79
evaluation/Returns Min                            2715.66
evaluation/Actions Mean                             -0.0257679
evaluation/Actions Std                               0.812338
evaluation/Actions Max                               0.999983
evaluation/Actions Min                              -0.999998
evaluation/Num Paths                                 5
evaluation/Average Returns                        2812.55
evaluation/env_infos/final/reward_run Mean           3.74977
evaluation/env_infos/final/reward_run Std            0.645282
evaluation/env_infos/final/reward_run Max            4.37415
evaluation/env_infos/final/reward_run Min            2.52622
evaluation/env_infos/initial/reward_run Mean         0.07398
evaluation/env_infos/initial/reward_run Std          0.0795991
evaluation/env_infos/initial/reward_run Max          0.155794
evaluation/env_infos/initial/reward_run Min         -0.0258886
evaluation/env_infos/reward_run Mean                 3.20889
evaluation/env_infos/reward_run Std                  0.876769
evaluation/env_infos/reward_run Max                  5.54864
evaluation/env_infos/reward_run Min                 -0.667756
evaluation/env_infos/final/reward_ctrl Mean         -0.352502
evaluation/env_infos/final/reward_ctrl Std           0.0943209
evaluation/env_infos/final/reward_ctrl Max          -0.216441
evaluation/env_infos/final/reward_ctrl Min          -0.478465
evaluation/env_infos/initial/reward_ctrl Mean       -0.0704937
evaluation/env_infos/initial/reward_ctrl Std         0.02266
evaluation/env_infos/initial/reward_ctrl Max        -0.0309449
evaluation/env_infos/initial/reward_ctrl Min        -0.0970779
evaluation/env_infos/reward_ctrl Mean               -0.396334
evaluation/env_infos/reward_ctrl Std                 0.0986554
evaluation/env_infos/reward_ctrl Max                -0.0309449
evaluation/env_infos/reward_ctrl Min                -0.591473
time/data storing (s)                                0.00679549
time/evaluation sampling (s)                         3.41596
time/exploration sampling (s)                        0.61934
time/logging (s)                                     0.0423544
time/saving (s)                                      0.0160269
time/training (s)                                   32.1957
time/epoch (s)                                      36.2962
time/total (s)                                    2184.89
Epoch                                               51
----------------------------------------------  ---------------
2020-07-08 20:35:11.043501 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 52 finished
----------------------------------------------  ---------------
replay_buffer/size                               54000
trainer/QF1 Loss                                     6.68179
trainer/QF2 Loss                                     6.39197
trainer/Policy Loss                                -70.1251
trainer/Q1 Predictions Mean                         75.7346
trainer/Q1 Predictions Std                          57.7774
trainer/Q1 Predictions Max                         153.263
trainer/Q1 Predictions Min                           2.55879
trainer/Q2 Predictions Mean                         75.6101
trainer/Q2 Predictions Std                          57.7218
trainer/Q2 Predictions Max                         155.138
trainer/Q2 Predictions Min                           3.06231
trainer/Q Targets Mean                              75.0053
trainer/Q Targets Std                               57.4596
trainer/Q Targets Max                              154.878
trainer/Q Targets Min                                2.29899
trainer/Log Pis Mean                                 5.91188
trainer/Log Pis Std                                  5.54504
trainer/Log Pis Max                                 22.9283
trainer/Log Pis Min                                 -4.86428
trainer/Policy mu Mean                               0.0391413
trainer/Policy mu Std                                1.58083
trainer/Policy mu Max                                4.62838
trainer/Policy mu Min                               -5.42394
trainer/Policy log std Mean                         -0.733247
trainer/Policy log std Std                           0.285599
trainer/Policy log std Max                           0.0263097
trainer/Policy log std Min                          -1.89994
trainer/Alpha                                        0.0389469
trainer/Alpha Loss                                  -0.286013
exploration/num steps total                      54000
exploration/num paths total                         54
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.55276
exploration/Rewards Std                              0.958274
exploration/Rewards Max                              4.76363
exploration/Rewards Min                             -1.09355
exploration/Returns Mean                          2552.76
exploration/Returns Std                              0
exploration/Returns Max                           2552.76
exploration/Returns Min                           2552.76
exploration/Actions Mean                            -0.004624
exploration/Actions Std                              0.814465
exploration/Actions Max                              0.999993
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       2552.76
exploration/env_infos/final/reward_run Mean          2.46013
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.46013
exploration/env_infos/final/reward_run Min           2.46013
exploration/env_infos/initial/reward_run Mean       -0.0448234
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0448234
exploration/env_infos/initial/reward_run Min        -0.0448234
exploration/env_infos/reward_run Mean                2.95078
exploration/env_infos/reward_run Std                 0.947864
exploration/env_infos/reward_run Max                 5.23967
exploration/env_infos/reward_run Min                -0.527208
exploration/env_infos/final/reward_ctrl Mean        -0.572708
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.572708
exploration/env_infos/final/reward_ctrl Min         -0.572708
exploration/env_infos/initial/reward_ctrl Mean      -0.0968018
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0968018
exploration/env_infos/initial/reward_ctrl Min       -0.0968018
exploration/env_infos/reward_ctrl Mean              -0.398025
exploration/env_infos/reward_ctrl Std                0.10001
exploration/env_infos/reward_ctrl Max               -0.0966462
exploration/env_infos/reward_ctrl Min               -0.593178
evaluation/num steps total                      265000
evaluation/num paths total                         265
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.9186
evaluation/Rewards Std                               0.91628
evaluation/Rewards Max                               5.40206
evaluation/Rewards Min                              -1.14482
evaluation/Returns Mean                           2918.6
evaluation/Returns Std                              95.4021
evaluation/Returns Max                            3096.47
evaluation/Returns Min                            2835.98
evaluation/Actions Mean                              0.00647458
evaluation/Actions Std                               0.816122
evaluation/Actions Max                               0.999971
evaluation/Actions Min                              -0.999999
evaluation/Num Paths                                 5
evaluation/Average Returns                        2918.6
evaluation/env_infos/final/reward_run Mean           3.0301
evaluation/env_infos/final/reward_run Std            0.95861
evaluation/env_infos/final/reward_run Max            4.08719
evaluation/env_infos/final/reward_run Min            1.65211
evaluation/env_infos/initial/reward_run Mean         0.275844
evaluation/env_infos/initial/reward_run Std          0.1827
evaluation/env_infos/initial/reward_run Max          0.498393
evaluation/env_infos/initial/reward_run Min          0.0239311
evaluation/env_infos/reward_run Mean                 3.31825
evaluation/env_infos/reward_run Std                  0.905148
evaluation/env_infos/reward_run Max                  5.86072
evaluation/env_infos/reward_run Min                 -0.664288
evaluation/env_infos/final/reward_ctrl Mean         -0.471428
evaluation/env_infos/final/reward_ctrl Std           0.0870172
evaluation/env_infos/final/reward_ctrl Max          -0.312679
evaluation/env_infos/final/reward_ctrl Min          -0.564873
evaluation/env_infos/initial/reward_ctrl Mean       -0.0556464
evaluation/env_infos/initial/reward_ctrl Std         0.0309992
evaluation/env_infos/initial/reward_ctrl Max        -0.0354607
evaluation/env_infos/initial/reward_ctrl Min        -0.116539
evaluation/env_infos/reward_ctrl Mean               -0.399658
evaluation/env_infos/reward_ctrl Std                 0.095845
evaluation/env_infos/reward_ctrl Max                -0.0354607
evaluation/env_infos/reward_ctrl Min                -0.594583
time/data storing (s)                                0.00679553
time/evaluation sampling (s)                         2.389
time/exploration sampling (s)                        0.611925
time/logging (s)                                     0.0403868
time/saving (s)                                      0.0160846
time/training (s)                                   31.0399
time/epoch (s)                                      34.1041
time/total (s)                                    2219.11
Epoch                                               52
----------------------------------------------  ---------------
2020-07-08 20:35:52.048051 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 53 finished
----------------------------------------------  ---------------
replay_buffer/size                               55000
trainer/QF1 Loss                                     4.79697
trainer/QF2 Loss                                     5.32251
trainer/Policy Loss                                -77.2281
trainer/Q1 Predictions Mean                         82.9628
trainer/Q1 Predictions Std                          59.6941
trainer/Q1 Predictions Max                         156.356
trainer/Q1 Predictions Min                           2.15869
trainer/Q2 Predictions Mean                         82.9817
trainer/Q2 Predictions Std                          59.6592
trainer/Q2 Predictions Max                         158.193
trainer/Q2 Predictions Min                          -0.742333
trainer/Q Targets Mean                              83.2067
trainer/Q Targets Std                               59.787
trainer/Q Targets Max                              159.972
trainer/Q Targets Min                                0.579277
trainer/Log Pis Mean                                 6.09852
trainer/Log Pis Std                                  5.62748
trainer/Log Pis Max                                 35.5467
trainer/Log Pis Min                                 -7.59221
trainer/Policy mu Mean                               0.00586684
trainer/Policy mu Std                                1.57296
trainer/Policy mu Max                                5.0364
trainer/Policy mu Min                               -5.36015
trainer/Policy log std Mean                         -0.767882
trainer/Policy log std Std                           0.301943
trainer/Policy log std Max                           0.0341515
trainer/Policy log std Min                          -1.86826
trainer/Alpha                                        0.0392715
trainer/Alpha Loss                                   0.318939
exploration/num steps total                      55000
exploration/num paths total                         55
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.498
exploration/Rewards Std                              0.901717
exploration/Rewards Max                              4.89285
exploration/Rewards Min                             -0.477504
exploration/Returns Mean                          2498
exploration/Returns Std                              0
exploration/Returns Max                           2498
exploration/Returns Min                           2498
exploration/Actions Mean                             0.0074693
exploration/Actions Std                              0.798613
exploration/Actions Max                              0.999953
exploration/Actions Min                             -0.999999
exploration/Num Paths                                1
exploration/Average Returns                       2498
exploration/env_infos/final/reward_run Mean          3.8804
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.8804
exploration/env_infos/final/reward_run Min           3.8804
exploration/env_infos/initial/reward_run Mean        0.134605
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.134605
exploration/env_infos/initial/reward_run Min         0.134605
exploration/env_infos/reward_run Mean                2.8807
exploration/env_infos/reward_run Std                 0.888168
exploration/env_infos/reward_run Max                 5.31709
exploration/env_infos/reward_run Min                -0.0758427
exploration/env_infos/final/reward_ctrl Mean        -0.387475
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.387475
exploration/env_infos/final/reward_ctrl Min         -0.387475
exploration/env_infos/initial/reward_ctrl Mean      -0.090664
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.090664
exploration/env_infos/initial/reward_ctrl Min       -0.090664
exploration/env_infos/reward_ctrl Mean              -0.382703
exploration/env_infos/reward_ctrl Std                0.0970088
exploration/env_infos/reward_ctrl Max               -0.090664
exploration/env_infos/reward_ctrl Min               -0.588202
evaluation/num steps total                      270000
evaluation/num paths total                         270
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.87508
evaluation/Rewards Std                               0.873542
evaluation/Rewards Max                               5.24227
evaluation/Rewards Min                              -0.912568
evaluation/Returns Mean                           2875.08
evaluation/Returns Std                              65.9741
evaluation/Returns Max                            2978.21
evaluation/Returns Min                            2800.9
evaluation/Actions Mean                              0.00495366
evaluation/Actions Std                               0.796287
evaluation/Actions Max                               0.999991
evaluation/Actions Min                              -0.999999
evaluation/Num Paths                                 5
evaluation/Average Returns                        2875.08
evaluation/env_infos/final/reward_run Mean           3.41052
evaluation/env_infos/final/reward_run Std            0.839736
evaluation/env_infos/final/reward_run Max            4.78947
evaluation/env_infos/final/reward_run Min            2.18151
evaluation/env_infos/initial/reward_run Mean         0.231635
evaluation/env_infos/initial/reward_run Std          0.192791
evaluation/env_infos/initial/reward_run Max          0.537775
evaluation/env_infos/initial/reward_run Min         -0.0214491
evaluation/env_infos/reward_run Mean                 3.25554
evaluation/env_infos/reward_run Std                  0.864515
evaluation/env_infos/reward_run Max                  5.72566
evaluation/env_infos/reward_run Min                 -0.36343
evaluation/env_infos/final/reward_ctrl Mean         -0.373256
evaluation/env_infos/final/reward_ctrl Std           0.0572585
evaluation/env_infos/final/reward_ctrl Max          -0.286904
evaluation/env_infos/final/reward_ctrl Min          -0.461943
evaluation/env_infos/initial/reward_ctrl Mean       -0.053099
evaluation/env_infos/initial/reward_ctrl Std         0.0088182
evaluation/env_infos/initial/reward_ctrl Max        -0.038584
evaluation/env_infos/initial/reward_ctrl Min        -0.0661186
evaluation/env_infos/reward_ctrl Mean               -0.380458
evaluation/env_infos/reward_ctrl Std                 0.096401
evaluation/env_infos/reward_ctrl Max                -0.0371508
evaluation/env_infos/reward_ctrl Min                -0.591563
time/data storing (s)                                0.00732034
time/evaluation sampling (s)                         2.47743
time/exploration sampling (s)                        0.604545
time/logging (s)                                     0.0396801
time/saving (s)                                      0.0162731
time/training (s)                                   37.8257
time/epoch (s)                                      40.971
time/total (s)                                    2260.11
Epoch                                               53
----------------------------------------------  ---------------
2020-07-08 20:36:37.159535 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 54 finished
----------------------------------------------  ---------------
replay_buffer/size                               56000
trainer/QF1 Loss                                     6.78333
trainer/QF2 Loss                                     6.25561
trainer/Policy Loss                                -72.7168
trainer/Q1 Predictions Mean                         77.4149
trainer/Q1 Predictions Std                          62.4424
trainer/Q1 Predictions Max                         162.812
trainer/Q1 Predictions Min                           1.37946
trainer/Q2 Predictions Mean                         77.686
trainer/Q2 Predictions Std                          62.7558
trainer/Q2 Predictions Max                         163.397
trainer/Q2 Predictions Min                           2.03745
trainer/Q Targets Mean                              77.4536
trainer/Q Targets Std                               62.8138
trainer/Q Targets Max                              164.272
trainer/Q Targets Min                                0.248431
trainer/Log Pis Mean                                 5.01361
trainer/Log Pis Std                                  5.47451
trainer/Log Pis Max                                 24.1764
trainer/Log Pis Min                                 -7.0914
trainer/Policy mu Mean                              -0.0556134
trainer/Policy mu Std                                1.5422
trainer/Policy mu Max                                4.44727
trainer/Policy mu Min                               -4.83047
trainer/Policy log std Mean                         -0.739355
trainer/Policy log std Std                           0.286329
trainer/Policy log std Max                           0.0355373
trainer/Policy log std Min                          -1.71332
trainer/Alpha                                        0.0394932
trainer/Alpha Loss                                  -3.18768
exploration/num steps total                      56000
exploration/num paths total                         56
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.94186
exploration/Rewards Std                              0.923921
exploration/Rewards Max                              5.07105
exploration/Rewards Min                             -0.98102
exploration/Returns Mean                          2941.86
exploration/Returns Std                              0
exploration/Returns Max                           2941.86
exploration/Returns Min                           2941.86
exploration/Actions Mean                            -0.0104299
exploration/Actions Std                              0.809436
exploration/Actions Max                              0.999964
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       2941.86
exploration/env_infos/final/reward_run Mean          2.73019
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.73019
exploration/env_infos/final/reward_run Min           2.73019
exploration/env_infos/initial/reward_run Mean        0.112827
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.112827
exploration/env_infos/initial/reward_run Min         0.112827
exploration/env_infos/reward_run Mean                3.33503
exploration/env_infos/reward_run Std                 0.909246
exploration/env_infos/reward_run Max                 5.53264
exploration/env_infos/reward_run Min                -0.53003
exploration/env_infos/final/reward_ctrl Mean        -0.526646
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.526646
exploration/env_infos/final/reward_ctrl Min         -0.526646
exploration/env_infos/initial/reward_ctrl Mean      -0.207904
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.207904
exploration/env_infos/initial/reward_ctrl Min       -0.207904
exploration/env_infos/reward_ctrl Mean              -0.393177
exploration/env_infos/reward_ctrl Std                0.0920213
exploration/env_infos/reward_ctrl Max               -0.141217
exploration/env_infos/reward_ctrl Min               -0.592305
evaluation/num steps total                      275000
evaluation/num paths total                         275
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.13044
evaluation/Rewards Std                               0.945406
evaluation/Rewards Max                               5.52802
evaluation/Rewards Min                              -1.29006
evaluation/Returns Mean                           3130.44
evaluation/Returns Std                              75.4717
evaluation/Returns Max                            3243.12
evaluation/Returns Min                            3008.7
evaluation/Actions Mean                             -0.00563565
evaluation/Actions Std                               0.809574
evaluation/Actions Max                               0.999972
evaluation/Actions Min                              -0.999998
evaluation/Num Paths                                 5
evaluation/Average Returns                        3130.44
evaluation/env_infos/final/reward_run Mean           3.65553
evaluation/env_infos/final/reward_run Std            0.663988
evaluation/env_infos/final/reward_run Max            4.95667
evaluation/env_infos/final/reward_run Min            3.07754
evaluation/env_infos/initial/reward_run Mean         0.327769
evaluation/env_infos/initial/reward_run Std          0.126693
evaluation/env_infos/initial/reward_run Max          0.435094
evaluation/env_infos/initial/reward_run Min          0.0845964
evaluation/env_infos/reward_run Mean                 3.5237
evaluation/env_infos/reward_run Std                  0.934189
evaluation/env_infos/reward_run Max                  5.94137
evaluation/env_infos/reward_run Min                 -0.819969
evaluation/env_infos/final/reward_ctrl Mean         -0.373813
evaluation/env_infos/final/reward_ctrl Std           0.0810293
evaluation/env_infos/final/reward_ctrl Max          -0.26367
evaluation/env_infos/final/reward_ctrl Min          -0.499219
evaluation/env_infos/initial/reward_ctrl Mean       -0.0836552
evaluation/env_infos/initial/reward_ctrl Std         0.0144971
evaluation/env_infos/initial/reward_ctrl Max        -0.0581046
evaluation/env_infos/initial/reward_ctrl Min        -0.0996347
evaluation/env_infos/reward_ctrl Mean               -0.393265
evaluation/env_infos/reward_ctrl Std                 0.0927998
evaluation/env_infos/reward_ctrl Max                -0.0581046
evaluation/env_infos/reward_ctrl Min                -0.591587
time/data storing (s)                                0.00700214
time/evaluation sampling (s)                         3.11161
time/exploration sampling (s)                        0.638328
time/logging (s)                                     0.0456665
time/saving (s)                                      0.0159413
time/training (s)                                   41.2079
time/epoch (s)                                      45.0264
time/total (s)                                    2305.22
Epoch                                               54
----------------------------------------------  ---------------
2020-07-08 20:37:22.519899 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 55 finished
----------------------------------------------  ---------------
replay_buffer/size                               57000
trainer/QF1 Loss                                     4.7599
trainer/QF2 Loss                                     5.97595
trainer/Policy Loss                                -80.9304
trainer/Q1 Predictions Mean                         86.6286
trainer/Q1 Predictions Std                          62.4506
trainer/Q1 Predictions Max                         164.153
trainer/Q1 Predictions Min                           2.96743
trainer/Q2 Predictions Mean                         86.7547
trainer/Q2 Predictions Std                          62.5922
trainer/Q2 Predictions Max                         164.867
trainer/Q2 Predictions Min                           3.38189
trainer/Q Targets Mean                              86.6896
trainer/Q Targets Std                               62.3426
trainer/Q Targets Max                              167.38
trainer/Q Targets Min                                2.42353
trainer/Log Pis Mean                                 6.04655
trainer/Log Pis Std                                  5.32282
trainer/Log Pis Max                                 21.8448
trainer/Log Pis Min                                 -7.32141
trainer/Policy mu Mean                               0.0325224
trainer/Policy mu Std                                1.55798
trainer/Policy mu Max                                4.59899
trainer/Policy mu Min                               -4.22938
trainer/Policy log std Mean                         -0.776081
trainer/Policy log std Std                           0.304715
trainer/Policy log std Max                          -0.0328459
trainer/Policy log std Min                          -2.0837
trainer/Alpha                                        0.0406202
trainer/Alpha Loss                                   0.149142
exploration/num steps total                      57000
exploration/num paths total                         57
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.83751
exploration/Rewards Std                              0.974504
exploration/Rewards Max                              5.40745
exploration/Rewards Min                             -0.69689
exploration/Returns Mean                          2837.51
exploration/Returns Std                              0
exploration/Returns Max                           2837.51
exploration/Returns Min                           2837.51
exploration/Actions Mean                             0.0074818
exploration/Actions Std                              0.800767
exploration/Actions Max                              0.999997
exploration/Actions Min                             -0.999992
exploration/Num Paths                                1
exploration/Average Returns                       2837.51
exploration/env_infos/final/reward_run Mean          2.51489
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.51489
exploration/env_infos/final/reward_run Min           2.51489
exploration/env_infos/initial/reward_run Mean        0.244932
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.244932
exploration/env_infos/initial/reward_run Min         0.244932
exploration/env_infos/reward_run Mean                3.22228
exploration/env_infos/reward_run Std                 0.960955
exploration/env_infos/reward_run Max                 5.71535
exploration/env_infos/reward_run Min                -0.347595
exploration/env_infos/final/reward_ctrl Mean        -0.400415
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.400415
exploration/env_infos/final/reward_ctrl Min         -0.400415
exploration/env_infos/initial/reward_ctrl Mean      -0.0622905
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0622905
exploration/env_infos/initial/reward_ctrl Min       -0.0622905
exploration/env_infos/reward_ctrl Mean              -0.38477
exploration/env_infos/reward_ctrl Std                0.0935089
exploration/env_infos/reward_ctrl Max               -0.0622905
exploration/env_infos/reward_ctrl Min               -0.58425
evaluation/num steps total                      280000
evaluation/num paths total                         280
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.93155
evaluation/Rewards Std                               0.983412
evaluation/Rewards Max                               5.51628
evaluation/Rewards Min                              -1.70483
evaluation/Returns Mean                           2931.55
evaluation/Returns Std                              83.1194
evaluation/Returns Max                            3041.53
evaluation/Returns Min                            2805.2
evaluation/Actions Mean                              0.00597009
evaluation/Actions Std                               0.808267
evaluation/Actions Max                               0.999975
evaluation/Actions Min                              -0.999994
evaluation/Num Paths                                 5
evaluation/Average Returns                        2931.55
evaluation/env_infos/final/reward_run Mean           3.263
evaluation/env_infos/final/reward_run Std            0.831078
evaluation/env_infos/final/reward_run Max            4.49699
evaluation/env_infos/final/reward_run Min            2.32475
evaluation/env_infos/initial/reward_run Mean        -0.0867793
evaluation/env_infos/initial/reward_run Std          0.159113
evaluation/env_infos/initial/reward_run Max          0.0825278
evaluation/env_infos/initial/reward_run Min         -0.368839
evaluation/env_infos/reward_run Mean                 3.32355
evaluation/env_infos/reward_run Std                  0.977084
evaluation/env_infos/reward_run Max                  5.91844
evaluation/env_infos/reward_run Min                 -1.22422
evaluation/env_infos/final/reward_ctrl Mean         -0.275139
evaluation/env_infos/final/reward_ctrl Std           0.0939171
evaluation/env_infos/final/reward_ctrl Max          -0.162071
evaluation/env_infos/final/reward_ctrl Min          -0.441735
evaluation/env_infos/initial/reward_ctrl Mean       -0.15383
evaluation/env_infos/initial/reward_ctrl Std         0.0789002
evaluation/env_infos/initial/reward_ctrl Max        -0.0610849
evaluation/env_infos/initial/reward_ctrl Min        -0.281159
evaluation/env_infos/reward_ctrl Mean               -0.391999
evaluation/env_infos/reward_ctrl Std                 0.0932643
evaluation/env_infos/reward_ctrl Max                -0.0610849
evaluation/env_infos/reward_ctrl Min                -0.590986
time/data storing (s)                                0.00695755
time/evaluation sampling (s)                         3.24047
time/exploration sampling (s)                        0.705105
time/logging (s)                                     0.0452178
time/saving (s)                                      0.0179307
time/training (s)                                   41.3284
time/epoch (s)                                      45.3441
time/total (s)                                    2350.58
Epoch                                               55
----------------------------------------------  ---------------
2020-07-08 20:38:13.262947 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 56 finished
----------------------------------------------  ---------------
replay_buffer/size                               58000
trainer/QF1 Loss                                     5.11133
trainer/QF2 Loss                                     6.57041
trainer/Policy Loss                                -72.6912
trainer/Q1 Predictions Mean                         78.1941
trainer/Q1 Predictions Std                          64.4914
trainer/Q1 Predictions Max                         167.488
trainer/Q1 Predictions Min                           3.45861
trainer/Q2 Predictions Mean                         77.9802
trainer/Q2 Predictions Std                          64.4913
trainer/Q2 Predictions Max                         169.909
trainer/Q2 Predictions Min                           2.65218
trainer/Q Targets Mean                              78.4246
trainer/Q Targets Std                               64.6823
trainer/Q Targets Max                              172.518
trainer/Q Targets Min                                3.15771
trainer/Log Pis Mean                                 5.74128
trainer/Log Pis Std                                  5.09759
trainer/Log Pis Max                                 22.1079
trainer/Log Pis Min                                 -3.94282
trainer/Policy mu Mean                               0.0477335
trainer/Policy mu Std                                1.5325
trainer/Policy mu Max                                4.73791
trainer/Policy mu Min                               -3.91896
trainer/Policy log std Mean                         -0.725931
trainer/Policy log std Std                           0.280026
trainer/Policy log std Max                          -0.0801538
trainer/Policy log std Min                          -2.1281
trainer/Alpha                                        0.0416116
trainer/Alpha Loss                                  -0.822624
exploration/num steps total                      58000
exploration/num paths total                         58
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.10197
exploration/Rewards Std                              1.02623
exploration/Rewards Max                              5.15571
exploration/Rewards Min                             -1.24297
exploration/Returns Mean                          3101.97
exploration/Returns Std                              0
exploration/Returns Max                           3101.97
exploration/Returns Min                           3101.97
exploration/Actions Mean                             0.0206505
exploration/Actions Std                              0.81601
exploration/Actions Max                              0.99999
exploration/Actions Min                             -0.999997
exploration/Num Paths                                1
exploration/Average Returns                       3101.97
exploration/env_infos/final/reward_run Mean          4.81445
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.81445
exploration/env_infos/final/reward_run Min           4.81445
exploration/env_infos/initial/reward_run Mean       -0.344759
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.344759
exploration/env_infos/initial/reward_run Min        -0.344759
exploration/env_infos/reward_run Mean                3.50175
exploration/env_infos/reward_run Std                 1.01751
exploration/env_infos/reward_run Max                 5.58737
exploration/env_infos/reward_run Min                -0.809602
exploration/env_infos/final/reward_ctrl Mean        -0.35355
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.35355
exploration/env_infos/final/reward_ctrl Min         -0.35355
exploration/env_infos/initial/reward_ctrl Mean      -0.194975
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.194975
exploration/env_infos/initial/reward_ctrl Min       -0.194975
exploration/env_infos/reward_ctrl Mean              -0.399779
exploration/env_infos/reward_ctrl Std                0.093157
exploration/env_infos/reward_ctrl Max               -0.107494
exploration/env_infos/reward_ctrl Min               -0.593816
evaluation/num steps total                      285000
evaluation/num paths total                         285
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.02293
evaluation/Rewards Std                               1.01815
evaluation/Rewards Max                               5.45978
evaluation/Rewards Min                              -1.26559
evaluation/Returns Mean                           3022.93
evaluation/Returns Std                             139.797
evaluation/Returns Max                            3192.15
evaluation/Returns Min                            2822.19
evaluation/Actions Mean                              0.0203664
evaluation/Actions Std                               0.820146
evaluation/Actions Max                               0.999986
evaluation/Actions Min                              -0.999984
evaluation/Num Paths                                 5
evaluation/Average Returns                        3022.93
evaluation/env_infos/final/reward_run Mean           3.28904
evaluation/env_infos/final/reward_run Std            1.64614
evaluation/env_infos/final/reward_run Max            5.76412
evaluation/env_infos/final/reward_run Min            0.713743
evaluation/env_infos/initial/reward_run Mean         0.0531321
evaluation/env_infos/initial/reward_run Std          0.252587
evaluation/env_infos/initial/reward_run Max          0.47659
evaluation/env_infos/initial/reward_run Min         -0.22888
evaluation/env_infos/reward_run Mean                 3.42677
evaluation/env_infos/reward_run Std                  1.00742
evaluation/env_infos/reward_run Max                  5.86411
evaluation/env_infos/reward_run Min                 -0.7448
evaluation/env_infos/final/reward_ctrl Mean         -0.398712
evaluation/env_infos/final/reward_ctrl Std           0.0334717
evaluation/env_infos/final/reward_ctrl Max          -0.36123
evaluation/env_infos/final/reward_ctrl Min          -0.443821
evaluation/env_infos/initial/reward_ctrl Mean       -0.115653
evaluation/env_infos/initial/reward_ctrl Std         0.0566136
evaluation/env_infos/initial/reward_ctrl Max        -0.0478798
evaluation/env_infos/initial/reward_ctrl Min        -0.191165
evaluation/env_infos/reward_ctrl Mean               -0.403833
evaluation/env_infos/reward_ctrl Std                 0.0911728
evaluation/env_infos/reward_ctrl Max                -0.0478798
evaluation/env_infos/reward_ctrl Min                -0.592473
time/data storing (s)                                0.00671099
time/evaluation sampling (s)                         3.10569
time/exploration sampling (s)                        0.712297
time/logging (s)                                     0.0426855
time/saving (s)                                      0.0165344
time/training (s)                                   46.8384
time/epoch (s)                                      50.7223
time/total (s)                                    2401.32
Epoch                                               56
----------------------------------------------  ---------------
2020-07-08 20:39:02.667897 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 57 finished
----------------------------------------------  --------------
replay_buffer/size                               59000
trainer/QF1 Loss                                     7.86316
trainer/QF2 Loss                                     8.12444
trainer/Policy Loss                                -82.1868
trainer/Q1 Predictions Mean                         88.1425
trainer/Q1 Predictions Std                          65.345
trainer/Q1 Predictions Max                         173.263
trainer/Q1 Predictions Min                           2.71669
trainer/Q2 Predictions Mean                         87.8079
trainer/Q2 Predictions Std                          65.0728
trainer/Q2 Predictions Max                         171.869
trainer/Q2 Predictions Min                           2.88219
trainer/Q Targets Mean                              87.9301
trainer/Q Targets Std                               65.1831
trainer/Q Targets Max                              177.925
trainer/Q Targets Min                                1.64394
trainer/Log Pis Mean                                 6.00843
trainer/Log Pis Std                                  5.37911
trainer/Log Pis Max                                 21.4788
trainer/Log Pis Min                                 -5.5594
trainer/Policy mu Mean                               0.116837
trainer/Policy mu Std                                1.55516
trainer/Policy mu Max                                4.73482
trainer/Policy mu Min                               -5.57031
trainer/Policy log std Mean                         -0.781048
trainer/Policy log std Std                           0.307945
trainer/Policy log std Max                           0.145973
trainer/Policy log std Min                          -1.90931
trainer/Alpha                                        0.042041
trainer/Alpha Loss                                   0.0267244
exploration/num steps total                      59000
exploration/num paths total                         59
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.92887
exploration/Rewards Std                              0.984471
exploration/Rewards Max                              5.37851
exploration/Rewards Min                             -0.992223
exploration/Returns Mean                          2928.87
exploration/Returns Std                              0
exploration/Returns Max                           2928.87
exploration/Returns Min                           2928.87
exploration/Actions Mean                             0.0270818
exploration/Actions Std                              0.810683
exploration/Actions Max                              0.999951
exploration/Actions Min                             -0.999998
exploration/Num Paths                                1
exploration/Average Returns                       2928.87
exploration/env_infos/final/reward_run Mean          3.7864
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.7864
exploration/env_infos/final/reward_run Min           3.7864
exploration/env_infos/initial/reward_run Mean       -0.339968
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.339968
exploration/env_infos/initial/reward_run Min        -0.339968
exploration/env_infos/reward_run Mean                3.32364
exploration/env_infos/reward_run Std                 0.967183
exploration/env_infos/reward_run Max                 5.80343
exploration/env_infos/reward_run Min                -0.527124
exploration/env_infos/final/reward_ctrl Mean        -0.521382
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.521382
exploration/env_infos/final/reward_ctrl Min         -0.521382
exploration/env_infos/initial/reward_ctrl Mean      -0.212529
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.212529
exploration/env_infos/initial/reward_ctrl Min       -0.212529
exploration/env_infos/reward_ctrl Mean              -0.394764
exploration/env_infos/reward_ctrl Std                0.0955006
exploration/env_infos/reward_ctrl Max               -0.105724
exploration/env_infos/reward_ctrl Min               -0.589853
evaluation/num steps total                      290000
evaluation/num paths total                         290
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.788
evaluation/Rewards Std                               1.3722
evaluation/Rewards Max                               5.68353
evaluation/Rewards Min                              -2.02001
evaluation/Returns Mean                           2788
evaluation/Returns Std                             581.459
evaluation/Returns Max                            3185.32
evaluation/Returns Min                            1638.08
evaluation/Actions Mean                              0.0210498
evaluation/Actions Std                               0.808037
evaluation/Actions Max                               0.999985
evaluation/Actions Min                              -0.999998
evaluation/Num Paths                                 5
evaluation/Average Returns                        2788
evaluation/env_infos/final/reward_run Mean           2.00459
evaluation/env_infos/final/reward_run Std            1.42904
evaluation/env_infos/final/reward_run Max            4.13673
evaluation/env_infos/final/reward_run Min            0.266117
evaluation/env_infos/initial/reward_run Mean         0.148815
evaluation/env_infos/initial/reward_run Std          0.27077
evaluation/env_infos/initial/reward_run Max          0.41212
evaluation/env_infos/initial/reward_run Min         -0.347273
evaluation/env_infos/reward_run Mean                 3.18002
evaluation/env_infos/reward_run Std                  1.37338
evaluation/env_infos/reward_run Max                  6.09425
evaluation/env_infos/reward_run Min                 -1.69465
evaluation/env_infos/final/reward_ctrl Mean         -0.467294
evaluation/env_infos/final/reward_ctrl Std           0.0580276
evaluation/env_infos/final/reward_ctrl Max          -0.410749
evaluation/env_infos/final/reward_ctrl Min          -0.572694
evaluation/env_infos/initial/reward_ctrl Mean       -0.108793
evaluation/env_infos/initial/reward_ctrl Std         0.0812127
evaluation/env_infos/initial/reward_ctrl Max        -0.0197669
evaluation/env_infos/initial/reward_ctrl Min        -0.259117
evaluation/env_infos/reward_ctrl Mean               -0.39202
evaluation/env_infos/reward_ctrl Std                 0.0920158
evaluation/env_infos/reward_ctrl Max                -0.0197669
evaluation/env_infos/reward_ctrl Min                -0.589521
time/data storing (s)                                0.0075132
time/evaluation sampling (s)                         2.72976
time/exploration sampling (s)                        0.71508
time/logging (s)                                     0.0727457
time/saving (s)                                      0.024321
time/training (s)                                   45.854
time/epoch (s)                                      49.4035
time/total (s)                                    2450.75
Epoch                                               57
----------------------------------------------  --------------
2020-07-08 20:39:45.338561 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 58 finished
----------------------------------------------  ---------------
replay_buffer/size                               60000
trainer/QF1 Loss                                     6.30508
trainer/QF2 Loss                                     7.77743
trainer/Policy Loss                                -87.2668
trainer/Q1 Predictions Mean                         93.6658
trainer/Q1 Predictions Std                          65.2847
trainer/Q1 Predictions Max                         181.016
trainer/Q1 Predictions Min                           2.70468
trainer/Q2 Predictions Mean                         93.9439
trainer/Q2 Predictions Std                          65.4569
trainer/Q2 Predictions Max                         181.69
trainer/Q2 Predictions Min                           2.88708
trainer/Q Targets Mean                              93.4643
trainer/Q Targets Std                               65.4006
trainer/Q Targets Max                              178.378
trainer/Q Targets Min                                3.08086
trainer/Log Pis Mean                                 6.95057
trainer/Log Pis Std                                  6.14377
trainer/Log Pis Max                                 27.0389
trainer/Log Pis Min                                 -4.12599
trainer/Policy mu Mean                              -0.0408288
trainer/Policy mu Std                                1.68503
trainer/Policy mu Max                                4.90081
trainer/Policy mu Min                               -6.5041
trainer/Policy log std Mean                         -0.746192
trainer/Policy log std Std                           0.304667
trainer/Policy log std Max                           0.436635
trainer/Policy log std Min                          -2.00986
trainer/Alpha                                        0.0430617
trainer/Alpha Loss                                   2.98966
exploration/num steps total                      60000
exploration/num paths total                         60
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.33917
exploration/Rewards Std                              0.979057
exploration/Rewards Max                              5.45459
exploration/Rewards Min                             -0.863814
exploration/Returns Mean                          3339.17
exploration/Returns Std                              0
exploration/Returns Max                           3339.17
exploration/Returns Min                           3339.17
exploration/Actions Mean                             0.00188507
exploration/Actions Std                              0.808824
exploration/Actions Max                              0.999982
exploration/Actions Min                             -0.999998
exploration/Num Paths                                1
exploration/Average Returns                       3339.17
exploration/env_infos/final/reward_run Mean          2.35555
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.35555
exploration/env_infos/final/reward_run Min           2.35555
exploration/env_infos/initial/reward_run Mean        0.0410243
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0410243
exploration/env_infos/initial/reward_run Min         0.0410243
exploration/env_infos/reward_run Mean                3.73169
exploration/env_infos/reward_run Std                 0.964281
exploration/env_infos/reward_run Max                 5.8985
exploration/env_infos/reward_run Min                -0.406553
exploration/env_infos/final/reward_ctrl Mean        -0.528013
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.528013
exploration/env_infos/final/reward_ctrl Min         -0.528013
exploration/env_infos/initial/reward_ctrl Mean      -0.0668232
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0668232
exploration/env_infos/initial/reward_ctrl Min       -0.0668232
exploration/env_infos/reward_ctrl Mean              -0.39252
exploration/env_infos/reward_ctrl Std                0.0911241
exploration/env_infos/reward_ctrl Max               -0.0668232
exploration/env_infos/reward_ctrl Min               -0.579659
evaluation/num steps total                      295000
evaluation/num paths total                         295
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.2338
evaluation/Rewards Std                               1.04316
evaluation/Rewards Max                               5.72283
evaluation/Rewards Min                              -1.66323
evaluation/Returns Mean                           3233.8
evaluation/Returns Std                              66.7396
evaluation/Returns Max                            3331.43
evaluation/Returns Min                            3146.97
evaluation/Actions Mean                             -0.00540643
evaluation/Actions Std                               0.814416
evaluation/Actions Max                               0.999991
evaluation/Actions Min                              -0.999999
evaluation/Num Paths                                 5
evaluation/Average Returns                        3233.8
evaluation/env_infos/final/reward_run Mean           2.65499
evaluation/env_infos/final/reward_run Std            0.560912
evaluation/env_infos/final/reward_run Max            3.17026
evaluation/env_infos/final/reward_run Min            1.58871
evaluation/env_infos/initial/reward_run Mean         0.13402
evaluation/env_infos/initial/reward_run Std          0.300241
evaluation/env_infos/initial/reward_run Max          0.547869
evaluation/env_infos/initial/reward_run Min         -0.351622
evaluation/env_infos/reward_run Mean                 3.63179
evaluation/env_infos/reward_run Std                  1.02512
evaluation/env_infos/reward_run Max                  6.21162
evaluation/env_infos/reward_run Min                 -1.20675
evaluation/env_infos/final/reward_ctrl Mean         -0.44608
evaluation/env_infos/final/reward_ctrl Std           0.0858569
evaluation/env_infos/final/reward_ctrl Max          -0.303063
evaluation/env_infos/final/reward_ctrl Min          -0.573814
evaluation/env_infos/initial/reward_ctrl Mean       -0.0998628
evaluation/env_infos/initial/reward_ctrl Std         0.0778983
evaluation/env_infos/initial/reward_ctrl Max        -0.0387664
evaluation/env_infos/initial/reward_ctrl Min        -0.25254
evaluation/env_infos/reward_ctrl Mean               -0.397982
evaluation/env_infos/reward_ctrl Std                 0.0904476
evaluation/env_infos/reward_ctrl Max                -0.0387664
evaluation/env_infos/reward_ctrl Min                -0.594184
time/data storing (s)                                0.00944837
time/evaluation sampling (s)                         2.92776
time/exploration sampling (s)                        0.801071
time/logging (s)                                     0.0435821
time/saving (s)                                      0.0167138
time/training (s)                                   38.6421
time/epoch (s)                                      42.4407
time/total (s)                                    2493.38
Epoch                                               58
----------------------------------------------  ---------------
2020-07-08 20:40:36.533083 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 59 finished
----------------------------------------------  ---------------
replay_buffer/size                               61000
trainer/QF1 Loss                                     5.80523
trainer/QF2 Loss                                     6.13745
trainer/Policy Loss                                -85.9609
trainer/Q1 Predictions Mean                         91.0601
trainer/Q1 Predictions Std                          68.2701
trainer/Q1 Predictions Max                         184.503
trainer/Q1 Predictions Min                           3.88418
trainer/Q2 Predictions Mean                         90.964
trainer/Q2 Predictions Std                          68.3417
trainer/Q2 Predictions Max                         183.144
trainer/Q2 Predictions Min                           3.62183
trainer/Q Targets Mean                              90.5694
trainer/Q Targets Std                               68.2611
trainer/Q Targets Max                              188.263
trainer/Q Targets Min                                2.74165
trainer/Log Pis Mean                                 5.36604
trainer/Log Pis Std                                  5.09742
trainer/Log Pis Max                                 19.9097
trainer/Log Pis Min                                 -5.92463
trainer/Policy mu Mean                              -0.0432525
trainer/Policy mu Std                                1.52557
trainer/Policy mu Max                                4.97763
trainer/Policy mu Min                               -4.10434
trainer/Policy log std Mean                         -0.752516
trainer/Policy log std Std                           0.301116
trainer/Policy log std Max                           0.185038
trainer/Policy log std Min                          -1.87676
trainer/Alpha                                        0.0435356
trainer/Alpha Loss                                  -1.98693
exploration/num steps total                      61000
exploration/num paths total                         61
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.17026
exploration/Rewards Std                              0.925891
exploration/Rewards Max                              5.27427
exploration/Rewards Min                             -0.607729
exploration/Returns Mean                          3170.26
exploration/Returns Std                              0
exploration/Returns Max                           3170.26
exploration/Returns Min                           3170.26
exploration/Actions Mean                             0.00844706
exploration/Actions Std                              0.808154
exploration/Actions Max                              0.999977
exploration/Actions Min                             -0.999983
exploration/Num Paths                                1
exploration/Average Returns                       3170.26
exploration/env_infos/final/reward_run Mean          3.14496
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.14496
exploration/env_infos/final/reward_run Min           3.14496
exploration/env_infos/initial/reward_run Mean        0.216549
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.216549
exploration/env_infos/initial/reward_run Min         0.216549
exploration/env_infos/reward_run Mean                3.56217
exploration/env_infos/reward_run Std                 0.906672
exploration/env_infos/reward_run Max                 5.69177
exploration/env_infos/reward_run Min                -0.183914
exploration/env_infos/final/reward_ctrl Mean        -0.329137
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.329137
exploration/env_infos/final/reward_ctrl Min         -0.329137
exploration/env_infos/initial/reward_ctrl Mean      -0.0622111
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0622111
exploration/env_infos/initial/reward_ctrl Min       -0.0622111
exploration/env_infos/reward_ctrl Mean              -0.391911
exploration/env_infos/reward_ctrl Std                0.0953489
exploration/env_infos/reward_ctrl Max               -0.0622111
exploration/env_infos/reward_ctrl Min               -0.590763
evaluation/num steps total                      300000
evaluation/num paths total                         300
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.24148
evaluation/Rewards Std                               0.986117
evaluation/Rewards Max                               5.31149
evaluation/Rewards Min                              -1.32394
evaluation/Returns Mean                           3241.48
evaluation/Returns Std                              88.525
evaluation/Returns Max                            3354.21
evaluation/Returns Min                            3125.78
evaluation/Actions Mean                              0.0137194
evaluation/Actions Std                               0.822103
evaluation/Actions Max                               0.999977
evaluation/Actions Min                              -0.999991
evaluation/Num Paths                                 5
evaluation/Average Returns                        3241.48
evaluation/env_infos/final/reward_run Mean           3.64292
evaluation/env_infos/final/reward_run Std            0.855402
evaluation/env_infos/final/reward_run Max            4.93217
evaluation/env_infos/final/reward_run Min            2.75053
evaluation/env_infos/initial/reward_run Mean        -0.0332933
evaluation/env_infos/initial/reward_run Std          0.237594
evaluation/env_infos/initial/reward_run Max          0.34102
evaluation/env_infos/initial/reward_run Min         -0.402392
evaluation/env_infos/reward_run Mean                 3.64711
evaluation/env_infos/reward_run Std                  0.967899
evaluation/env_infos/reward_run Max                  5.70164
evaluation/env_infos/reward_run Min                 -0.792078
evaluation/env_infos/final/reward_ctrl Mean         -0.403918
evaluation/env_infos/final/reward_ctrl Std           0.109272
evaluation/env_infos/final/reward_ctrl Max          -0.250459
evaluation/env_infos/final/reward_ctrl Min          -0.547726
evaluation/env_infos/initial/reward_ctrl Mean       -0.121769
evaluation/env_infos/initial/reward_ctrl Std         0.0986704
evaluation/env_infos/initial/reward_ctrl Max        -0.0515374
evaluation/env_infos/initial/reward_ctrl Min        -0.314953
evaluation/env_infos/reward_ctrl Mean               -0.405625
evaluation/env_infos/reward_ctrl Std                 0.0900737
evaluation/env_infos/reward_ctrl Max                -0.0515374
evaluation/env_infos/reward_ctrl Min                -0.592847
time/data storing (s)                                0.0143835
time/evaluation sampling (s)                         4.12474
time/exploration sampling (s)                        0.758275
time/logging (s)                                     0.0417945
time/saving (s)                                      0.0205996
time/training (s)                                   46.1222
time/epoch (s)                                      51.082
time/total (s)                                    2544.57
Epoch                                               59
----------------------------------------------  ---------------
2020-07-08 20:41:22.215993 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 60 finished
----------------------------------------------  ---------------
replay_buffer/size                               62000
trainer/QF1 Loss                                     5.4604
trainer/QF2 Loss                                     6.73656
trainer/Policy Loss                                -95.8275
trainer/Q1 Predictions Mean                        101.31
trainer/Q1 Predictions Std                          68.461
trainer/Q1 Predictions Max                         180.423
trainer/Q1 Predictions Min                           2.71506
trainer/Q2 Predictions Mean                        101.02
trainer/Q2 Predictions Std                          68.295
trainer/Q2 Predictions Max                         181.569
trainer/Q2 Predictions Min                           2.05549
trainer/Q Targets Mean                             101.072
trainer/Q Targets Std                               68.3188
trainer/Q Targets Max                              182.437
trainer/Q Targets Min                                0.91447
trainer/Log Pis Mean                                 5.77954
trainer/Log Pis Std                                  5.68467
trainer/Log Pis Max                                 24.2173
trainer/Log Pis Min                                 -6.29666
trainer/Policy mu Mean                               0.0554164
trainer/Policy mu Std                                1.53709
trainer/Policy mu Max                                5.06041
trainer/Policy mu Min                               -4.51767
trainer/Policy log std Mean                         -0.796135
trainer/Policy log std Std                           0.322821
trainer/Policy log std Max                           0.113286
trainer/Policy log std Min                          -2.0732
trainer/Alpha                                        0.0439744
trainer/Alpha Loss                                  -0.688779
exploration/num steps total                      62000
exploration/num paths total                         62
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.18899
exploration/Rewards Std                              0.984462
exploration/Rewards Max                              5.43529
exploration/Rewards Min                             -0.984348
exploration/Returns Mean                          3188.99
exploration/Returns Std                              0
exploration/Returns Max                           3188.99
exploration/Returns Min                           3188.99
exploration/Actions Mean                            -0.013036
exploration/Actions Std                              0.826113
exploration/Actions Max                              0.999979
exploration/Actions Min                             -0.999999
exploration/Num Paths                                1
exploration/Average Returns                       3188.99
exploration/env_infos/final/reward_run Mean          4.30131
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.30131
exploration/env_infos/final/reward_run Min           4.30131
exploration/env_infos/initial/reward_run Mean       -0.118798
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.118798
exploration/env_infos/initial/reward_run Min        -0.118798
exploration/env_infos/reward_run Mean                3.59857
exploration/env_infos/reward_run Std                 0.972736
exploration/env_infos/reward_run Max                 5.80201
exploration/env_infos/reward_run Min                -0.608627
exploration/env_infos/final/reward_ctrl Mean        -0.427035
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.427035
exploration/env_infos/final/reward_ctrl Min         -0.427035
exploration/env_infos/initial/reward_ctrl Mean      -0.0604773
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0604773
exploration/env_infos/initial/reward_ctrl Min       -0.0604773
exploration/env_infos/reward_ctrl Mean              -0.40958
exploration/env_infos/reward_ctrl Std                0.0923198
exploration/env_infos/reward_ctrl Max               -0.0604773
exploration/env_infos/reward_ctrl Min               -0.595155
evaluation/num steps total                      305000
evaluation/num paths total                         305
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.257
evaluation/Rewards Std                               1.03394
evaluation/Rewards Max                               5.70509
evaluation/Rewards Min                              -1.44612
evaluation/Returns Mean                           3257
evaluation/Returns Std                              49.1634
evaluation/Returns Max                            3348.95
evaluation/Returns Min                            3211.04
evaluation/Actions Mean                             -0.0115561
evaluation/Actions Std                               0.834704
evaluation/Actions Max                               0.999988
evaluation/Actions Min                              -0.999993
evaluation/Num Paths                                 5
evaluation/Average Returns                        3257
evaluation/env_infos/final/reward_run Mean           2.71453
evaluation/env_infos/final/reward_run Std            0.78728
evaluation/env_infos/final/reward_run Max            3.76846
evaluation/env_infos/final/reward_run Min            1.57623
evaluation/env_infos/initial/reward_run Mean         0.00978602
evaluation/env_infos/initial/reward_run Std          0.310222
evaluation/env_infos/initial/reward_run Max          0.387099
evaluation/env_infos/initial/reward_run Min         -0.384471
evaluation/env_infos/reward_run Mean                 3.67512
evaluation/env_infos/reward_run Std                  1.02523
evaluation/env_infos/reward_run Max                  6.21695
evaluation/env_infos/reward_run Min                 -1.00501
evaluation/env_infos/final/reward_ctrl Mean         -0.473191
evaluation/env_infos/final/reward_ctrl Std           0.0630676
evaluation/env_infos/final/reward_ctrl Max          -0.359398
evaluation/env_infos/final/reward_ctrl Min          -0.535505
evaluation/env_infos/initial/reward_ctrl Mean       -0.117451
evaluation/env_infos/initial/reward_ctrl Std         0.084066
evaluation/env_infos/initial/reward_ctrl Max        -0.0445198
evaluation/env_infos/initial/reward_ctrl Min        -0.255526
evaluation/env_infos/reward_ctrl Mean               -0.418119
evaluation/env_infos/reward_ctrl Std                 0.0928265
evaluation/env_infos/reward_ctrl Max                -0.0317467
evaluation/env_infos/reward_ctrl Min                -0.595087
time/data storing (s)                                0.00805796
time/evaluation sampling (s)                         4.32675
time/exploration sampling (s)                        0.671881
time/logging (s)                                     0.0463117
time/saving (s)                                      0.0167785
time/training (s)                                   40.6017
time/epoch (s)                                      45.6715
time/total (s)                                    2590.25
Epoch                                               60
----------------------------------------------  ---------------
2020-07-08 20:42:01.169084 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 61 finished
----------------------------------------------  ---------------
replay_buffer/size                               63000
trainer/QF1 Loss                                     4.90309
trainer/QF2 Loss                                     5.75405
trainer/Policy Loss                                -93.902
trainer/Q1 Predictions Mean                         99.6586
trainer/Q1 Predictions Std                          70.044
trainer/Q1 Predictions Max                         189.358
trainer/Q1 Predictions Min                           2.63078
trainer/Q2 Predictions Mean                         99.5778
trainer/Q2 Predictions Std                          69.9973
trainer/Q2 Predictions Max                         187.702
trainer/Q2 Predictions Min                           2.64249
trainer/Q Targets Mean                              99.8484
trainer/Q Targets Std                               69.9786
trainer/Q Targets Max                              189.544
trainer/Q Targets Min                                2.40992
trainer/Log Pis Mean                                 6.10171
trainer/Log Pis Std                                  5.91962
trainer/Log Pis Max                                 27.2274
trainer/Log Pis Min                                 -4.20018
trainer/Policy mu Mean                               0.0108287
trainer/Policy mu Std                                1.597
trainer/Policy mu Max                                3.98404
trainer/Policy mu Min                               -5.80008
trainer/Policy log std Mean                         -0.723078
trainer/Policy log std Std                           0.315162
trainer/Policy log std Max                           0.0929185
trainer/Policy log std Min                          -2.39569
trainer/Alpha                                        0.0454455
trainer/Alpha Loss                                   0.31439
exploration/num steps total                      63000
exploration/num paths total                         63
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.06179
exploration/Rewards Std                              1.07784
exploration/Rewards Max                              5.93654
exploration/Rewards Min                             -1.10121
exploration/Returns Mean                          3061.79
exploration/Returns Std                              0
exploration/Returns Max                           3061.79
exploration/Returns Min                           3061.79
exploration/Actions Mean                            -0.0126287
exploration/Actions Std                              0.82935
exploration/Actions Max                              1
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       3061.79
exploration/env_infos/final/reward_run Mean          3.78088
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.78088
exploration/env_infos/final/reward_run Min           3.78088
exploration/env_infos/initial/reward_run Mean       -0.262336
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.262336
exploration/env_infos/initial/reward_run Min        -0.262336
exploration/env_infos/reward_run Mean                3.47457
exploration/env_infos/reward_run Std                 1.07253
exploration/env_infos/reward_run Max                 6.30031
exploration/env_infos/reward_run Min                -0.590077
exploration/env_infos/final/reward_ctrl Mean        -0.431422
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.431422
exploration/env_infos/final/reward_ctrl Min         -0.431422
exploration/env_infos/initial/reward_ctrl Mean      -0.307954
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.307954
exploration/env_infos/initial/reward_ctrl Min       -0.307954
exploration/env_infos/reward_ctrl Mean              -0.412788
exploration/env_infos/reward_ctrl Std                0.0937104
exploration/env_infos/reward_ctrl Max               -0.0880069
exploration/env_infos/reward_ctrl Min               -0.585251
evaluation/num steps total                      310000
evaluation/num paths total                         310
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.30855
evaluation/Rewards Std                               1.05508
evaluation/Rewards Max                               5.62675
evaluation/Rewards Min                              -1.38726
evaluation/Returns Mean                           3308.55
evaluation/Returns Std                              65.8729
evaluation/Returns Max                            3405.56
evaluation/Returns Min                            3221.34
evaluation/Actions Mean                             -0.00405341
evaluation/Actions Std                               0.836966
evaluation/Actions Max                               0.999992
evaluation/Actions Min                              -0.999998
evaluation/Num Paths                                 5
evaluation/Average Returns                        3308.55
evaluation/env_infos/final/reward_run Mean           4.17298
evaluation/env_infos/final/reward_run Std            0.409969
evaluation/env_infos/final/reward_run Max            4.7046
evaluation/env_infos/final/reward_run Min            3.47178
evaluation/env_infos/initial/reward_run Mean         0.289071
evaluation/env_infos/initial/reward_run Std          0.20195
evaluation/env_infos/initial/reward_run Max          0.520256
evaluation/env_infos/initial/reward_run Min         -0.0346432
evaluation/env_infos/reward_run Mean                 3.72887
evaluation/env_infos/reward_run Std                  1.04745
evaluation/env_infos/reward_run Max                  5.99444
evaluation/env_infos/reward_run Min                 -0.853877
evaluation/env_infos/final/reward_ctrl Mean         -0.433592
evaluation/env_infos/final/reward_ctrl Std           0.108747
evaluation/env_infos/final/reward_ctrl Max          -0.2281
evaluation/env_infos/final/reward_ctrl Min          -0.531399
evaluation/env_infos/initial/reward_ctrl Mean       -0.0596968
evaluation/env_infos/initial/reward_ctrl Std         0.0257778
evaluation/env_infos/initial/reward_ctrl Max        -0.0317541
evaluation/env_infos/initial/reward_ctrl Min        -0.0999733
evaluation/env_infos/reward_ctrl Mean               -0.420317
evaluation/env_infos/reward_ctrl Std                 0.0933064
evaluation/env_infos/reward_ctrl Max                -0.0317541
evaluation/env_infos/reward_ctrl Min                -0.593914
time/data storing (s)                                0.00692015
time/evaluation sampling (s)                         2.87513
time/exploration sampling (s)                        0.660069
time/logging (s)                                     0.0479223
time/saving (s)                                      0.0306502
time/training (s)                                   35.2021
time/epoch (s)                                      38.8228
time/total (s)                                    2629.09
Epoch                                               61
----------------------------------------------  ---------------
2020-07-08 21:16:24.545943 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 62 finished
----------------------------------------------  --------------
replay_buffer/size                               64000
trainer/QF1 Loss                                     7.1493
trainer/QF2 Loss                                     8.59387
trainer/Policy Loss                                -97.6115
trainer/Q1 Predictions Mean                        102.69
trainer/Q1 Predictions Std                          70.7538
trainer/Q1 Predictions Max                         188.463
trainer/Q1 Predictions Min                           3.03792
trainer/Q2 Predictions Mean                        103.137
trainer/Q2 Predictions Std                          70.9139
trainer/Q2 Predictions Max                         190.642
trainer/Q2 Predictions Min                           2.0737
trainer/Q Targets Mean                             102.694
trainer/Q Targets Std                               70.7021
trainer/Q Targets Max                              192.983
trainer/Q Targets Min                                3.66185
trainer/Log Pis Mean                                 5.54171
trainer/Log Pis Std                                  5.56126
trainer/Log Pis Max                                 30.1561
trainer/Log Pis Min                                 -3.59735
trainer/Policy mu Mean                               0.102602
trainer/Policy mu Std                                1.52757
trainer/Policy mu Max                                4.74693
trainer/Policy mu Min                               -5.60427
trainer/Policy log std Mean                         -0.760476
trainer/Policy log std Std                           0.308691
trainer/Policy log std Max                          -0.0131697
trainer/Policy log std Min                          -1.88825
trainer/Alpha                                        0.0465962
trainer/Alpha Loss                                  -1.40516
exploration/num steps total                      64000
exploration/num paths total                         64
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.08994
exploration/Rewards Std                              0.912673
exploration/Rewards Max                              5.32859
exploration/Rewards Min                             -0.963576
exploration/Returns Mean                          3089.94
exploration/Returns Std                              0
exploration/Returns Max                           3089.94
exploration/Returns Min                           3089.94
exploration/Actions Mean                             0.0539825
exploration/Actions Std                              0.796855
exploration/Actions Max                              0.999975
exploration/Actions Min                             -0.999987
exploration/Num Paths                                1
exploration/Average Returns                       3089.94
exploration/env_infos/final/reward_run Mean          4.77877
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.77877
exploration/env_infos/final/reward_run Min           4.77877
exploration/env_infos/initial/reward_run Mean        0.233949
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.233949
exploration/env_infos/initial/reward_run Min         0.233949
exploration/env_infos/reward_run Mean                3.47267
exploration/env_infos/reward_run Std                 0.896206
exploration/env_infos/reward_run Max                 5.64782
exploration/env_infos/reward_run Min                -0.525476
exploration/env_infos/final/reward_ctrl Mean        -0.338384
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.338384
exploration/env_infos/final/reward_ctrl Min         -0.338384
exploration/env_infos/initial/reward_ctrl Mean      -0.0616049
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0616049
exploration/env_infos/initial/reward_ctrl Min       -0.0616049
exploration/env_infos/reward_ctrl Mean              -0.382735
exploration/env_infos/reward_ctrl Std                0.0946118
exploration/env_infos/reward_ctrl Max               -0.0616049
exploration/env_infos/reward_ctrl Min               -0.58948
evaluation/num steps total                      315000
evaluation/num paths total                         315
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.17385
evaluation/Rewards Std                               0.997938
evaluation/Rewards Max                               5.72247
evaluation/Rewards Min                              -1.35476
evaluation/Returns Mean                           3173.85
evaluation/Returns Std                              50.9988
evaluation/Returns Max                            3228.64
evaluation/Returns Min                            3089.48
evaluation/Actions Mean                              0.0507596
evaluation/Actions Std                               0.809014
evaluation/Actions Max                               0.999963
evaluation/Actions Min                              -0.999999
evaluation/Num Paths                                 5
evaluation/Average Returns                        3173.85
evaluation/env_infos/final/reward_run Mean           3.85492
evaluation/env_infos/final/reward_run Std            0.700902
evaluation/env_infos/final/reward_run Max            5.08473
evaluation/env_infos/final/reward_run Min            3.17994
evaluation/env_infos/initial/reward_run Mean         0.237967
evaluation/env_infos/initial/reward_run Std          0.328342
evaluation/env_infos/initial/reward_run Max          0.691181
evaluation/env_infos/initial/reward_run Min         -0.236605
evaluation/env_infos/reward_run Mean                 3.5681
evaluation/env_infos/reward_run Std                  0.985591
evaluation/env_infos/reward_run Max                  6.24053
evaluation/env_infos/reward_run Min                 -0.955486
evaluation/env_infos/final/reward_ctrl Mean         -0.340113
evaluation/env_infos/final/reward_ctrl Std           0.0864638
evaluation/env_infos/final/reward_ctrl Max          -0.214072
evaluation/env_infos/final/reward_ctrl Min          -0.476227
evaluation/env_infos/initial/reward_ctrl Mean       -0.0720691
evaluation/env_infos/initial/reward_ctrl Std         0.02452
evaluation/env_infos/initial/reward_ctrl Max        -0.0471222
evaluation/env_infos/initial/reward_ctrl Min        -0.114127
evaluation/env_infos/reward_ctrl Mean               -0.394248
evaluation/env_infos/reward_ctrl Std                 0.0944751
evaluation/env_infos/reward_ctrl Max                -0.0471222
evaluation/env_infos/reward_ctrl Min                -0.5897
time/data storing (s)                                0.0109368
time/evaluation sampling (s)                        10.4166
time/exploration sampling (s)                        0.945211
time/logging (s)                                     0.0414534
time/saving (s)                                      0.0181702
time/training (s)                                   33.2532
time/epoch (s)                                      44.6855
time/total (s)                                    2674.15
Epoch                                               62
----------------------------------------------  --------------
2020-07-08 21:31:32.333863 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 63 finished
----------------------------------------------  ---------------
replay_buffer/size                               65000
trainer/QF1 Loss                                     4.76432
trainer/QF2 Loss                                     6.01742
trainer/Policy Loss                               -102.439
trainer/Q1 Predictions Mean                        108.545
trainer/Q1 Predictions Std                          71.7349
trainer/Q1 Predictions Max                         194.2
trainer/Q1 Predictions Min                           2.98693
trainer/Q2 Predictions Mean                        108.385
trainer/Q2 Predictions Std                          71.7851
trainer/Q2 Predictions Max                         191.368
trainer/Q2 Predictions Min                           3.21727
trainer/Q Targets Mean                             108.245
trainer/Q Targets Std                               71.6175
trainer/Q Targets Max                              194.488
trainer/Q Targets Min                                2.78232
trainer/Log Pis Mean                                 6.51445
trainer/Log Pis Std                                  5.95422
trainer/Log Pis Max                                 28.1944
trainer/Log Pis Min                                 -5.30274
trainer/Policy mu Mean                               0.0557378
trainer/Policy mu Std                                1.62166
trainer/Policy mu Max                                8.82981
trainer/Policy mu Min                               -4.92768
trainer/Policy log std Mean                         -0.768511
trainer/Policy log std Std                           0.317996
trainer/Policy log std Max                           0.313734
trainer/Policy log std Min                          -1.91438
trainer/Alpha                                        0.0478848
trainer/Alpha Loss                                   1.56347
exploration/num steps total                      65000
exploration/num paths total                         65
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.21059
exploration/Rewards Std                              0.996536
exploration/Rewards Max                              5.38867
exploration/Rewards Min                             -1.14015
exploration/Returns Mean                          3210.59
exploration/Returns Std                              0
exploration/Returns Max                           3210.59
exploration/Returns Min                           3210.59
exploration/Actions Mean                             0.0129124
exploration/Actions Std                              0.831681
exploration/Actions Max                              0.999914
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       3210.59
exploration/env_infos/final/reward_run Mean          4.08375
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.08375
exploration/env_infos/final/reward_run Min           4.08375
exploration/env_infos/initial/reward_run Mean       -0.531274
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.531274
exploration/env_infos/initial/reward_run Min        -0.531274
exploration/env_infos/reward_run Mean                3.62571
exploration/env_infos/reward_run Std                 0.979126
exploration/env_infos/reward_run Max                 5.86378
exploration/env_infos/reward_run Min                -0.712549
exploration/env_infos/final/reward_ctrl Mean        -0.323492
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.323492
exploration/env_infos/final/reward_ctrl Min         -0.323492
exploration/env_infos/initial/reward_ctrl Mean      -0.232989
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.232989
exploration/env_infos/initial/reward_ctrl Min       -0.232989
exploration/env_infos/reward_ctrl Mean              -0.415116
exploration/env_infos/reward_ctrl Std                0.091658
exploration/env_infos/reward_ctrl Max               -0.0645822
exploration/env_infos/reward_ctrl Min               -0.595537
evaluation/num steps total                      320000
evaluation/num paths total                         320
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.30433
evaluation/Rewards Std                               1.01712
evaluation/Rewards Max                               5.57286
evaluation/Rewards Min                              -1.09404
evaluation/Returns Mean                           3304.33
evaluation/Returns Std                              47.2341
evaluation/Returns Max                            3387.55
evaluation/Returns Min                            3254.66
evaluation/Actions Mean                              0.00579085
evaluation/Actions Std                               0.831972
evaluation/Actions Max                               0.999965
evaluation/Actions Min                              -0.999994
evaluation/Num Paths                                 5
evaluation/Average Returns                        3304.33
evaluation/env_infos/final/reward_run Mean           3.66152
evaluation/env_infos/final/reward_run Std            0.622852
evaluation/env_infos/final/reward_run Max            4.6893
evaluation/env_infos/final/reward_run Min            2.8789
evaluation/env_infos/initial/reward_run Mean         0.131715
evaluation/env_infos/initial/reward_run Std          0.418819
evaluation/env_infos/initial/reward_run Max          0.645012
evaluation/env_infos/initial/reward_run Min         -0.435442
evaluation/env_infos/reward_run Mean                 3.71966
evaluation/env_infos/reward_run Std                  1.00073
evaluation/env_infos/reward_run Max                  5.98213
evaluation/env_infos/reward_run Min                 -0.546852
evaluation/env_infos/final/reward_ctrl Mean         -0.35711
evaluation/env_infos/final/reward_ctrl Std           0.0870468
evaluation/env_infos/final/reward_ctrl Max          -0.25159
evaluation/env_infos/final/reward_ctrl Min          -0.484494
evaluation/env_infos/initial/reward_ctrl Mean       -0.109239
evaluation/env_infos/initial/reward_ctrl Std         0.0540896
evaluation/env_infos/initial/reward_ctrl Max        -0.0588264
evaluation/env_infos/initial/reward_ctrl Min        -0.192483
evaluation/env_infos/reward_ctrl Mean               -0.415326
evaluation/env_infos/reward_ctrl Std                 0.0951099
evaluation/env_infos/reward_ctrl Max                -0.0588264
evaluation/env_infos/reward_ctrl Min                -0.596379
time/data storing (s)                                0.00653271
time/evaluation sampling (s)                         2.45571
time/exploration sampling (s)                        0.581247
time/logging (s)                                     0.042442
time/saving (s)                                      0.0177171
time/training (s)                                   32.2768
time/epoch (s)                                      35.3804
time/total (s)                                    2709.55
Epoch                                               63
----------------------------------------------  ---------------
2020-07-08 21:46:36.235381 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 64 finished
----------------------------------------------  ---------------
replay_buffer/size                               66000
trainer/QF1 Loss                                     4.77092
trainer/QF2 Loss                                     5.16452
trainer/Policy Loss                                -98.7159
trainer/Q1 Predictions Mean                        103.973
trainer/Q1 Predictions Std                          76.103
trainer/Q1 Predictions Max                         198.152
trainer/Q1 Predictions Min                           3.03745
trainer/Q2 Predictions Mean                        104.221
trainer/Q2 Predictions Std                          76.3058
trainer/Q2 Predictions Max                         198.034
trainer/Q2 Predictions Min                           3.41813
trainer/Q Targets Mean                             103.783
trainer/Q Targets Std                               76.0201
trainer/Q Targets Max                              197.878
trainer/Q Targets Min                                3.09374
trainer/Log Pis Mean                                 5.65902
trainer/Log Pis Std                                  5.19457
trainer/Log Pis Max                                 26.3292
trainer/Log Pis Min                                 -5.67763
trainer/Policy mu Mean                               0.0943837
trainer/Policy mu Std                                1.53024
trainer/Policy mu Max                                3.89427
trainer/Policy mu Min                               -4.77792
trainer/Policy log std Mean                         -0.752095
trainer/Policy log std Std                           0.295538
trainer/Policy log std Max                          -0.0236382
trainer/Policy log std Min                          -1.74888
trainer/Alpha                                        0.0480224
trainer/Alpha Loss                                  -1.03528
exploration/num steps total                      66000
exploration/num paths total                         66
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.07884
exploration/Rewards Std                              1.01578
exploration/Rewards Max                              5.2188
exploration/Rewards Min                             -1.26241
exploration/Returns Mean                          3078.84
exploration/Returns Std                              0
exploration/Returns Max                           3078.84
exploration/Returns Min                           3078.84
exploration/Actions Mean                             0.0268334
exploration/Actions Std                              0.825074
exploration/Actions Max                              0.999964
exploration/Actions Min                             -0.999974
exploration/Num Paths                                1
exploration/Average Returns                       3078.84
exploration/env_infos/final/reward_run Mean          3.27093
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.27093
exploration/env_infos/final/reward_run Min           3.27093
exploration/env_infos/initial/reward_run Mean       -0.529131
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.529131
exploration/env_infos/initial/reward_run Min        -0.529131
exploration/env_infos/reward_run Mean                3.48772
exploration/env_infos/reward_run Std                 1.00872
exploration/env_infos/reward_run Max                 5.64702
exploration/env_infos/reward_run Min                -0.706265
exploration/env_infos/final/reward_ctrl Mean        -0.244092
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.244092
exploration/env_infos/final/reward_ctrl Min         -0.244092
exploration/env_infos/initial/reward_ctrl Mean      -0.204242
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.204242
exploration/env_infos/initial/reward_ctrl Min       -0.204242
exploration/env_infos/reward_ctrl Mean              -0.40888
exploration/env_infos/reward_ctrl Std                0.0931429
exploration/env_infos/reward_ctrl Max               -0.112916
exploration/env_infos/reward_ctrl Min               -0.587221
evaluation/num steps total                      325000
evaluation/num paths total                         325
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.9647
evaluation/Rewards Std                               1.22936
evaluation/Rewards Max                               5.35723
evaluation/Rewards Min                              -1.42229
evaluation/Returns Mean                           2964.7
evaluation/Returns Std                             393.99
evaluation/Returns Max                            3234.74
evaluation/Returns Min                            2182.35
evaluation/Actions Mean                              0.0209913
evaluation/Actions Std                               0.828699
evaluation/Actions Max                               0.999967
evaluation/Actions Min                              -0.999993
evaluation/Num Paths                                 5
evaluation/Average Returns                        2964.7
evaluation/env_infos/final/reward_run Mean           3.64463
evaluation/env_infos/final/reward_run Std            1.03794
evaluation/env_infos/final/reward_run Max            4.64678
evaluation/env_infos/final/reward_run Min            2.0388
evaluation/env_infos/initial/reward_run Mean        -0.111244
evaluation/env_infos/initial/reward_run Std          0.345112
evaluation/env_infos/initial/reward_run Max          0.325002
evaluation/env_infos/initial/reward_run Min         -0.627431
evaluation/env_infos/reward_run Mean                 3.37701
evaluation/env_infos/reward_run Std                  1.23266
evaluation/env_infos/reward_run Max                  5.74673
evaluation/env_infos/reward_run Min                 -1.10873
evaluation/env_infos/final/reward_ctrl Mean         -0.406035
evaluation/env_infos/final/reward_ctrl Std           0.0947689
evaluation/env_infos/final/reward_ctrl Max          -0.274641
evaluation/env_infos/final/reward_ctrl Min          -0.559212
evaluation/env_infos/initial/reward_ctrl Mean       -0.113328
evaluation/env_infos/initial/reward_ctrl Std         0.0859715
evaluation/env_infos/initial/reward_ctrl Max        -0.0400175
evaluation/env_infos/initial/reward_ctrl Min        -0.279556
evaluation/env_infos/reward_ctrl Mean               -0.412309
evaluation/env_infos/reward_ctrl Std                 0.0941542
evaluation/env_infos/reward_ctrl Max                -0.0400175
evaluation/env_infos/reward_ctrl Min                -0.591048
time/data storing (s)                                0.00706578
time/evaluation sampling (s)                         2.28695
time/exploration sampling (s)                        0.572452
time/logging (s)                                     0.0389617
time/saving (s)                                      0.0170864
time/training (s)                                   36.6785
time/epoch (s)                                      39.601
time/total (s)                                    2749.16
Epoch                                               64
----------------------------------------------  ---------------
2020-07-08 21:53:12.013682 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 65 finished
----------------------------------------------  --------------
replay_buffer/size                               67000
trainer/QF1 Loss                                     5.5895
trainer/QF2 Loss                                     6.76686
trainer/Policy Loss                               -106.704
trainer/Q1 Predictions Mean                        113.389
trainer/Q1 Predictions Std                          75.5116
trainer/Q1 Predictions Max                         199.261
trainer/Q1 Predictions Min                           3.50188
trainer/Q2 Predictions Mean                        113.21
trainer/Q2 Predictions Std                          75.398
trainer/Q2 Predictions Max                         198.841
trainer/Q2 Predictions Min                           3.31429
trainer/Q Targets Mean                             113.709
trainer/Q Targets Std                               75.7684
trainer/Q Targets Max                              200.761
trainer/Q Targets Min                                2.74533
trainer/Log Pis Mean                                 6.96183
trainer/Log Pis Std                                  5.71924
trainer/Log Pis Max                                 33.3615
trainer/Log Pis Min                                 -6.45163
trainer/Policy mu Mean                               0.185329
trainer/Policy mu Std                                1.63366
trainer/Policy mu Max                                7.01476
trainer/Policy mu Min                               -3.93531
trainer/Policy log std Mean                         -0.761977
trainer/Policy log std Std                           0.2955
trainer/Policy log std Max                          -0.0635545
trainer/Policy log std Min                          -1.92019
trainer/Alpha                                        0.0489254
trainer/Alpha Loss                                   2.9025
exploration/num steps total                      67000
exploration/num paths total                         67
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.32208
exploration/Rewards Std                              1.04641
exploration/Rewards Max                              5.58121
exploration/Rewards Min                             -0.740573
exploration/Returns Mean                          3322.08
exploration/Returns Std                              0
exploration/Returns Max                           3322.08
exploration/Returns Min                           3322.08
exploration/Actions Mean                             0.0316925
exploration/Actions Std                              0.843329
exploration/Actions Max                              0.999925
exploration/Actions Min                             -0.999988
exploration/Num Paths                                1
exploration/Average Returns                       3322.08
exploration/env_infos/final/reward_run Mean          3.04603
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.04603
exploration/env_infos/final/reward_run Min           3.04603
exploration/env_infos/initial/reward_run Mean        0.380428
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.380428
exploration/env_infos/initial/reward_run Min         0.380428
exploration/env_infos/reward_run Mean                3.7494
exploration/env_infos/reward_run Std                 1.04009
exploration/env_infos/reward_run Max                 6.05322
exploration/env_infos/reward_run Min                -0.179593
exploration/env_infos/final/reward_ctrl Mean        -0.491703
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.491703
exploration/env_infos/final/reward_ctrl Min         -0.491703
exploration/env_infos/initial/reward_ctrl Mean      -0.0829326
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0829326
exploration/env_infos/initial/reward_ctrl Min       -0.0829326
exploration/env_infos/reward_ctrl Mean              -0.427325
exploration/env_infos/reward_ctrl Std                0.0907503
exploration/env_infos/reward_ctrl Max               -0.0829326
exploration/env_infos/reward_ctrl Min               -0.587748
evaluation/num steps total                      330000
evaluation/num paths total                         330
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              2.76092
evaluation/Rewards Std                               1.49642
evaluation/Rewards Max                               5.66074
evaluation/Rewards Min                              -1.68863
evaluation/Returns Mean                           2760.92
evaluation/Returns Std                             981.667
evaluation/Returns Max                            3290.51
evaluation/Returns Min                             798.644
evaluation/Actions Mean                              0.0211799
evaluation/Actions Std                               0.835825
evaluation/Actions Max                               0.999993
evaluation/Actions Min                              -0.999905
evaluation/Num Paths                                 5
evaluation/Average Returns                        2760.92
evaluation/env_infos/final/reward_run Mean           3.4633
evaluation/env_infos/final/reward_run Std            1.43107
evaluation/env_infos/final/reward_run Max            5.12456
evaluation/env_infos/final/reward_run Min            1.08539
evaluation/env_infos/initial/reward_run Mean         0.195236
evaluation/env_infos/initial/reward_run Std          0.262359
evaluation/env_infos/initial/reward_run Max          0.451628
evaluation/env_infos/initial/reward_run Min         -0.243293
evaluation/env_infos/reward_run Mean                 3.18035
evaluation/env_infos/reward_run Std                  1.51735
evaluation/env_infos/reward_run Max                  6.17265
evaluation/env_infos/reward_run Min                 -1.27611
evaluation/env_infos/final/reward_ctrl Mean         -0.401501
evaluation/env_infos/final/reward_ctrl Std           0.0890784
evaluation/env_infos/final/reward_ctrl Max          -0.268855
evaluation/env_infos/final/reward_ctrl Min          -0.50566
evaluation/env_infos/initial/reward_ctrl Mean       -0.111485
evaluation/env_infos/initial/reward_ctrl Std         0.0399371
evaluation/env_infos/initial/reward_ctrl Max        -0.072173
evaluation/env_infos/initial/reward_ctrl Min        -0.17703
evaluation/env_infos/reward_ctrl Mean               -0.419431
evaluation/env_infos/reward_ctrl Std                 0.098185
evaluation/env_infos/reward_ctrl Max                -0.04453
evaluation/env_infos/reward_ctrl Min                -0.592402
time/data storing (s)                                0.0119394
time/evaluation sampling (s)                         2.30041
time/exploration sampling (s)                        2.54632
time/logging (s)                                     0.0503684
time/saving (s)                                      0.0222303
time/training (s)                                   67.3126
time/epoch (s)                                      72.2439
time/total (s)                                    2821.47
Epoch                                               65
----------------------------------------------  --------------
2020-07-08 21:53:56.793429 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 66 finished
----------------------------------------------  ---------------
replay_buffer/size                               68000
trainer/QF1 Loss                                     8.49771
trainer/QF2 Loss                                     8.57826
trainer/Policy Loss                               -103.158
trainer/Q1 Predictions Mean                        108.615
trainer/Q1 Predictions Std                          77.6864
trainer/Q1 Predictions Max                         208.163
trainer/Q1 Predictions Min                           3.03487
trainer/Q2 Predictions Mean                        109.112
trainer/Q2 Predictions Std                          77.9098
trainer/Q2 Predictions Max                         208.14
trainer/Q2 Predictions Min                           3.3138
trainer/Q Targets Mean                             109.165
trainer/Q Targets Std                               78.0941
trainer/Q Targets Max                              204.575
trainer/Q Targets Min                                1.53563
trainer/Log Pis Mean                                 6.11873
trainer/Log Pis Std                                  5.81915
trainer/Log Pis Max                                 25.0171
trainer/Log Pis Min                                 -8.32339
trainer/Policy mu Mean                               0.0344068
trainer/Policy mu Std                                1.57196
trainer/Policy mu Max                                4.53174
trainer/Policy mu Min                               -4.65483
trainer/Policy log std Mean                         -0.756006
trainer/Policy log std Std                           0.303091
trainer/Policy log std Max                          -0.00638568
trainer/Policy log std Min                          -1.87065
trainer/Alpha                                        0.0500337
trainer/Alpha Loss                                   0.3556
exploration/num steps total                      68000
exploration/num paths total                         68
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.46645
exploration/Rewards Std                              0.970483
exploration/Rewards Max                              5.41813
exploration/Rewards Min                             -1.01096
exploration/Returns Mean                          3466.45
exploration/Returns Std                              0
exploration/Returns Max                           3466.45
exploration/Returns Min                           3466.45
exploration/Actions Mean                             0.00149124
exploration/Actions Std                              0.835506
exploration/Actions Max                              0.999903
exploration/Actions Min                             -0.999962
exploration/Num Paths                                1
exploration/Average Returns                       3466.45
exploration/env_infos/final/reward_run Mean          4.70717
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.70717
exploration/env_infos/final/reward_run Min           4.70717
exploration/env_infos/initial/reward_run Mean       -0.331624
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.331624
exploration/env_infos/initial/reward_run Min        -0.331624
exploration/env_infos/reward_run Mean                3.88529
exploration/env_infos/reward_run Std                 0.961689
exploration/env_infos/reward_run Max                 5.81643
exploration/env_infos/reward_run Min                -0.469621
exploration/env_infos/final/reward_ctrl Mean        -0.461652
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.461652
exploration/env_infos/final/reward_ctrl Min         -0.461652
exploration/env_infos/initial/reward_ctrl Mean      -0.163057
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.163057
exploration/env_infos/initial/reward_ctrl Min       -0.163057
exploration/env_infos/reward_ctrl Mean              -0.418844
exploration/env_infos/reward_ctrl Std                0.0905682
exploration/env_infos/reward_ctrl Max               -0.129096
exploration/env_infos/reward_ctrl Min               -0.581465
evaluation/num steps total                      335000
evaluation/num paths total                         335
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.43883
evaluation/Rewards Std                               1.05026
evaluation/Rewards Max                               5.5897
evaluation/Rewards Min                              -1.15846
evaluation/Returns Mean                           3438.83
evaluation/Returns Std                              54.1076
evaluation/Returns Max                            3523.19
evaluation/Returns Min                            3389.43
evaluation/Actions Mean                             -0.00252835
evaluation/Actions Std                               0.845074
evaluation/Actions Max                               0.999968
evaluation/Actions Min                              -0.999969
evaluation/Num Paths                                 5
evaluation/Average Returns                        3438.83
evaluation/env_infos/final/reward_run Mean           4.52146
evaluation/env_infos/final/reward_run Std            1.30342
evaluation/env_infos/final/reward_run Max            5.7461
evaluation/env_infos/final/reward_run Min            2.3222
evaluation/env_infos/initial/reward_run Mean         0.409274
evaluation/env_infos/initial/reward_run Std          0.371935
evaluation/env_infos/initial/reward_run Max          0.943638
evaluation/env_infos/initial/reward_run Min         -0.157167
evaluation/env_infos/reward_run Mean                 3.86732
evaluation/env_infos/reward_run Std                  1.04247
evaluation/env_infos/reward_run Max                  6.06136
evaluation/env_infos/reward_run Min                 -0.654784
evaluation/env_infos/final/reward_ctrl Mean         -0.41223
evaluation/env_infos/final/reward_ctrl Std           0.0852984
evaluation/env_infos/final/reward_ctrl Max          -0.317858
evaluation/env_infos/final/reward_ctrl Min          -0.5332
evaluation/env_infos/initial/reward_ctrl Mean       -0.108782
evaluation/env_infos/initial/reward_ctrl Std         0.0602406
evaluation/env_infos/initial/reward_ctrl Max        -0.0393612
evaluation/env_infos/initial/reward_ctrl Min        -0.212258
evaluation/env_infos/reward_ctrl Mean               -0.428494
evaluation/env_infos/reward_ctrl Std                 0.0935748
evaluation/env_infos/reward_ctrl Max                -0.0393612
evaluation/env_infos/reward_ctrl Min                -0.595426
time/data storing (s)                                0.0072846
time/evaluation sampling (s)                         2.60051
time/exploration sampling (s)                        0.619546
time/logging (s)                                     0.0485274
time/saving (s)                                      0.0170544
time/training (s)                                   41.466
time/epoch (s)                                      44.7589
time/total (s)                                    2866.25
Epoch                                               66
----------------------------------------------  ---------------
2020-07-08 21:54:45.739310 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 67 finished
----------------------------------------------  ---------------
replay_buffer/size                               69000
trainer/QF1 Loss                                     7.1145
trainer/QF2 Loss                                     6.05298
trainer/Policy Loss                               -102.327
trainer/Q1 Predictions Mean                        108.028
trainer/Q1 Predictions Std                          77.9823
trainer/Q1 Predictions Max                         207.857
trainer/Q1 Predictions Min                           3.58458
trainer/Q2 Predictions Mean                        108.205
trainer/Q2 Predictions Std                          78.1435
trainer/Q2 Predictions Max                         205.569
trainer/Q2 Predictions Min                           3.43057
trainer/Q Targets Mean                             108.538
trainer/Q Targets Std                               78.3933
trainer/Q Targets Max                              207.714
trainer/Q Targets Min                                3.4886
trainer/Log Pis Mean                                 6.21383
trainer/Log Pis Std                                  5.81773
trainer/Log Pis Max                                 23.9495
trainer/Log Pis Min                                 -6.78071
trainer/Policy mu Mean                               0.0829293
trainer/Policy mu Std                                1.59594
trainer/Policy mu Max                                5.33728
trainer/Policy mu Min                               -4.64592
trainer/Policy log std Mean                         -0.746098
trainer/Policy log std Std                           0.310316
trainer/Policy log std Max                           0.138854
trainer/Policy log std Min                          -1.89431
trainer/Alpha                                        0.0501892
trainer/Alpha Loss                                   0.639736
exploration/num steps total                      69000
exploration/num paths total                         69
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.37282
exploration/Rewards Std                              0.99123
exploration/Rewards Max                              5.4376
exploration/Rewards Min                             -0.437002
exploration/Returns Mean                          3372.82
exploration/Returns Std                              0
exploration/Returns Max                           3372.82
exploration/Returns Min                           3372.82
exploration/Actions Mean                             0.0138725
exploration/Actions Std                              0.820598
exploration/Actions Max                              0.99999
exploration/Actions Min                             -0.99999
exploration/Num Paths                                1
exploration/Average Returns                       3372.82
exploration/env_infos/final/reward_run Mean          4.013
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.013
exploration/env_infos/final/reward_run Min           4.013
exploration/env_infos/initial/reward_run Mean       -0.236016
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.236016
exploration/env_infos/initial/reward_run Min        -0.236016
exploration/env_infos/reward_run Mean                3.77696
exploration/env_infos/reward_run Std                 0.97768
exploration/env_infos/reward_run Max                 5.69668
exploration/env_infos/reward_run Min                -0.236016
exploration/env_infos/final/reward_ctrl Mean        -0.458914
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.458914
exploration/env_infos/final/reward_ctrl Min         -0.458914
exploration/env_infos/initial/reward_ctrl Mean      -0.200985
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.200985
exploration/env_infos/initial/reward_ctrl Min       -0.200985
exploration/env_infos/reward_ctrl Mean              -0.404144
exploration/env_infos/reward_ctrl Std                0.0902764
exploration/env_infos/reward_ctrl Max               -0.144408
exploration/env_infos/reward_ctrl Min               -0.58435
evaluation/num steps total                      340000
evaluation/num paths total                         340
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.48418
evaluation/Rewards Std                               1.05322
evaluation/Rewards Max                               5.67244
evaluation/Rewards Min                              -1.32411
evaluation/Returns Mean                           3484.18
evaluation/Returns Std                             129.788
evaluation/Returns Max                            3620.34
evaluation/Returns Min                            3248.66
evaluation/Actions Mean                              0.0302193
evaluation/Actions Std                               0.827951
evaluation/Actions Max                               0.999923
evaluation/Actions Min                              -0.99993
evaluation/Num Paths                                 5
evaluation/Average Returns                        3484.18
evaluation/env_infos/final/reward_run Mean           4.49145
evaluation/env_infos/final/reward_run Std            0.732106
evaluation/env_infos/final/reward_run Max            5.2658
evaluation/env_infos/final/reward_run Min            3.39329
evaluation/env_infos/initial/reward_run Mean         0.259078
evaluation/env_infos/initial/reward_run Std          0.315966
evaluation/env_infos/initial/reward_run Max          0.575803
evaluation/env_infos/initial/reward_run Min         -0.305809
evaluation/env_infos/reward_run Mean                 3.89603
evaluation/env_infos/reward_run Std                  1.03583
evaluation/env_infos/reward_run Max                  6.03038
evaluation/env_infos/reward_run Min                 -0.810951
evaluation/env_infos/final/reward_ctrl Mean         -0.456004
evaluation/env_infos/final/reward_ctrl Std           0.0472882
evaluation/env_infos/final/reward_ctrl Max          -0.367446
evaluation/env_infos/final/reward_ctrl Min          -0.506224
evaluation/env_infos/initial/reward_ctrl Mean       -0.108843
evaluation/env_infos/initial/reward_ctrl Std         0.0517138
evaluation/env_infos/initial/reward_ctrl Max        -0.0508695
evaluation/env_infos/initial/reward_ctrl Min        -0.188574
evaluation/env_infos/reward_ctrl Mean               -0.41185
evaluation/env_infos/reward_ctrl Std                 0.089315
evaluation/env_infos/reward_ctrl Max                -0.0508695
evaluation/env_infos/reward_ctrl Min                -0.593948
time/data storing (s)                                0.00716676
time/evaluation sampling (s)                         3.62812
time/exploration sampling (s)                        0.856666
time/logging (s)                                     0.0510751
time/saving (s)                                      0.0204795
time/training (s)                                   44.3638
time/epoch (s)                                      48.9273
time/total (s)                                    2915.19
Epoch                                               67
----------------------------------------------  ---------------
2020-07-08 21:55:50.890184 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 68 finished
----------------------------------------------  ---------------
replay_buffer/size                               70000
trainer/QF1 Loss                                     6.25065
trainer/QF2 Loss                                     6.99568
trainer/Policy Loss                               -112.103
trainer/Q1 Predictions Mean                        118.067
trainer/Q1 Predictions Std                          76.6616
trainer/Q1 Predictions Max                         209.445
trainer/Q1 Predictions Min                           3.73752
trainer/Q2 Predictions Mean                        118.065
trainer/Q2 Predictions Std                          76.7612
trainer/Q2 Predictions Max                         208.975
trainer/Q2 Predictions Min                           3.23287
trainer/Q Targets Mean                             118.251
trainer/Q Targets Std                               76.9804
trainer/Q Targets Max                              209.232
trainer/Q Targets Min                                2.4618
trainer/Log Pis Mean                                 6.58552
trainer/Log Pis Std                                  5.83065
trainer/Log Pis Max                                 33.3242
trainer/Log Pis Min                                 -5.57853
trainer/Policy mu Mean                               0.18555
trainer/Policy mu Std                                1.62927
trainer/Policy mu Max                                5.87061
trainer/Policy mu Min                               -4.99093
trainer/Policy log std Mean                         -0.781233
trainer/Policy log std Std                           0.320947
trainer/Policy log std Max                           0.118247
trainer/Policy log std Min                          -1.92257
trainer/Alpha                                        0.0511669
trainer/Alpha Loss                                   1.7405
exploration/num steps total                      70000
exploration/num paths total                         70
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.25856
exploration/Rewards Std                              1.12044
exploration/Rewards Max                              5.64927
exploration/Rewards Min                             -0.737383
exploration/Returns Mean                          3258.56
exploration/Returns Std                              0
exploration/Returns Max                           3258.56
exploration/Returns Min                           3258.56
exploration/Actions Mean                             0.00249026
exploration/Actions Std                              0.826902
exploration/Actions Max                              0.999997
exploration/Actions Min                             -0.999977
exploration/Num Paths                                1
exploration/Average Returns                       3258.56
exploration/env_infos/final/reward_run Mean          1.45635
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.45635
exploration/env_infos/final/reward_run Min           1.45635
exploration/env_infos/initial/reward_run Mean        0.214469
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.214469
exploration/env_infos/initial/reward_run Min         0.214469
exploration/env_infos/reward_run Mean                3.66882
exploration/env_infos/reward_run Std                 1.11587
exploration/env_infos/reward_run Max                 5.99378
exploration/env_infos/reward_run Min                -0.236364
exploration/env_infos/final/reward_ctrl Mean        -0.537861
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.537861
exploration/env_infos/final/reward_ctrl Min         -0.537861
exploration/env_infos/initial/reward_ctrl Mean      -0.0687683
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0687683
exploration/env_infos/initial/reward_ctrl Min       -0.0687683
exploration/env_infos/reward_ctrl Mean              -0.410264
exploration/env_infos/reward_ctrl Std                0.0881809
exploration/env_infos/reward_ctrl Max               -0.0687683
exploration/env_infos/reward_ctrl Min               -0.578601
evaluation/num steps total                      345000
evaluation/num paths total                         345
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.38148
evaluation/Rewards Std                               1.11466
evaluation/Rewards Max                               5.87322
evaluation/Rewards Min                              -1.16228
evaluation/Returns Mean                           3381.48
evaluation/Returns Std                              66.7191
evaluation/Returns Max                            3458.11
evaluation/Returns Min                            3274.84
evaluation/Actions Mean                              0.0134387
evaluation/Actions Std                               0.837336
evaluation/Actions Max                               0.999987
evaluation/Actions Min                              -0.999998
evaluation/Num Paths                                 5
evaluation/Average Returns                        3381.48
evaluation/env_infos/final/reward_run Mean           3.52914
evaluation/env_infos/final/reward_run Std            1.29327
evaluation/env_infos/final/reward_run Max            5.36406
evaluation/env_infos/final/reward_run Min            1.96249
evaluation/env_infos/initial/reward_run Mean         0.105039
evaluation/env_infos/initial/reward_run Std          0.285279
evaluation/env_infos/initial/reward_run Max          0.506546
evaluation/env_infos/initial/reward_run Min         -0.198191
evaluation/env_infos/reward_run Mean                 3.80227
evaluation/env_infos/reward_run Std                  1.11081
evaluation/env_infos/reward_run Max                  6.29614
evaluation/env_infos/reward_run Min                 -0.769616
evaluation/env_infos/final/reward_ctrl Mean         -0.402939
evaluation/env_infos/final/reward_ctrl Std           0.0647151
evaluation/env_infos/final/reward_ctrl Max          -0.308873
evaluation/env_infos/final/reward_ctrl Min          -0.501674
evaluation/env_infos/initial/reward_ctrl Mean       -0.108484
evaluation/env_infos/initial/reward_ctrl Std         0.0252591
evaluation/env_infos/initial/reward_ctrl Max        -0.0794973
evaluation/env_infos/initial/reward_ctrl Min        -0.149144
evaluation/env_infos/reward_ctrl Mean               -0.420787
evaluation/env_infos/reward_ctrl Std                 0.0860213
evaluation/env_infos/reward_ctrl Max                -0.069738
evaluation/env_infos/reward_ctrl Min                -0.594403
time/data storing (s)                                0.00709953
time/evaluation sampling (s)                         2.80448
time/exploration sampling (s)                        0.78227
time/logging (s)                                     0.040424
time/saving (s)                                      0.0165729
time/training (s)                                   61.4458
time/epoch (s)                                      65.0967
time/total (s)                                    2980.33
Epoch                                               68
----------------------------------------------  ---------------
2020-07-08 21:56:30.111301 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 69 finished
----------------------------------------------  --------------
replay_buffer/size                               71000
trainer/QF1 Loss                                     6.18278
trainer/QF2 Loss                                     7.4562
trainer/Policy Loss                               -119.492
trainer/Q1 Predictions Mean                        125.166
trainer/Q1 Predictions Std                          80.4467
trainer/Q1 Predictions Max                         210.859
trainer/Q1 Predictions Min                           2.24614
trainer/Q2 Predictions Mean                        124.514
trainer/Q2 Predictions Std                          80.0597
trainer/Q2 Predictions Max                         211.396
trainer/Q2 Predictions Min                           1.74716
trainer/Q Targets Mean                             124.792
trainer/Q Targets Std                               80.4698
trainer/Q Targets Max                              212.731
trainer/Q Targets Min                                1.68023
trainer/Log Pis Mean                                 5.6601
trainer/Log Pis Std                                  5.21917
trainer/Log Pis Max                                 29.9583
trainer/Log Pis Min                                 -4.72056
trainer/Policy mu Mean                               0.0618584
trainer/Policy mu Std                                1.52833
trainer/Policy mu Max                                5.66476
trainer/Policy mu Min                               -4.21714
trainer/Policy log std Mean                         -0.775301
trainer/Policy log std Std                           0.311483
trainer/Policy log std Max                           0.0787508
trainer/Policy log std Min                          -1.94682
trainer/Alpha                                        0.0525783
trainer/Alpha Loss                                  -1.00116
exploration/num steps total                      71000
exploration/num paths total                         71
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.28974
exploration/Rewards Std                              1.12501
exploration/Rewards Max                              5.6005
exploration/Rewards Min                             -0.452546
exploration/Returns Mean                          3289.74
exploration/Returns Std                              0
exploration/Returns Max                           3289.74
exploration/Returns Min                           3289.74
exploration/Actions Mean                             0.0131782
exploration/Actions Std                              0.806142
exploration/Actions Max                              0.999933
exploration/Actions Min                             -0.999965
exploration/Num Paths                                1
exploration/Average Returns                       3289.74
exploration/env_infos/final/reward_run Mean          5.18163
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.18163
exploration/env_infos/final/reward_run Min           5.18163
exploration/env_infos/initial/reward_run Mean       -0.195038
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.195038
exploration/env_infos/initial/reward_run Min        -0.195038
exploration/env_infos/reward_run Mean                3.67976
exploration/env_infos/reward_run Std                 1.12161
exploration/env_infos/reward_run Max                 6.01888
exploration/env_infos/reward_run Min                -0.195038
exploration/env_infos/final/reward_ctrl Mean        -0.40099
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.40099
exploration/env_infos/final/reward_ctrl Min         -0.40099
exploration/env_infos/initial/reward_ctrl Mean      -0.23963
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.23963
exploration/env_infos/initial/reward_ctrl Min       -0.23963
exploration/env_infos/reward_ctrl Mean              -0.390023
exploration/env_infos/reward_ctrl Std                0.0889695
exploration/env_infos/reward_ctrl Max               -0.06004
exploration/env_infos/reward_ctrl Min               -0.57565
evaluation/num steps total                      350000
evaluation/num paths total                         350
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.47391
evaluation/Rewards Std                               1.06543
evaluation/Rewards Max                               5.75321
evaluation/Rewards Min                              -0.889379
evaluation/Returns Mean                           3473.91
evaluation/Returns Std                             153.615
evaluation/Returns Max                            3733.52
evaluation/Returns Min                            3276.06
evaluation/Actions Mean                              0.0102518
evaluation/Actions Std                               0.817806
evaluation/Actions Max                               0.999939
evaluation/Actions Min                              -0.999967
evaluation/Num Paths                                 5
evaluation/Average Returns                        3473.91
evaluation/env_infos/final/reward_run Mean           3.51822
evaluation/env_infos/final/reward_run Std            0.612611
evaluation/env_infos/final/reward_run Max            4.68727
evaluation/env_infos/final/reward_run Min            2.99775
evaluation/env_infos/initial/reward_run Mean         0.29768
evaluation/env_infos/initial/reward_run Std          0.21419
evaluation/env_infos/initial/reward_run Max          0.536275
evaluation/env_infos/initial/reward_run Min         -0.0368695
evaluation/env_infos/reward_run Mean                 3.87526
evaluation/env_infos/reward_run Std                  1.05761
evaluation/env_infos/reward_run Max                  6.26113
evaluation/env_infos/reward_run Min                 -0.449921
evaluation/env_infos/final/reward_ctrl Mean         -0.431131
evaluation/env_infos/final/reward_ctrl Std           0.0539553
evaluation/env_infos/final/reward_ctrl Max          -0.367722
evaluation/env_infos/final/reward_ctrl Min          -0.524809
evaluation/env_infos/initial/reward_ctrl Mean       -0.102967
evaluation/env_infos/initial/reward_ctrl Std         0.0426421
evaluation/env_infos/initial/reward_ctrl Max        -0.0431203
evaluation/env_infos/initial/reward_ctrl Min        -0.163274
evaluation/env_infos/reward_ctrl Mean               -0.401347
evaluation/env_infos/reward_ctrl Std                 0.0925033
evaluation/env_infos/reward_ctrl Max                -0.0431203
evaluation/env_infos/reward_ctrl Min                -0.584581
time/data storing (s)                                0.0138823
time/evaluation sampling (s)                         4.33731
time/exploration sampling (s)                        1.13875
time/logging (s)                                     0.040215
time/saving (s)                                      0.0192787
time/training (s)                                   33.6403
time/epoch (s)                                      39.1897
time/total (s)                                    3019.54
Epoch                                               69
----------------------------------------------  --------------
2020-07-08 21:57:07.790282 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 70 finished
----------------------------------------------  ----------------
replay_buffer/size                               72000
trainer/QF1 Loss                                     5.9185
trainer/QF2 Loss                                     6.23805
trainer/Policy Loss                               -112.123
trainer/Q1 Predictions Mean                        117.934
trainer/Q1 Predictions Std                          83.0828
trainer/Q1 Predictions Max                         216.277
trainer/Q1 Predictions Min                           3.25552
trainer/Q2 Predictions Mean                        118.088
trainer/Q2 Predictions Std                          83.2858
trainer/Q2 Predictions Max                         215.438
trainer/Q2 Predictions Min                           2.9919
trainer/Q Targets Mean                             118.291
trainer/Q Targets Std                               83.4048
trainer/Q Targets Max                              216.115
trainer/Q Targets Min                                2.83093
trainer/Log Pis Mean                                 6.29914
trainer/Log Pis Std                                  5.36573
trainer/Log Pis Max                                 21.7811
trainer/Log Pis Min                                 -5.45168
trainer/Policy mu Mean                               0.0258221
trainer/Policy mu Std                                1.58368
trainer/Policy mu Max                                5.30469
trainer/Policy mu Min                               -4.08589
trainer/Policy log std Mean                         -0.730782
trainer/Policy log std Std                           0.290658
trainer/Policy log std Max                           0.0376633
trainer/Policy log std Min                          -2.00166
trainer/Alpha                                        0.053333
trainer/Alpha Loss                                   0.876818
exploration/num steps total                      72000
exploration/num paths total                         72
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.41969
exploration/Rewards Std                              0.986558
exploration/Rewards Max                              5.60672
exploration/Rewards Min                             -0.608694
exploration/Returns Mean                          3419.69
exploration/Returns Std                              0
exploration/Returns Max                           3419.69
exploration/Returns Min                           3419.69
exploration/Actions Mean                             0.00186209
exploration/Actions Std                              0.825896
exploration/Actions Max                              0.999941
exploration/Actions Min                             -1
exploration/Num Paths                                1
exploration/Average Returns                       3419.69
exploration/env_infos/final/reward_run Mean          2.82352
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.82352
exploration/env_infos/final/reward_run Min           2.82352
exploration/env_infos/initial/reward_run Mean       -0.111463
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.111463
exploration/env_infos/initial/reward_run Min        -0.111463
exploration/env_infos/reward_run Mean                3.82895
exploration/env_infos/reward_run Std                 0.976577
exploration/env_infos/reward_run Max                 5.89602
exploration/env_infos/reward_run Min                -0.157272
exploration/env_infos/final/reward_ctrl Mean        -0.566137
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.566137
exploration/env_infos/final/reward_ctrl Min         -0.566137
exploration/env_infos/initial/reward_ctrl Mean      -0.0546199
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0546199
exploration/env_infos/initial/reward_ctrl Min       -0.0546199
exploration/env_infos/reward_ctrl Mean              -0.409265
exploration/env_infos/reward_ctrl Std                0.0868824
exploration/env_infos/reward_ctrl Max               -0.0546199
exploration/env_infos/reward_ctrl Min               -0.587885
evaluation/num steps total                      355000
evaluation/num paths total                         355
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.47233
evaluation/Rewards Std                               1.05117
evaluation/Rewards Max                               5.65572
evaluation/Rewards Min                              -1.15179
evaluation/Returns Mean                           3472.33
evaluation/Returns Std                             133.497
evaluation/Returns Max                            3611.52
evaluation/Returns Min                            3301.19
evaluation/Actions Mean                              0.000591738
evaluation/Actions Std                               0.83082
evaluation/Actions Max                               0.99968
evaluation/Actions Min                              -0.999968
evaluation/Num Paths                                 5
evaluation/Average Returns                        3472.33
evaluation/env_infos/final/reward_run Mean           4.05325
evaluation/env_infos/final/reward_run Std            0.927479
evaluation/env_infos/final/reward_run Max            5.36504
evaluation/env_infos/final/reward_run Min            2.83727
evaluation/env_infos/initial/reward_run Mean        -0.0640507
evaluation/env_infos/initial/reward_run Std          0.161738
evaluation/env_infos/initial/reward_run Max          0.153405
evaluation/env_infos/initial/reward_run Min         -0.28457
evaluation/env_infos/reward_run Mean                 3.88648
evaluation/env_infos/reward_run Std                  1.04301
evaluation/env_infos/reward_run Max                  6.09378
evaluation/env_infos/reward_run Min                 -0.759331
evaluation/env_infos/final/reward_ctrl Mean         -0.448393
evaluation/env_infos/final/reward_ctrl Std           0.0824721
evaluation/env_infos/final/reward_ctrl Max          -0.356272
evaluation/env_infos/final/reward_ctrl Min          -0.580262
evaluation/env_infos/initial/reward_ctrl Mean       -0.130312
evaluation/env_infos/initial/reward_ctrl Std         0.0516901
evaluation/env_infos/initial/reward_ctrl Max        -0.068049
evaluation/env_infos/initial/reward_ctrl Min        -0.19426
evaluation/env_infos/reward_ctrl Mean               -0.414158
evaluation/env_infos/reward_ctrl Std                 0.0877888
evaluation/env_infos/reward_ctrl Max                -0.068049
evaluation/env_infos/reward_ctrl Min                -0.59105
time/data storing (s)                                0.00671752
time/evaluation sampling (s)                         2.71454
time/exploration sampling (s)                        0.728793
time/logging (s)                                     0.0520497
time/saving (s)                                      0.021009
time/training (s)                                   34.126
time/epoch (s)                                      37.6492
time/total (s)                                    3057.23
Epoch                                               70
----------------------------------------------  ----------------
2020-07-08 21:57:55.384934 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 71 finished
----------------------------------------------  ---------------
replay_buffer/size                               73000
trainer/QF1 Loss                                     8.2649
trainer/QF2 Loss                                     7.83684
trainer/Policy Loss                               -125.295
trainer/Q1 Predictions Mean                        130.639
trainer/Q1 Predictions Std                          80.9317
trainer/Q1 Predictions Max                         221.038
trainer/Q1 Predictions Min                           3.09122
trainer/Q2 Predictions Mean                        130.567
trainer/Q2 Predictions Std                          80.8542
trainer/Q2 Predictions Max                         221.256
trainer/Q2 Predictions Min                           3.33591
trainer/Q Targets Mean                             130.729
trainer/Q Targets Std                               80.6857
trainer/Q Targets Max                              219.938
trainer/Q Targets Min                                2.77018
trainer/Log Pis Mean                                 5.86488
trainer/Log Pis Std                                  5.25981
trainer/Log Pis Max                                 23.0463
trainer/Log Pis Min                                 -6.29207
trainer/Policy mu Mean                               0.138941
trainer/Policy mu Std                                1.52717
trainer/Policy mu Max                                4.18919
trainer/Policy mu Min                               -4.74787
trainer/Policy log std Mean                         -0.792842
trainer/Policy log std Std                           0.30648
trainer/Policy log std Max                           0.0540411
trainer/Policy log std Min                          -1.90525
trainer/Alpha                                        0.0528661
trainer/Alpha Loss                                  -0.397235
exploration/num steps total                      73000
exploration/num paths total                         73
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.30649
exploration/Rewards Std                              1.05846
exploration/Rewards Max                              5.79767
exploration/Rewards Min                             -0.822344
exploration/Returns Mean                          3306.49
exploration/Returns Std                              0
exploration/Returns Max                           3306.49
exploration/Returns Min                           3306.49
exploration/Actions Mean                             0.0249001
exploration/Actions Std                              0.815867
exploration/Actions Max                              0.99984
exploration/Actions Min                             -0.999991
exploration/Num Paths                                1
exploration/Average Returns                       3306.49
exploration/env_infos/final/reward_run Mean          3.65751
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.65751
exploration/env_infos/final/reward_run Min           3.65751
exploration/env_infos/initial/reward_run Mean       -0.401798
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.401798
exploration/env_infos/initial/reward_run Min        -0.401798
exploration/env_infos/reward_run Mean                3.70625
exploration/env_infos/reward_run Std                 1.05144
exploration/env_infos/reward_run Max                 6.24307
exploration/env_infos/reward_run Min                -0.437338
exploration/env_infos/final/reward_ctrl Mean        -0.402278
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.402278
exploration/env_infos/final/reward_ctrl Min         -0.402278
exploration/env_infos/initial/reward_ctrl Mean      -0.163724
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.163724
exploration/env_infos/initial/reward_ctrl Min       -0.163724
exploration/env_infos/reward_ctrl Mean              -0.399755
exploration/env_infos/reward_ctrl Std                0.0851107
exploration/env_infos/reward_ctrl Max               -0.112731
exploration/env_infos/reward_ctrl Min               -0.583756
evaluation/num steps total                      360000
evaluation/num paths total                         360
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.46764
evaluation/Rewards Std                               1.07122
evaluation/Rewards Max                               5.80303
evaluation/Rewards Min                              -1.14482
evaluation/Returns Mean                           3467.64
evaluation/Returns Std                              95.3776
evaluation/Returns Max                            3653.77
evaluation/Returns Min                            3405.26
evaluation/Actions Mean                              0.0295182
evaluation/Actions Std                               0.824386
evaluation/Actions Max                               0.999852
evaluation/Actions Min                              -0.999931
evaluation/Num Paths                                 5
evaluation/Average Returns                        3467.64
evaluation/env_infos/final/reward_run Mean           4.09531
evaluation/env_infos/final/reward_run Std            0.855712
evaluation/env_infos/final/reward_run Max            5.09892
evaluation/env_infos/final/reward_run Min            3.05433
evaluation/env_infos/initial/reward_run Mean        -0.0649108
evaluation/env_infos/initial/reward_run Std          0.463782
evaluation/env_infos/initial/reward_run Max          0.74177
evaluation/env_infos/initial/reward_run Min         -0.513798
evaluation/env_infos/reward_run Mean                 3.87593
evaluation/env_infos/reward_run Std                  1.06503
evaluation/env_infos/reward_run Max                  6.09098
evaluation/env_infos/reward_run Min                 -0.578498
evaluation/env_infos/final/reward_ctrl Mean         -0.442675
evaluation/env_infos/final/reward_ctrl Std           0.0774725
evaluation/env_infos/final/reward_ctrl Max          -0.294854
evaluation/env_infos/final/reward_ctrl Min          -0.520214
evaluation/env_infos/initial/reward_ctrl Mean       -0.128202
evaluation/env_infos/initial/reward_ctrl Std         0.0661522
evaluation/env_infos/initial/reward_ctrl Max        -0.0294614
evaluation/env_infos/initial/reward_ctrl Min        -0.217271
evaluation/env_infos/reward_ctrl Mean               -0.40829
evaluation/env_infos/reward_ctrl Std                 0.0883756
evaluation/env_infos/reward_ctrl Max                -0.0294614
evaluation/env_infos/reward_ctrl Min                -0.593379
time/data storing (s)                                0.00811269
time/evaluation sampling (s)                         2.90179
time/exploration sampling (s)                        0.786924
time/logging (s)                                     0.0438644
time/saving (s)                                      0.0160303
time/training (s)                                   43.811
time/epoch (s)                                      47.5677
time/total (s)                                    3104.81
Epoch                                               71
----------------------------------------------  ---------------
2020-07-08 21:58:48.728550 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 72 finished
----------------------------------------------  ---------------
replay_buffer/size                               74000
trainer/QF1 Loss                                     7.94726
trainer/QF2 Loss                                     8.48911
trainer/Policy Loss                               -118.436
trainer/Q1 Predictions Mean                        123.521
trainer/Q1 Predictions Std                          84.2461
trainer/Q1 Predictions Max                         227.037
trainer/Q1 Predictions Min                           3.59067
trainer/Q2 Predictions Mean                        123.627
trainer/Q2 Predictions Std                          84.1792
trainer/Q2 Predictions Max                         230.469
trainer/Q2 Predictions Min                           3.28004
trainer/Q Targets Mean                             123.659
trainer/Q Targets Std                               84.4145
trainer/Q Targets Max                              227.488
trainer/Q Targets Min                                2.84735
trainer/Log Pis Mean                                 5.44461
trainer/Log Pis Std                                  5.48209
trainer/Log Pis Max                                 25.8717
trainer/Log Pis Min                                 -5.48057
trainer/Policy mu Mean                               0.0925465
trainer/Policy mu Std                                1.54194
trainer/Policy mu Max                                5.11372
trainer/Policy mu Min                               -5.06733
trainer/Policy log std Mean                         -0.747912
trainer/Policy log std Std                           0.311599
trainer/Policy log std Max                           0.270949
trainer/Policy log std Min                          -2.38372
trainer/Alpha                                        0.0549397
trainer/Alpha Loss                                  -1.61149
exploration/num steps total                      74000
exploration/num paths total                         74
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.29083
exploration/Rewards Std                              1.02645
exploration/Rewards Max                              5.42257
exploration/Rewards Min                             -0.938346
exploration/Returns Mean                          3290.83
exploration/Returns Std                              0
exploration/Returns Max                           3290.83
exploration/Returns Min                           3290.83
exploration/Actions Mean                             0.0332934
exploration/Actions Std                              0.810726
exploration/Actions Max                              0.999982
exploration/Actions Min                             -0.999966
exploration/Num Paths                                1
exploration/Average Returns                       3290.83
exploration/env_infos/final/reward_run Mean          1.16417
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.16417
exploration/env_infos/final/reward_run Min           1.16417
exploration/env_infos/initial/reward_run Mean        0.874798
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.874798
exploration/env_infos/initial/reward_run Min         0.874798
exploration/env_infos/reward_run Mean                3.68587
exploration/env_infos/reward_run Std                 1.02008
exploration/env_infos/reward_run Max                 5.83643
exploration/env_infos/reward_run Min                -0.392567
exploration/env_infos/final/reward_ctrl Mean        -0.314185
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.314185
exploration/env_infos/final/reward_ctrl Min         -0.314185
exploration/env_infos/initial/reward_ctrl Mean      -0.171662
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.171662
exploration/env_infos/initial/reward_ctrl Min       -0.171662
exploration/env_infos/reward_ctrl Mean              -0.395031
exploration/env_infos/reward_ctrl Std                0.0923696
exploration/env_infos/reward_ctrl Max               -0.12177
exploration/env_infos/reward_ctrl Min               -0.58316
evaluation/num steps total                      365000
evaluation/num paths total                         365
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.45962
evaluation/Rewards Std                               1.05615
evaluation/Rewards Max                               5.69
evaluation/Rewards Min                              -1.88254
evaluation/Returns Mean                           3459.62
evaluation/Returns Std                              63.129
evaluation/Returns Max                            3567.87
evaluation/Returns Min                            3390.72
evaluation/Actions Mean                              0.0378632
evaluation/Actions Std                               0.821609
evaluation/Actions Max                               0.999957
evaluation/Actions Min                              -0.999948
evaluation/Num Paths                                 5
evaluation/Average Returns                        3459.62
evaluation/env_infos/final/reward_run Mean           3.15444
evaluation/env_infos/final/reward_run Std            0.814999
evaluation/env_infos/final/reward_run Max            4.77888
evaluation/env_infos/final/reward_run Min            2.65972
evaluation/env_infos/initial/reward_run Mean        -0.107324
evaluation/env_infos/initial/reward_run Std          0.369266
evaluation/env_infos/initial/reward_run Max          0.441841
evaluation/env_infos/initial/reward_run Min         -0.509879
evaluation/env_infos/reward_run Mean                 3.86551
evaluation/env_infos/reward_run Std                  1.04245
evaluation/env_infos/reward_run Max                  6.24256
evaluation/env_infos/reward_run Min                 -1.40398
evaluation/env_infos/final/reward_ctrl Mean         -0.430278
evaluation/env_infos/final/reward_ctrl Std           0.0757422
evaluation/env_infos/final/reward_ctrl Max          -0.334464
evaluation/env_infos/final/reward_ctrl Min          -0.567403
evaluation/env_infos/initial/reward_ctrl Mean       -0.179627
evaluation/env_infos/initial/reward_ctrl Std         0.101921
evaluation/env_infos/initial/reward_ctrl Max        -0.0372873
evaluation/env_infos/initial/reward_ctrl Min        -0.309184
evaluation/env_infos/reward_ctrl Mean               -0.405885
evaluation/env_infos/reward_ctrl Std                 0.0907233
evaluation/env_infos/reward_ctrl Max                -0.0372873
evaluation/env_infos/reward_ctrl Min                -0.592982
time/data storing (s)                                0.00702922
time/evaluation sampling (s)                         2.63438
time/exploration sampling (s)                        1.10067
time/logging (s)                                     0.04449
time/saving (s)                                      0.0204781
time/training (s)                                   49.5192
time/epoch (s)                                      53.3262
time/total (s)                                    3158.15
Epoch                                               72
----------------------------------------------  ---------------
2020-07-08 21:59:38.270772 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 73 finished
----------------------------------------------  ---------------
replay_buffer/size                               75000
trainer/QF1 Loss                                     6.27426
trainer/QF2 Loss                                     6.83103
trainer/Policy Loss                               -122.947
trainer/Q1 Predictions Mean                        128.254
trainer/Q1 Predictions Std                          85.502
trainer/Q1 Predictions Max                         228.025
trainer/Q1 Predictions Min                           4.89328
trainer/Q2 Predictions Mean                        128.321
trainer/Q2 Predictions Std                          85.4048
trainer/Q2 Predictions Max                         227.171
trainer/Q2 Predictions Min                           4.30033
trainer/Q Targets Mean                             128.647
trainer/Q Targets Std                               85.7498
trainer/Q Targets Max                              228.13
trainer/Q Targets Min                                3.96895
trainer/Log Pis Mean                                 5.82256
trainer/Log Pis Std                                  5.18294
trainer/Log Pis Max                                 19.2505
trainer/Log Pis Min                                 -5.63245
trainer/Policy mu Mean                               0.0563161
trainer/Policy mu Std                                1.53071
trainer/Policy mu Max                                5.97822
trainer/Policy mu Min                               -4.60963
trainer/Policy log std Mean                         -0.769783
trainer/Policy log std Std                           0.323164
trainer/Policy log std Max                           0.313603
trainer/Policy log std Min                          -1.84543
trainer/Alpha                                        0.0550932
trainer/Alpha Loss                                  -0.514324
exploration/num steps total                      75000
exploration/num paths total                         75
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.55225
exploration/Rewards Std                              1.01268
exploration/Rewards Max                              5.48908
exploration/Rewards Min                             -0.87111
exploration/Returns Mean                          3552.25
exploration/Returns Std                              0
exploration/Returns Max                           3552.25
exploration/Returns Min                           3552.25
exploration/Actions Mean                             0.011698
exploration/Actions Std                              0.804337
exploration/Actions Max                              0.999959
exploration/Actions Min                             -0.999999
exploration/Num Paths                                1
exploration/Average Returns                       3552.25
exploration/env_infos/final/reward_run Mean          2.61995
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.61995
exploration/env_infos/final/reward_run Min           2.61995
exploration/env_infos/initial/reward_run Mean        0.212909
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.212909
exploration/env_infos/initial/reward_run Min         0.212909
exploration/env_infos/reward_run Mean                3.94051
exploration/env_infos/reward_run Std                 1.00031
exploration/env_infos/reward_run Max                 5.93185
exploration/env_infos/reward_run Min                -0.486123
exploration/env_infos/final/reward_ctrl Mean        -0.439804
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.439804
exploration/env_infos/final/reward_ctrl Min         -0.439804
exploration/env_infos/initial/reward_ctrl Mean      -0.0555592
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0555592
exploration/env_infos/initial/reward_ctrl Min       -0.0555592
exploration/env_infos/reward_ctrl Mean              -0.388257
exploration/env_infos/reward_ctrl Std                0.0925273
exploration/env_infos/reward_ctrl Max               -0.0516355
exploration/env_infos/reward_ctrl Min               -0.590449
evaluation/num steps total                      370000
evaluation/num paths total                         370
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.47771
evaluation/Rewards Std                               1.10309
evaluation/Rewards Max                               5.90623
evaluation/Rewards Min                              -1.23252
evaluation/Returns Mean                           3477.71
evaluation/Returns Std                             137.841
evaluation/Returns Max                            3695.51
evaluation/Returns Min                            3267.91
evaluation/Actions Mean                              0.00289604
evaluation/Actions Std                               0.814024
evaluation/Actions Max                               0.999913
evaluation/Actions Min                              -0.999995
evaluation/Num Paths                                 5
evaluation/Average Returns                        3477.71
evaluation/env_infos/final/reward_run Mean           4.45959
evaluation/env_infos/final/reward_run Std            0.776988
evaluation/env_infos/final/reward_run Max            5.6068
evaluation/env_infos/final/reward_run Min            3.24366
evaluation/env_infos/initial/reward_run Mean        -0.0458119
evaluation/env_infos/initial/reward_run Std          0.275502
evaluation/env_infos/initial/reward_run Max          0.385378
evaluation/env_infos/initial/reward_run Min         -0.477146
evaluation/env_infos/reward_run Mean                 3.87529
evaluation/env_infos/reward_run Std                  1.09695
evaluation/env_infos/reward_run Max                  6.26567
evaluation/env_infos/reward_run Min                 -0.793581
evaluation/env_infos/final/reward_ctrl Mean         -0.42529
evaluation/env_infos/final/reward_ctrl Std           0.0546416
evaluation/env_infos/final/reward_ctrl Max          -0.333557
evaluation/env_infos/final/reward_ctrl Min          -0.480967
evaluation/env_infos/initial/reward_ctrl Mean       -0.0954485
evaluation/env_infos/initial/reward_ctrl Std         0.0522106
evaluation/env_infos/initial/reward_ctrl Max        -0.0403259
evaluation/env_infos/initial/reward_ctrl Min        -0.171604
evaluation/env_infos/reward_ctrl Mean               -0.397586
evaluation/env_infos/reward_ctrl Std                 0.093366
evaluation/env_infos/reward_ctrl Max                -0.0297855
evaluation/env_infos/reward_ctrl Min                -0.591568
time/data storing (s)                                0.00669296
time/evaluation sampling (s)                         3.55443
time/exploration sampling (s)                        1.89468
time/logging (s)                                     0.045707
time/saving (s)                                      0.0185339
time/training (s)                                   43.9955
time/epoch (s)                                      49.5155
time/total (s)                                    3207.69
Epoch                                               73
----------------------------------------------  ---------------
2020-07-08 22:00:19.917646 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 74 finished
----------------------------------------------  ---------------
replay_buffer/size                               76000
trainer/QF1 Loss                                     6.09549
trainer/QF2 Loss                                     7.40135
trainer/Policy Loss                               -135.735
trainer/Q1 Predictions Mean                        141.32
trainer/Q1 Predictions Std                          83.6696
trainer/Q1 Predictions Max                         228.602
trainer/Q1 Predictions Min                           5.01288
trainer/Q2 Predictions Mean                        141.191
trainer/Q2 Predictions Std                          83.4981
trainer/Q2 Predictions Max                         226.997
trainer/Q2 Predictions Min                           5.26712
trainer/Q Targets Mean                             140.874
trainer/Q Targets Std                               83.5788
trainer/Q Targets Max                              229.862
trainer/Q Targets Min                                3.17035
trainer/Log Pis Mean                                 5.89525
trainer/Log Pis Std                                  4.88596
trainer/Log Pis Max                                 22.2126
trainer/Log Pis Min                                 -5.38721
trainer/Policy mu Mean                               0.101099
trainer/Policy mu Std                                1.53103
trainer/Policy mu Max                                4.11402
trainer/Policy mu Min                               -4.40761
trainer/Policy log std Mean                         -0.779245
trainer/Policy log std Std                           0.310087
trainer/Policy log std Max                          -0.0615057
trainer/Policy log std Min                          -1.91758
trainer/Alpha                                        0.0549064
trainer/Alpha Loss                                  -0.303983
exploration/num steps total                      76000
exploration/num paths total                         76
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.16238
exploration/Rewards Std                              1.06801
exploration/Rewards Max                              5.33305
exploration/Rewards Min                             -1.04923
exploration/Returns Mean                          3162.38
exploration/Returns Std                              0
exploration/Returns Max                           3162.38
exploration/Returns Min                           3162.38
exploration/Actions Mean                             0.00677708
exploration/Actions Std                              0.815333
exploration/Actions Max                              0.999949
exploration/Actions Min                             -0.999979
exploration/Num Paths                                1
exploration/Average Returns                       3162.38
exploration/env_infos/final/reward_run Mean          4.891
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.891
exploration/env_infos/final/reward_run Min           4.891
exploration/env_infos/initial/reward_run Mean        0.298065
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.298065
exploration/env_infos/initial/reward_run Min         0.298065
exploration/env_infos/reward_run Mean                3.56127
exploration/env_infos/reward_run Std                 1.06427
exploration/env_infos/reward_run Max                 5.70569
exploration/env_infos/reward_run Min                -0.548741
exploration/env_infos/final/reward_ctrl Mean        -0.51165
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.51165
exploration/env_infos/final/reward_ctrl Min         -0.51165
exploration/env_infos/initial/reward_ctrl Mean      -0.145334
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.145334
exploration/env_infos/initial/reward_ctrl Min       -0.145334
exploration/env_infos/reward_ctrl Mean              -0.398888
exploration/env_infos/reward_ctrl Std                0.0914308
exploration/env_infos/reward_ctrl Max               -0.0704395
exploration/env_infos/reward_ctrl Min               -0.588087
evaluation/num steps total                      375000
evaluation/num paths total                         375
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.54554
evaluation/Rewards Std                               1.09424
evaluation/Rewards Max                               5.80545
evaluation/Rewards Min                              -1.03862
evaluation/Returns Mean                           3545.54
evaluation/Returns Std                              39.5966
evaluation/Returns Max                            3587.81
evaluation/Returns Min                            3479.94
evaluation/Actions Mean                              0.00356908
evaluation/Actions Std                               0.819505
evaluation/Actions Max                               0.999842
evaluation/Actions Min                              -0.999995
evaluation/Num Paths                                 5
evaluation/Average Returns                        3545.54
evaluation/env_infos/final/reward_run Mean           3.66809
evaluation/env_infos/final/reward_run Std            0.915581
evaluation/env_infos/final/reward_run Max            5.302
evaluation/env_infos/final/reward_run Min            2.5167
evaluation/env_infos/initial/reward_run Mean         0.145722
evaluation/env_infos/initial/reward_run Std          0.229604
evaluation/env_infos/initial/reward_run Max          0.492361
evaluation/env_infos/initial/reward_run Min         -0.118793
evaluation/env_infos/reward_run Mean                 3.9485
evaluation/env_infos/reward_run Std                  1.09008
evaluation/env_infos/reward_run Max                  6.16009
evaluation/env_infos/reward_run Min                 -0.513679
evaluation/env_infos/final/reward_ctrl Mean         -0.416946
evaluation/env_infos/final/reward_ctrl Std           0.0731795
evaluation/env_infos/final/reward_ctrl Max          -0.325256
evaluation/env_infos/final/reward_ctrl Min          -0.492494
evaluation/env_infos/initial/reward_ctrl Mean       -0.110948
evaluation/env_infos/initial/reward_ctrl Std         0.0450785
evaluation/env_infos/initial/reward_ctrl Max        -0.0499066
evaluation/env_infos/initial/reward_ctrl Min        -0.181437
evaluation/env_infos/reward_ctrl Mean               -0.40296
evaluation/env_infos/reward_ctrl Std                 0.0867652
evaluation/env_infos/reward_ctrl Max                -0.0499066
evaluation/env_infos/reward_ctrl Min                -0.592198
time/data storing (s)                                0.00726466
time/evaluation sampling (s)                         3.36451
time/exploration sampling (s)                        0.720961
time/logging (s)                                     0.0427967
time/saving (s)                                      0.0163449
time/training (s)                                   37.4736
time/epoch (s)                                      41.6255
time/total (s)                                    3249.33
Epoch                                               74
----------------------------------------------  ---------------
2020-07-08 22:00:59.471746 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 75 finished
----------------------------------------------  ---------------
replay_buffer/size                               77000
trainer/QF1 Loss                                     7.70398
trainer/QF2 Loss                                     7.61172
trainer/Policy Loss                               -135.486
trainer/Q1 Predictions Mean                        141.344
trainer/Q1 Predictions Std                          84.8517
trainer/Q1 Predictions Max                         231.085
trainer/Q1 Predictions Min                           3.38564
trainer/Q2 Predictions Mean                        141.113
trainer/Q2 Predictions Std                          84.9509
trainer/Q2 Predictions Max                         231.844
trainer/Q2 Predictions Min                           1.8562
trainer/Q Targets Mean                             141.664
trainer/Q Targets Std                               85.2788
trainer/Q Targets Max                              231.292
trainer/Q Targets Min                                4.30128
trainer/Log Pis Mean                                 6.0893
trainer/Log Pis Std                                  5.34313
trainer/Log Pis Max                                 23.4246
trainer/Log Pis Min                                 -5.40078
trainer/Policy mu Mean                               0.118489
trainer/Policy mu Std                                1.56168
trainer/Policy mu Max                                4.92674
trainer/Policy mu Min                               -4.44729
trainer/Policy log std Mean                         -0.771018
trainer/Policy log std Std                           0.313096
trainer/Policy log std Max                           0.0596988
trainer/Policy log std Min                          -2.00644
trainer/Alpha                                        0.0570434
trainer/Alpha Loss                                   0.255743
exploration/num steps total                      77000
exploration/num paths total                         77
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.51824
exploration/Rewards Std                              1.1467
exploration/Rewards Max                              5.88082
exploration/Rewards Min                             -0.590579
exploration/Returns Mean                          3518.24
exploration/Returns Std                              0
exploration/Returns Max                           3518.24
exploration/Returns Min                           3518.24
exploration/Actions Mean                             0.0104418
exploration/Actions Std                              0.834526
exploration/Actions Max                              0.999958
exploration/Actions Min                             -0.999996
exploration/Num Paths                                1
exploration/Average Returns                       3518.24
exploration/env_infos/final/reward_run Mean          4.51085
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.51085
exploration/env_infos/final/reward_run Min           4.51085
exploration/env_infos/initial/reward_run Mean       -0.461875
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.461875
exploration/env_infos/initial/reward_run Min        -0.461875
exploration/env_infos/reward_run Mean                3.93617
exploration/env_infos/reward_run Std                 1.15279
exploration/env_infos/reward_run Max                 6.32778
exploration/env_infos/reward_run Min                -0.461875
exploration/env_infos/final/reward_ctrl Mean        -0.191742
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.191742
exploration/env_infos/final/reward_ctrl Min         -0.191742
exploration/env_infos/initial/reward_ctrl Mean      -0.10084
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.10084
exploration/env_infos/initial/reward_ctrl Min       -0.10084
exploration/env_infos/reward_ctrl Mean              -0.417925
exploration/env_infos/reward_ctrl Std                0.0892816
exploration/env_infos/reward_ctrl Max               -0.099746
exploration/env_infos/reward_ctrl Min               -0.583081
evaluation/num steps total                      380000
evaluation/num paths total                         380
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.43056
evaluation/Rewards Std                               1.15374
evaluation/Rewards Max                               5.95057
evaluation/Rewards Min                              -1.47363
evaluation/Returns Mean                           3430.56
evaluation/Returns Std                             155.383
evaluation/Returns Max                            3691.05
evaluation/Returns Min                            3243.88
evaluation/Actions Mean                              0.00332436
evaluation/Actions Std                               0.845916
evaluation/Actions Max                               0.999913
evaluation/Actions Min                              -0.999995
evaluation/Num Paths                                 5
evaluation/Average Returns                        3430.56
evaluation/env_infos/final/reward_run Mean           3.25121
evaluation/env_infos/final/reward_run Std            0.688173
evaluation/env_infos/final/reward_run Max            4.09981
evaluation/env_infos/final/reward_run Min            2.18061
evaluation/env_infos/initial/reward_run Mean         0.198184
evaluation/env_infos/initial/reward_run Std          0.382474
evaluation/env_infos/initial/reward_run Max          0.660189
evaluation/env_infos/initial/reward_run Min         -0.292994
evaluation/env_infos/reward_run Mean                 3.85991
evaluation/env_infos/reward_run Std                  1.15271
evaluation/env_infos/reward_run Max                  6.36554
evaluation/env_infos/reward_run Min                 -1.02012
evaluation/env_infos/final/reward_ctrl Mean         -0.459801
evaluation/env_infos/final/reward_ctrl Std           0.0484626
evaluation/env_infos/final/reward_ctrl Max          -0.394205
evaluation/env_infos/final/reward_ctrl Min          -0.530015
evaluation/env_infos/initial/reward_ctrl Mean       -0.14968
evaluation/env_infos/initial/reward_ctrl Std         0.0738158
evaluation/env_infos/initial/reward_ctrl Max        -0.0483223
evaluation/env_infos/initial/reward_ctrl Min        -0.233167
evaluation/env_infos/reward_ctrl Mean               -0.429351
evaluation/env_infos/reward_ctrl Std                 0.0863665
evaluation/env_infos/reward_ctrl Max                -0.0483223
evaluation/env_infos/reward_ctrl Min                -0.591058
time/data storing (s)                                0.00840864
time/evaluation sampling (s)                         2.85514
time/exploration sampling (s)                        0.985938
time/logging (s)                                     0.066794
time/saving (s)                                      0.0179776
time/training (s)                                   35.6277
time/epoch (s)                                      39.562
time/total (s)                                    3288.9
Epoch                                               75
----------------------------------------------  ---------------
2020-07-08 22:01:41.320417 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 76 finished
----------------------------------------------  ---------------
replay_buffer/size                               78000
trainer/QF1 Loss                                    10.0466
trainer/QF2 Loss                                    11.8061
trainer/Policy Loss                               -140.962
trainer/Q1 Predictions Mean                        147.028
trainer/Q1 Predictions Std                          83.5769
trainer/Q1 Predictions Max                         234.915
trainer/Q1 Predictions Min                           3.81888
trainer/Q2 Predictions Mean                        146.824
trainer/Q2 Predictions Std                          83.5418
trainer/Q2 Predictions Max                         235.263
trainer/Q2 Predictions Min                           4.05619
trainer/Q Targets Mean                             146.744
trainer/Q Targets Std                               83.4569
trainer/Q Targets Max                              234.09
trainer/Q Targets Min                                3.09886
trainer/Log Pis Mean                                 6.46736
trainer/Log Pis Std                                  5.8178
trainer/Log Pis Max                                 26.214
trainer/Log Pis Min                                 -4.57018
trainer/Policy mu Mean                              -0.0434908
trainer/Policy mu Std                                1.60276
trainer/Policy mu Max                                4.87958
trainer/Policy mu Min                               -4.93283
trainer/Policy log std Mean                         -0.798657
trainer/Policy log std Std                           0.333231
trainer/Policy log std Max                           0.285711
trainer/Policy log std Min                          -2.08242
trainer/Alpha                                        0.0571486
trainer/Alpha Loss                                   1.33765
exploration/num steps total                      78000
exploration/num paths total                         78
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.40592
exploration/Rewards Std                              1.06442
exploration/Rewards Max                              5.62233
exploration/Rewards Min                             -0.97961
exploration/Returns Mean                          3405.92
exploration/Returns Std                              0
exploration/Returns Max                           3405.92
exploration/Returns Min                           3405.92
exploration/Actions Mean                            -0.0210698
exploration/Actions Std                              0.815933
exploration/Actions Max                              0.99996
exploration/Actions Min                             -0.999997
exploration/Num Paths                                1
exploration/Average Returns                       3405.92
exploration/env_infos/final/reward_run Mean          5.366
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.366
exploration/env_infos/final/reward_run Min           5.366
exploration/env_infos/initial/reward_run Mean        0.23307
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.23307
exploration/env_infos/initial/reward_run Min         0.23307
exploration/env_infos/reward_run Mean                3.80563
exploration/env_infos/reward_run Std                 1.061
exploration/env_infos/reward_run Max                 6.01592
exploration/env_infos/reward_run Min                -0.579427
exploration/env_infos/final/reward_ctrl Mean        -0.5723
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.5723
exploration/env_infos/final/reward_ctrl Min         -0.5723
exploration/env_infos/initial/reward_ctrl Mean      -0.0475695
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0475695
exploration/env_infos/initial/reward_ctrl Min       -0.0475695
exploration/env_infos/reward_ctrl Mean              -0.399714
exploration/env_infos/reward_ctrl Std                0.086444
exploration/env_infos/reward_ctrl Max               -0.0475695
exploration/env_infos/reward_ctrl Min               -0.590438
evaluation/num steps total                      385000
evaluation/num paths total                         385
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.62613
evaluation/Rewards Std                               1.04681
evaluation/Rewards Max                               5.96457
evaluation/Rewards Min                              -0.966964
evaluation/Returns Mean                           3626.13
evaluation/Returns Std                              87.0767
evaluation/Returns Max                            3707.28
evaluation/Returns Min                            3503.5
evaluation/Actions Mean                             -0.0154758
evaluation/Actions Std                               0.824624
evaluation/Actions Max                               0.999853
evaluation/Actions Min                              -0.999966
evaluation/Num Paths                                 5
evaluation/Average Returns                        3626.13
evaluation/env_infos/final/reward_run Mean           4.26565
evaluation/env_infos/final/reward_run Std            0.919154
evaluation/env_infos/final/reward_run Max            5.78253
evaluation/env_infos/final/reward_run Min            3.17145
evaluation/env_infos/initial/reward_run Mean         0.0794729
evaluation/env_infos/initial/reward_run Std          0.383265
evaluation/env_infos/initial/reward_run Max          0.726192
evaluation/env_infos/initial/reward_run Min         -0.386983
evaluation/env_infos/reward_run Mean                 4.03428
evaluation/env_infos/reward_run Std                  1.03997
evaluation/env_infos/reward_run Max                  6.44318
evaluation/env_infos/reward_run Min                 -0.487431
evaluation/env_infos/final/reward_ctrl Mean         -0.419112
evaluation/env_infos/final/reward_ctrl Std           0.058267
evaluation/env_infos/final/reward_ctrl Max          -0.366121
evaluation/env_infos/final/reward_ctrl Min          -0.521631
evaluation/env_infos/initial/reward_ctrl Mean       -0.128639
evaluation/env_infos/initial/reward_ctrl Std         0.0811366
evaluation/env_infos/initial/reward_ctrl Max        -0.0373758
evaluation/env_infos/initial/reward_ctrl Min        -0.247961
evaluation/env_infos/reward_ctrl Mean               -0.408146
evaluation/env_infos/reward_ctrl Std                 0.0850173
evaluation/env_infos/reward_ctrl Max                -0.0373758
evaluation/env_infos/reward_ctrl Min                -0.586356
time/data storing (s)                                0.00671734
time/evaluation sampling (s)                         2.8122
time/exploration sampling (s)                        0.727663
time/logging (s)                                     0.0421987
time/saving (s)                                      0.0167564
time/training (s)                                   38.1995
time/epoch (s)                                      41.805
time/total (s)                                    3330.72
Epoch                                               76
----------------------------------------------  ---------------
2020-07-08 22:02:26.454197 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 77 finished
----------------------------------------------  ---------------
replay_buffer/size                               79000
trainer/QF1 Loss                                     7.68831
trainer/QF2 Loss                                    10.1977
trainer/Policy Loss                               -131.286
trainer/Q1 Predictions Mean                        137.002
trainer/Q1 Predictions Std                          86.7817
trainer/Q1 Predictions Max                         236.895
trainer/Q1 Predictions Min                           4.19753
trainer/Q2 Predictions Mean                        136.418
trainer/Q2 Predictions Std                          86.4556
trainer/Q2 Predictions Max                         235.2
trainer/Q2 Predictions Min                           4.5109
trainer/Q Targets Mean                             137.048
trainer/Q Targets Std                               86.9699
trainer/Q Targets Max                              236.689
trainer/Q Targets Min                                3.95872
trainer/Log Pis Mean                                 5.63855
trainer/Log Pis Std                                  5.36115
trainer/Log Pis Max                                 22.4745
trainer/Log Pis Min                                 -5.01082
trainer/Policy mu Mean                               0.0379805
trainer/Policy mu Std                                1.52323
trainer/Policy mu Max                                4.59236
trainer/Policy mu Min                               -4.83994
trainer/Policy log std Mean                         -0.786305
trainer/Policy log std Std                           0.339718
trainer/Policy log std Max                           0.117548
trainer/Policy log std Min                          -1.93253
trainer/Alpha                                        0.0575197
trainer/Alpha Loss                                  -1.03219
exploration/num steps total                      79000
exploration/num paths total                         79
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.29826
exploration/Rewards Std                              1.05925
exploration/Rewards Max                              5.64098
exploration/Rewards Min                             -0.768116
exploration/Returns Mean                          3298.26
exploration/Returns Std                              0
exploration/Returns Max                           3298.26
exploration/Returns Min                           3298.26
exploration/Actions Mean                            -0.0283086
exploration/Actions Std                              0.829951
exploration/Actions Max                              0.999979
exploration/Actions Min                             -0.999994
exploration/Num Paths                                1
exploration/Average Returns                       3298.26
exploration/env_infos/final/reward_run Mean          4.3949
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.3949
exploration/env_infos/final/reward_run Min           4.3949
exploration/env_infos/initial/reward_run Mean       -0.228982
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.228982
exploration/env_infos/initial/reward_run Min        -0.228982
exploration/env_infos/reward_run Mean                3.71203
exploration/env_infos/reward_run Std                 1.05624
exploration/env_infos/reward_run Max                 6.1071
exploration/env_infos/reward_run Min                -0.349472
exploration/env_infos/final/reward_ctrl Mean        -0.34394
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.34394
exploration/env_infos/final/reward_ctrl Min         -0.34394
exploration/env_infos/initial/reward_ctrl Mean      -0.07757
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.07757
exploration/env_infos/initial/reward_ctrl Min       -0.07757
exploration/env_infos/reward_ctrl Mean              -0.413772
exploration/env_infos/reward_ctrl Std                0.0895075
exploration/env_infos/reward_ctrl Max               -0.07757
exploration/env_infos/reward_ctrl Min               -0.591705
evaluation/num steps total                      390000
evaluation/num paths total                         390
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.62316
evaluation/Rewards Std                               1.02564
evaluation/Rewards Max                               5.90817
evaluation/Rewards Min                              -1.26994
evaluation/Returns Mean                           3623.16
evaluation/Returns Std                              93.1671
evaluation/Returns Max                            3736.99
evaluation/Returns Min                            3459.16
evaluation/Actions Mean                             -0.0233947
evaluation/Actions Std                               0.835894
evaluation/Actions Max                               0.999649
evaluation/Actions Min                              -0.999943
evaluation/Num Paths                                 5
evaluation/Average Returns                        3623.16
evaluation/env_infos/final/reward_run Mean           4.13509
evaluation/env_infos/final/reward_run Std            0.890714
evaluation/env_infos/final/reward_run Max            5.16899
evaluation/env_infos/final/reward_run Min            3.05601
evaluation/env_infos/initial/reward_run Mean         0.219882
evaluation/env_infos/initial/reward_run Std          0.333281
evaluation/env_infos/initial/reward_run Max          0.777511
evaluation/env_infos/initial/reward_run Min         -0.248269
evaluation/env_infos/reward_run Mean                 4.04272
evaluation/env_infos/reward_run Std                  1.02397
evaluation/env_infos/reward_run Max                  6.29081
evaluation/env_infos/reward_run Min                 -0.80987
evaluation/env_infos/final/reward_ctrl Mean         -0.450465
evaluation/env_infos/final/reward_ctrl Std           0.0542666
evaluation/env_infos/final/reward_ctrl Max          -0.381174
evaluation/env_infos/final/reward_ctrl Min          -0.537585
evaluation/env_infos/initial/reward_ctrl Mean       -0.102002
evaluation/env_infos/initial/reward_ctrl Std         0.078044
evaluation/env_infos/initial/reward_ctrl Max        -0.0270011
evaluation/env_infos/initial/reward_ctrl Min        -0.235485
evaluation/env_infos/reward_ctrl Mean               -0.41956
evaluation/env_infos/reward_ctrl Std                 0.0909797
evaluation/env_infos/reward_ctrl Max                -0.0270011
evaluation/env_infos/reward_ctrl Min                -0.596583
time/data storing (s)                                0.00699593
time/evaluation sampling (s)                         3.1632
time/exploration sampling (s)                        0.689299
time/logging (s)                                     0.0401159
time/saving (s)                                      0.0161457
time/training (s)                                   41.1731
time/epoch (s)                                      45.0888
time/total (s)                                    3375.85
Epoch                                               77
----------------------------------------------  ---------------
2020-07-08 22:03:14.935171 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 78 finished
----------------------------------------------  ---------------
replay_buffer/size                               80000
trainer/QF1 Loss                                     5.72667
trainer/QF2 Loss                                     5.49307
trainer/Policy Loss                               -134.353
trainer/Q1 Predictions Mean                        140.374
trainer/Q1 Predictions Std                          89.6944
trainer/Q1 Predictions Max                         240.088
trainer/Q1 Predictions Min                           4.10164
trainer/Q2 Predictions Mean                        140.056
trainer/Q2 Predictions Std                          89.4537
trainer/Q2 Predictions Max                         240.526
trainer/Q2 Predictions Min                           3.31466
trainer/Q Targets Mean                             139.606
trainer/Q Targets Std                               89.3505
trainer/Q Targets Max                              239.918
trainer/Q Targets Min                                2.25417
trainer/Log Pis Mean                                 6.2874
trainer/Log Pis Std                                  5.8353
trainer/Log Pis Max                                 28.9658
trainer/Log Pis Min                                 -7.17702
trainer/Policy mu Mean                              -0.0643785
trainer/Policy mu Std                                1.62097
trainer/Policy mu Max                                5.54649
trainer/Policy mu Min                               -7.65968
trainer/Policy log std Mean                         -0.748628
trainer/Policy log std Std                           0.311177
trainer/Policy log std Max                           0.231934
trainer/Policy log std Min                          -1.97332
trainer/Alpha                                        0.0584805
trainer/Alpha Loss                                   0.815995
exploration/num steps total                      80000
exploration/num paths total                         80
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.50596
exploration/Rewards Std                              1.07772
exploration/Rewards Max                              5.63536
exploration/Rewards Min                             -0.673156
exploration/Returns Mean                          3505.96
exploration/Returns Std                              0
exploration/Returns Max                           3505.96
exploration/Returns Min                           3505.96
exploration/Actions Mean                            -0.0155238
exploration/Actions Std                              0.827094
exploration/Actions Max                              0.999947
exploration/Actions Min                             -0.999946
exploration/Num Paths                                1
exploration/Average Returns                       3505.96
exploration/env_infos/final/reward_run Mean          5.17878
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.17878
exploration/env_infos/final/reward_run Min           5.17878
exploration/env_infos/initial/reward_run Mean        0.00363999
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.00363999
exploration/env_infos/initial/reward_run Min         0.00363999
exploration/env_infos/reward_run Mean                3.91655
exploration/env_infos/reward_run Std                 1.07867
exploration/env_infos/reward_run Max                 6.18651
exploration/env_infos/reward_run Min                -0.19885
exploration/env_infos/final/reward_ctrl Mean        -0.445707
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.445707
exploration/env_infos/final/reward_ctrl Min         -0.445707
exploration/env_infos/initial/reward_ctrl Mean      -0.0641743
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0641743
exploration/env_infos/initial/reward_ctrl Min       -0.0641743
exploration/env_infos/reward_ctrl Mean              -0.410596
exploration/env_infos/reward_ctrl Std                0.0861237
exploration/env_infos/reward_ctrl Max               -0.0641743
exploration/env_infos/reward_ctrl Min               -0.585125
evaluation/num steps total                      395000
evaluation/num paths total                         395
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.6829
evaluation/Rewards Std                               1.03459
evaluation/Rewards Max                               5.82225
evaluation/Rewards Min                              -1.26698
evaluation/Returns Mean                           3682.9
evaluation/Returns Std                              71.5888
evaluation/Returns Max                            3812.9
evaluation/Returns Min                            3599.13
evaluation/Actions Mean                             -0.0152499
evaluation/Actions Std                               0.831993
evaluation/Actions Max                               0.999718
evaluation/Actions Min                              -0.999934
evaluation/Num Paths                                 5
evaluation/Average Returns                        3682.9
evaluation/env_infos/final/reward_run Mean           4.2794
evaluation/env_infos/final/reward_run Std            0.9467
evaluation/env_infos/final/reward_run Max            4.94816
evaluation/env_infos/final/reward_run Min            2.44858
evaluation/env_infos/initial/reward_run Mean         0.00943522
evaluation/env_infos/initial/reward_run Std          0.279968
evaluation/env_infos/initial/reward_run Max          0.514177
evaluation/env_infos/initial/reward_run Min         -0.203342
evaluation/env_infos/reward_run Mean                 4.09837
evaluation/env_infos/reward_run Std                  1.02894
evaluation/env_infos/reward_run Max                  6.22284
evaluation/env_infos/reward_run Min                 -0.724162
evaluation/env_infos/final/reward_ctrl Mean         -0.437465
evaluation/env_infos/final/reward_ctrl Std           0.0637978
evaluation/env_infos/final/reward_ctrl Max          -0.332108
evaluation/env_infos/final/reward_ctrl Min          -0.505208
evaluation/env_infos/initial/reward_ctrl Mean       -0.0661111
evaluation/env_infos/initial/reward_ctrl Std         0.0297375
evaluation/env_infos/initial/reward_ctrl Max        -0.0329789
evaluation/env_infos/initial/reward_ctrl Min        -0.113646
evaluation/env_infos/reward_ctrl Mean               -0.415467
evaluation/env_infos/reward_ctrl Std                 0.0874022
evaluation/env_infos/reward_ctrl Max                -0.0329789
evaluation/env_infos/reward_ctrl Min                -0.586358
time/data storing (s)                                0.00668573
time/evaluation sampling (s)                         3.02671
time/exploration sampling (s)                        0.797903
time/logging (s)                                     0.0411469
time/saving (s)                                      0.015732
time/training (s)                                   44.5773
time/epoch (s)                                      48.4655
time/total (s)                                    3424.33
Epoch                                               78
----------------------------------------------  ---------------
2020-07-08 22:03:54.140332 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 79 finished
----------------------------------------------  ---------------
replay_buffer/size                               81000
trainer/QF1 Loss                                     4.69321
trainer/QF2 Loss                                     5.44663
trainer/Policy Loss                               -134.344
trainer/Q1 Predictions Mean                        139.667
trainer/Q1 Predictions Std                          92.1084
trainer/Q1 Predictions Max                         239.858
trainer/Q1 Predictions Min                           4.42933
trainer/Q2 Predictions Mean                        139.378
trainer/Q2 Predictions Std                          91.9314
trainer/Q2 Predictions Max                         236.323
trainer/Q2 Predictions Min                           3.57796
trainer/Q Targets Mean                             139.593
trainer/Q Targets Std                               92.151
trainer/Q Targets Max                              239.481
trainer/Q Targets Min                                3.38294
trainer/Log Pis Mean                                 5.33095
trainer/Log Pis Std                                  5.1077
trainer/Log Pis Max                                 20.774
trainer/Log Pis Min                                 -5.42756
trainer/Policy mu Mean                              -0.0688851
trainer/Policy mu Std                                1.4422
trainer/Policy mu Max                                4.18859
trainer/Policy mu Min                               -4.35181
trainer/Policy log std Mean                         -0.80255
trainer/Policy log std Std                           0.345094
trainer/Policy log std Max                           0.229076
trainer/Policy log std Min                          -2.11765
trainer/Alpha                                        0.057726
trainer/Alpha Loss                                  -1.90813
exploration/num steps total                      81000
exploration/num paths total                         81
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.49852
exploration/Rewards Std                              0.959531
exploration/Rewards Max                              5.67072
exploration/Rewards Min                             -1.02054
exploration/Returns Mean                          3498.52
exploration/Returns Std                              0
exploration/Returns Max                           3498.52
exploration/Returns Min                           3498.52
exploration/Actions Mean                            -0.00228452
exploration/Actions Std                              0.813152
exploration/Actions Max                              0.999971
exploration/Actions Min                             -0.999875
exploration/Num Paths                                1
exploration/Average Returns                       3498.52
exploration/env_infos/final/reward_run Mean          4.86786
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.86786
exploration/env_infos/final/reward_run Min           4.86786
exploration/env_infos/initial/reward_run Mean       -0.348968
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.348968
exploration/env_infos/initial/reward_run Min        -0.348968
exploration/env_infos/reward_run Mean                3.89525
exploration/env_infos/reward_run Std                 0.952029
exploration/env_infos/reward_run Max                 6.09658
exploration/env_infos/reward_run Min                -0.593199
exploration/env_infos/final/reward_ctrl Mean        -0.497201
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.497201
exploration/env_infos/final/reward_ctrl Min         -0.497201
exploration/env_infos/initial/reward_ctrl Mean      -0.192693
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.192693
exploration/env_infos/initial/reward_ctrl Min       -0.192693
exploration/env_infos/reward_ctrl Mean              -0.396733
exploration/env_infos/reward_ctrl Std                0.0910711
exploration/env_infos/reward_ctrl Max               -0.0897196
exploration/env_infos/reward_ctrl Min               -0.580212
evaluation/num steps total                      400000
evaluation/num paths total                         400
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.67426
evaluation/Rewards Std                               1.02176
evaluation/Rewards Max                               6.05845
evaluation/Rewards Min                              -1.52829
evaluation/Returns Mean                           3674.26
evaluation/Returns Std                              48.8094
evaluation/Returns Max                            3732.6
evaluation/Returns Min                            3609.41
evaluation/Actions Mean                             -0.0162114
evaluation/Actions Std                               0.819344
evaluation/Actions Max                               0.999789
evaluation/Actions Min                              -0.999879
evaluation/Num Paths                                 5
evaluation/Average Returns                        3674.26
evaluation/env_infos/final/reward_run Mean           4.78836
evaluation/env_infos/final/reward_run Std            0.745119
evaluation/env_infos/final/reward_run Max            5.83386
evaluation/env_infos/final/reward_run Min            3.69348
evaluation/env_infos/initial/reward_run Mean        -0.248762
evaluation/env_infos/initial/reward_run Std          0.223969
evaluation/env_infos/initial/reward_run Max          0.159581
evaluation/env_infos/initial/reward_run Min         -0.511609
evaluation/env_infos/reward_run Mean                 4.07721
evaluation/env_infos/reward_run Std                  1.01621
evaluation/env_infos/reward_run Max                  6.48111
evaluation/env_infos/reward_run Min                 -1.12966
evaluation/env_infos/final/reward_ctrl Mean         -0.430914
evaluation/env_infos/final/reward_ctrl Std           0.0649905
evaluation/env_infos/final/reward_ctrl Max          -0.317617
evaluation/env_infos/final/reward_ctrl Min          -0.490896
evaluation/env_infos/initial/reward_ctrl Mean       -0.112659
evaluation/env_infos/initial/reward_ctrl Std         0.0743507
evaluation/env_infos/initial/reward_ctrl Max        -0.0358347
evaluation/env_infos/initial/reward_ctrl Min        -0.210785
evaluation/env_infos/reward_ctrl Mean               -0.402953
evaluation/env_infos/reward_ctrl Std                 0.0902451
evaluation/env_infos/reward_ctrl Max                -0.0358347
evaluation/env_infos/reward_ctrl Min                -0.590444
time/data storing (s)                                0.00688956
time/evaluation sampling (s)                         2.54805
time/exploration sampling (s)                        0.635704
time/logging (s)                                     0.0445874
time/saving (s)                                      0.0172642
time/training (s)                                   35.9237
time/epoch (s)                                      39.1762
time/total (s)                                    3463.53
Epoch                                               79
----------------------------------------------  ---------------
2020-07-08 22:04:43.466637 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 80 finished
----------------------------------------------  ---------------
replay_buffer/size                               82000
trainer/QF1 Loss                                     9.59188
trainer/QF2 Loss                                    10.1989
trainer/Policy Loss                               -143.731
trainer/Q1 Predictions Mean                        149.549
trainer/Q1 Predictions Std                          86.8956
trainer/Q1 Predictions Max                         245.287
trainer/Q1 Predictions Min                           4.94648
trainer/Q2 Predictions Mean                        149.088
trainer/Q2 Predictions Std                          86.657
trainer/Q2 Predictions Max                         244.54
trainer/Q2 Predictions Min                           4.92279
trainer/Q Targets Mean                             149.44
trainer/Q Targets Std                               86.9759
trainer/Q Targets Max                              245.763
trainer/Q Targets Min                                4.15931
trainer/Log Pis Mean                                 5.92966
trainer/Log Pis Std                                  5.40414
trainer/Log Pis Max                                 24.5559
trainer/Log Pis Min                                 -5.11042
trainer/Policy mu Mean                               0.0845335
trainer/Policy mu Std                                1.55244
trainer/Policy mu Max                                4.88299
trainer/Policy mu Min                               -4.19509
trainer/Policy log std Mean                         -0.801298
trainer/Policy log std Std                           0.331351
trainer/Policy log std Max                           0.0487413
trainer/Policy log std Min                          -2.14637
trainer/Alpha                                        0.0590317
trainer/Alpha Loss                                  -0.199038
exploration/num steps total                      82000
exploration/num paths total                         82
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.37238
exploration/Rewards Std                              1.17472
exploration/Rewards Max                              5.90218
exploration/Rewards Min                             -0.89781
exploration/Returns Mean                          3372.38
exploration/Returns Std                              0
exploration/Returns Max                           3372.38
exploration/Returns Min                           3372.38
exploration/Actions Mean                             0.00950579
exploration/Actions Std                              0.815426
exploration/Actions Max                              0.999933
exploration/Actions Min                             -0.99998
exploration/Num Paths                                1
exploration/Average Returns                       3372.38
exploration/env_infos/final/reward_run Mean          3.27819
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.27819
exploration/env_infos/final/reward_run Min           3.27819
exploration/env_infos/initial/reward_run Mean       -0.576398
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.576398
exploration/env_infos/initial/reward_run Min        -0.576398
exploration/env_infos/reward_run Mean                3.77139
exploration/env_infos/reward_run Std                 1.17709
exploration/env_infos/reward_run Max                 6.14759
exploration/env_infos/reward_run Min                -0.584984
exploration/env_infos/final/reward_ctrl Mean        -0.369901
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.369901
exploration/env_infos/final/reward_ctrl Min         -0.369901
exploration/env_infos/initial/reward_ctrl Mean      -0.128594
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.128594
exploration/env_infos/initial/reward_ctrl Min       -0.128594
exploration/env_infos/reward_ctrl Mean              -0.399006
exploration/env_infos/reward_ctrl Std                0.0880611
exploration/env_infos/reward_ctrl Max               -0.0786436
exploration/env_infos/reward_ctrl Min               -0.580983
evaluation/num steps total                      405000
evaluation/num paths total                         405
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.72183
evaluation/Rewards Std                               1.03198
evaluation/Rewards Max                               5.92769
evaluation/Rewards Min                              -1.09906
evaluation/Returns Mean                           3721.83
evaluation/Returns Std                              64.9669
evaluation/Returns Max                            3778.11
evaluation/Returns Min                            3598.2
evaluation/Actions Mean                             -0.00216382
evaluation/Actions Std                               0.820467
evaluation/Actions Max                               0.999683
evaluation/Actions Min                              -0.999886
evaluation/Num Paths                                 5
evaluation/Average Returns                        3721.83
evaluation/env_infos/final/reward_run Mean           3.86144
evaluation/env_infos/final/reward_run Std            0.870544
evaluation/env_infos/final/reward_run Max            5.19476
evaluation/env_infos/final/reward_run Min            2.92284
evaluation/env_infos/initial/reward_run Mean         0.0653738
evaluation/env_infos/initial/reward_run Std          0.0712202
evaluation/env_infos/initial/reward_run Max          0.164803
evaluation/env_infos/initial/reward_run Min         -0.0217277
evaluation/env_infos/reward_run Mean                 4.12573
evaluation/env_infos/reward_run Std                  1.03315
evaluation/env_infos/reward_run Max                  6.3885
evaluation/env_infos/reward_run Min                 -0.605899
evaluation/env_infos/final/reward_ctrl Mean         -0.409555
evaluation/env_infos/final/reward_ctrl Std           0.0655279
evaluation/env_infos/final/reward_ctrl Max          -0.28722
evaluation/env_infos/final/reward_ctrl Min          -0.466745
evaluation/env_infos/initial/reward_ctrl Mean       -0.0484348
evaluation/env_infos/initial/reward_ctrl Std         0.017217
evaluation/env_infos/initial/reward_ctrl Max        -0.0216098
evaluation/env_infos/initial/reward_ctrl Min        -0.0662998
evaluation/env_infos/reward_ctrl Mean               -0.403902
evaluation/env_infos/reward_ctrl Std                 0.0875645
evaluation/env_infos/reward_ctrl Max                -0.0216098
evaluation/env_infos/reward_ctrl Min                -0.584058
time/data storing (s)                                0.00759734
time/evaluation sampling (s)                         3.72852
time/exploration sampling (s)                        0.875074
time/logging (s)                                     0.0439236
time/saving (s)                                      0.0168688
time/training (s)                                   44.6306
time/epoch (s)                                      49.3026
time/total (s)                                    3512.86
Epoch                                               80
----------------------------------------------  ---------------
2020-07-08 22:05:32.246494 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 81 finished
----------------------------------------------  ---------------
replay_buffer/size                               83000
trainer/QF1 Loss                                    10.1724
trainer/QF2 Loss                                     8.61722
trainer/Policy Loss                               -145.232
trainer/Q1 Predictions Mean                        150.752
trainer/Q1 Predictions Std                          90.8868
trainer/Q1 Predictions Max                         243.253
trainer/Q1 Predictions Min                           3.44575
trainer/Q2 Predictions Mean                        150.521
trainer/Q2 Predictions Std                          90.6493
trainer/Q2 Predictions Max                         243.177
trainer/Q2 Predictions Min                           3.96749
trainer/Q Targets Mean                             150.508
trainer/Q Targets Std                               90.7785
trainer/Q Targets Max                              244.526
trainer/Q Targets Min                                2.34044
trainer/Log Pis Mean                                 5.58045
trainer/Log Pis Std                                  5.01828
trainer/Log Pis Max                                 20.7841
trainer/Log Pis Min                                 -7.02739
trainer/Policy mu Mean                              -0.0482654
trainer/Policy mu Std                                1.5291
trainer/Policy mu Max                                4.81044
trainer/Policy mu Min                               -5.61362
trainer/Policy log std Mean                         -0.800446
trainer/Policy log std Std                           0.330589
trainer/Policy log std Max                          -0.00478047
trainer/Policy log std Min                          -1.92036
trainer/Alpha                                        0.0609185
trainer/Alpha Loss                                  -1.174
exploration/num steps total                      83000
exploration/num paths total                         83
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.48697
exploration/Rewards Std                              1.00957
exploration/Rewards Max                              5.91491
exploration/Rewards Min                             -0.861234
exploration/Returns Mean                          3486.97
exploration/Returns Std                              0
exploration/Returns Max                           3486.97
exploration/Returns Min                           3486.97
exploration/Actions Mean                            -0.00790465
exploration/Actions Std                              0.808858
exploration/Actions Max                              0.999888
exploration/Actions Min                             -0.999927
exploration/Num Paths                                1
exploration/Average Returns                       3486.97
exploration/env_infos/final/reward_run Mean          3.21316
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.21316
exploration/env_infos/final/reward_run Min           3.21316
exploration/env_infos/initial/reward_run Mean        0.0668183
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0668183
exploration/env_infos/initial/reward_run Min         0.0668183
exploration/env_infos/reward_run Mean                3.87956
exploration/env_infos/reward_run Std                 1.00611
exploration/env_infos/reward_run Max                 6.3227
exploration/env_infos/reward_run Min                -0.560185
exploration/env_infos/final/reward_ctrl Mean        -0.4389
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.4389
exploration/env_infos/final/reward_ctrl Min         -0.4389
exploration/env_infos/initial/reward_ctrl Mean      -0.0864065
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0864065
exploration/env_infos/initial/reward_ctrl Min       -0.0864065
exploration/env_infos/reward_ctrl Mean              -0.392588
exploration/env_infos/reward_ctrl Std                0.0880081
exploration/env_infos/reward_ctrl Max               -0.0864065
exploration/env_infos/reward_ctrl Min               -0.576675
evaluation/num steps total                      410000
evaluation/num paths total                         410
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.64666
evaluation/Rewards Std                               1.0537
evaluation/Rewards Max                               5.85656
evaluation/Rewards Min                              -1.2812
evaluation/Returns Mean                           3646.66
evaluation/Returns Std                              61.6386
evaluation/Returns Max                            3747.87
evaluation/Returns Min                            3583.97
evaluation/Actions Mean                             -0.0152331
evaluation/Actions Std                               0.812755
evaluation/Actions Max                               0.999807
evaluation/Actions Min                              -0.999899
evaluation/Num Paths                                 5
evaluation/Average Returns                        3646.66
evaluation/env_infos/final/reward_run Mean           3.97931
evaluation/env_infos/final/reward_run Std            0.704544
evaluation/env_infos/final/reward_run Max            4.91264
evaluation/env_infos/final/reward_run Min            2.99316
evaluation/env_infos/initial/reward_run Mean         0.0552577
evaluation/env_infos/initial/reward_run Std          0.448733
evaluation/env_infos/initial/reward_run Max          0.650475
evaluation/env_infos/initial/reward_run Min         -0.489306
evaluation/env_infos/reward_run Mean                 4.04314
evaluation/env_infos/reward_run Std                  1.05624
evaluation/env_infos/reward_run Max                  6.3312
evaluation/env_infos/reward_run Min                 -0.862563
evaluation/env_infos/final/reward_ctrl Mean         -0.415883
evaluation/env_infos/final/reward_ctrl Std           0.0850738
evaluation/env_infos/final/reward_ctrl Max          -0.295448
evaluation/env_infos/final/reward_ctrl Min          -0.512215
evaluation/env_infos/initial/reward_ctrl Mean       -0.144078
evaluation/env_infos/initial/reward_ctrl Std         0.0347675
evaluation/env_infos/initial/reward_ctrl Max        -0.0895372
evaluation/env_infos/initial/reward_ctrl Min        -0.192968
evaluation/env_infos/reward_ctrl Mean               -0.396482
evaluation/env_infos/reward_ctrl Std                 0.0868346
evaluation/env_infos/reward_ctrl Max                -0.082168
evaluation/env_infos/reward_ctrl Min                -0.588988
time/data storing (s)                                0.00692522
time/evaluation sampling (s)                         2.91575
time/exploration sampling (s)                        0.81494
time/logging (s)                                     0.0459388
time/saving (s)                                      0.0175051
time/training (s)                                   44.9331
time/epoch (s)                                      48.7342
time/total (s)                                    3561.63
Epoch                                               81
----------------------------------------------  ---------------
2020-07-08 22:06:16.455444 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 82 finished
----------------------------------------------  ----------------
replay_buffer/size                               84000
trainer/QF1 Loss                                     6.67772
trainer/QF2 Loss                                     6.22211
trainer/Policy Loss                               -155.297
trainer/Q1 Predictions Mean                        161.236
trainer/Q1 Predictions Std                          88.8431
trainer/Q1 Predictions Max                         246.776
trainer/Q1 Predictions Min                           3.87271
trainer/Q2 Predictions Mean                        161.27
trainer/Q2 Predictions Std                          88.9498
trainer/Q2 Predictions Max                         248.85
trainer/Q2 Predictions Min                           3.97504
trainer/Q Targets Mean                             161.685
trainer/Q Targets Std                               89.2164
trainer/Q Targets Max                              250.131
trainer/Q Targets Min                                5.03086
trainer/Log Pis Mean                                 6.12815
trainer/Log Pis Std                                  5.25313
trainer/Log Pis Max                                 25.2397
trainer/Log Pis Min                                 -6.61052
trainer/Policy mu Mean                               0.0264526
trainer/Policy mu Std                                1.52145
trainer/Policy mu Max                                4.34689
trainer/Policy mu Min                               -4.94939
trainer/Policy log std Mean                         -0.842447
trainer/Policy log std Std                           0.344641
trainer/Policy log std Max                          -0.0651665
trainer/Policy log std Min                          -1.95397
trainer/Alpha                                        0.0601
trainer/Alpha Loss                                   0.360352
exploration/num steps total                      84000
exploration/num paths total                         84
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.46915
exploration/Rewards Std                              1.08507
exploration/Rewards Max                              5.9891
exploration/Rewards Min                             -0.494257
exploration/Returns Mean                          3469.15
exploration/Returns Std                              0
exploration/Returns Max                           3469.15
exploration/Returns Min                           3469.15
exploration/Actions Mean                             0.000473087
exploration/Actions Std                              0.818813
exploration/Actions Max                              0.999915
exploration/Actions Min                             -0.999962
exploration/Num Paths                                1
exploration/Average Returns                       3469.15
exploration/env_infos/final/reward_run Mean          5.04409
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.04409
exploration/env_infos/final/reward_run Min           5.04409
exploration/env_infos/initial/reward_run Mean       -0.349516
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.349516
exploration/env_infos/initial/reward_run Min        -0.349516
exploration/env_infos/reward_run Mean                3.87142
exploration/env_infos/reward_run Std                 1.0854
exploration/env_infos/reward_run Max                 6.32715
exploration/env_infos/reward_run Min                -0.349516
exploration/env_infos/final/reward_ctrl Mean        -0.328275
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.328275
exploration/env_infos/final/reward_ctrl Min         -0.328275
exploration/env_infos/initial/reward_ctrl Mean      -0.0880465
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0880465
exploration/env_infos/initial/reward_ctrl Min       -0.0880465
exploration/env_infos/reward_ctrl Mean              -0.402273
exploration/env_infos/reward_ctrl Std                0.0908286
exploration/env_infos/reward_ctrl Max               -0.0880465
exploration/env_infos/reward_ctrl Min               -0.584859
evaluation/num steps total                      415000
evaluation/num paths total                         415
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.69688
evaluation/Rewards Std                               0.992231
evaluation/Rewards Max                               5.84673
evaluation/Rewards Min                              -1.0422
evaluation/Returns Mean                           3696.88
evaluation/Returns Std                              87.4285
evaluation/Returns Max                            3830.64
evaluation/Returns Min                            3588.41
evaluation/Actions Mean                             -0.004203
evaluation/Actions Std                               0.825703
evaluation/Actions Max                               0.999797
evaluation/Actions Min                              -0.99997
evaluation/Num Paths                                 5
evaluation/Average Returns                        3696.88
evaluation/env_infos/final/reward_run Mean           4.59202
evaluation/env_infos/final/reward_run Std            0.42389
evaluation/env_infos/final/reward_run Max            5.00525
evaluation/env_infos/final/reward_run Min            4.07141
evaluation/env_infos/initial/reward_run Mean         0.221442
evaluation/env_infos/initial/reward_run Std          0.159155
evaluation/env_infos/initial/reward_run Max          0.501218
evaluation/env_infos/initial/reward_run Min          0.0438718
evaluation/env_infos/reward_run Mean                 4.10596
evaluation/env_infos/reward_run Std                  0.991528
evaluation/env_infos/reward_run Max                  6.25907
evaluation/env_infos/reward_run Min                 -0.654293
evaluation/env_infos/final/reward_ctrl Mean         -0.42012
evaluation/env_infos/final/reward_ctrl Std           0.079516
evaluation/env_infos/final/reward_ctrl Max          -0.325196
evaluation/env_infos/final/reward_ctrl Min          -0.55201
evaluation/env_infos/initial/reward_ctrl Mean       -0.0678968
evaluation/env_infos/initial/reward_ctrl Std         0.042645
evaluation/env_infos/initial/reward_ctrl Max        -0.018056
evaluation/env_infos/initial/reward_ctrl Min        -0.13808
evaluation/env_infos/reward_ctrl Mean               -0.409082
evaluation/env_infos/reward_ctrl Std                 0.0915392
evaluation/env_infos/reward_ctrl Max                -0.018056
evaluation/env_infos/reward_ctrl Min                -0.590162
time/data storing (s)                                0.0135135
time/evaluation sampling (s)                         4.09429
time/exploration sampling (s)                        1.00725
time/logging (s)                                     0.0536391
time/saving (s)                                      0.0170927
time/training (s)                                   39.0096
time/epoch (s)                                      44.1954
time/total (s)                                    3605.85
Epoch                                               82
----------------------------------------------  ----------------
2020-07-08 22:07:00.299920 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 83 finished
----------------------------------------------  ---------------
replay_buffer/size                               85000
trainer/QF1 Loss                                    11.675
trainer/QF2 Loss                                    10.6351
trainer/Policy Loss                               -154.168
trainer/Q1 Predictions Mean                        160.317
trainer/Q1 Predictions Std                          86.4707
trainer/Q1 Predictions Max                         248.289
trainer/Q1 Predictions Min                           5.64741
trainer/Q2 Predictions Mean                        160.129
trainer/Q2 Predictions Std                          86.3928
trainer/Q2 Predictions Max                         244.825
trainer/Q2 Predictions Min                           5.62619
trainer/Q Targets Mean                             160.24
trainer/Q Targets Std                               86.5194
trainer/Q Targets Max                              247.777
trainer/Q Targets Min                                5.27173
trainer/Log Pis Mean                                 6.48531
trainer/Log Pis Std                                  5.64161
trainer/Log Pis Max                                 25.315
trainer/Log Pis Min                                 -7.52062
trainer/Policy mu Mean                              -0.00559085
trainer/Policy mu Std                                1.60107
trainer/Policy mu Max                                4.16462
trainer/Policy mu Min                               -5.58412
trainer/Policy log std Mean                         -0.80822
trainer/Policy log std Std                           0.329072
trainer/Policy log std Max                           0.00132233
trainer/Policy log std Min                          -2.04656
trainer/Alpha                                        0.06133
trainer/Alpha Loss                                   1.35485
exploration/num steps total                      85000
exploration/num paths total                         85
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.54439
exploration/Rewards Std                              1.0029
exploration/Rewards Max                              5.55574
exploration/Rewards Min                             -0.783059
exploration/Returns Mean                          3544.39
exploration/Returns Std                              0
exploration/Returns Max                           3544.39
exploration/Returns Min                           3544.39
exploration/Actions Mean                            -0.0173945
exploration/Actions Std                              0.817284
exploration/Actions Max                              0.999917
exploration/Actions Min                             -0.999956
exploration/Num Paths                                1
exploration/Average Returns                       3544.39
exploration/env_infos/final/reward_run Mean          2.66699
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.66699
exploration/env_infos/final/reward_run Min           2.66699
exploration/env_infos/initial/reward_run Mean       -0.151173
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.151173
exploration/env_infos/initial/reward_run Min        -0.151173
exploration/env_infos/reward_run Mean                3.94534
exploration/env_infos/reward_run Std                 0.999458
exploration/env_infos/reward_run Max                 6.01895
exploration/env_infos/reward_run Min                -0.210431
exploration/env_infos/final/reward_ctrl Mean        -0.268108
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.268108
exploration/env_infos/final/reward_ctrl Min         -0.268108
exploration/env_infos/initial/reward_ctrl Mean      -0.05357
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.05357
exploration/env_infos/initial/reward_ctrl Min       -0.05357
exploration/env_infos/reward_ctrl Mean              -0.400954
exploration/env_infos/reward_ctrl Std                0.0900082
exploration/env_infos/reward_ctrl Max               -0.05357
exploration/env_infos/reward_ctrl Min               -0.597162
evaluation/num steps total                      420000
evaluation/num paths total                         420
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.56813
evaluation/Rewards Std                               1.07238
evaluation/Rewards Max                               5.85908
evaluation/Rewards Min                              -0.868595
evaluation/Returns Mean                           3568.13
evaluation/Returns Std                              38.744
evaluation/Returns Max                            3637.04
evaluation/Returns Min                            3524.22
evaluation/Actions Mean                             -0.0289142
evaluation/Actions Std                               0.821589
evaluation/Actions Max                               0.999823
evaluation/Actions Min                              -0.999957
evaluation/Num Paths                                 5
evaluation/Average Returns                        3568.13
evaluation/env_infos/final/reward_run Mean           4.29053
evaluation/env_infos/final/reward_run Std            1.36829
evaluation/env_infos/final/reward_run Max            6.16352
evaluation/env_infos/final/reward_run Min            2.57373
evaluation/env_infos/initial/reward_run Mean         0.0987359
evaluation/env_infos/initial/reward_run Std          0.102424
evaluation/env_infos/initial/reward_run Max          0.203475
evaluation/env_infos/initial/reward_run Min         -0.0532358
evaluation/env_infos/reward_run Mean                 3.97364
evaluation/env_infos/reward_run Std                  1.07546
evaluation/env_infos/reward_run Max                  6.36694
evaluation/env_infos/reward_run Min                 -0.490632
evaluation/env_infos/final/reward_ctrl Mean         -0.312515
evaluation/env_infos/final/reward_ctrl Std           0.063331
evaluation/env_infos/final/reward_ctrl Max          -0.224807
evaluation/env_infos/final/reward_ctrl Min          -0.388482
evaluation/env_infos/initial/reward_ctrl Mean       -0.0528153
evaluation/env_infos/initial/reward_ctrl Std         0.0415356
evaluation/env_infos/initial/reward_ctrl Max        -0.00986921
evaluation/env_infos/initial/reward_ctrl Min        -0.107226
evaluation/env_infos/reward_ctrl Mean               -0.405507
evaluation/env_infos/reward_ctrl Std                 0.0930944
evaluation/env_infos/reward_ctrl Max                -0.00986921
evaluation/env_infos/reward_ctrl Min                -0.588858
time/data storing (s)                                0.00851601
time/evaluation sampling (s)                         3.4769
time/exploration sampling (s)                        1.01228
time/logging (s)                                     0.182236
time/saving (s)                                      0.0178825
time/training (s)                                   39.2387
time/epoch (s)                                      43.9366
time/total (s)                                    3649.81
Epoch                                               83
----------------------------------------------  ---------------
2020-07-08 22:07:39.248581 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 84 finished
----------------------------------------------  ---------------
replay_buffer/size                               86000
trainer/QF1 Loss                                     9.37932
trainer/QF2 Loss                                    10.0001
trainer/Policy Loss                               -151.792
trainer/Q1 Predictions Mean                        157.745
trainer/Q1 Predictions Std                          91.3399
trainer/Q1 Predictions Max                         251.473
trainer/Q1 Predictions Min                           3.22951
trainer/Q2 Predictions Mean                        157.426
trainer/Q2 Predictions Std                          91.2143
trainer/Q2 Predictions Max                         248.542
trainer/Q2 Predictions Min                           3.46887
trainer/Q Targets Mean                             157.5
trainer/Q Targets Std                               91.5177
trainer/Q Targets Max                              249.812
trainer/Q Targets Min                                2.17119
trainer/Log Pis Mean                                 6.13361
trainer/Log Pis Std                                  5.51249
trainer/Log Pis Max                                 36.602
trainer/Log Pis Min                                 -4.17516
trainer/Policy mu Mean                               0.0140919
trainer/Policy mu Std                                1.55777
trainer/Policy mu Max                                4.98663
trainer/Policy mu Min                               -7.76177
trainer/Policy log std Mean                         -0.806366
trainer/Policy log std Std                           0.34843
trainer/Policy log std Max                           0.170894
trainer/Policy log std Min                          -2.16014
trainer/Alpha                                        0.0626699
trainer/Alpha Loss                                   0.370082
exploration/num steps total                      86000
exploration/num paths total                         86
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.55031
exploration/Rewards Std                              1.03719
exploration/Rewards Max                              5.93353
exploration/Rewards Min                             -1.42717
exploration/Returns Mean                          3550.31
exploration/Returns Std                              0
exploration/Returns Max                           3550.31
exploration/Returns Min                           3550.31
exploration/Actions Mean                            -0.00704742
exploration/Actions Std                              0.815574
exploration/Actions Max                              0.99991
exploration/Actions Min                             -0.999972
exploration/Num Paths                                1
exploration/Average Returns                       3550.31
exploration/env_infos/final/reward_run Mean          2.95554
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.95554
exploration/env_infos/final/reward_run Min           2.95554
exploration/env_infos/initial/reward_run Mean       -0.355415
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.355415
exploration/env_infos/initial/reward_run Min        -0.355415
exploration/env_infos/reward_run Mean                3.94944
exploration/env_infos/reward_run Std                 1.0401
exploration/env_infos/reward_run Max                 6.24507
exploration/env_infos/reward_run Min                -1.02159
exploration/env_infos/final/reward_ctrl Mean        -0.514913
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.514913
exploration/env_infos/final/reward_ctrl Min         -0.514913
exploration/env_infos/initial/reward_ctrl Mean      -0.168752
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.168752
exploration/env_infos/initial/reward_ctrl Min       -0.168752
exploration/env_infos/reward_ctrl Mean              -0.399127
exploration/env_infos/reward_ctrl Std                0.0929344
exploration/env_infos/reward_ctrl Max               -0.0954229
exploration/env_infos/reward_ctrl Min               -0.58599
evaluation/num steps total                      425000
evaluation/num paths total                         425
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.55308
evaluation/Rewards Std                               1.08469
evaluation/Rewards Max                               6.03554
evaluation/Rewards Min                              -1.09851
evaluation/Returns Mean                           3553.08
evaluation/Returns Std                              85.2756
evaluation/Returns Max                            3644.41
evaluation/Returns Min                            3399.6
evaluation/Actions Mean                             -0.00682231
evaluation/Actions Std                               0.820061
evaluation/Actions Max                               0.999756
evaluation/Actions Min                              -0.999965
evaluation/Num Paths                                 5
evaluation/Average Returns                        3553.08
evaluation/env_infos/final/reward_run Mean           4.52313
evaluation/env_infos/final/reward_run Std            0.52302
evaluation/env_infos/final/reward_run Max            5.28511
evaluation/env_infos/final/reward_run Min            3.64706
evaluation/env_infos/initial/reward_run Mean         0.120713
evaluation/env_infos/initial/reward_run Std          0.327436
evaluation/env_infos/initial/reward_run Max          0.495541
evaluation/env_infos/initial/reward_run Min         -0.475671
evaluation/env_infos/reward_run Mean                 3.9566
evaluation/env_infos/reward_run Std                  1.08538
evaluation/env_infos/reward_run Max                  6.4394
evaluation/env_infos/reward_run Min                 -0.534745
evaluation/env_infos/final/reward_ctrl Mean         -0.466465
evaluation/env_infos/final/reward_ctrl Std           0.0151336
evaluation/env_infos/final/reward_ctrl Max          -0.44705
evaluation/env_infos/final/reward_ctrl Min          -0.487343
evaluation/env_infos/initial/reward_ctrl Mean       -0.0943393
evaluation/env_infos/initial/reward_ctrl Std         0.0574502
evaluation/env_infos/initial/reward_ctrl Max        -0.0113242
evaluation/env_infos/initial/reward_ctrl Min        -0.165494
evaluation/env_infos/reward_ctrl Mean               -0.403528
evaluation/env_infos/reward_ctrl Std                 0.0911641
evaluation/env_infos/reward_ctrl Max                -0.0113242
evaluation/env_infos/reward_ctrl Min                -0.592649
time/data storing (s)                                0.00717092
time/evaluation sampling (s)                         3.09313
time/exploration sampling (s)                        0.851123
time/logging (s)                                     0.0433279
time/saving (s)                                      0.0163728
time/training (s)                                   34.7844
time/epoch (s)                                      38.7955
time/total (s)                                    3688.62
Epoch                                               84
----------------------------------------------  ---------------
2020-07-08 22:08:14.764303 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 85 finished
----------------------------------------------  ----------------
replay_buffer/size                               87000
trainer/QF1 Loss                                     7.82231
trainer/QF2 Loss                                     7.6785
trainer/Policy Loss                               -156.784
trainer/Q1 Predictions Mean                        163.165
trainer/Q1 Predictions Std                          88.9863
trainer/Q1 Predictions Max                         252.552
trainer/Q1 Predictions Min                           4.09889
trainer/Q2 Predictions Mean                        162.833
trainer/Q2 Predictions Std                          88.9231
trainer/Q2 Predictions Max                         252.629
trainer/Q2 Predictions Min                           5.01704
trainer/Q Targets Mean                             163.186
trainer/Q Targets Std                               89.3147
trainer/Q Targets Max                              252.259
trainer/Q Targets Min                                4.73934
trainer/Log Pis Mean                                 6.52841
trainer/Log Pis Std                                  5.55174
trainer/Log Pis Max                                 28.6777
trainer/Log Pis Min                                 -4.77001
trainer/Policy mu Mean                               0.149356
trainer/Policy mu Std                                1.58339
trainer/Policy mu Max                                4.23752
trainer/Policy mu Min                               -4.12293
trainer/Policy log std Mean                         -0.823261
trainer/Policy log std Std                           0.33282
trainer/Policy log std Max                           0.0672958
trainer/Policy log std Min                          -2.08827
trainer/Alpha                                        0.0613391
trainer/Alpha Loss                                   1.4749
exploration/num steps total                      87000
exploration/num paths total                         87
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.17027
exploration/Rewards Std                              1.36366
exploration/Rewards Max                              5.70824
exploration/Rewards Min                             -0.905284
exploration/Returns Mean                          3170.27
exploration/Returns Std                              0
exploration/Returns Max                           3170.27
exploration/Returns Min                           3170.27
exploration/Actions Mean                            -0.000236071
exploration/Actions Std                              0.803282
exploration/Actions Max                              0.999784
exploration/Actions Min                             -0.999941
exploration/Num Paths                                1
exploration/Average Returns                       3170.27
exploration/env_infos/final/reward_run Mean          4.04881
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.04881
exploration/env_infos/final/reward_run Min           4.04881
exploration/env_infos/initial/reward_run Mean        0.0173365
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0173365
exploration/env_infos/initial/reward_run Min         0.0173365
exploration/env_infos/reward_run Mean                3.55743
exploration/env_infos/reward_run Std                 1.39057
exploration/env_infos/reward_run Max                 6.11707
exploration/env_infos/reward_run Min                -0.514822
exploration/env_infos/final/reward_ctrl Mean        -0.467524
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.467524
exploration/env_infos/final/reward_ctrl Min         -0.467524
exploration/env_infos/initial/reward_ctrl Mean      -0.0219633
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0219633
exploration/env_infos/initial/reward_ctrl Min       -0.0219633
exploration/env_infos/reward_ctrl Mean              -0.387157
exploration/env_infos/reward_ctrl Std                0.097158
exploration/env_infos/reward_ctrl Max               -0.0219633
exploration/env_infos/reward_ctrl Min               -0.587405
evaluation/num steps total                      430000
evaluation/num paths total                         430
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.61878
evaluation/Rewards Std                               1.18438
evaluation/Rewards Max                               5.88811
evaluation/Rewards Min                              -1.33247
evaluation/Returns Mean                           3618.78
evaluation/Returns Std                             175.832
evaluation/Returns Max                            3762.35
evaluation/Returns Min                            3295.75
evaluation/Actions Mean                              0.0058082
evaluation/Actions Std                               0.82412
evaluation/Actions Max                               0.999849
evaluation/Actions Min                              -0.999915
evaluation/Num Paths                                 5
evaluation/Average Returns                        3618.78
evaluation/env_infos/final/reward_run Mean           4.1946
evaluation/env_infos/final/reward_run Std            0.78048
evaluation/env_infos/final/reward_run Max            4.96893
evaluation/env_infos/final/reward_run Min            2.98211
evaluation/env_infos/initial/reward_run Mean         0.230571
evaluation/env_infos/initial/reward_run Std          0.279059
evaluation/env_infos/initial/reward_run Max          0.609625
evaluation/env_infos/initial/reward_run Min         -0.143278
evaluation/env_infos/reward_run Mean                 4.0263
evaluation/env_infos/reward_run Std                  1.19549
evaluation/env_infos/reward_run Max                  6.29933
evaluation/env_infos/reward_run Min                 -0.96693
evaluation/env_infos/final/reward_ctrl Mean         -0.470689
evaluation/env_infos/final/reward_ctrl Std           0.0910876
evaluation/env_infos/final/reward_ctrl Max          -0.304591
evaluation/env_infos/final/reward_ctrl Min          -0.55378
evaluation/env_infos/initial/reward_ctrl Mean       -0.103135
evaluation/env_infos/initial/reward_ctrl Std         0.0624864
evaluation/env_infos/initial/reward_ctrl Max        -0.0275788
evaluation/env_infos/initial/reward_ctrl Min        -0.211576
evaluation/env_infos/reward_ctrl Mean               -0.407525
evaluation/env_infos/reward_ctrl Std                 0.0875857
evaluation/env_infos/reward_ctrl Max                -0.0223085
evaluation/env_infos/reward_ctrl Min                -0.587561
time/data storing (s)                                0.00683584
time/evaluation sampling (s)                         2.64339
time/exploration sampling (s)                        0.661832
time/logging (s)                                     0.042708
time/saving (s)                                      0.0166431
time/training (s)                                   32.1256
time/epoch (s)                                      35.497
time/total (s)                                    3724.13
Epoch                                               85
----------------------------------------------  ----------------
2020-07-08 22:08:53.315700 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 86 finished
----------------------------------------------  --------------
replay_buffer/size                               88000
trainer/QF1 Loss                                     7.11407
trainer/QF2 Loss                                     8.23177
trainer/Policy Loss                               -164.023
trainer/Q1 Predictions Mean                        169.86
trainer/Q1 Predictions Std                          88.9085
trainer/Q1 Predictions Max                         255.763
trainer/Q1 Predictions Min                           4.62993
trainer/Q2 Predictions Mean                        170.134
trainer/Q2 Predictions Std                          89.0108
trainer/Q2 Predictions Max                         257.317
trainer/Q2 Predictions Min                           5.76842
trainer/Q Targets Mean                             169.355
trainer/Q Targets Std                               88.7808
trainer/Q Targets Max                              256.675
trainer/Q Targets Min                                4.85333
trainer/Log Pis Mean                                 6.09029
trainer/Log Pis Std                                  4.97311
trainer/Log Pis Max                                 24.0928
trainer/Log Pis Min                                 -5.63725
trainer/Policy mu Mean                               0.0208729
trainer/Policy mu Std                                1.49437
trainer/Policy mu Max                                4.14738
trainer/Policy mu Min                               -5.45069
trainer/Policy log std Mean                         -0.863417
trainer/Policy log std Std                           0.34295
trainer/Policy log std Max                           0.0599944
trainer/Policy log std Min                          -2.01803
trainer/Alpha                                        0.0623219
trainer/Alpha Loss                                   0.250615
exploration/num steps total                      88000
exploration/num paths total                         88
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.69178
exploration/Rewards Std                              0.963211
exploration/Rewards Max                              5.78455
exploration/Rewards Min                             -0.986164
exploration/Returns Mean                          3691.78
exploration/Returns Std                              0
exploration/Returns Max                           3691.78
exploration/Returns Min                           3691.78
exploration/Actions Mean                            -0.0269723
exploration/Actions Std                              0.811245
exploration/Actions Max                              0.999973
exploration/Actions Min                             -0.999973
exploration/Num Paths                                1
exploration/Average Returns                       3691.78
exploration/env_infos/final/reward_run Mean          4.62215
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.62215
exploration/env_infos/final/reward_run Min           4.62215
exploration/env_infos/initial/reward_run Mean       -0.0458043
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0458043
exploration/env_infos/initial/reward_run Min        -0.0458043
exploration/env_infos/reward_run Mean                4.08709
exploration/env_infos/reward_run Std                 0.955168
exploration/env_infos/reward_run Max                 6.17913
exploration/env_infos/reward_run Min                -0.651373
exploration/env_infos/final/reward_ctrl Mean        -0.274102
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.274102
exploration/env_infos/final/reward_ctrl Min         -0.274102
exploration/env_infos/initial/reward_ctrl Mean      -0.159282
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.159282
exploration/env_infos/initial/reward_ctrl Min       -0.159282
exploration/env_infos/reward_ctrl Mean              -0.395307
exploration/env_infos/reward_ctrl Std                0.0908992
exploration/env_infos/reward_ctrl Max               -0.111155
exploration/env_infos/reward_ctrl Min               -0.582576
evaluation/num steps total                      435000
evaluation/num paths total                         435
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.76162
evaluation/Rewards Std                               1.04668
evaluation/Rewards Max                               5.91912
evaluation/Rewards Min                              -0.870865
evaluation/Returns Mean                           3761.62
evaluation/Returns Std                              61.9431
evaluation/Returns Max                            3857.71
evaluation/Returns Min                            3684.93
evaluation/Actions Mean                             -0.0226976
evaluation/Actions Std                               0.814399
evaluation/Actions Max                               0.999796
evaluation/Actions Min                              -0.999991
evaluation/Num Paths                                 5
evaluation/Average Returns                        3761.62
evaluation/env_infos/final/reward_run Mean           3.99511
evaluation/env_infos/final/reward_run Std            0.501388
evaluation/env_infos/final/reward_run Max            4.84894
evaluation/env_infos/final/reward_run Min            3.32325
evaluation/env_infos/initial/reward_run Mean         0.133217
evaluation/env_infos/initial/reward_run Std          0.0430213
evaluation/env_infos/initial/reward_run Max          0.186118
evaluation/env_infos/initial/reward_run Min          0.060596
evaluation/env_infos/reward_run Mean                 4.15988
evaluation/env_infos/reward_run Std                  1.04361
evaluation/env_infos/reward_run Max                  6.29373
evaluation/env_infos/reward_run Min                 -0.514192
evaluation/env_infos/final/reward_ctrl Mean         -0.40108
evaluation/env_infos/final/reward_ctrl Std           0.0951051
evaluation/env_infos/final/reward_ctrl Max          -0.26689
evaluation/env_infos/final/reward_ctrl Min          -0.545609
evaluation/env_infos/initial/reward_ctrl Mean       -0.0617364
evaluation/env_infos/initial/reward_ctrl Std         0.0318695
evaluation/env_infos/initial/reward_ctrl Max        -0.0103285
evaluation/env_infos/initial/reward_ctrl Min        -0.104642
evaluation/env_infos/reward_ctrl Mean               -0.398256
evaluation/env_infos/reward_ctrl Std                 0.0903111
evaluation/env_infos/reward_ctrl Max                -0.0103285
evaluation/env_infos/reward_ctrl Min                -0.592194
time/data storing (s)                                0.0065
time/evaluation sampling (s)                         2.57661
time/exploration sampling (s)                        0.648968
time/logging (s)                                     0.0421719
time/saving (s)                                      0.0166205
time/training (s)                                   35.2424
time/epoch (s)                                      38.5333
time/total (s)                                    3762.68
Epoch                                               86
----------------------------------------------  --------------
2020-07-08 22:09:33.418152 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 87 finished
----------------------------------------------  ---------------
replay_buffer/size                               89000
trainer/QF1 Loss                                    11.9422
trainer/QF2 Loss                                    13.0251
trainer/Policy Loss                               -163.638
trainer/Q1 Predictions Mean                        169.354
trainer/Q1 Predictions Std                          89.4399
trainer/Q1 Predictions Max                         252.836
trainer/Q1 Predictions Min                           5.07442
trainer/Q2 Predictions Mean                        169.617
trainer/Q2 Predictions Std                          89.4278
trainer/Q2 Predictions Max                         253.384
trainer/Q2 Predictions Min                           5.41578
trainer/Q Targets Mean                             169.044
trainer/Q Targets Std                               89.4898
trainer/Q Targets Max                              252.65
trainer/Q Targets Min                                4.99896
trainer/Log Pis Mean                                 6.13411
trainer/Log Pis Std                                  5.14633
trainer/Log Pis Max                                 24.3674
trainer/Log Pis Min                                 -4.25493
trainer/Policy mu Mean                              -0.0469482
trainer/Policy mu Std                                1.55078
trainer/Policy mu Max                                4.80566
trainer/Policy mu Min                               -4.87136
trainer/Policy log std Mean                         -0.810814
trainer/Policy log std Std                           0.340401
trainer/Policy log std Max                          -0.00231954
trainer/Policy log std Min                          -2.00359
trainer/Alpha                                        0.063791
trainer/Alpha Loss                                   0.369115
exploration/num steps total                      89000
exploration/num paths total                         89
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.62594
exploration/Rewards Std                              1.04814
exploration/Rewards Max                              5.8492
exploration/Rewards Min                             -0.95072
exploration/Returns Mean                          3625.94
exploration/Returns Std                              0
exploration/Returns Max                           3625.94
exploration/Returns Min                           3625.94
exploration/Actions Mean                            -0.020746
exploration/Actions Std                              0.814816
exploration/Actions Max                              0.999934
exploration/Actions Min                             -0.999965
exploration/Num Paths                                1
exploration/Average Returns                       3625.94
exploration/env_infos/final/reward_run Mean          2.88954
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.88954
exploration/env_infos/final/reward_run Min           2.88954
exploration/env_infos/initial/reward_run Mean        0.254986
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.254986
exploration/env_infos/initial/reward_run Min         0.254986
exploration/env_infos/reward_run Mean                4.02455
exploration/env_infos/reward_run Std                 1.04623
exploration/env_infos/reward_run Max                 6.217
exploration/env_infos/reward_run Min                -0.526847
exploration/env_infos/final/reward_ctrl Mean        -0.577717
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.577717
exploration/env_infos/final/reward_ctrl Min         -0.577717
exploration/env_infos/initial/reward_ctrl Mean      -0.135563
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.135563
exploration/env_infos/initial/reward_ctrl Min       -0.135563
exploration/env_infos/reward_ctrl Mean              -0.398614
exploration/env_infos/reward_ctrl Std                0.088636
exploration/env_infos/reward_ctrl Max               -0.0726246
exploration/env_infos/reward_ctrl Min               -0.59103
evaluation/num steps total                      440000
evaluation/num paths total                         440
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.81616
evaluation/Rewards Std                               1.02665
evaluation/Rewards Max                               6.13819
evaluation/Rewards Min                              -1.14724
evaluation/Returns Mean                           3816.16
evaluation/Returns Std                              81.2035
evaluation/Returns Max                            3949.2
evaluation/Returns Min                            3734.46
evaluation/Actions Mean                             -0.0216129
evaluation/Actions Std                               0.828121
evaluation/Actions Max                               0.999718
evaluation/Actions Min                              -0.999974
evaluation/Num Paths                                 5
evaluation/Average Returns                        3816.16
evaluation/env_infos/final/reward_run Mean           4.95462
evaluation/env_infos/final/reward_run Std            0.980018
evaluation/env_infos/final/reward_run Max            6.01951
evaluation/env_infos/final/reward_run Min            3.11016
evaluation/env_infos/initial/reward_run Mean         0.215218
evaluation/env_infos/initial/reward_run Std          0.156443
evaluation/env_infos/initial/reward_run Max          0.400068
evaluation/env_infos/initial/reward_run Min         -0.0676841
evaluation/env_infos/reward_run Mean                 4.22791
evaluation/env_infos/reward_run Std                  1.01955
evaluation/env_infos/reward_run Max                  6.57324
evaluation/env_infos/reward_run Min                 -0.657019
evaluation/env_infos/final/reward_ctrl Mean         -0.435601
evaluation/env_infos/final/reward_ctrl Std           0.0665908
evaluation/env_infos/final/reward_ctrl Max          -0.332389
evaluation/env_infos/final/reward_ctrl Min          -0.512616
evaluation/env_infos/initial/reward_ctrl Mean       -0.0490269
evaluation/env_infos/initial/reward_ctrl Std         0.0338399
evaluation/env_infos/initial/reward_ctrl Max        -0.00924965
evaluation/env_infos/initial/reward_ctrl Min        -0.108946
evaluation/env_infos/reward_ctrl Mean               -0.411751
evaluation/env_infos/reward_ctrl Std                 0.0853552
evaluation/env_infos/reward_ctrl Max                -0.00924965
evaluation/env_infos/reward_ctrl Min                -0.591566
time/data storing (s)                                0.00662167
time/evaluation sampling (s)                         2.8679
time/exploration sampling (s)                        0.691519
time/logging (s)                                     0.0447602
time/saving (s)                                      0.0172105
time/training (s)                                   36.4601
time/epoch (s)                                      40.0881
time/total (s)                                    3802.78
Epoch                                               87
----------------------------------------------  ---------------
2020-07-08 22:10:19.224551 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 88 finished
----------------------------------------------  ---------------
replay_buffer/size                               90000
trainer/QF1 Loss                                     9.54715
trainer/QF2 Loss                                     9.12821
trainer/Policy Loss                               -159.492
trainer/Q1 Predictions Mean                        165.506
trainer/Q1 Predictions Std                          93.1436
trainer/Q1 Predictions Max                         256.715
trainer/Q1 Predictions Min                           4.75734
trainer/Q2 Predictions Mean                        165.66
trainer/Q2 Predictions Std                          93.1704
trainer/Q2 Predictions Max                         256.772
trainer/Q2 Predictions Min                           5.20792
trainer/Q Targets Mean                             165.602
trainer/Q Targets Std                               93.1523
trainer/Q Targets Max                              258.154
trainer/Q Targets Min                                4.18609
trainer/Log Pis Mean                                 6.26554
trainer/Log Pis Std                                  5.3791
trainer/Log Pis Max                                 22.7287
trainer/Log Pis Min                                 -5.21048
trainer/Policy mu Mean                               0.0477981
trainer/Policy mu Std                                1.56905
trainer/Policy mu Max                                4.05742
trainer/Policy mu Min                               -4.41886
trainer/Policy log std Mean                         -0.810856
trainer/Policy log std Std                           0.344139
trainer/Policy log std Max                          -0.0250416
trainer/Policy log std Min                          -2.02323
trainer/Alpha                                        0.0649679
trainer/Alpha Loss                                   0.725943
exploration/num steps total                      90000
exploration/num paths total                         90
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.74696
exploration/Rewards Std                              1.03874
exploration/Rewards Max                              5.82311
exploration/Rewards Min                             -0.765889
exploration/Returns Mean                          3746.96
exploration/Returns Std                              0
exploration/Returns Max                           3746.96
exploration/Returns Min                           3746.96
exploration/Actions Mean                             0.0209607
exploration/Actions Std                              0.8182
exploration/Actions Max                              0.999985
exploration/Actions Min                             -0.999986
exploration/Num Paths                                1
exploration/Average Returns                       3746.96
exploration/env_infos/final/reward_run Mean          5.02733
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.02733
exploration/env_infos/final/reward_run Min           5.02733
exploration/env_infos/initial/reward_run Mean        0.18481
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.18481
exploration/env_infos/initial/reward_run Min         0.18481
exploration/env_infos/reward_run Mean                4.1489
exploration/env_infos/reward_run Std                 1.03215
exploration/env_infos/reward_run Max                 6.20537
exploration/env_infos/reward_run Min                -0.490565
exploration/env_infos/final/reward_ctrl Mean        -0.378922
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.378922
exploration/env_infos/final/reward_ctrl Min         -0.378922
exploration/env_infos/initial/reward_ctrl Mean      -0.0276142
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0276142
exploration/env_infos/initial/reward_ctrl Min       -0.0276142
exploration/env_infos/reward_ctrl Mean              -0.401934
exploration/env_infos/reward_ctrl Std                0.0813297
exploration/env_infos/reward_ctrl Max               -0.0276142
exploration/env_infos/reward_ctrl Min               -0.581196
evaluation/num steps total                      445000
evaluation/num paths total                         445
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.63509
evaluation/Rewards Std                               1.22881
evaluation/Rewards Max                               5.96682
evaluation/Rewards Min                              -1.12168
evaluation/Returns Mean                           3635.09
evaluation/Returns Std                              97.0791
evaluation/Returns Max                            3734.78
evaluation/Returns Min                            3478.78
evaluation/Actions Mean                              0.00790382
evaluation/Actions Std                               0.819324
evaluation/Actions Max                               0.999888
evaluation/Actions Min                              -0.999957
evaluation/Num Paths                                 5
evaluation/Average Returns                        3635.09
evaluation/env_infos/final/reward_run Mean           3.38588
evaluation/env_infos/final/reward_run Std            0.572704
evaluation/env_infos/final/reward_run Max            4.12346
evaluation/env_infos/final/reward_run Min            2.5472
evaluation/env_infos/initial/reward_run Mean         0.0184635
evaluation/env_infos/initial/reward_run Std          0.0782753
evaluation/env_infos/initial/reward_run Max          0.144987
evaluation/env_infos/initial/reward_run Min         -0.0544667
evaluation/env_infos/reward_run Mean                 4.03791
evaluation/env_infos/reward_run Std                  1.24097
evaluation/env_infos/reward_run Max                  6.39262
evaluation/env_infos/reward_run Min                 -0.865647
evaluation/env_infos/final/reward_ctrl Mean         -0.426856
evaluation/env_infos/final/reward_ctrl Std           0.0660432
evaluation/env_infos/final/reward_ctrl Max          -0.323579
evaluation/env_infos/final/reward_ctrl Min          -0.506825
evaluation/env_infos/initial/reward_ctrl Mean       -0.050431
evaluation/env_infos/initial/reward_ctrl Std         0.0217906
evaluation/env_infos/initial/reward_ctrl Max        -0.0171132
evaluation/env_infos/initial/reward_ctrl Min        -0.0836085
evaluation/env_infos/reward_ctrl Mean               -0.402813
evaluation/env_infos/reward_ctrl Std                 0.0862365
evaluation/env_infos/reward_ctrl Max                -0.0171132
evaluation/env_infos/reward_ctrl Min                -0.585898
time/data storing (s)                                0.00662612
time/evaluation sampling (s)                         3.94368
time/exploration sampling (s)                        0.865568
time/logging (s)                                     0.048773
time/saving (s)                                      0.0216119
time/training (s)                                   40.9046
time/epoch (s)                                      45.7908
time/total (s)                                    3848.58
Epoch                                               88
----------------------------------------------  ---------------
2020-07-08 22:11:05.680830 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 89 finished
----------------------------------------------  --------------
replay_buffer/size                               91000
trainer/QF1 Loss                                    11.4148
trainer/QF2 Loss                                    11.5888
trainer/Policy Loss                               -168.902
trainer/Q1 Predictions Mean                        174.606
trainer/Q1 Predictions Std                          91.0413
trainer/Q1 Predictions Max                         259.374
trainer/Q1 Predictions Min                           5.35739
trainer/Q2 Predictions Mean                        174.638
trainer/Q2 Predictions Std                          91.0403
trainer/Q2 Predictions Max                         261.811
trainer/Q2 Predictions Min                           5.17066
trainer/Q Targets Mean                             174.639
trainer/Q Targets Std                               90.9532
trainer/Q Targets Max                              259.231
trainer/Q Targets Min                                4.81901
trainer/Log Pis Mean                                 5.91238
trainer/Log Pis Std                                  5.02946
trainer/Log Pis Max                                 24.3582
trainer/Log Pis Min                                 -6.41702
trainer/Policy mu Mean                               0.113602
trainer/Policy mu Std                                1.51472
trainer/Policy mu Max                                4.27432
trainer/Policy mu Min                               -4.66417
trainer/Policy log std Mean                         -0.815686
trainer/Policy log std Std                           0.319553
trainer/Policy log std Max                          -0.0818177
trainer/Policy log std Min                          -2.23937
trainer/Alpha                                        0.0653701
trainer/Alpha Loss                                  -0.23897
exploration/num steps total                      91000
exploration/num paths total                         91
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.75307
exploration/Rewards Std                              1.09164
exploration/Rewards Max                              6.00358
exploration/Rewards Min                             -0.213759
exploration/Returns Mean                          3753.07
exploration/Returns Std                              0
exploration/Returns Max                           3753.07
exploration/Returns Min                           3753.07
exploration/Actions Mean                             0.017382
exploration/Actions Std                              0.819976
exploration/Actions Max                              0.999966
exploration/Actions Min                             -0.999914
exploration/Num Paths                                1
exploration/Average Returns                       3753.07
exploration/env_infos/final/reward_run Mean          5.11873
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.11873
exploration/env_infos/final/reward_run Min           5.11873
exploration/env_infos/initial/reward_run Mean       -0.0739906
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0739906
exploration/env_infos/initial/reward_run Min        -0.0739906
exploration/env_infos/reward_run Mean                4.15667
exploration/env_infos/reward_run Std                 1.08594
exploration/env_infos/reward_run Max                 6.28481
exploration/env_infos/reward_run Min                -0.0739906
exploration/env_infos/final/reward_ctrl Mean        -0.315359
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.315359
exploration/env_infos/final/reward_ctrl Min         -0.315359
exploration/env_infos/initial/reward_ctrl Mean      -0.117058
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.117058
exploration/env_infos/initial/reward_ctrl Min       -0.117058
exploration/env_infos/reward_ctrl Mean              -0.403598
exploration/env_infos/reward_ctrl Std                0.0819157
exploration/env_infos/reward_ctrl Max               -0.0848621
exploration/env_infos/reward_ctrl Min               -0.591882
evaluation/num steps total                      450000
evaluation/num paths total                         450
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.91449
evaluation/Rewards Std                               1.11441
evaluation/Rewards Max                               6.13202
evaluation/Rewards Min                              -1.01602
evaluation/Returns Mean                           3914.49
evaluation/Returns Std                             100.404
evaluation/Returns Max                            4049.8
evaluation/Returns Min                            3738.02
evaluation/Actions Mean                              0.0122146
evaluation/Actions Std                               0.827837
evaluation/Actions Max                               0.999776
evaluation/Actions Min                              -0.99986
evaluation/Num Paths                                 5
evaluation/Average Returns                        3914.49
evaluation/env_infos/final/reward_run Mean           4.64545
evaluation/env_infos/final/reward_run Std            0.688573
evaluation/env_infos/final/reward_run Max            5.80929
evaluation/env_infos/final/reward_run Min            3.8976
evaluation/env_infos/initial/reward_run Mean        -0.0413381
evaluation/env_infos/initial/reward_run Std          0.123229
evaluation/env_infos/initial/reward_run Max          0.158256
evaluation/env_infos/initial/reward_run Min         -0.222632
evaluation/env_infos/reward_run Mean                 4.32577
evaluation/env_infos/reward_run Std                  1.10987
evaluation/env_infos/reward_run Max                  6.53067
evaluation/env_infos/reward_run Min                 -0.662795
evaluation/env_infos/final/reward_ctrl Mean         -0.481127
evaluation/env_infos/final/reward_ctrl Std           0.052648
evaluation/env_infos/final/reward_ctrl Max          -0.409503
evaluation/env_infos/final/reward_ctrl Min          -0.544908
evaluation/env_infos/initial/reward_ctrl Mean       -0.0502348
evaluation/env_infos/initial/reward_ctrl Std         0.0273247
evaluation/env_infos/initial/reward_ctrl Max        -0.020125
evaluation/env_infos/initial/reward_ctrl Min        -0.0914237
evaluation/env_infos/reward_ctrl Mean               -0.411278
evaluation/env_infos/reward_ctrl Std                 0.0838359
evaluation/env_infos/reward_ctrl Max                -0.020125
evaluation/env_infos/reward_ctrl Min                -0.589148
time/data storing (s)                                0.0073508
time/evaluation sampling (s)                         4.44635
time/exploration sampling (s)                        1.00921
time/logging (s)                                     0.0402612
time/saving (s)                                      0.0175474
time/training (s)                                   40.8801
time/epoch (s)                                      46.4009
time/total (s)                                    3895.03
Epoch                                               89
----------------------------------------------  --------------
2020-07-08 22:11:42.420315 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 90 finished
----------------------------------------------  ---------------
replay_buffer/size                               92000
trainer/QF1 Loss                                     7.64909
trainer/QF2 Loss                                     9.47288
trainer/Policy Loss                               -171.323
trainer/Q1 Predictions Mean                        177
trainer/Q1 Predictions Std                          89.97
trainer/Q1 Predictions Max                         264.478
trainer/Q1 Predictions Min                           5.76918
trainer/Q2 Predictions Mean                        177.11
trainer/Q2 Predictions Std                          90.1315
trainer/Q2 Predictions Max                         269.448
trainer/Q2 Predictions Min                           5.61804
trainer/Q Targets Mean                             176.659
trainer/Q Targets Std                               89.8454
trainer/Q Targets Max                              265.681
trainer/Q Targets Min                                5.44867
trainer/Log Pis Mean                                 5.7812
trainer/Log Pis Std                                  5.2345
trainer/Log Pis Max                                 21.6627
trainer/Log Pis Min                                 -7.49711
trainer/Policy mu Mean                               0.192309
trainer/Policy mu Std                                1.5189
trainer/Policy mu Max                                5.27243
trainer/Policy mu Min                               -4.74436
trainer/Policy log std Mean                         -0.823528
trainer/Policy log std Std                           0.335514
trainer/Policy log std Max                           0.0320296
trainer/Policy log std Min                          -2.05322
trainer/Alpha                                        0.0667705
trainer/Alpha Loss                                  -0.592162
exploration/num steps total                      92000
exploration/num paths total                         92
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.59588
exploration/Rewards Std                              1.20896
exploration/Rewards Max                              5.67135
exploration/Rewards Min                             -1.09237
exploration/Returns Mean                          3595.88
exploration/Returns Std                              0
exploration/Returns Max                           3595.88
exploration/Returns Min                           3595.88
exploration/Actions Mean                             0.0336122
exploration/Actions Std                              0.809884
exploration/Actions Max                              0.999897
exploration/Actions Min                             -0.999857
exploration/Num Paths                                1
exploration/Average Returns                       3595.88
exploration/env_infos/final/reward_run Mean          5.21862
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.21862
exploration/env_infos/final/reward_run Min           5.21862
exploration/env_infos/initial/reward_run Mean        0.226573
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.226573
exploration/env_infos/initial/reward_run Min         0.226573
exploration/env_infos/reward_run Mean                3.99011
exploration/env_infos/reward_run Std                 1.21428
exploration/env_infos/reward_run Max                 6.01054
exploration/env_infos/reward_run Min                -0.559201
exploration/env_infos/final/reward_ctrl Mean        -0.324484
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.324484
exploration/env_infos/final/reward_ctrl Min         -0.324484
exploration/env_infos/initial/reward_ctrl Mean      -0.0353058
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0353058
exploration/env_infos/initial/reward_ctrl Min       -0.0353058
exploration/env_infos/reward_ctrl Mean              -0.394225
exploration/env_infos/reward_ctrl Std                0.0875798
exploration/env_infos/reward_ctrl Max               -0.0353058
exploration/env_infos/reward_ctrl Min               -0.586908
evaluation/num steps total                      455000
evaluation/num paths total                         455
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.70386
evaluation/Rewards Std                               1.12023
evaluation/Rewards Max                               5.9828
evaluation/Rewards Min                              -1.26871
evaluation/Returns Mean                           3703.86
evaluation/Returns Std                             104.91
evaluation/Returns Max                            3886.51
evaluation/Returns Min                            3573.3
evaluation/Actions Mean                              0.0289779
evaluation/Actions Std                               0.814854
evaluation/Actions Max                               0.999683
evaluation/Actions Min                              -0.999944
evaluation/Num Paths                                 5
evaluation/Average Returns                        3703.86
evaluation/env_infos/final/reward_run Mean           4.23282
evaluation/env_infos/final/reward_run Std            0.913847
evaluation/env_infos/final/reward_run Max            5.42231
evaluation/env_infos/final/reward_run Min            3.05878
evaluation/env_infos/initial/reward_run Mean         0.178051
evaluation/env_infos/initial/reward_run Std          0.193616
evaluation/env_infos/initial/reward_run Max          0.457159
evaluation/env_infos/initial/reward_run Min         -0.0980932
evaluation/env_infos/reward_run Mean                 4.10276
evaluation/env_infos/reward_run Std                  1.11444
evaluation/env_infos/reward_run Max                  6.36837
evaluation/env_infos/reward_run Min                 -0.77173
evaluation/env_infos/final/reward_ctrl Mean         -0.40624
evaluation/env_infos/final/reward_ctrl Std           0.037357
evaluation/env_infos/final/reward_ctrl Max          -0.352966
evaluation/env_infos/final/reward_ctrl Min          -0.443574
evaluation/env_infos/initial/reward_ctrl Mean       -0.0757715
evaluation/env_infos/initial/reward_ctrl Std         0.0382455
evaluation/env_infos/initial/reward_ctrl Max        -0.0170399
evaluation/env_infos/initial/reward_ctrl Min        -0.11993
evaluation/env_infos/reward_ctrl Mean               -0.398896
evaluation/env_infos/reward_ctrl Std                 0.0844312
evaluation/env_infos/reward_ctrl Max                -0.0170399
evaluation/env_infos/reward_ctrl Min                -0.590719
time/data storing (s)                                0.00692965
time/evaluation sampling (s)                         2.5489
time/exploration sampling (s)                        0.647671
time/logging (s)                                     0.0413323
time/saving (s)                                      0.0174589
time/training (s)                                   33.4298
time/epoch (s)                                      36.6921
time/total (s)                                    3931.76
Epoch                                               90
----------------------------------------------  ---------------
2020-07-08 22:12:17.135007 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 91 finished
----------------------------------------------  ---------------
replay_buffer/size                               93000
trainer/QF1 Loss                                     7.36224
trainer/QF2 Loss                                     6.8158
trainer/Policy Loss                               -166.81
trainer/Q1 Predictions Mean                        172.323
trainer/Q1 Predictions Std                          94.162
trainer/Q1 Predictions Max                         265.887
trainer/Q1 Predictions Min                           5.28649
trainer/Q2 Predictions Mean                        172.255
trainer/Q2 Predictions Std                          94.224
trainer/Q2 Predictions Max                         268.047
trainer/Q2 Predictions Min                           5.42604
trainer/Q Targets Mean                             172.232
trainer/Q Targets Std                               94.2036
trainer/Q Targets Max                              265.826
trainer/Q Targets Min                                4.20486
trainer/Log Pis Mean                                 5.69399
trainer/Log Pis Std                                  5.56623
trainer/Log Pis Max                                 27.3436
trainer/Log Pis Min                                 -7.01922
trainer/Policy mu Mean                               0.00852247
trainer/Policy mu Std                                1.49909
trainer/Policy mu Max                                4.25556
trainer/Policy mu Min                               -5.78804
trainer/Policy log std Mean                         -0.817395
trainer/Policy log std Std                           0.333841
trainer/Policy log std Max                           0.163394
trainer/Policy log std Min                          -2.22143
trainer/Alpha                                        0.0658997
trainer/Alpha Loss                                  -0.832173
exploration/num steps total                      93000
exploration/num paths total                         93
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.53064
exploration/Rewards Std                              0.989158
exploration/Rewards Max                              5.66707
exploration/Rewards Min                             -0.357047
exploration/Returns Mean                          3530.64
exploration/Returns Std                              0
exploration/Returns Max                           3530.64
exploration/Returns Min                           3530.64
exploration/Actions Mean                             0.0153497
exploration/Actions Std                              0.790726
exploration/Actions Max                              0.999945
exploration/Actions Min                             -0.999849
exploration/Num Paths                                1
exploration/Average Returns                       3530.64
exploration/env_infos/final/reward_run Mean          3.1409
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.1409
exploration/env_infos/final/reward_run Min           3.1409
exploration/env_infos/initial/reward_run Mean       -0.0175879
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0175879
exploration/env_infos/initial/reward_run Min        -0.0175879
exploration/env_infos/reward_run Mean                3.90593
exploration/env_infos/reward_run Std                 0.971181
exploration/env_infos/reward_run Max                 5.91451
exploration/env_infos/reward_run Min                -0.0175879
exploration/env_infos/final/reward_ctrl Mean        -0.571246
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.571246
exploration/env_infos/final/reward_ctrl Min         -0.571246
exploration/env_infos/initial/reward_ctrl Mean      -0.00974758
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.00974758
exploration/env_infos/initial/reward_ctrl Min       -0.00974758
exploration/env_infos/reward_ctrl Mean              -0.37529
exploration/env_infos/reward_ctrl Std                0.088572
exploration/env_infos/reward_ctrl Max               -0.00974758
exploration/env_infos/reward_ctrl Min               -0.582911
evaluation/num steps total                      460000
evaluation/num paths total                         460
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.25587
evaluation/Rewards Std                               1.56552
evaluation/Rewards Max                               6.05557
evaluation/Rewards Min                              -1.47407
evaluation/Returns Mean                           3255.87
evaluation/Returns Std                            1140.78
evaluation/Returns Max                            3894.72
evaluation/Returns Min                             975.738
evaluation/Actions Mean                             -0.00778047
evaluation/Actions Std                               0.784773
evaluation/Actions Max                               0.999976
evaluation/Actions Min                              -0.999962
evaluation/Num Paths                                 5
evaluation/Average Returns                        3255.87
evaluation/env_infos/final/reward_run Mean           3.60247
evaluation/env_infos/final/reward_run Std            1.59633
evaluation/env_infos/final/reward_run Max            5.12215
evaluation/env_infos/final/reward_run Min            0.565581
evaluation/env_infos/initial/reward_run Mean         0.158438
evaluation/env_infos/initial/reward_run Std          0.103998
evaluation/env_infos/initial/reward_run Max          0.296263
evaluation/env_infos/initial/reward_run Min          0.037893
evaluation/env_infos/reward_run Mean                 3.62543
evaluation/env_infos/reward_run Std                  1.57873
evaluation/env_infos/reward_run Max                  6.45983
evaluation/env_infos/reward_run Min                 -1.10321
evaluation/env_infos/final/reward_ctrl Mean         -0.295914
evaluation/env_infos/final/reward_ctrl Std           0.0799524
evaluation/env_infos/final/reward_ctrl Max          -0.178671
evaluation/env_infos/final/reward_ctrl Min          -0.387758
evaluation/env_infos/initial/reward_ctrl Mean       -0.0847958
evaluation/env_infos/initial/reward_ctrl Std         0.0205004
evaluation/env_infos/initial/reward_ctrl Max        -0.0505193
evaluation/env_infos/initial/reward_ctrl Min        -0.104845
evaluation/env_infos/reward_ctrl Mean               -0.369558
evaluation/env_infos/reward_ctrl Std                 0.0905855
evaluation/env_infos/reward_ctrl Max                -0.0498597
evaluation/env_infos/reward_ctrl Min                -0.587885
time/data storing (s)                                0.00689722
time/evaluation sampling (s)                         2.5254
time/exploration sampling (s)                        0.618121
time/logging (s)                                     0.0403864
time/saving (s)                                      0.0162616
time/training (s)                                   31.4901
time/epoch (s)                                      34.6972
time/total (s)                                    3966.47
Epoch                                               91
----------------------------------------------  ---------------
2020-07-08 22:12:52.220420 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 92 finished
----------------------------------------------  ---------------
replay_buffer/size                               94000
trainer/QF1 Loss                                     9.07708
trainer/QF2 Loss                                     9.25659
trainer/Policy Loss                               -168.25
trainer/Q1 Predictions Mean                        173.749
trainer/Q1 Predictions Std                          93.8292
trainer/Q1 Predictions Max                         264.348
trainer/Q1 Predictions Min                           5.82073
trainer/Q2 Predictions Mean                        173.417
trainer/Q2 Predictions Std                          93.528
trainer/Q2 Predictions Max                         266.464
trainer/Q2 Predictions Min                           6.24464
trainer/Q Targets Mean                             173.708
trainer/Q Targets Std                               93.6934
trainer/Q Targets Max                              265.532
trainer/Q Targets Min                                5.82788
trainer/Log Pis Mean                                 5.43812
trainer/Log Pis Std                                  5.01858
trainer/Log Pis Max                                 22.4588
trainer/Log Pis Min                                 -4.83376
trainer/Policy mu Mean                               0.123498
trainer/Policy mu Std                                1.49599
trainer/Policy mu Max                                4.53811
trainer/Policy mu Min                               -4.15295
trainer/Policy log std Mean                         -0.803433
trainer/Policy log std Std                           0.343383
trainer/Policy log std Max                           0.00251637
trainer/Policy log std Min                          -2.05999
trainer/Alpha                                        0.0666433
trainer/Alpha Loss                                  -1.52174
exploration/num steps total                      94000
exploration/num paths total                         94
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.72997
exploration/Rewards Std                              1.03698
exploration/Rewards Max                              6.00163
exploration/Rewards Min                             -1.12265
exploration/Returns Mean                          3729.97
exploration/Returns Std                              0
exploration/Returns Max                           3729.97
exploration/Returns Min                           3729.97
exploration/Actions Mean                            -0.0195543
exploration/Actions Std                              0.819224
exploration/Actions Max                              0.999856
exploration/Actions Min                             -0.999972
exploration/Num Paths                                1
exploration/Average Returns                       3729.97
exploration/env_infos/final/reward_run Mean          4.06229
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.06229
exploration/env_infos/final/reward_run Min           4.06229
exploration/env_infos/initial/reward_run Mean       -0.346918
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.346918
exploration/env_infos/initial/reward_run Min        -0.346918
exploration/env_infos/reward_run Mean                4.13288
exploration/env_infos/reward_run Std                 1.03172
exploration/env_infos/reward_run Max                 6.44332
exploration/env_infos/reward_run Min                -0.73131
exploration/env_infos/final/reward_ctrl Mean        -0.42252
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.42252
exploration/env_infos/final/reward_ctrl Min         -0.42252
exploration/env_infos/initial/reward_ctrl Mean      -0.148109
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.148109
exploration/env_infos/initial/reward_ctrl Min       -0.148109
exploration/env_infos/reward_ctrl Mean              -0.402906
exploration/env_infos/reward_ctrl Std                0.0885351
exploration/env_infos/reward_ctrl Max               -0.148109
exploration/env_infos/reward_ctrl Min               -0.587787
evaluation/num steps total                      465000
evaluation/num paths total                         465
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.78033
evaluation/Rewards Std                               1.08755
evaluation/Rewards Max                               6.15273
evaluation/Rewards Min                              -0.926802
evaluation/Returns Mean                           3780.33
evaluation/Returns Std                             105.173
evaluation/Returns Max                            3932.24
evaluation/Returns Min                            3651
evaluation/Actions Mean                             -0.0255773
evaluation/Actions Std                               0.825588
evaluation/Actions Max                               0.99982
evaluation/Actions Min                              -0.999853
evaluation/Num Paths                                 5
evaluation/Average Returns                        3780.33
evaluation/env_infos/final/reward_run Mean           3.73094
evaluation/env_infos/final/reward_run Std            0.504731
evaluation/env_infos/final/reward_run Max            4.31344
evaluation/env_infos/final/reward_run Min            2.99572
evaluation/env_infos/initial/reward_run Mean         0.0445084
evaluation/env_infos/initial/reward_run Std          0.124226
evaluation/env_infos/initial/reward_run Max          0.213165
evaluation/env_infos/initial/reward_run Min         -0.140139
evaluation/env_infos/reward_run Mean                 4.18968
evaluation/env_infos/reward_run Std                  1.08288
evaluation/env_infos/reward_run Max                  6.5974
evaluation/env_infos/reward_run Min                 -0.458997
evaluation/env_infos/final/reward_ctrl Mean         -0.460838
evaluation/env_infos/final/reward_ctrl Std           0.0502597
evaluation/env_infos/final/reward_ctrl Max          -0.373166
evaluation/env_infos/final/reward_ctrl Min          -0.504682
evaluation/env_infos/initial/reward_ctrl Mean       -0.0539361
evaluation/env_infos/initial/reward_ctrl Std         0.0259827
evaluation/env_infos/initial/reward_ctrl Max        -0.0146044
evaluation/env_infos/initial/reward_ctrl Min        -0.0774312
evaluation/env_infos/reward_ctrl Mean               -0.40935
evaluation/env_infos/reward_ctrl Std                 0.0885059
evaluation/env_infos/reward_ctrl Max                -0.0146044
evaluation/env_infos/reward_ctrl Min                -0.590018
time/data storing (s)                                0.00679251
time/evaluation sampling (s)                         2.52851
time/exploration sampling (s)                        0.64798
time/logging (s)                                     0.0423815
time/saving (s)                                      0.017699
time/training (s)                                   31.7976
time/epoch (s)                                      35.041
time/total (s)                                    4001.56
Epoch                                               92
----------------------------------------------  ---------------
2020-07-08 22:13:27.511741 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 93 finished
----------------------------------------------  ---------------
replay_buffer/size                               95000
trainer/QF1 Loss                                     7.30408
trainer/QF2 Loss                                     7.59341
trainer/Policy Loss                               -166.589
trainer/Q1 Predictions Mean                        172.213
trainer/Q1 Predictions Std                          97.0667
trainer/Q1 Predictions Max                         266.532
trainer/Q1 Predictions Min                           5.41321
trainer/Q2 Predictions Mean                        172.226
trainer/Q2 Predictions Std                          96.9787
trainer/Q2 Predictions Max                         266.748
trainer/Q2 Predictions Min                           5.73546
trainer/Q Targets Mean                             172.035
trainer/Q Targets Std                               97.0835
trainer/Q Targets Max                              267.986
trainer/Q Targets Min                                5.62375
trainer/Log Pis Mean                                 5.99445
trainer/Log Pis Std                                  5.43193
trainer/Log Pis Max                                 22.4816
trainer/Log Pis Min                                 -6.16531
trainer/Policy mu Mean                               0.0455363
trainer/Policy mu Std                                1.52148
trainer/Policy mu Max                                4.68612
trainer/Policy mu Min                               -4.75411
trainer/Policy log std Mean                         -0.80753
trainer/Policy log std Std                           0.359562
trainer/Policy log std Max                           0.0605294
trainer/Policy log std Min                          -2.26515
trainer/Alpha                                        0.0672994
trainer/Alpha Loss                                  -0.0149668
exploration/num steps total                      95000
exploration/num paths total                         95
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.84897
exploration/Rewards Std                              0.95541
exploration/Rewards Max                              5.75356
exploration/Rewards Min                             -1.00197
exploration/Returns Mean                          3848.97
exploration/Returns Std                              0
exploration/Returns Max                           3848.97
exploration/Returns Min                           3848.97
exploration/Actions Mean                            -0.0118636
exploration/Actions Std                              0.805855
exploration/Actions Max                              0.99999
exploration/Actions Min                             -0.99998
exploration/Num Paths                                1
exploration/Average Returns                       3848.97
exploration/env_infos/final/reward_run Mean          3.33848
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.33848
exploration/env_infos/final/reward_run Min           3.33848
exploration/env_infos/initial/reward_run Mean       -0.0225699
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0225699
exploration/env_infos/initial/reward_run Min        -0.0225699
exploration/env_infos/reward_run Mean                4.2387
exploration/env_infos/reward_run Std                 0.942105
exploration/env_infos/reward_run Max                 6.19824
exploration/env_infos/reward_run Min                -0.520505
exploration/env_infos/final/reward_ctrl Mean        -0.448337
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.448337
exploration/env_infos/final/reward_ctrl Min         -0.448337
exploration/env_infos/initial/reward_ctrl Mean      -0.059534
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.059534
exploration/env_infos/initial/reward_ctrl Min       -0.059534
exploration/env_infos/reward_ctrl Mean              -0.389726
exploration/env_infos/reward_ctrl Std                0.0880261
exploration/env_infos/reward_ctrl Max               -0.059534
exploration/env_infos/reward_ctrl Min               -0.583066
evaluation/num steps total                      470000
evaluation/num paths total                         470
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.88388
evaluation/Rewards Std                               1.01716
evaluation/Rewards Max                               5.96167
evaluation/Rewards Min                              -0.766659
evaluation/Returns Mean                           3883.88
evaluation/Returns Std                              59.311
evaluation/Returns Max                            3955.54
evaluation/Returns Min                            3785.81
evaluation/Actions Mean                             -0.0291212
evaluation/Actions Std                               0.820868
evaluation/Actions Max                               0.999981
evaluation/Actions Min                              -0.99992
evaluation/Num Paths                                 5
evaluation/Average Returns                        3883.88
evaluation/env_infos/final/reward_run Mean           4.29862
evaluation/env_infos/final/reward_run Std            1.17746
evaluation/env_infos/final/reward_run Max            5.73672
evaluation/env_infos/final/reward_run Min            2.22553
evaluation/env_infos/initial/reward_run Mean         0.172446
evaluation/env_infos/initial/reward_run Std          0.0832835
evaluation/env_infos/initial/reward_run Max          0.320942
evaluation/env_infos/initial/reward_run Min          0.089738
evaluation/env_infos/reward_run Mean                 4.28869
evaluation/env_infos/reward_run Std                  1.00665
evaluation/env_infos/reward_run Max                  6.35065
evaluation/env_infos/reward_run Min                 -0.400452
evaluation/env_infos/final/reward_ctrl Mean         -0.45763
evaluation/env_infos/final/reward_ctrl Std           0.0396483
evaluation/env_infos/final/reward_ctrl Max          -0.419799
evaluation/env_infos/final/reward_ctrl Min          -0.530843
evaluation/env_infos/initial/reward_ctrl Mean       -0.0342679
evaluation/env_infos/initial/reward_ctrl Std         0.0266562
evaluation/env_infos/initial/reward_ctrl Max        -0.00562272
evaluation/env_infos/initial/reward_ctrl Min        -0.0817234
evaluation/env_infos/reward_ctrl Mean               -0.404803
evaluation/env_infos/reward_ctrl Std                 0.0883019
evaluation/env_infos/reward_ctrl Max                -0.00562272
evaluation/env_infos/reward_ctrl Min                -0.592083
time/data storing (s)                                0.00687895
time/evaluation sampling (s)                         2.53806
time/exploration sampling (s)                        0.64643
time/logging (s)                                     0.0418621
time/saving (s)                                      0.0170252
time/training (s)                                   31.8816
time/epoch (s)                                      35.1318
time/total (s)                                    4036.84
Epoch                                               93
----------------------------------------------  ---------------
2020-07-08 22:14:11.570978 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 94 finished
----------------------------------------------  ---------------
replay_buffer/size                               96000
trainer/QF1 Loss                                     9.89727
trainer/QF2 Loss                                    10.2686
trainer/Policy Loss                               -170.856
trainer/Q1 Predictions Mean                        176.273
trainer/Q1 Predictions Std                          91.4498
trainer/Q1 Predictions Max                         275.223
trainer/Q1 Predictions Min                           4.70636
trainer/Q2 Predictions Mean                        176.21
trainer/Q2 Predictions Std                          91.4253
trainer/Q2 Predictions Max                         272.293
trainer/Q2 Predictions Min                           4.6269
trainer/Q Targets Mean                             175.816
trainer/Q Targets Std                               91.2868
trainer/Q Targets Max                              273.166
trainer/Q Targets Min                                3.65668
trainer/Log Pis Mean                                 5.68527
trainer/Log Pis Std                                  5.16312
trainer/Log Pis Max                                 21.8569
trainer/Log Pis Min                                 -5.88714
trainer/Policy mu Mean                               0.0763933
trainer/Policy mu Std                                1.49951
trainer/Policy mu Max                                4.74138
trainer/Policy mu Min                               -4.26198
trainer/Policy log std Mean                         -0.791289
trainer/Policy log std Std                           0.340485
trainer/Policy log std Max                           0.0815926
trainer/Policy log std Min                          -2.21003
trainer/Alpha                                        0.0690765
trainer/Alpha Loss                                  -0.841104
exploration/num steps total                      96000
exploration/num paths total                         96
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.74925
exploration/Rewards Std                              1.0331
exploration/Rewards Max                              5.87842
exploration/Rewards Min                             -0.69487
exploration/Returns Mean                          3749.25
exploration/Returns Std                              0
exploration/Returns Max                           3749.25
exploration/Returns Min                           3749.25
exploration/Actions Mean                             0.0275066
exploration/Actions Std                              0.806264
exploration/Actions Max                              0.999863
exploration/Actions Min                             -0.999892
exploration/Num Paths                                1
exploration/Average Returns                       3749.25
exploration/env_infos/final/reward_run Mean          4.50224
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.50224
exploration/env_infos/final/reward_run Min           4.50224
exploration/env_infos/initial/reward_run Mean        0.22334
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.22334
exploration/env_infos/initial/reward_run Min         0.22334
exploration/env_infos/reward_run Mean                4.13974
exploration/env_infos/reward_run Std                 1.0173
exploration/env_infos/reward_run Max                 6.27598
exploration/env_infos/reward_run Min                -0.276831
exploration/env_infos/final/reward_ctrl Mean        -0.398926
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.398926
exploration/env_infos/final/reward_ctrl Min         -0.398926
exploration/env_infos/initial/reward_ctrl Mean      -0.0845472
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0845472
exploration/env_infos/initial/reward_ctrl Min       -0.0845472
exploration/env_infos/reward_ctrl Mean              -0.390491
exploration/env_infos/reward_ctrl Std                0.0869492
exploration/env_infos/reward_ctrl Max               -0.0845472
exploration/env_infos/reward_ctrl Min               -0.576891
evaluation/num steps total                      475000
evaluation/num paths total                         475
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.8529
evaluation/Rewards Std                               1.04241
evaluation/Rewards Max                               6.16483
evaluation/Rewards Min                              -1.01634
evaluation/Returns Mean                           3852.9
evaluation/Returns Std                             108.713
evaluation/Returns Max                            3963.22
evaluation/Returns Min                            3711.77
evaluation/Actions Mean                              0.00677723
evaluation/Actions Std                               0.81326
evaluation/Actions Max                               0.999911
evaluation/Actions Min                              -0.999936
evaluation/Num Paths                                 5
evaluation/Average Returns                        3852.9
evaluation/env_infos/final/reward_run Mean           4.88755
evaluation/env_infos/final/reward_run Std            0.844884
evaluation/env_infos/final/reward_run Max            5.68853
evaluation/env_infos/final/reward_run Min            3.24709
evaluation/env_infos/initial/reward_run Mean        -0.0408892
evaluation/env_infos/initial/reward_run Std          0.121848
evaluation/env_infos/initial/reward_run Max          0.139917
evaluation/env_infos/initial/reward_run Min         -0.215258
evaluation/env_infos/reward_run Mean                 4.24976
evaluation/env_infos/reward_run Std                  1.02966
evaluation/env_infos/reward_run Max                  6.60187
evaluation/env_infos/reward_run Min                 -0.517243
evaluation/env_infos/final/reward_ctrl Mean         -0.3835
evaluation/env_infos/final/reward_ctrl Std           0.0437992
evaluation/env_infos/final/reward_ctrl Max          -0.325683
evaluation/env_infos/final/reward_ctrl Min          -0.448703
evaluation/env_infos/initial/reward_ctrl Mean       -0.0495004
evaluation/env_infos/initial/reward_ctrl Std         0.0312471
evaluation/env_infos/initial/reward_ctrl Max        -0.00833057
evaluation/env_infos/initial/reward_ctrl Min        -0.100349
evaluation/env_infos/reward_ctrl Mean               -0.396862
evaluation/env_infos/reward_ctrl Std                 0.0883695
evaluation/env_infos/reward_ctrl Max                -0.00833057
evaluation/env_infos/reward_ctrl Min                -0.592229
time/data storing (s)                                0.00661951
time/evaluation sampling (s)                         2.47448
time/exploration sampling (s)                        0.631764
time/logging (s)                                     0.0431879
time/saving (s)                                      0.017254
time/training (s)                                   40.867
time/epoch (s)                                      44.0403
time/total (s)                                    4080.9
Epoch                                               94
----------------------------------------------  ---------------
2020-07-08 22:14:54.533301 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 95 finished
----------------------------------------------  ---------------
replay_buffer/size                               97000
trainer/QF1 Loss                                     9.2084
trainer/QF2 Loss                                     8.61266
trainer/Policy Loss                               -168.093
trainer/Q1 Predictions Mean                        173.725
trainer/Q1 Predictions Std                          95.5981
trainer/Q1 Predictions Max                         269.234
trainer/Q1 Predictions Min                           5.75673
trainer/Q2 Predictions Mean                        173.872
trainer/Q2 Predictions Std                          95.611
trainer/Q2 Predictions Max                         267.5
trainer/Q2 Predictions Min                           1.12068
trainer/Q Targets Mean                             173.983
trainer/Q Targets Std                               95.7041
trainer/Q Targets Max                              274.636
trainer/Q Targets Min                                3.90003
trainer/Log Pis Mean                                 6.25797
trainer/Log Pis Std                                  5.32891
trainer/Log Pis Max                                 23.3785
trainer/Log Pis Min                                 -5.6002
trainer/Policy mu Mean                               0.130428
trainer/Policy mu Std                                1.52698
trainer/Policy mu Max                                5.3265
trainer/Policy mu Min                               -4.78043
trainer/Policy log std Mean                         -0.779267
trainer/Policy log std Std                           0.322086
trainer/Policy log std Max                          -0.0913167
trainer/Policy log std Min                          -2.2635
trainer/Alpha                                        0.0697526
trainer/Alpha Loss                                   0.686945
exploration/num steps total                      97000
exploration/num paths total                         97
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.64508
exploration/Rewards Std                              1.03216
exploration/Rewards Max                              5.88795
exploration/Rewards Min                             -0.737199
exploration/Returns Mean                          3645.08
exploration/Returns Std                              0
exploration/Returns Max                           3645.08
exploration/Returns Min                           3645.08
exploration/Actions Mean                             0.011123
exploration/Actions Std                              0.812398
exploration/Actions Max                              0.999947
exploration/Actions Min                             -0.99996
exploration/Num Paths                                1
exploration/Average Returns                       3645.08
exploration/env_infos/final/reward_run Mean          5.18258
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.18258
exploration/env_infos/final/reward_run Min           5.18258
exploration/env_infos/initial/reward_run Mean        0.489168
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.489168
exploration/env_infos/initial/reward_run Min         0.489168
exploration/env_infos/reward_run Mean                4.04115
exploration/env_infos/reward_run Std                 1.01554
exploration/env_infos/reward_run Max                 6.17722
exploration/env_infos/reward_run Min                -0.451143
exploration/env_infos/final/reward_ctrl Mean        -0.37641
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.37641
exploration/env_infos/final/reward_ctrl Min         -0.37641
exploration/env_infos/initial/reward_ctrl Mean      -0.0846216
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0846216
exploration/env_infos/initial/reward_ctrl Min       -0.0846216
exploration/env_infos/reward_ctrl Mean              -0.396068
exploration/env_infos/reward_ctrl Std                0.0874796
exploration/env_infos/reward_ctrl Max               -0.0846216
exploration/env_infos/reward_ctrl Min               -0.586843
evaluation/num steps total                      480000
evaluation/num paths total                         480
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.83778
evaluation/Rewards Std                               1.03422
evaluation/Rewards Max                               6.2852
evaluation/Rewards Min                              -0.896251
evaluation/Returns Mean                           3837.78
evaluation/Returns Std                              57.8839
evaluation/Returns Max                            3941.28
evaluation/Returns Min                            3775.14
evaluation/Actions Mean                             -0.00898264
evaluation/Actions Std                               0.826688
evaluation/Actions Max                               0.999892
evaluation/Actions Min                              -0.999981
evaluation/Num Paths                                 5
evaluation/Average Returns                        3837.78
evaluation/env_infos/final/reward_run Mean           4.70076
evaluation/env_infos/final/reward_run Std            0.558739
evaluation/env_infos/final/reward_run Max            5.53584
evaluation/env_infos/final/reward_run Min            3.86176
evaluation/env_infos/initial/reward_run Mean         0.0370092
evaluation/env_infos/initial/reward_run Std          0.0874921
evaluation/env_infos/initial/reward_run Max          0.105406
evaluation/env_infos/initial/reward_run Min         -0.135863
evaluation/env_infos/reward_run Mean                 4.24788
evaluation/env_infos/reward_run Std                  1.02346
evaluation/env_infos/reward_run Max                  6.67732
evaluation/env_infos/reward_run Min                 -0.471949
evaluation/env_infos/final/reward_ctrl Mean         -0.433087
evaluation/env_infos/final/reward_ctrl Std           0.0711706
evaluation/env_infos/final/reward_ctrl Max          -0.355818
evaluation/env_infos/final/reward_ctrl Min          -0.545153
evaluation/env_infos/initial/reward_ctrl Mean       -0.0425649
evaluation/env_infos/initial/reward_ctrl Std         0.0276214
evaluation/env_infos/initial/reward_ctrl Max        -0.0136407
evaluation/env_infos/initial/reward_ctrl Min        -0.0812497
evaluation/env_infos/reward_ctrl Mean               -0.410096
evaluation/env_infos/reward_ctrl Std                 0.0863696
evaluation/env_infos/reward_ctrl Max                -0.0136407
evaluation/env_infos/reward_ctrl Min                -0.58184
time/data storing (s)                                0.00703854
time/evaluation sampling (s)                         4.07735
time/exploration sampling (s)                        0.774983
time/logging (s)                                     0.048154
time/saving (s)                                      0.0233322
time/training (s)                                   38.0182
time/epoch (s)                                      42.9491
time/total (s)                                    4123.86
Epoch                                               95
----------------------------------------------  ---------------
2020-07-08 22:15:34.153467 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 96 finished
----------------------------------------------  ---------------
replay_buffer/size                               98000
trainer/QF1 Loss                                     8.43877
trainer/QF2 Loss                                     8.44901
trainer/Policy Loss                               -173.244
trainer/Q1 Predictions Mean                        178.847
trainer/Q1 Predictions Std                          95.2608
trainer/Q1 Predictions Max                         268.507
trainer/Q1 Predictions Min                           5.30778
trainer/Q2 Predictions Mean                        179.013
trainer/Q2 Predictions Std                          95.2584
trainer/Q2 Predictions Max                         269.176
trainer/Q2 Predictions Min                           6.47292
trainer/Q Targets Mean                             178.666
trainer/Q Targets Std                               95.2684
trainer/Q Targets Max                              269.945
trainer/Q Targets Min                                5.11576
trainer/Log Pis Mean                                 5.99445
trainer/Log Pis Std                                  4.92712
trainer/Log Pis Max                                 18.2605
trainer/Log Pis Min                                 -5.2181
trainer/Policy mu Mean                               0.0363988
trainer/Policy mu Std                                1.51114
trainer/Policy mu Max                                3.84048
trainer/Policy mu Min                               -4.16009
trainer/Policy log std Mean                         -0.805699
trainer/Policy log std Std                           0.339734
trainer/Policy log std Max                           0.0463514
trainer/Policy log std Min                          -1.97909
trainer/Alpha                                        0.0689365
trainer/Alpha Loss                                  -0.0148484
exploration/num steps total                      98000
exploration/num paths total                         98
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.77408
exploration/Rewards Std                              1.00147
exploration/Rewards Max                              6.2758
exploration/Rewards Min                             -0.952583
exploration/Returns Mean                          3774.08
exploration/Returns Std                              0
exploration/Returns Max                           3774.08
exploration/Returns Min                           3774.08
exploration/Actions Mean                            -0.00521448
exploration/Actions Std                              0.819697
exploration/Actions Max                              0.999979
exploration/Actions Min                             -0.999841
exploration/Num Paths                                1
exploration/Average Returns                       3774.08
exploration/env_infos/final/reward_run Mean          4.98431
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.98431
exploration/env_infos/final/reward_run Min           4.98431
exploration/env_infos/initial/reward_run Mean        0.400197
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.400197
exploration/env_infos/initial/reward_run Min         0.400197
exploration/env_infos/reward_run Mean                4.17724
exploration/env_infos/reward_run Std                 0.996008
exploration/env_infos/reward_run Max                 6.73681
exploration/env_infos/reward_run Min                -0.441586
exploration/env_infos/final/reward_ctrl Mean        -0.474351
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.474351
exploration/env_infos/final/reward_ctrl Min         -0.474351
exploration/env_infos/initial/reward_ctrl Mean      -0.149527
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.149527
exploration/env_infos/initial/reward_ctrl Min       -0.149527
exploration/env_infos/reward_ctrl Mean              -0.403158
exploration/env_infos/reward_ctrl Std                0.0871577
exploration/env_infos/reward_ctrl Max               -0.0850083
exploration/env_infos/reward_ctrl Min               -0.590604
evaluation/num steps total                      485000
evaluation/num paths total                         485
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.85038
evaluation/Rewards Std                               1.05913
evaluation/Rewards Max                               6.07981
evaluation/Rewards Min                              -0.91659
evaluation/Returns Mean                           3850.38
evaluation/Returns Std                              94.6294
evaluation/Returns Max                            4026.16
evaluation/Returns Min                            3759.77
evaluation/Actions Mean                             -0.0136409
evaluation/Actions Std                               0.823211
evaluation/Actions Max                               0.999723
evaluation/Actions Min                              -0.999796
evaluation/Num Paths                                 5
evaluation/Average Returns                        3850.38
evaluation/env_infos/final/reward_run Mean           4.90229
evaluation/env_infos/final/reward_run Std            0.968254
evaluation/env_infos/final/reward_run Max            5.95632
evaluation/env_infos/final/reward_run Min            3.67836
evaluation/env_infos/initial/reward_run Mean         0.0929071
evaluation/env_infos/initial/reward_run Std          0.17737
evaluation/env_infos/initial/reward_run Max          0.344379
evaluation/env_infos/initial/reward_run Min         -0.105283
evaluation/env_infos/reward_run Mean                 4.2571
evaluation/env_infos/reward_run Std                  1.05209
evaluation/env_infos/reward_run Max                  6.48113
evaluation/env_infos/reward_run Min                 -0.412609
evaluation/env_infos/final/reward_ctrl Mean         -0.390706
evaluation/env_infos/final/reward_ctrl Std           0.0990312
evaluation/env_infos/final/reward_ctrl Max          -0.201823
evaluation/env_infos/final/reward_ctrl Min          -0.486223
evaluation/env_infos/initial/reward_ctrl Mean       -0.0843742
evaluation/env_infos/initial/reward_ctrl Std         0.0460158
evaluation/env_infos/initial/reward_ctrl Max        -0.028323
evaluation/env_infos/initial/reward_ctrl Min        -0.131191
evaluation/env_infos/reward_ctrl Mean               -0.406718
evaluation/env_infos/reward_ctrl Std                 0.0895599
evaluation/env_infos/reward_ctrl Max                -0.028323
evaluation/env_infos/reward_ctrl Min                -0.582809
time/data storing (s)                                0.00833948
time/evaluation sampling (s)                         3.54028
time/exploration sampling (s)                        1.41186
time/logging (s)                                     0.0426391
time/saving (s)                                      0.0159306
time/training (s)                                   34.5768
time/epoch (s)                                      39.5959
time/total (s)                                    4163.47
Epoch                                               96
----------------------------------------------  ---------------
2020-07-08 22:16:11.662644 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 97 finished
----------------------------------------------  ---------------
replay_buffer/size                               99000
trainer/QF1 Loss                                     6.65755
trainer/QF2 Loss                                     7.27253
trainer/Policy Loss                               -162.209
trainer/Q1 Predictions Mean                        167.116
trainer/Q1 Predictions Std                         103.687
trainer/Q1 Predictions Max                         271.187
trainer/Q1 Predictions Min                           5.35788
trainer/Q2 Predictions Mean                        167.484
trainer/Q2 Predictions Std                         103.854
trainer/Q2 Predictions Max                         271.367
trainer/Q2 Predictions Min                           5.25635
trainer/Q Targets Mean                             167.54
trainer/Q Targets Std                              103.953
trainer/Q Targets Max                              271.671
trainer/Q Targets Min                                4.83341
trainer/Log Pis Mean                                 5.21224
trainer/Log Pis Std                                  5.39002
trainer/Log Pis Max                                 23.0647
trainer/Log Pis Min                                 -6.83459
trainer/Policy mu Mean                               0.0436004
trainer/Policy mu Std                                1.4731
trainer/Policy mu Max                                3.99902
trainer/Policy mu Min                               -4.80688
trainer/Policy log std Mean                         -0.763778
trainer/Policy log std Std                           0.334711
trainer/Policy log std Max                           0.297716
trainer/Policy log std Min                          -1.99038
trainer/Alpha                                        0.0717148
trainer/Alpha Loss                                  -2.07555
exploration/num steps total                      99000
exploration/num paths total                         99
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.68736
exploration/Rewards Std                              0.986703
exploration/Rewards Max                              5.96895
exploration/Rewards Min                             -0.781794
exploration/Returns Mean                          3687.36
exploration/Returns Std                              0
exploration/Returns Max                           3687.36
exploration/Returns Min                           3687.36
exploration/Actions Mean                            -0.0042823
exploration/Actions Std                              0.80598
exploration/Actions Max                              0.999912
exploration/Actions Min                             -0.999882
exploration/Num Paths                                1
exploration/Average Returns                       3687.36
exploration/env_infos/final/reward_run Mean          5.23041
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.23041
exploration/env_infos/final/reward_run Min           5.23041
exploration/env_infos/initial/reward_run Mean        0.259832
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.259832
exploration/env_infos/initial/reward_run Min         0.259832
exploration/env_infos/reward_run Mean                4.07714
exploration/env_infos/reward_run Std                 0.977434
exploration/env_infos/reward_run Max                 6.2442
exploration/env_infos/reward_run Min                -0.274872
exploration/env_infos/final/reward_ctrl Mean        -0.539873
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.539873
exploration/env_infos/final/reward_ctrl Min         -0.539873
exploration/env_infos/initial/reward_ctrl Mean      -0.155483
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.155483
exploration/env_infos/initial/reward_ctrl Min       -0.155483
exploration/env_infos/reward_ctrl Mean              -0.389774
exploration/env_infos/reward_ctrl Std                0.0903871
exploration/env_infos/reward_ctrl Max               -0.0468258
exploration/env_infos/reward_ctrl Min               -0.585536
evaluation/num steps total                      490000
evaluation/num paths total                         490
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.87987
evaluation/Rewards Std                               1.01889
evaluation/Rewards Max                               6.28757
evaluation/Rewards Min                              -1.05207
evaluation/Returns Mean                           3879.87
evaluation/Returns Std                              62.7109
evaluation/Returns Max                            3960
evaluation/Returns Min                            3802.93
evaluation/Actions Mean                             -0.00402283
evaluation/Actions Std                               0.805131
evaluation/Actions Max                               0.999796
evaluation/Actions Min                              -0.999901
evaluation/Num Paths                                 5
evaluation/Average Returns                        3879.87
evaluation/env_infos/final/reward_run Mean           4.97476
evaluation/env_infos/final/reward_run Std            0.615878
evaluation/env_infos/final/reward_run Max            5.71443
evaluation/env_infos/final/reward_run Min            4.02344
evaluation/env_infos/initial/reward_run Mean         0.12026
evaluation/env_infos/initial/reward_run Std          0.168652
evaluation/env_infos/initial/reward_run Max          0.369642
evaluation/env_infos/initial/reward_run Min         -0.101997
evaluation/env_infos/reward_run Mean                 4.26882
evaluation/env_infos/reward_run Std                  1.00891
evaluation/env_infos/reward_run Max                  6.68358
evaluation/env_infos/reward_run Min                 -0.56322
evaluation/env_infos/final/reward_ctrl Mean         -0.424583
evaluation/env_infos/final/reward_ctrl Std           0.0804142
evaluation/env_infos/final/reward_ctrl Max          -0.32313
evaluation/env_infos/final/reward_ctrl Min          -0.517434
evaluation/env_infos/initial/reward_ctrl Mean       -0.0652315
evaluation/env_infos/initial/reward_ctrl Std         0.0460558
evaluation/env_infos/initial/reward_ctrl Max        -0.00522492
evaluation/env_infos/initial/reward_ctrl Min        -0.137116
evaluation/env_infos/reward_ctrl Mean               -0.388952
evaluation/env_infos/reward_ctrl Std                 0.0883877
evaluation/env_infos/reward_ctrl Max                -0.00522492
evaluation/env_infos/reward_ctrl Min                -0.590995
time/data storing (s)                                0.0068128
time/evaluation sampling (s)                         2.58191
time/exploration sampling (s)                        0.655154
time/logging (s)                                     0.0477514
time/saving (s)                                      0.0198728
time/training (s)                                   34.0656
time/epoch (s)                                      37.3771
time/total (s)                                    4200.98
Epoch                                               97
----------------------------------------------  ---------------
2020-07-08 22:16:55.966040 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 98 finished
----------------------------------------------  ---------------
replay_buffer/size                              100000
trainer/QF1 Loss                                     8.24553
trainer/QF2 Loss                                     8.1722
trainer/Policy Loss                               -179.777
trainer/Q1 Predictions Mean                        185.015
trainer/Q1 Predictions Std                          95.2485
trainer/Q1 Predictions Max                         272.812
trainer/Q1 Predictions Min                           5.47
trainer/Q2 Predictions Mean                        184.999
trainer/Q2 Predictions Std                          95.18
trainer/Q2 Predictions Max                         270.623
trainer/Q2 Predictions Min                           3.04557
trainer/Q Targets Mean                             184.414
trainer/Q Targets Std                               94.9956
trainer/Q Targets Max                              271.496
trainer/Q Targets Min                                1.76276
trainer/Log Pis Mean                                 5.53211
trainer/Log Pis Std                                  4.81723
trainer/Log Pis Max                                 20.691
trainer/Log Pis Min                                 -4.89937
trainer/Policy mu Mean                               0.00957909
trainer/Policy mu Std                                1.46398
trainer/Policy mu Max                                4.07552
trainer/Policy mu Min                               -3.86547
trainer/Policy log std Mean                         -0.835882
trainer/Policy log std Std                           0.348119
trainer/Policy log std Max                          -0.039092
trainer/Policy log std Min                          -2.384
trainer/Alpha                                        0.0710778
trainer/Alpha Loss                                  -1.23716
exploration/num steps total                     100000
exploration/num paths total                        100
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.80795
exploration/Rewards Std                              1.03695
exploration/Rewards Max                              5.91268
exploration/Rewards Min                             -0.944709
exploration/Returns Mean                          3807.95
exploration/Returns Std                              0
exploration/Returns Max                           3807.95
exploration/Returns Min                           3807.95
exploration/Actions Mean                            -0.0142276
exploration/Actions Std                              0.8127
exploration/Actions Max                              0.999891
exploration/Actions Min                             -0.999966
exploration/Num Paths                                1
exploration/Average Returns                       3807.95
exploration/env_infos/final/reward_run Mean          5.17713
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.17713
exploration/env_infos/final/reward_run Min           5.17713
exploration/env_infos/initial/reward_run Mean       -0.232615
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.232615
exploration/env_infos/initial/reward_run Min        -0.232615
exploration/env_infos/reward_run Mean                4.20436
exploration/env_infos/reward_run Std                 1.02644
exploration/env_infos/reward_run Max                 6.28666
exploration/env_infos/reward_run Min                -0.532319
exploration/env_infos/final/reward_ctrl Mean        -0.29973
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.29973
exploration/env_infos/final/reward_ctrl Min         -0.29973
exploration/env_infos/initial/reward_ctrl Mean      -0.046085
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.046085
exploration/env_infos/initial/reward_ctrl Min       -0.046085
exploration/env_infos/reward_ctrl Mean              -0.396411
exploration/env_infos/reward_ctrl Std                0.0859518
exploration/env_infos/reward_ctrl Max               -0.046085
exploration/env_infos/reward_ctrl Min               -0.592679
evaluation/num steps total                      495000
evaluation/num paths total                         495
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.9412
evaluation/Rewards Std                               1.0618
evaluation/Rewards Max                               6.23925
evaluation/Rewards Min                              -1.02336
evaluation/Returns Mean                           3941.2
evaluation/Returns Std                              82.4081
evaluation/Returns Max                            4081.72
evaluation/Returns Min                            3835.92
evaluation/Actions Mean                             -0.0212534
evaluation/Actions Std                               0.818795
evaluation/Actions Max                               0.999722
evaluation/Actions Min                              -0.99997
evaluation/Num Paths                                 5
evaluation/Average Returns                        3941.2
evaluation/env_infos/final/reward_run Mean           4.54055
evaluation/env_infos/final/reward_run Std            0.949233
evaluation/env_infos/final/reward_run Max            5.54637
evaluation/env_infos/final/reward_run Min            3.11973
evaluation/env_infos/initial/reward_run Mean         0.154355
evaluation/env_infos/initial/reward_run Std          0.195098
evaluation/env_infos/initial/reward_run Max          0.461538
evaluation/env_infos/initial/reward_run Min         -0.0303426
evaluation/env_infos/reward_run Mean                 4.34373
evaluation/env_infos/reward_run Std                  1.05554
evaluation/env_infos/reward_run Max                  6.63019
evaluation/env_infos/reward_run Min                 -0.547669
evaluation/env_infos/final/reward_ctrl Mean         -0.409403
evaluation/env_infos/final/reward_ctrl Std           0.0597356
evaluation/env_infos/final/reward_ctrl Max          -0.341492
evaluation/env_infos/final/reward_ctrl Min          -0.515835
evaluation/env_infos/initial/reward_ctrl Mean       -0.0767125
evaluation/env_infos/initial/reward_ctrl Std         0.0232601
evaluation/env_infos/initial/reward_ctrl Max        -0.0482135
evaluation/env_infos/initial/reward_ctrl Min        -0.105083
evaluation/env_infos/reward_ctrl Mean               -0.402526
evaluation/env_infos/reward_ctrl Std                 0.0861638
evaluation/env_infos/reward_ctrl Max                -0.0482135
evaluation/env_infos/reward_ctrl Min                -0.591135
time/data storing (s)                                0.00656338
time/evaluation sampling (s)                         2.52616
time/exploration sampling (s)                        0.648275
time/logging (s)                                     0.0470158
time/saving (s)                                      0.0162049
time/training (s)                                   40.9292
time/epoch (s)                                      44.1734
time/total (s)                                    4245.28
Epoch                                               98
----------------------------------------------  ---------------
2020-07-08 22:17:36.793852 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 99 finished
----------------------------------------------  ---------------
replay_buffer/size                              101000
trainer/QF1 Loss                                     7.87936
trainer/QF2 Loss                                     7.43349
trainer/Policy Loss                               -180.191
trainer/Q1 Predictions Mean                        186.111
trainer/Q1 Predictions Std                          95.1541
trainer/Q1 Predictions Max                         278.565
trainer/Q1 Predictions Min                           4.80568
trainer/Q2 Predictions Mean                        186.547
trainer/Q2 Predictions Std                          95.3928
trainer/Q2 Predictions Max                         279.184
trainer/Q2 Predictions Min                           5.10929
trainer/Q Targets Mean                             186.409
trainer/Q Targets Std                               95.4163
trainer/Q Targets Max                              277.849
trainer/Q Targets Min                                3.8893
trainer/Log Pis Mean                                 6.28191
trainer/Log Pis Std                                  5.37054
trainer/Log Pis Max                                 25.9505
trainer/Log Pis Min                                 -5.38264
trainer/Policy mu Mean                               0.0237426
trainer/Policy mu Std                                1.53031
trainer/Policy mu Max                                4.27817
trainer/Policy mu Min                               -4.66891
trainer/Policy log std Mean                         -0.830468
trainer/Policy log std Std                           0.354074
trainer/Policy log std Max                           0.204263
trainer/Policy log std Min                          -2.05607
trainer/Alpha                                        0.0718448
trainer/Alpha Loss                                   0.742354
exploration/num steps total                     101000
exploration/num paths total                        101
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.85697
exploration/Rewards Std                              1.01791
exploration/Rewards Max                              6.19441
exploration/Rewards Min                             -0.739183
exploration/Returns Mean                          3856.97
exploration/Returns Std                              0
exploration/Returns Max                           3856.97
exploration/Returns Min                           3856.97
exploration/Actions Mean                             0.00835279
exploration/Actions Std                              0.801351
exploration/Actions Max                              0.999989
exploration/Actions Min                             -0.999908
exploration/Num Paths                                1
exploration/Average Returns                       3856.97
exploration/env_infos/final/reward_run Mean          3.9098
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.9098
exploration/env_infos/final/reward_run Min           3.9098
exploration/env_infos/initial/reward_run Mean       -0.126306
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.126306
exploration/env_infos/initial/reward_run Min        -0.126306
exploration/env_infos/reward_run Mean                4.24231
exploration/env_infos/reward_run Std                 1.00235
exploration/env_infos/reward_run Max                 6.55816
exploration/env_infos/reward_run Min                -0.319023
exploration/env_infos/final/reward_ctrl Mean        -0.259511
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.259511
exploration/env_infos/final/reward_ctrl Min         -0.259511
exploration/env_infos/initial/reward_ctrl Mean      -0.0725588
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0725588
exploration/env_infos/initial/reward_ctrl Min       -0.0725588
exploration/env_infos/reward_ctrl Mean              -0.385339
exploration/env_infos/reward_ctrl Std                0.0871736
exploration/env_infos/reward_ctrl Max               -0.0725588
exploration/env_infos/reward_ctrl Min               -0.581247
evaluation/num steps total                      500000
evaluation/num paths total                         500
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.94582
evaluation/Rewards Std                               1.04876
evaluation/Rewards Max                               6.32804
evaluation/Rewards Min                              -0.976075
evaluation/Returns Mean                           3945.82
evaluation/Returns Std                              68.5598
evaluation/Returns Max                            4064.97
evaluation/Returns Min                            3871.82
evaluation/Actions Mean                              0.0108373
evaluation/Actions Std                               0.806196
evaluation/Actions Max                               0.999413
evaluation/Actions Min                              -0.999839
evaluation/Num Paths                                 5
evaluation/Average Returns                        3945.82
evaluation/env_infos/final/reward_run Mean           4.59194
evaluation/env_infos/final/reward_run Std            0.635064
evaluation/env_infos/final/reward_run Max            5.32172
evaluation/env_infos/final/reward_run Min            3.43465
evaluation/env_infos/initial/reward_run Mean        -0.194682
evaluation/env_infos/initial/reward_run Std          0.184393
evaluation/env_infos/initial/reward_run Max          0.021614
evaluation/env_infos/initial/reward_run Min         -0.528217
evaluation/env_infos/reward_run Mean                 4.33586
evaluation/env_infos/reward_run Std                  1.0362
evaluation/env_infos/reward_run Max                  6.61237
evaluation/env_infos/reward_run Min                 -0.528217
evaluation/env_infos/final/reward_ctrl Mean         -0.390557
evaluation/env_infos/final/reward_ctrl Std           0.0665287
evaluation/env_infos/final/reward_ctrl Max          -0.273549
evaluation/env_infos/final/reward_ctrl Min          -0.480556
evaluation/env_infos/initial/reward_ctrl Mean       -0.0710204
evaluation/env_infos/initial/reward_ctrl Std         0.0315962
evaluation/env_infos/initial/reward_ctrl Max        -0.0325854
evaluation/env_infos/initial/reward_ctrl Min        -0.119506
evaluation/env_infos/reward_ctrl Mean               -0.390042
evaluation/env_infos/reward_ctrl Std                 0.0844711
evaluation/env_infos/reward_ctrl Max                -0.0325854
evaluation/env_infos/reward_ctrl Min                -0.58065
time/data storing (s)                                0.00722751
time/evaluation sampling (s)                         3.04798
time/exploration sampling (s)                        1.34052
time/logging (s)                                     0.040781
time/saving (s)                                      0.0171092
time/training (s)                                   36.348
time/epoch (s)                                      40.8016
time/total (s)                                    4286.09
Epoch                                               99
----------------------------------------------  ---------------
2020-07-08 22:18:17.310752 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 100 finished
----------------------------------------------  ---------------
replay_buffer/size                              102000
trainer/QF1 Loss                                     5.97375
trainer/QF2 Loss                                     7.35652
trainer/Policy Loss                               -173.035
trainer/Q1 Predictions Mean                        178.16
trainer/Q1 Predictions Std                          98.5459
trainer/Q1 Predictions Max                         275.142
trainer/Q1 Predictions Min                           4.79429
trainer/Q2 Predictions Mean                        178.491
trainer/Q2 Predictions Std                          98.7588
trainer/Q2 Predictions Max                         276.889
trainer/Q2 Predictions Min                           4.40427
trainer/Q Targets Mean                             178.62
trainer/Q Targets Std                               98.8033
trainer/Q Targets Max                              275.868
trainer/Q Targets Min                                4.33921
trainer/Log Pis Mean                                 5.62043
trainer/Log Pis Std                                  5.22206
trainer/Log Pis Max                                 20.2196
trainer/Log Pis Min                                 -7.9151
trainer/Policy mu Mean                               0.0269877
trainer/Policy mu Std                                1.48473
trainer/Policy mu Max                                4.35299
trainer/Policy mu Min                               -4.23917
trainer/Policy log std Mean                         -0.805851
trainer/Policy log std Std                           0.361225
trainer/Policy log std Max                           0.276526
trainer/Policy log std Min                          -2.55023
trainer/Alpha                                        0.0727413
trainer/Alpha Loss                                  -0.994743
exploration/num steps total                     102000
exploration/num paths total                        102
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.83849
exploration/Rewards Std                              1.00114
exploration/Rewards Max                              5.77055
exploration/Rewards Min                             -1.21911
exploration/Returns Mean                          3838.49
exploration/Returns Std                              0
exploration/Returns Max                           3838.49
exploration/Returns Min                           3838.49
exploration/Actions Mean                            -0.00664146
exploration/Actions Std                              0.80939
exploration/Actions Max                              0.999887
exploration/Actions Min                             -0.999954
exploration/Num Paths                                1
exploration/Average Returns                       3838.49
exploration/env_infos/final/reward_run Mean          5.04517
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.04517
exploration/env_infos/final/reward_run Min           5.04517
exploration/env_infos/initial/reward_run Mean       -0.118656
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.118656
exploration/env_infos/initial/reward_run Min        -0.118656
exploration/env_infos/reward_run Mean                4.23158
exploration/env_infos/reward_run Std                 0.99213
exploration/env_infos/reward_run Max                 6.20383
exploration/env_infos/reward_run Min                -0.930518
exploration/env_infos/final/reward_ctrl Mean        -0.378946
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.378946
exploration/env_infos/final/reward_ctrl Min         -0.378946
exploration/env_infos/initial/reward_ctrl Mean      -0.158748
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.158748
exploration/env_infos/initial/reward_ctrl Min       -0.158748
exploration/env_infos/reward_ctrl Mean              -0.393094
exploration/env_infos/reward_ctrl Std                0.0903094
exploration/env_infos/reward_ctrl Max               -0.106839
exploration/env_infos/reward_ctrl Min               -0.585062
evaluation/num steps total                      505000
evaluation/num paths total                         505
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.9478
evaluation/Rewards Std                               1.02309
evaluation/Rewards Max                               6.14435
evaluation/Rewards Min                              -0.952254
evaluation/Returns Mean                           3947.8
evaluation/Returns Std                              59.011
evaluation/Returns Max                            4046
evaluation/Returns Min                            3873.49
evaluation/Actions Mean                             -0.0186279
evaluation/Actions Std                               0.815857
evaluation/Actions Max                               0.999927
evaluation/Actions Min                              -0.999848
evaluation/Num Paths                                 5
evaluation/Average Returns                        3947.8
evaluation/env_infos/final/reward_run Mean           4.34883
evaluation/env_infos/final/reward_run Std            1.24905
evaluation/env_infos/final/reward_run Max            6.10872
evaluation/env_infos/final/reward_run Min            3.18965
evaluation/env_infos/initial/reward_run Mean         0.0429133
evaluation/env_infos/initial/reward_run Std          0.16031
evaluation/env_infos/initial/reward_run Max          0.279199
evaluation/env_infos/initial/reward_run Min         -0.174584
evaluation/env_infos/reward_run Mean                 4.34738
evaluation/env_infos/reward_run Std                  1.01434
evaluation/env_infos/reward_run Max                  6.54935
evaluation/env_infos/reward_run Min                 -0.503989
evaluation/env_infos/final/reward_ctrl Mean         -0.366187
evaluation/env_infos/final/reward_ctrl Std           0.0725341
evaluation/env_infos/final/reward_ctrl Max          -0.25693
evaluation/env_infos/final/reward_ctrl Min          -0.432429
evaluation/env_infos/initial/reward_ctrl Mean       -0.0283028
evaluation/env_infos/initial/reward_ctrl Std         0.0150387
evaluation/env_infos/initial/reward_ctrl Max        -0.00782759
evaluation/env_infos/initial/reward_ctrl Min        -0.0457659
evaluation/env_infos/reward_ctrl Mean               -0.399581
evaluation/env_infos/reward_ctrl Std                 0.0894655
evaluation/env_infos/reward_ctrl Max                -0.00782759
evaluation/env_infos/reward_ctrl Min                -0.590601
time/data storing (s)                                0.00679516
time/evaluation sampling (s)                         3.79365
time/exploration sampling (s)                        0.851166
time/logging (s)                                     0.0426021
time/saving (s)                                      0.0171967
time/training (s)                                   35.7599
time/epoch (s)                                      40.4713
time/total (s)                                    4326.61
Epoch                                              100
----------------------------------------------  ---------------
2020-07-08 22:19:00.108745 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 101 finished
----------------------------------------------  ---------------
replay_buffer/size                              103000
trainer/QF1 Loss                                     9.14392
trainer/QF2 Loss                                     8.77487
trainer/Policy Loss                               -184.41
trainer/Q1 Predictions Mean                        190.343
trainer/Q1 Predictions Std                          93.1018
trainer/Q1 Predictions Max                         281.199
trainer/Q1 Predictions Min                           4.27055
trainer/Q2 Predictions Mean                        190.602
trainer/Q2 Predictions Std                          93.0734
trainer/Q2 Predictions Max                         280.541
trainer/Q2 Predictions Min                           4.36439
trainer/Q Targets Mean                             190.336
trainer/Q Targets Std                               93.2286
trainer/Q Targets Max                              281.422
trainer/Q Targets Min                                2.62449
trainer/Log Pis Mean                                 6.2604
trainer/Log Pis Std                                  5.4876
trainer/Log Pis Max                                 21.3004
trainer/Log Pis Min                                 -6.12139
trainer/Policy mu Mean                               0.0905031
trainer/Policy mu Std                                1.56883
trainer/Policy mu Max                                5.10771
trainer/Policy mu Min                               -4.43376
trainer/Policy log std Mean                         -0.829717
trainer/Policy log std Std                           0.333835
trainer/Policy log std Max                           0.0136124
trainer/Policy log std Min                          -2.08815
trainer/Alpha                                        0.0724465
trainer/Alpha Loss                                   0.683521
exploration/num steps total                     103000
exploration/num paths total                        103
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.63047
exploration/Rewards Std                              1.02212
exploration/Rewards Max                              5.95761
exploration/Rewards Min                             -0.570446
exploration/Returns Mean                          3630.47
exploration/Returns Std                              0
exploration/Returns Max                           3630.47
exploration/Returns Min                           3630.47
exploration/Actions Mean                             0.00578362
exploration/Actions Std                              0.807467
exploration/Actions Max                              0.999831
exploration/Actions Min                             -0.99999
exploration/Num Paths                                1
exploration/Average Returns                       3630.47
exploration/env_infos/final/reward_run Mean          4.54388
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.54388
exploration/env_infos/final/reward_run Min           4.54388
exploration/env_infos/initial/reward_run Mean        0.0137708
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0137708
exploration/env_infos/initial/reward_run Min         0.0137708
exploration/env_infos/reward_run Mean                4.02169
exploration/env_infos/reward_run Std                 1.0061
exploration/env_infos/reward_run Max                 6.32313
exploration/env_infos/reward_run Min                -0.120808
exploration/env_infos/final/reward_ctrl Mean        -0.321105
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.321105
exploration/env_infos/final/reward_ctrl Min         -0.321105
exploration/env_infos/initial/reward_ctrl Mean      -0.052835
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.052835
exploration/env_infos/initial/reward_ctrl Min       -0.052835
exploration/env_infos/reward_ctrl Mean              -0.391222
exploration/env_infos/reward_ctrl Std                0.0928058
exploration/env_infos/reward_ctrl Max               -0.0518565
exploration/env_infos/reward_ctrl Min               -0.576528
evaluation/num steps total                      510000
evaluation/num paths total                         510
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.92517
evaluation/Rewards Std                               1.0436
evaluation/Rewards Max                               6.40288
evaluation/Rewards Min                              -0.936306
evaluation/Returns Mean                           3925.17
evaluation/Returns Std                              63.0259
evaluation/Returns Max                            4039.77
evaluation/Returns Min                            3852.32
evaluation/Actions Mean                              0.00250482
evaluation/Actions Std                               0.812691
evaluation/Actions Max                               0.999861
evaluation/Actions Min                              -0.999953
evaluation/Num Paths                                 5
evaluation/Average Returns                        3925.17
evaluation/env_infos/final/reward_run Mean           4.55885
evaluation/env_infos/final/reward_run Std            0.821044
evaluation/env_infos/final/reward_run Max            5.49836
evaluation/env_infos/final/reward_run Min            3.45668
evaluation/env_infos/initial/reward_run Mean        -0.0623359
evaluation/env_infos/initial/reward_run Std          0.176823
evaluation/env_infos/initial/reward_run Max          0.127372
evaluation/env_infos/initial/reward_run Min         -0.282423
evaluation/env_infos/reward_run Mean                 4.32145
evaluation/env_infos/reward_run Std                  1.02825
evaluation/env_infos/reward_run Max                  6.85322
evaluation/env_infos/reward_run Min                 -0.482277
evaluation/env_infos/final/reward_ctrl Mean         -0.359774
evaluation/env_infos/final/reward_ctrl Std           0.0859778
evaluation/env_infos/final/reward_ctrl Max          -0.263346
evaluation/env_infos/final/reward_ctrl Min          -0.510085
evaluation/env_infos/initial/reward_ctrl Mean       -0.0487119
evaluation/env_infos/initial/reward_ctrl Std         0.0519642
evaluation/env_infos/initial/reward_ctrl Max        -0.00530032
evaluation/env_infos/initial/reward_ctrl Min        -0.147545
evaluation/env_infos/reward_ctrl Mean               -0.396283
evaluation/env_infos/reward_ctrl Std                 0.0866366
evaluation/env_infos/reward_ctrl Max                -0.00530032
evaluation/env_infos/reward_ctrl Min                -0.5882
time/data storing (s)                                0.00723068
time/evaluation sampling (s)                         4.11894
time/exploration sampling (s)                        0.926117
time/logging (s)                                     0.0457915
time/saving (s)                                      0.0178423
time/training (s)                                   37.6642
time/epoch (s)                                      42.7801
time/total (s)                                    4369.4
Epoch                                              101
----------------------------------------------  ---------------
2020-07-08 22:19:36.404822 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 102 finished
----------------------------------------------  ---------------
replay_buffer/size                              104000
trainer/QF1 Loss                                     6.56564
trainer/QF2 Loss                                     8.12546
trainer/Policy Loss                               -179.955
trainer/Q1 Predictions Mean                        186.744
trainer/Q1 Predictions Std                          96.3129
trainer/Q1 Predictions Max                         275.645
trainer/Q1 Predictions Min                           6.20106
trainer/Q2 Predictions Mean                        186.285
trainer/Q2 Predictions Std                          96.1954
trainer/Q2 Predictions Max                         274.448
trainer/Q2 Predictions Min                           6.27884
trainer/Q Targets Mean                             186.716
trainer/Q Targets Std                               96.3554
trainer/Q Targets Max                              276.367
trainer/Q Targets Min                                6.76988
trainer/Log Pis Mean                                 6.66618
trainer/Log Pis Std                                  5.5536
trainer/Log Pis Max                                 25.153
trainer/Log Pis Min                                 -5.59829
trainer/Policy mu Mean                               0.041571
trainer/Policy mu Std                                1.59454
trainer/Policy mu Max                                5.1732
trainer/Policy mu Min                               -4.70332
trainer/Policy log std Mean                         -0.798078
trainer/Policy log std Std                           0.340258
trainer/Policy log std Max                           0.119413
trainer/Policy log std Min                          -2.1237
trainer/Alpha                                        0.0728526
trainer/Alpha Loss                                   1.74498
exploration/num steps total                     104000
exploration/num paths total                        104
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.71772
exploration/Rewards Std                              0.982335
exploration/Rewards Max                              5.7356
exploration/Rewards Min                             -0.392587
exploration/Returns Mean                          3717.72
exploration/Returns Std                              0
exploration/Returns Max                           3717.72
exploration/Returns Min                           3717.72
exploration/Actions Mean                             0.0158125
exploration/Actions Std                              0.804228
exploration/Actions Max                              0.999774
exploration/Actions Min                             -0.999763
exploration/Num Paths                                1
exploration/Average Returns                       3717.72
exploration/env_infos/final/reward_run Mean          5.28771
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.28771
exploration/env_infos/final/reward_run Min           5.28771
exploration/env_infos/initial/reward_run Mean        0.232527
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.232527
exploration/env_infos/initial/reward_run Min         0.232527
exploration/env_infos/reward_run Mean                4.10594
exploration/env_infos/reward_run Std                 0.971297
exploration/env_infos/reward_run Max                 6.06828
exploration/env_infos/reward_run Min                -0.0276716
exploration/env_infos/final/reward_ctrl Mean        -0.476373
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.476373
exploration/env_infos/final/reward_ctrl Min         -0.476373
exploration/env_infos/initial/reward_ctrl Mean      -0.0983309
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0983309
exploration/env_infos/initial/reward_ctrl Min       -0.0983309
exploration/env_infos/reward_ctrl Mean              -0.38822
exploration/env_infos/reward_ctrl Std                0.0923994
exploration/env_infos/reward_ctrl Max               -0.0983309
exploration/env_infos/reward_ctrl Min               -0.583711
evaluation/num steps total                      515000
evaluation/num paths total                         515
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.89426
evaluation/Rewards Std                               1.04589
evaluation/Rewards Max                               6.4421
evaluation/Rewards Min                              -0.696734
evaluation/Returns Mean                           3894.26
evaluation/Returns Std                              41.7017
evaluation/Returns Max                            3959.08
evaluation/Returns Min                            3838.23
evaluation/Actions Mean                              0.0011134
evaluation/Actions Std                               0.815519
evaluation/Actions Max                               0.999886
evaluation/Actions Min                              -0.999816
evaluation/Num Paths                                 5
evaluation/Average Returns                        3894.26
evaluation/env_infos/final/reward_run Mean           5.26043
evaluation/env_infos/final/reward_run Std            0.897162
evaluation/env_infos/final/reward_run Max            6.1506
evaluation/env_infos/final/reward_run Min            3.66452
evaluation/env_infos/initial/reward_run Mean         0.187204
evaluation/env_infos/initial/reward_run Std          0.238015
evaluation/env_infos/initial/reward_run Max          0.519796
evaluation/env_infos/initial/reward_run Min         -0.0912942
evaluation/env_infos/reward_run Mean                 4.2933
evaluation/env_infos/reward_run Std                  1.03532
evaluation/env_infos/reward_run Max                  6.75529
evaluation/env_infos/reward_run Min                 -0.283546
evaluation/env_infos/final/reward_ctrl Mean         -0.41768
evaluation/env_infos/final/reward_ctrl Std           0.0600821
evaluation/env_infos/final/reward_ctrl Max          -0.342224
evaluation/env_infos/final/reward_ctrl Min          -0.498874
evaluation/env_infos/initial/reward_ctrl Mean       -0.0764655
evaluation/env_infos/initial/reward_ctrl Std         0.0368005
evaluation/env_infos/initial/reward_ctrl Max        -0.0518005
evaluation/env_infos/initial/reward_ctrl Min        -0.14897
evaluation/env_infos/reward_ctrl Mean               -0.399044
evaluation/env_infos/reward_ctrl Std                 0.0874461
evaluation/env_infos/reward_ctrl Max                -0.0518005
evaluation/env_infos/reward_ctrl Min                -0.589235
time/data storing (s)                                0.00793083
time/evaluation sampling (s)                         2.61577
time/exploration sampling (s)                        0.647172
time/logging (s)                                     0.0430619
time/saving (s)                                      0.016209
time/training (s)                                   32.9457
time/epoch (s)                                      36.2759
time/total (s)                                    4405.69
Epoch                                              102
----------------------------------------------  ---------------
2020-07-08 22:20:21.378151 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 103 finished
----------------------------------------------  ---------------
replay_buffer/size                              105000
trainer/QF1 Loss                                     7.08392
trainer/QF2 Loss                                     6.45565
trainer/Policy Loss                               -190.956
trainer/Q1 Predictions Mean                        196.57
trainer/Q1 Predictions Std                          94.3582
trainer/Q1 Predictions Max                         279.666
trainer/Q1 Predictions Min                           6.38031
trainer/Q2 Predictions Mean                        195.919
trainer/Q2 Predictions Std                          94.1205
trainer/Q2 Predictions Max                         279.468
trainer/Q2 Predictions Min                           6.132
trainer/Q Targets Mean                             195.797
trainer/Q Targets Std                               94.0623
trainer/Q Targets Max                              278.326
trainer/Q Targets Min                                4.75876
trainer/Log Pis Mean                                 5.48414
trainer/Log Pis Std                                  4.85781
trainer/Log Pis Max                                 20.1159
trainer/Log Pis Min                                 -6.79039
trainer/Policy mu Mean                               0.20478
trainer/Policy mu Std                                1.48971
trainer/Policy mu Max                                4.29809
trainer/Policy mu Min                               -3.62551
trainer/Policy log std Mean                         -0.815251
trainer/Policy log std Std                           0.336204
trainer/Policy log std Max                           0.0649855
trainer/Policy log std Min                          -2.17323
trainer/Alpha                                        0.0734153
trainer/Alpha Loss                                  -1.34713
exploration/num steps total                     105000
exploration/num paths total                        105
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.81889
exploration/Rewards Std                              1.06952
exploration/Rewards Max                              6.15843
exploration/Rewards Min                             -0.812604
exploration/Returns Mean                          3818.89
exploration/Returns Std                              0
exploration/Returns Max                           3818.89
exploration/Returns Min                           3818.89
exploration/Actions Mean                             0.0528525
exploration/Actions Std                              0.812856
exploration/Actions Max                              0.999814
exploration/Actions Min                             -0.99988
exploration/Num Paths                                1
exploration/Average Returns                       3818.89
exploration/env_infos/final/reward_run Mean          5.37426
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.37426
exploration/env_infos/final/reward_run Min           5.37426
exploration/env_infos/initial/reward_run Mean        0.352033
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.352033
exploration/env_infos/initial/reward_run Min         0.352033
exploration/env_infos/reward_run Mean                4.21701
exploration/env_infos/reward_run Std                 1.0604
exploration/env_infos/reward_run Max                 6.48453
exploration/env_infos/reward_run Min                -0.415451
exploration/env_infos/final/reward_ctrl Mean        -0.451203
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.451203
exploration/env_infos/final/reward_ctrl Min         -0.451203
exploration/env_infos/initial/reward_ctrl Mean      -0.165889
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.165889
exploration/env_infos/initial/reward_ctrl Min       -0.165889
exploration/env_infos/reward_ctrl Mean              -0.398117
exploration/env_infos/reward_ctrl Std                0.0859136
exploration/env_infos/reward_ctrl Max               -0.106569
exploration/env_infos/reward_ctrl Min               -0.582645
evaluation/num steps total                      520000
evaluation/num paths total                         520
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.94329
evaluation/Rewards Std                               1.06871
evaluation/Rewards Max                               6.30023
evaluation/Rewards Min                              -0.823885
evaluation/Returns Mean                           3943.29
evaluation/Returns Std                              40.7955
evaluation/Returns Max                            3997.02
evaluation/Returns Min                            3894.94
evaluation/Actions Mean                              0.0353365
evaluation/Actions Std                               0.816841
evaluation/Actions Max                               0.999946
evaluation/Actions Min                              -0.999822
evaluation/Num Paths                                 5
evaluation/Average Returns                        3943.29
evaluation/env_infos/final/reward_run Mean           4.36723
evaluation/env_infos/final/reward_run Std            0.655055
evaluation/env_infos/final/reward_run Max            5.3767
evaluation/env_infos/final/reward_run Min            3.45278
evaluation/env_infos/initial/reward_run Mean         0.197316
evaluation/env_infos/initial/reward_run Std          0.115964
evaluation/env_infos/initial/reward_run Max          0.369693
evaluation/env_infos/initial/reward_run Min          0.0223469
evaluation/env_infos/reward_run Mean                 4.34438
evaluation/env_infos/reward_run Std                  1.05853
evaluation/env_infos/reward_run Max                  6.70484
evaluation/env_infos/reward_run Min                 -0.331536
evaluation/env_infos/final/reward_ctrl Mean         -0.3418
evaluation/env_infos/final/reward_ctrl Std           0.0572958
evaluation/env_infos/final/reward_ctrl Max          -0.253137
evaluation/env_infos/final/reward_ctrl Min          -0.395309
evaluation/env_infos/initial/reward_ctrl Mean       -0.053313
evaluation/env_infos/initial/reward_ctrl Std         0.0468472
evaluation/env_infos/initial/reward_ctrl Max        -0.00927041
evaluation/env_infos/initial/reward_ctrl Min        -0.143531
evaluation/env_infos/reward_ctrl Mean               -0.401087
evaluation/env_infos/reward_ctrl Std                 0.0835017
evaluation/env_infos/reward_ctrl Max                -0.00927041
evaluation/env_infos/reward_ctrl Min                -0.587732
time/data storing (s)                                0.00708855
time/evaluation sampling (s)                         2.63512
time/exploration sampling (s)                        0.736486
time/logging (s)                                     0.0435755
time/saving (s)                                      0.0192814
time/training (s)                                   41.5149
time/epoch (s)                                      44.9564
time/total (s)                                    4450.66
Epoch                                              103
----------------------------------------------  ---------------
2020-07-08 22:21:07.161779 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 104 finished
----------------------------------------------  ---------------
replay_buffer/size                              106000
trainer/QF1 Loss                                     8.21911
trainer/QF2 Loss                                     7.90949
trainer/Policy Loss                               -187.192
trainer/Q1 Predictions Mean                        193.531
trainer/Q1 Predictions Std                          96.1652
trainer/Q1 Predictions Max                         280.037
trainer/Q1 Predictions Min                           5.12834
trainer/Q2 Predictions Mean                        192.936
trainer/Q2 Predictions Std                          95.9648
trainer/Q2 Predictions Max                         279.937
trainer/Q2 Predictions Min                           4.91687
trainer/Q Targets Mean                             192.689
trainer/Q Targets Std                               95.7989
trainer/Q Targets Max                              280.178
trainer/Q Targets Min                                5.51575
trainer/Log Pis Mean                                 6.3025
trainer/Log Pis Std                                  5.16102
trainer/Log Pis Max                                 17.5453
trainer/Log Pis Min                                 -6.36322
trainer/Policy mu Mean                               0.147444
trainer/Policy mu Std                                1.5736
trainer/Policy mu Max                                4.05149
trainer/Policy mu Min                               -3.95345
trainer/Policy log std Mean                         -0.79691
trainer/Policy log std Std                           0.352714
trainer/Policy log std Max                           0.0428503
trainer/Policy log std Min                          -2.14483
trainer/Alpha                                        0.073561
trainer/Alpha Loss                                   0.789455
exploration/num steps total                     106000
exploration/num paths total                        106
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.70736
exploration/Rewards Std                              1.08474
exploration/Rewards Max                              5.961
exploration/Rewards Min                             -0.393947
exploration/Returns Mean                          3707.36
exploration/Returns Std                              0
exploration/Returns Max                           3707.36
exploration/Returns Min                           3707.36
exploration/Actions Mean                             0.0237539
exploration/Actions Std                              0.80859
exploration/Actions Max                              0.999977
exploration/Actions Min                             -0.999836
exploration/Num Paths                                1
exploration/Average Returns                       3707.36
exploration/env_infos/final/reward_run Mean          5.50835
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.50835
exploration/env_infos/final/reward_run Min           5.50835
exploration/env_infos/initial/reward_run Mean        0.047912
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.047912
exploration/env_infos/initial/reward_run Min         0.047912
exploration/env_infos/reward_run Mean                4.09999
exploration/env_infos/reward_run Std                 1.07872
exploration/env_infos/reward_run Max                 6.36377
exploration/env_infos/reward_run Min                -0.0656432
exploration/env_infos/final/reward_ctrl Mean        -0.503128
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.503128
exploration/env_infos/final/reward_ctrl Min         -0.503128
exploration/env_infos/initial/reward_ctrl Mean      -0.0137901
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0137901
exploration/env_infos/initial/reward_ctrl Min       -0.0137901
exploration/env_infos/reward_ctrl Mean              -0.392629
exploration/env_infos/reward_ctrl Std                0.0888919
exploration/env_infos/reward_ctrl Max               -0.0137901
exploration/env_infos/reward_ctrl Min               -0.579558
evaluation/num steps total                      525000
evaluation/num paths total                         525
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.94596
evaluation/Rewards Std                               1.03014
evaluation/Rewards Max                               6.10345
evaluation/Rewards Min                              -0.903645
evaluation/Returns Mean                           3945.96
evaluation/Returns Std                              41.1062
evaluation/Returns Max                            4005.37
evaluation/Returns Min                            3882.68
evaluation/Actions Mean                              0.0119621
evaluation/Actions Std                               0.822462
evaluation/Actions Max                               0.999565
evaluation/Actions Min                              -0.99981
evaluation/Num Paths                                 5
evaluation/Average Returns                        3945.96
evaluation/env_infos/final/reward_run Mean           4.57461
evaluation/env_infos/final/reward_run Std            0.850871
evaluation/env_infos/final/reward_run Max            5.75957
evaluation/env_infos/final/reward_run Min            3.21204
evaluation/env_infos/initial/reward_run Mean         0.158528
evaluation/env_infos/initial/reward_run Std          0.090533
evaluation/env_infos/initial/reward_run Max          0.267312
evaluation/env_infos/initial/reward_run Min          0.0409302
evaluation/env_infos/reward_run Mean                 4.35191
evaluation/env_infos/reward_run Std                  1.02595
evaluation/env_infos/reward_run Max                  6.49489
evaluation/env_infos/reward_run Min                 -0.489746
evaluation/env_infos/final/reward_ctrl Mean         -0.449717
evaluation/env_infos/final/reward_ctrl Std           0.0696986
evaluation/env_infos/final/reward_ctrl Max          -0.318209
evaluation/env_infos/final/reward_ctrl Min          -0.519217
evaluation/env_infos/initial/reward_ctrl Mean       -0.0594527
evaluation/env_infos/initial/reward_ctrl Std         0.0356553
evaluation/env_infos/initial/reward_ctrl Max        -0.00676427
evaluation/env_infos/initial/reward_ctrl Min        -0.101263
evaluation/env_infos/reward_ctrl Mean               -0.405952
evaluation/env_infos/reward_ctrl Std                 0.0847946
evaluation/env_infos/reward_ctrl Max                -0.00676427
evaluation/env_infos/reward_ctrl Min                -0.589846
time/data storing (s)                                0.00693229
time/evaluation sampling (s)                         3.10949
time/exploration sampling (s)                        1.02468
time/logging (s)                                     0.04871
time/saving (s)                                      0.0174238
time/training (s)                                   41.5603
time/epoch (s)                                      45.7676
time/total (s)                                    4496.44
Epoch                                              104
----------------------------------------------  ---------------
2020-07-08 22:21:44.845565 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 105 finished
----------------------------------------------  ---------------
replay_buffer/size                              107000
trainer/QF1 Loss                                     6.45425
trainer/QF2 Loss                                     7.56187
trainer/Policy Loss                               -186.989
trainer/Q1 Predictions Mean                        192.93
trainer/Q1 Predictions Std                          98.9374
trainer/Q1 Predictions Max                         283.519
trainer/Q1 Predictions Min                           5.43532
trainer/Q2 Predictions Mean                        192.761
trainer/Q2 Predictions Std                          98.8709
trainer/Q2 Predictions Max                         280.931
trainer/Q2 Predictions Min                           5.5985
trainer/Q Targets Mean                             192.555
trainer/Q Targets Std                               99.0214
trainer/Q Targets Max                              284.346
trainer/Q Targets Min                                3.77691
trainer/Log Pis Mean                                 6.03816
trainer/Log Pis Std                                  5.04692
trainer/Log Pis Max                                 18.4082
trainer/Log Pis Min                                 -5.76012
trainer/Policy mu Mean                               0.0391412
trainer/Policy mu Std                                1.4807
trainer/Policy mu Max                                4.13669
trainer/Policy mu Min                               -3.86132
trainer/Policy log std Mean                         -0.827512
trainer/Policy log std Std                           0.353013
trainer/Policy log std Max                          -0.0205388
trainer/Policy log std Min                          -2.08688
trainer/Alpha                                        0.0743559
trainer/Alpha Loss                                   0.0991834
exploration/num steps total                     107000
exploration/num paths total                        107
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             2.18499
exploration/Rewards Std                              1.86493
exploration/Rewards Max                              5.99786
exploration/Rewards Min                             -1.38815
exploration/Returns Mean                          2184.99
exploration/Returns Std                              0
exploration/Returns Max                           2184.99
exploration/Returns Min                           2184.99
exploration/Actions Mean                            -0.0230675
exploration/Actions Std                              0.756739
exploration/Actions Max                              0.999976
exploration/Actions Min                             -0.999965
exploration/Num Paths                                1
exploration/Average Returns                       2184.99
exploration/env_infos/final/reward_run Mean          2.27538
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.27538
exploration/env_infos/final/reward_run Min           2.27538
exploration/env_infos/initial/reward_run Mean       -0.124066
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.124066
exploration/env_infos/initial/reward_run Min        -0.124066
exploration/env_infos/reward_run Mean                2.5289
exploration/env_infos/reward_run Std                 1.90025
exploration/env_infos/reward_run Max                 6.42828
exploration/env_infos/reward_run Min                -1.06776
exploration/env_infos/final/reward_ctrl Mean        -0.43337
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.43337
exploration/env_infos/final/reward_ctrl Min         -0.43337
exploration/env_infos/initial/reward_ctrl Mean      -0.0952729
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0952729
exploration/env_infos/initial/reward_ctrl Min       -0.0952729
exploration/env_infos/reward_ctrl Mean              -0.343911
exploration/env_infos/reward_ctrl Std                0.0979613
exploration/env_infos/reward_ctrl Max               -0.0850463
exploration/env_infos/reward_ctrl Min               -0.591573
evaluation/num steps total                      530000
evaluation/num paths total                         530
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.99969
evaluation/Rewards Std                               1.01082
evaluation/Rewards Max                               6.38585
evaluation/Rewards Min                              -0.952472
evaluation/Returns Mean                           3999.69
evaluation/Returns Std                              95.7908
evaluation/Returns Max                            4102.58
evaluation/Returns Min                            3846.55
evaluation/Actions Mean                             -0.0175528
evaluation/Actions Std                               0.814352
evaluation/Actions Max                               0.999793
evaluation/Actions Min                              -0.999955
evaluation/Num Paths                                 5
evaluation/Average Returns                        3999.69
evaluation/env_infos/final/reward_run Mean           4.13767
evaluation/env_infos/final/reward_run Std            0.542885
evaluation/env_infos/final/reward_run Max            4.84676
evaluation/env_infos/final/reward_run Min            3.50759
evaluation/env_infos/initial/reward_run Mean         0.0613508
evaluation/env_infos/initial/reward_run Std          0.131839
evaluation/env_infos/initial/reward_run Max          0.27936
evaluation/env_infos/initial/reward_run Min         -0.131211
evaluation/env_infos/reward_run Mean                 4.39777
evaluation/env_infos/reward_run Std                  1.00128
evaluation/env_infos/reward_run Max                  6.84118
evaluation/env_infos/reward_run Min                 -0.437709
evaluation/env_infos/final/reward_ctrl Mean         -0.353281
evaluation/env_infos/final/reward_ctrl Std           0.101118
evaluation/env_infos/final/reward_ctrl Max          -0.250074
evaluation/env_infos/final/reward_ctrl Min          -0.536207
evaluation/env_infos/initial/reward_ctrl Mean       -0.0464148
evaluation/env_infos/initial/reward_ctrl Std         0.036814
evaluation/env_infos/initial/reward_ctrl Max        -0.0113355
evaluation/env_infos/initial/reward_ctrl Min        -0.114833
evaluation/env_infos/reward_ctrl Mean               -0.398087
evaluation/env_infos/reward_ctrl Std                 0.0886757
evaluation/env_infos/reward_ctrl Max                -0.0113355
evaluation/env_infos/reward_ctrl Min                -0.591895
time/data storing (s)                                0.00663631
time/evaluation sampling (s)                         2.62303
time/exploration sampling (s)                        0.635365
time/logging (s)                                     0.0392667
time/saving (s)                                      0.0160263
time/training (s)                                   34.3358
time/epoch (s)                                      37.6562
time/total (s)                                    4534.11
Epoch                                              105
----------------------------------------------  ---------------
2020-07-08 22:22:25.649461 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 106 finished
----------------------------------------------  ---------------
replay_buffer/size                              108000
trainer/QF1 Loss                                     8.57158
trainer/QF2 Loss                                    11.0193
trainer/Policy Loss                               -186.955
trainer/Q1 Predictions Mean                        192.669
trainer/Q1 Predictions Std                          97.0925
trainer/Q1 Predictions Max                         283.736
trainer/Q1 Predictions Min                           6.58976
trainer/Q2 Predictions Mean                        192.436
trainer/Q2 Predictions Std                          96.9245
trainer/Q2 Predictions Max                         284.346
trainer/Q2 Predictions Min                           6.77637
trainer/Q Targets Mean                             192.943
trainer/Q Targets Std                               97.2091
trainer/Q Targets Max                              282.916
trainer/Q Targets Min                                5.80194
trainer/Log Pis Mean                                 5.90452
trainer/Log Pis Std                                  5.31046
trainer/Log Pis Max                                 24.822
trainer/Log Pis Min                                 -5.30787
trainer/Policy mu Mean                              -0.0346448
trainer/Policy mu Std                                1.51907
trainer/Policy mu Max                                3.89015
trainer/Policy mu Min                               -4.26078
trainer/Policy log std Mean                         -0.815586
trainer/Policy log std Std                           0.354322
trainer/Policy log std Max                          -0.0358763
trainer/Policy log std Min                          -2.19145
trainer/Alpha                                        0.0735174
trainer/Alpha Loss                                  -0.249222
exploration/num steps total                     108000
exploration/num paths total                        108
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.4661
exploration/Rewards Std                              1.43121
exploration/Rewards Max                              6.04327
exploration/Rewards Min                             -1.76357
exploration/Returns Mean                          3466.1
exploration/Returns Std                              0
exploration/Returns Max                           3466.1
exploration/Returns Min                           3466.1
exploration/Actions Mean                            -0.0191582
exploration/Actions Std                              0.815267
exploration/Actions Max                              0.999999
exploration/Actions Min                             -0.999971
exploration/Num Paths                                1
exploration/Average Returns                       3466.1
exploration/env_infos/final/reward_run Mean          3.22726
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.22726
exploration/env_infos/final/reward_run Min           3.22726
exploration/env_infos/initial/reward_run Mean        0.316866
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.316866
exploration/env_infos/initial/reward_run Min         0.316866
exploration/env_infos/reward_run Mean                3.86511
exploration/env_infos/reward_run Std                 1.42561
exploration/env_infos/reward_run Max                 6.42756
exploration/env_infos/reward_run Min                -1.47264
exploration/env_infos/final/reward_ctrl Mean        -0.425207
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.425207
exploration/env_infos/final/reward_ctrl Min         -0.425207
exploration/env_infos/initial/reward_ctrl Mean      -0.108023
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.108023
exploration/env_infos/initial/reward_ctrl Min       -0.108023
exploration/env_infos/reward_ctrl Mean              -0.399017
exploration/env_infos/reward_ctrl Std                0.0926984
exploration/env_infos/reward_ctrl Max               -0.0826634
exploration/env_infos/reward_ctrl Min               -0.592981
evaluation/num steps total                      535000
evaluation/num paths total                         535
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.06505
evaluation/Rewards Std                               1.04059
evaluation/Rewards Max                               6.70117
evaluation/Rewards Min                              -0.939927
evaluation/Returns Mean                           4065.05
evaluation/Returns Std                              77.0189
evaluation/Returns Max                            4148.04
evaluation/Returns Min                            3919.38
evaluation/Actions Mean                             -0.01924
evaluation/Actions Std                               0.824791
evaluation/Actions Max                               0.999727
evaluation/Actions Min                              -0.999621
evaluation/Num Paths                                 5
evaluation/Average Returns                        4065.05
evaluation/env_infos/final/reward_run Mean           4.6197
evaluation/env_infos/final/reward_run Std            0.769235
evaluation/env_infos/final/reward_run Max            5.10602
evaluation/env_infos/final/reward_run Min            3.09643
evaluation/env_infos/initial/reward_run Mean         0.0985057
evaluation/env_infos/initial/reward_run Std          0.190329
evaluation/env_infos/initial/reward_run Max          0.332853
evaluation/env_infos/initial/reward_run Min         -0.244144
evaluation/env_infos/reward_run Mean                 4.47344
evaluation/env_infos/reward_run Std                  1.02657
evaluation/env_infos/reward_run Max                  7.16875
evaluation/env_infos/reward_run Min                 -0.431371
evaluation/env_infos/final/reward_ctrl Mean         -0.39146
evaluation/env_infos/final/reward_ctrl Std           0.0803702
evaluation/env_infos/final/reward_ctrl Max          -0.281643
evaluation/env_infos/final/reward_ctrl Min          -0.509296
evaluation/env_infos/initial/reward_ctrl Mean       -0.0357049
evaluation/env_infos/initial/reward_ctrl Std         0.024977
evaluation/env_infos/initial/reward_ctrl Max        -0.00960916
evaluation/env_infos/initial/reward_ctrl Min        -0.0782102
evaluation/env_infos/reward_ctrl Mean               -0.408391
evaluation/env_infos/reward_ctrl Std                 0.0871318
evaluation/env_infos/reward_ctrl Max                -0.00960916
evaluation/env_infos/reward_ctrl Min                -0.589913
time/data storing (s)                                0.00678412
time/evaluation sampling (s)                         2.49423
time/exploration sampling (s)                        0.621474
time/logging (s)                                     0.0405593
time/saving (s)                                      0.0194859
time/training (s)                                   37.6055
time/epoch (s)                                      40.788
time/total (s)                                    4574.91
Epoch                                              106
----------------------------------------------  ---------------
2020-07-08 22:23:07.132243 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 107 finished
----------------------------------------------  ---------------
replay_buffer/size                              109000
trainer/QF1 Loss                                     7.91979
trainer/QF2 Loss                                     7.81158
trainer/Policy Loss                               -190.506
trainer/Q1 Predictions Mean                        196.196
trainer/Q1 Predictions Std                          96.8157
trainer/Q1 Predictions Max                         284.796
trainer/Q1 Predictions Min                           5.70778
trainer/Q2 Predictions Mean                        196.057
trainer/Q2 Predictions Std                          96.7734
trainer/Q2 Predictions Max                         286.149
trainer/Q2 Predictions Min                           6.31201
trainer/Q Targets Mean                             196.207
trainer/Q Targets Std                               96.9047
trainer/Q Targets Max                              283.181
trainer/Q Targets Min                                5.80043
trainer/Log Pis Mean                                 6.10648
trainer/Log Pis Std                                  5.64228
trainer/Log Pis Max                                 34.1908
trainer/Log Pis Min                                 -5.18953
trainer/Policy mu Mean                               0.0365152
trainer/Policy mu Std                                1.55033
trainer/Policy mu Max                                5.08407
trainer/Policy mu Min                               -4.47198
trainer/Policy log std Mean                         -0.818104
trainer/Policy log std Std                           0.348212
trainer/Policy log std Max                           0.0170535
trainer/Policy log std Min                          -2.46982
trainer/Alpha                                        0.0750748
trainer/Alpha Loss                                   0.275712
exploration/num steps total                     109000
exploration/num paths total                        109
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.69497
exploration/Rewards Std                              1.10529
exploration/Rewards Max                              6.27922
exploration/Rewards Min                             -0.731294
exploration/Returns Mean                          3694.97
exploration/Returns Std                              0
exploration/Returns Max                           3694.97
exploration/Returns Min                           3694.97
exploration/Actions Mean                            -0.00676413
exploration/Actions Std                              0.813016
exploration/Actions Max                              0.999852
exploration/Actions Min                             -0.99991
exploration/Num Paths                                1
exploration/Average Returns                       3694.97
exploration/env_infos/final/reward_run Mean          3.54022
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.54022
exploration/env_infos/final/reward_run Min           3.54022
exploration/env_infos/initial/reward_run Mean        0.265944
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.265944
exploration/env_infos/initial/reward_run Min         0.265944
exploration/env_infos/reward_run Mean                4.0916
exploration/env_infos/reward_run Std                 1.09866
exploration/env_infos/reward_run Max                 6.72352
exploration/env_infos/reward_run Min                -0.208056
exploration/env_infos/final/reward_ctrl Mean        -0.413932
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.413932
exploration/env_infos/final/reward_ctrl Min         -0.413932
exploration/env_infos/initial/reward_ctrl Mean      -0.13384
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.13384
exploration/env_infos/initial/reward_ctrl Min       -0.13384
exploration/env_infos/reward_ctrl Mean              -0.396624
exploration/env_infos/reward_ctrl Std                0.0931248
exploration/env_infos/reward_ctrl Max               -0.091478
exploration/env_infos/reward_ctrl Min               -0.588709
evaluation/num steps total                      540000
evaluation/num paths total                         540
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.97766
evaluation/Rewards Std                               1.01643
evaluation/Rewards Max                               6.49056
evaluation/Rewards Min                              -0.87156
evaluation/Returns Mean                           3977.66
evaluation/Returns Std                              73.7169
evaluation/Returns Max                            4043.97
evaluation/Returns Min                            3878.99
evaluation/Actions Mean                             -0.0153165
evaluation/Actions Std                               0.817648
evaluation/Actions Max                               0.999517
evaluation/Actions Min                              -0.999421
evaluation/Num Paths                                 5
evaluation/Average Returns                        3977.66
evaluation/env_infos/final/reward_run Mean           4.64739
evaluation/env_infos/final/reward_run Std            0.639394
evaluation/env_infos/final/reward_run Max            5.21383
evaluation/env_infos/final/reward_run Min            3.46245
evaluation/env_infos/initial/reward_run Mean         0.331846
evaluation/env_infos/initial/reward_run Std          0.190547
evaluation/env_infos/initial/reward_run Max          0.637391
evaluation/env_infos/initial/reward_run Min          0.0743661
evaluation/env_infos/reward_run Mean                 4.37893
evaluation/env_infos/reward_run Std                  1.00346
evaluation/env_infos/reward_run Max                  6.94596
evaluation/env_infos/reward_run Min                 -0.379889
evaluation/env_infos/final/reward_ctrl Mean         -0.43114
evaluation/env_infos/final/reward_ctrl Std           0.0637893
evaluation/env_infos/final/reward_ctrl Max          -0.339934
evaluation/env_infos/final/reward_ctrl Min          -0.493993
evaluation/env_infos/initial/reward_ctrl Mean       -0.0715716
evaluation/env_infos/initial/reward_ctrl Std         0.0373032
evaluation/env_infos/initial/reward_ctrl Max        -0.0187044
evaluation/env_infos/initial/reward_ctrl Min        -0.113507
evaluation/env_infos/reward_ctrl Mean               -0.40127
evaluation/env_infos/reward_ctrl Std                 0.088747
evaluation/env_infos/reward_ctrl Max                -0.0187044
evaluation/env_infos/reward_ctrl Min                -0.589478
time/data storing (s)                                0.00662091
time/evaluation sampling (s)                         3.23454
time/exploration sampling (s)                        0.847154
time/logging (s)                                     0.0414101
time/saving (s)                                      0.0166882
time/training (s)                                   37.2838
time/epoch (s)                                      41.4303
time/total (s)                                    4616.39
Epoch                                              107
----------------------------------------------  ---------------
2020-07-08 22:23:47.162394 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 108 finished
----------------------------------------------  ---------------
replay_buffer/size                              110000
trainer/QF1 Loss                                     7.69383
trainer/QF2 Loss                                    10.9931
trainer/Policy Loss                               -197.482
trainer/Q1 Predictions Mean                        203.569
trainer/Q1 Predictions Std                          87.146
trainer/Q1 Predictions Max                         284.319
trainer/Q1 Predictions Min                           6.21975
trainer/Q2 Predictions Mean                        203.835
trainer/Q2 Predictions Std                          87.2474
trainer/Q2 Predictions Max                         287.682
trainer/Q2 Predictions Min                           6.63928
trainer/Q Targets Mean                             203.312
trainer/Q Targets Std                               87.1489
trainer/Q Targets Max                              285.993
trainer/Q Targets Min                                6.89562
trainer/Log Pis Mean                                 6.61687
trainer/Log Pis Std                                  5.39504
trainer/Log Pis Max                                 25.2562
trainer/Log Pis Min                                 -6.46497
trainer/Policy mu Mean                               0.126148
trainer/Policy mu Std                                1.54309
trainer/Policy mu Max                                4.51206
trainer/Policy mu Min                               -4.25581
trainer/Policy log std Mean                         -0.843378
trainer/Policy log std Std                           0.356339
trainer/Policy log std Max                           0.10017
trainer/Policy log std Min                          -2.3529
trainer/Alpha                                        0.0760721
trainer/Alpha Loss                                   1.58914
exploration/num steps total                     110000
exploration/num paths total                        110
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.75646
exploration/Rewards Std                              1.07173
exploration/Rewards Max                              6.18724
exploration/Rewards Min                             -0.632521
exploration/Returns Mean                          3756.46
exploration/Returns Std                              0
exploration/Returns Max                           3756.46
exploration/Returns Min                           3756.46
exploration/Actions Mean                             0.00383214
exploration/Actions Std                              0.810585
exploration/Actions Max                              0.999896
exploration/Actions Min                             -0.999826
exploration/Num Paths                                1
exploration/Average Returns                       3756.46
exploration/env_infos/final/reward_run Mean          5.62574
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.62574
exploration/env_infos/final/reward_run Min           5.62574
exploration/env_infos/initial/reward_run Mean       -0.163307
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.163307
exploration/env_infos/initial/reward_run Min        -0.163307
exploration/env_infos/reward_run Mean                4.1507
exploration/env_infos/reward_run Std                 1.06561
exploration/env_infos/reward_run Max                 6.44028
exploration/env_infos/reward_run Min                -0.439673
exploration/env_infos/final/reward_ctrl Mean        -0.379312
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.379312
exploration/env_infos/final/reward_ctrl Min         -0.379312
exploration/env_infos/initial/reward_ctrl Mean      -0.0202753
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0202753
exploration/env_infos/initial/reward_ctrl Min       -0.0202753
exploration/env_infos/reward_ctrl Mean              -0.394238
exploration/env_infos/reward_ctrl Std                0.0926527
exploration/env_infos/reward_ctrl Max               -0.0202753
exploration/env_infos/reward_ctrl Min               -0.585413
evaluation/num steps total                      545000
evaluation/num paths total                         545
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.09133
evaluation/Rewards Std                               1.00517
evaluation/Rewards Max                               6.7468
evaluation/Rewards Min                              -0.871554
evaluation/Returns Mean                           4091.33
evaluation/Returns Std                              41.0763
evaluation/Returns Max                            4159.55
evaluation/Returns Min                            4036.04
evaluation/Actions Mean                             -0.0158805
evaluation/Actions Std                               0.820922
evaluation/Actions Max                               0.999785
evaluation/Actions Min                              -0.999893
evaluation/Num Paths                                 5
evaluation/Average Returns                        4091.33
evaluation/env_infos/final/reward_run Mean           4.6095
evaluation/env_infos/final/reward_run Std            0.958209
evaluation/env_infos/final/reward_run Max            6.05586
evaluation/env_infos/final/reward_run Min            3.22141
evaluation/env_infos/initial/reward_run Mean         0.0547501
evaluation/env_infos/initial/reward_run Std          0.0652475
evaluation/env_infos/initial/reward_run Max          0.116567
evaluation/env_infos/initial/reward_run Min         -0.0478172
evaluation/env_infos/reward_run Mean                 4.49583
evaluation/env_infos/reward_run Std                  0.995168
evaluation/env_infos/reward_run Max                  7.15112
evaluation/env_infos/reward_run Min                 -0.324104
evaluation/env_infos/final/reward_ctrl Mean         -0.386256
evaluation/env_infos/final/reward_ctrl Std           0.0460802
evaluation/env_infos/final/reward_ctrl Max          -0.318745
evaluation/env_infos/final/reward_ctrl Min          -0.449728
evaluation/env_infos/initial/reward_ctrl Mean       -0.0308112
evaluation/env_infos/initial/reward_ctrl Std         0.0229019
evaluation/env_infos/initial/reward_ctrl Max        -0.00332483
evaluation/env_infos/initial/reward_ctrl Min        -0.0616937
evaluation/env_infos/reward_ctrl Mean               -0.4045
evaluation/env_infos/reward_ctrl Std                 0.0886565
evaluation/env_infos/reward_ctrl Max                -0.00332483
evaluation/env_infos/reward_ctrl Min                -0.585653
time/data storing (s)                                0.00699566
time/evaluation sampling (s)                         3.33184
time/exploration sampling (s)                        0.853911
time/logging (s)                                     0.0406616
time/saving (s)                                      0.0167159
time/training (s)                                   35.7618
time/epoch (s)                                      40.0119
time/total (s)                                    4656.42
Epoch                                              108
----------------------------------------------  ---------------
2020-07-08 22:24:23.229002 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 109 finished
----------------------------------------------  ---------------
replay_buffer/size                              111000
trainer/QF1 Loss                                     6.09103
trainer/QF2 Loss                                     5.90802
trainer/Policy Loss                               -195.432
trainer/Q1 Predictions Mean                        201.297
trainer/Q1 Predictions Std                          93.2401
trainer/Q1 Predictions Max                         286.452
trainer/Q1 Predictions Min                           6.27256
trainer/Q2 Predictions Mean                        201.051
trainer/Q2 Predictions Std                          93.037
trainer/Q2 Predictions Max                         288.062
trainer/Q2 Predictions Min                           6.74429
trainer/Q Targets Mean                             201.143
trainer/Q Targets Std                               93.2265
trainer/Q Targets Max                              287.182
trainer/Q Targets Min                                6.5708
trainer/Log Pis Mean                                 5.88679
trainer/Log Pis Std                                  4.9102
trainer/Log Pis Max                                 19.6036
trainer/Log Pis Min                                 -4.07304
trainer/Policy mu Mean                               0.0357578
trainer/Policy mu Std                                1.49091
trainer/Policy mu Max                                3.9705
trainer/Policy mu Min                               -3.76125
trainer/Policy log std Mean                         -0.817955
trainer/Policy log std Std                           0.340918
trainer/Policy log std Max                           0.0161608
trainer/Policy log std Min                          -2.78679
trainer/Alpha                                        0.0772321
trainer/Alpha Loss                                  -0.289899
exploration/num steps total                     111000
exploration/num paths total                        111
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.70962
exploration/Rewards Std                              1.13599
exploration/Rewards Max                              6.18966
exploration/Rewards Min                             -1.35802
exploration/Returns Mean                          3709.62
exploration/Returns Std                              0
exploration/Returns Max                           3709.62
exploration/Returns Min                           3709.62
exploration/Actions Mean                             0.0104916
exploration/Actions Std                              0.793884
exploration/Actions Max                              0.99998
exploration/Actions Min                             -0.99995
exploration/Num Paths                                1
exploration/Average Returns                       3709.62
exploration/env_infos/final/reward_run Mean          4.16896
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.16896
exploration/env_infos/final/reward_run Min           4.16896
exploration/env_infos/initial/reward_run Mean       -0.312261
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.312261
exploration/env_infos/initial/reward_run Min        -0.312261
exploration/env_infos/reward_run Mean                4.08784
exploration/env_infos/reward_run Std                 1.12892
exploration/env_infos/reward_run Max                 6.60091
exploration/env_infos/reward_run Min                -0.812913
exploration/env_infos/final/reward_ctrl Mean        -0.315808
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.315808
exploration/env_infos/final/reward_ctrl Min         -0.315808
exploration/env_infos/initial/reward_ctrl Mean      -0.0603248
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0603248
exploration/env_infos/initial/reward_ctrl Min       -0.0603248
exploration/env_infos/reward_ctrl Mean              -0.378217
exploration/env_infos/reward_ctrl Std                0.0908744
exploration/env_infos/reward_ctrl Max               -0.0603248
exploration/env_infos/reward_ctrl Min               -0.589496
evaluation/num steps total                      550000
evaluation/num paths total                         550
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.95639
evaluation/Rewards Std                               1.01602
evaluation/Rewards Max                               6.38882
evaluation/Rewards Min                              -0.986165
evaluation/Returns Mean                           3956.39
evaluation/Returns Std                              71.3076
evaluation/Returns Max                            4042.77
evaluation/Returns Min                            3862.41
evaluation/Actions Mean                             -0.00822018
evaluation/Actions Std                               0.800699
evaluation/Actions Max                               0.999953
evaluation/Actions Min                              -0.999834
evaluation/Num Paths                                 5
evaluation/Average Returns                        3956.39
evaluation/env_infos/final/reward_run Mean           4.01996
evaluation/env_infos/final/reward_run Std            0.648114
evaluation/env_infos/final/reward_run Max            4.84194
evaluation/env_infos/final/reward_run Min            3.41078
evaluation/env_infos/initial/reward_run Mean         0.0199183
evaluation/env_infos/initial/reward_run Std          0.0763647
evaluation/env_infos/initial/reward_run Max          0.150201
evaluation/env_infos/initial/reward_run Min         -0.0842515
evaluation/env_infos/reward_run Mean                 4.3411
evaluation/env_infos/reward_run Std                  1.00579
evaluation/env_infos/reward_run Max                  6.69134
evaluation/env_infos/reward_run Min                 -0.740771
evaluation/env_infos/final/reward_ctrl Mean         -0.36685
evaluation/env_infos/final/reward_ctrl Std           0.0611434
evaluation/env_infos/final/reward_ctrl Max          -0.292266
evaluation/env_infos/final/reward_ctrl Min          -0.452652
evaluation/env_infos/initial/reward_ctrl Mean       -0.0274557
evaluation/env_infos/initial/reward_ctrl Std         0.024282
evaluation/env_infos/initial/reward_ctrl Max        -0.00840554
evaluation/env_infos/initial/reward_ctrl Min        -0.0742941
evaluation/env_infos/reward_ctrl Mean               -0.384712
evaluation/env_infos/reward_ctrl Std                 0.0909564
evaluation/env_infos/reward_ctrl Max                -0.00840554
evaluation/env_infos/reward_ctrl Min                -0.593502
time/data storing (s)                                0.00674562
time/evaluation sampling (s)                         2.80162
time/exploration sampling (s)                        0.646651
time/logging (s)                                     0.0401372
time/saving (s)                                      0.0162944
time/training (s)                                   32.515
time/epoch (s)                                      36.0265
time/total (s)                                    4692.48
Epoch                                              109
----------------------------------------------  ---------------
2020-07-08 22:24:57.945838 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 110 finished
----------------------------------------------  ---------------
replay_buffer/size                              112000
trainer/QF1 Loss                                     6.33537
trainer/QF2 Loss                                     6.8951
trainer/Policy Loss                               -190.366
trainer/Q1 Predictions Mean                        196.145
trainer/Q1 Predictions Std                         101.842
trainer/Q1 Predictions Max                         287.538
trainer/Q1 Predictions Min                           6.65958
trainer/Q2 Predictions Mean                        196.493
trainer/Q2 Predictions Std                         101.931
trainer/Q2 Predictions Max                         286.536
trainer/Q2 Predictions Min                           6.26988
trainer/Q Targets Mean                             195.996
trainer/Q Targets Std                              101.71
trainer/Q Targets Max                              287.041
trainer/Q Targets Min                                6.60286
trainer/Log Pis Mean                                 6.18185
trainer/Log Pis Std                                  5.45887
trainer/Log Pis Max                                 22.3814
trainer/Log Pis Min                                 -5.97642
trainer/Policy mu Mean                               0.0133607
trainer/Policy mu Std                                1.53744
trainer/Policy mu Max                                4.3853
trainer/Policy mu Min                               -5.07322
trainer/Policy log std Mean                         -0.812934
trainer/Policy log std Std                           0.359721
trainer/Policy log std Max                           0.0981718
trainer/Policy log std Min                          -2.40418
trainer/Alpha                                        0.0766632
trainer/Alpha Loss                                   0.467074
exploration/num steps total                     112000
exploration/num paths total                        112
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.80287
exploration/Rewards Std                              0.986819
exploration/Rewards Max                              5.82766
exploration/Rewards Min                             -0.81809
exploration/Returns Mean                          3802.87
exploration/Returns Std                              0
exploration/Returns Max                           3802.87
exploration/Returns Min                           3802.87
exploration/Actions Mean                             0.00808613
exploration/Actions Std                              0.802973
exploration/Actions Max                              0.999875
exploration/Actions Min                             -0.999928
exploration/Num Paths                                1
exploration/Average Returns                       3802.87
exploration/env_infos/final/reward_run Mean          4.44832
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.44832
exploration/env_infos/final/reward_run Min           4.44832
exploration/env_infos/initial/reward_run Mean        0.290898
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.290898
exploration/env_infos/initial/reward_run Min         0.290898
exploration/env_infos/reward_run Mean                4.18977
exploration/env_infos/reward_run Std                 0.977726
exploration/env_infos/reward_run Max                 6.2613
exploration/env_infos/reward_run Min                -0.292094
exploration/env_infos/final/reward_ctrl Mean        -0.227487
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.227487
exploration/env_infos/final/reward_ctrl Min         -0.227487
exploration/env_infos/initial/reward_ctrl Mean      -0.0691757
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0691757
exploration/env_infos/initial/reward_ctrl Min       -0.0691757
exploration/env_infos/reward_ctrl Mean              -0.386898
exploration/env_infos/reward_ctrl Std                0.0925603
exploration/env_infos/reward_ctrl Max               -0.0356276
exploration/env_infos/reward_ctrl Min               -0.588236
evaluation/num steps total                      555000
evaluation/num paths total                         555
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.97426
evaluation/Rewards Std                               1.01404
evaluation/Rewards Max                               6.46674
evaluation/Rewards Min                              -0.935857
evaluation/Returns Mean                           3974.26
evaluation/Returns Std                             110.94
evaluation/Returns Max                            4151.29
evaluation/Returns Min                            3813.11
evaluation/Actions Mean                             -0.00729452
evaluation/Actions Std                               0.811604
evaluation/Actions Max                               0.999907
evaluation/Actions Min                              -0.999869
evaluation/Num Paths                                 5
evaluation/Average Returns                        3974.26
evaluation/env_infos/final/reward_run Mean           4.56414
evaluation/env_infos/final/reward_run Std            0.750381
evaluation/env_infos/final/reward_run Max            5.67631
evaluation/env_infos/final/reward_run Min            3.69978
evaluation/env_infos/initial/reward_run Mean         0.316838
evaluation/env_infos/initial/reward_run Std          0.152928
evaluation/env_infos/initial/reward_run Max          0.600091
evaluation/env_infos/initial/reward_run Min          0.186629
evaluation/env_infos/reward_run Mean                 4.36951
evaluation/env_infos/reward_run Std                  1.00693
evaluation/env_infos/reward_run Max                  6.91259
evaluation/env_infos/reward_run Min                 -0.427355
evaluation/env_infos/final/reward_ctrl Mean         -0.449465
evaluation/env_infos/final/reward_ctrl Std           0.0711284
evaluation/env_infos/final/reward_ctrl Max          -0.347638
evaluation/env_infos/final/reward_ctrl Min          -0.543736
evaluation/env_infos/initial/reward_ctrl Mean       -0.0536797
evaluation/env_infos/initial/reward_ctrl Std         0.0486965
evaluation/env_infos/initial/reward_ctrl Max        -0.0219799
evaluation/env_infos/initial/reward_ctrl Min        -0.15038
evaluation/env_infos/reward_ctrl Mean               -0.395253
evaluation/env_infos/reward_ctrl Std                 0.0887411
evaluation/env_infos/reward_ctrl Max                -0.0219799
evaluation/env_infos/reward_ctrl Min                -0.586122
time/data storing (s)                                0.00753928
time/evaluation sampling (s)                         2.52199
time/exploration sampling (s)                        0.681326
time/logging (s)                                     0.040668
time/saving (s)                                      0.0208151
time/training (s)                                   31.4215
time/epoch (s)                                      34.6939
time/total (s)                                    4727.19
Epoch                                              110
----------------------------------------------  ---------------
2020-07-08 22:25:33.856823 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 111 finished
----------------------------------------------  ---------------
replay_buffer/size                              113000
trainer/QF1 Loss                                     8.40649
trainer/QF2 Loss                                     7.58023
trainer/Policy Loss                               -188.865
trainer/Q1 Predictions Mean                        195.339
trainer/Q1 Predictions Std                         100.469
trainer/Q1 Predictions Max                         283.811
trainer/Q1 Predictions Min                           6.63154
trainer/Q2 Predictions Mean                        195.612
trainer/Q2 Predictions Std                         100.465
trainer/Q2 Predictions Max                         282.998
trainer/Q2 Predictions Min                           5.52399
trainer/Q Targets Mean                             195.421
trainer/Q Targets Std                              100.526
trainer/Q Targets Max                              284.368
trainer/Q Targets Min                                4.71256
trainer/Log Pis Mean                                 6.8377
trainer/Log Pis Std                                  5.93367
trainer/Log Pis Max                                 24.0718
trainer/Log Pis Min                                 -7.57901
trainer/Policy mu Mean                              -0.0445027
trainer/Policy mu Std                                1.58308
trainer/Policy mu Max                                7.17965
trainer/Policy mu Min                               -5.73278
trainer/Policy log std Mean                         -0.812433
trainer/Policy log std Std                           0.358828
trainer/Policy log std Max                          -0.0229047
trainer/Policy log std Min                          -2.30053
trainer/Alpha                                        0.0781998
trainer/Alpha Loss                                   2.13487
exploration/num steps total                     113000
exploration/num paths total                        113
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.7432
exploration/Rewards Std                              0.984046
exploration/Rewards Max                              6.17697
exploration/Rewards Min                             -0.445506
exploration/Returns Mean                          3743.2
exploration/Returns Std                              0
exploration/Returns Max                           3743.2
exploration/Returns Min                           3743.2
exploration/Actions Mean                            -0.0164885
exploration/Actions Std                              0.794142
exploration/Actions Max                              0.999779
exploration/Actions Min                             -0.999933
exploration/Num Paths                                1
exploration/Average Returns                       3743.2
exploration/env_infos/final/reward_run Mean          3.63773
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.63773
exploration/env_infos/final/reward_run Min           3.63773
exploration/env_infos/initial/reward_run Mean        0.280335
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.280335
exploration/env_infos/initial/reward_run Min         0.280335
exploration/env_infos/reward_run Mean                4.12176
exploration/env_infos/reward_run Std                 0.974556
exploration/env_infos/reward_run Max                 6.6151
exploration/env_infos/reward_run Min                 0.0858906
exploration/env_infos/final/reward_ctrl Mean        -0.445489
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.445489
exploration/env_infos/final/reward_ctrl Min         -0.445489
exploration/env_infos/initial/reward_ctrl Mean      -0.0384597
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0384597
exploration/env_infos/initial/reward_ctrl Min       -0.0384597
exploration/env_infos/reward_ctrl Mean              -0.37856
exploration/env_infos/reward_ctrl Std                0.0944124
exploration/env_infos/reward_ctrl Max               -0.0384597
exploration/env_infos/reward_ctrl Min               -0.581997
evaluation/num steps total                      560000
evaluation/num paths total                         560
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.92762
evaluation/Rewards Std                               1.03257
evaluation/Rewards Max                               6.60749
evaluation/Rewards Min                              -1.0168
evaluation/Returns Mean                           3927.62
evaluation/Returns Std                              34.7748
evaluation/Returns Max                            3978.11
evaluation/Returns Min                            3872.83
evaluation/Actions Mean                             -0.0273611
evaluation/Actions Std                               0.802099
evaluation/Actions Max                               0.999972
evaluation/Actions Min                              -0.999784
evaluation/Num Paths                                 5
evaluation/Average Returns                        3927.62
evaluation/env_infos/final/reward_run Mean           4.76453
evaluation/env_infos/final/reward_run Std            0.728822
evaluation/env_infos/final/reward_run Max            5.56425
evaluation/env_infos/final/reward_run Min            3.7829
evaluation/env_infos/initial/reward_run Mean         0.109435
evaluation/env_infos/initial/reward_run Std          0.0971853
evaluation/env_infos/initial/reward_run Max          0.253352
evaluation/env_infos/initial/reward_run Min         -0.0351945
evaluation/env_infos/reward_run Mean                 4.31409
evaluation/env_infos/reward_run Std                  1.02372
evaluation/env_infos/reward_run Max                  7.01944
evaluation/env_infos/reward_run Min                 -0.598459
evaluation/env_infos/final/reward_ctrl Mean         -0.386346
evaluation/env_infos/final/reward_ctrl Std           0.06864
evaluation/env_infos/final/reward_ctrl Max          -0.299636
evaluation/env_infos/final/reward_ctrl Min          -0.487592
evaluation/env_infos/initial/reward_ctrl Mean       -0.045793
evaluation/env_infos/initial/reward_ctrl Std         0.0151892
evaluation/env_infos/initial/reward_ctrl Max        -0.0321893
evaluation/env_infos/initial/reward_ctrl Min        -0.0651421
evaluation/env_infos/reward_ctrl Mean               -0.386467
evaluation/env_infos/reward_ctrl Std                 0.0943555
evaluation/env_infos/reward_ctrl Max                -0.0321893
evaluation/env_infos/reward_ctrl Min                -0.587249
time/data storing (s)                                0.00810938
time/evaluation sampling (s)                         2.46085
time/exploration sampling (s)                        0.652318
time/logging (s)                                     0.041489
time/saving (s)                                      0.0159928
time/training (s)                                   32.7154
time/epoch (s)                                      35.8942
time/total (s)                                    4763.1
Epoch                                              111
----------------------------------------------  ---------------
2020-07-08 22:26:08.999846 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 112 finished
----------------------------------------------  ---------------
replay_buffer/size                              114000
trainer/QF1 Loss                                     6.61751
trainer/QF2 Loss                                     8.95999
trainer/Policy Loss                               -200.135
trainer/Q1 Predictions Mean                        207.335
trainer/Q1 Predictions Std                          89.6356
trainer/Q1 Predictions Max                         294.811
trainer/Q1 Predictions Min                           2.50403
trainer/Q2 Predictions Mean                        206.497
trainer/Q2 Predictions Std                          89.5075
trainer/Q2 Predictions Max                         292.416
trainer/Q2 Predictions Min                          -3.54886
trainer/Q Targets Mean                             207.406
trainer/Q Targets Std                               89.4979
trainer/Q Targets Max                              292.094
trainer/Q Targets Min                                2.1607
trainer/Log Pis Mean                                 7.06406
trainer/Log Pis Std                                  5.19848
trainer/Log Pis Max                                 21.2857
trainer/Log Pis Min                                 -4.71707
trainer/Policy mu Mean                               0.0844413
trainer/Policy mu Std                                1.60848
trainer/Policy mu Max                                6.72286
trainer/Policy mu Min                               -4.35573
trainer/Policy log std Mean                         -0.830888
trainer/Policy log std Std                           0.352831
trainer/Policy log std Max                          -0.0412865
trainer/Policy log std Min                          -2.5318
trainer/Alpha                                        0.0795237
trainer/Alpha Loss                                   2.69398
exploration/num steps total                     114000
exploration/num paths total                        114
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.81232
exploration/Rewards Std                              1.01852
exploration/Rewards Max                              5.83808
exploration/Rewards Min                             -0.244973
exploration/Returns Mean                          3812.32
exploration/Returns Std                              0
exploration/Returns Max                           3812.32
exploration/Returns Min                           3812.32
exploration/Actions Mean                            -0.00859996
exploration/Actions Std                              0.798876
exploration/Actions Max                              0.999826
exploration/Actions Min                             -0.999918
exploration/Num Paths                                1
exploration/Average Returns                       3812.32
exploration/env_infos/final/reward_run Mean          3.20198
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.20198
exploration/env_infos/final/reward_run Min           3.20198
exploration/env_infos/initial/reward_run Mean       -0.07321
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.07321
exploration/env_infos/initial/reward_run Min        -0.07321
exploration/env_infos/reward_run Mean                4.19528
exploration/env_infos/reward_run Std                 1.00634
exploration/env_infos/reward_run Max                 6.28011
exploration/env_infos/reward_run Min                -0.07321
exploration/env_infos/final/reward_ctrl Mean        -0.528525
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.528525
exploration/env_infos/final/reward_ctrl Min         -0.528525
exploration/env_infos/initial/reward_ctrl Mean      -0.054156
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.054156
exploration/env_infos/initial/reward_ctrl Min       -0.054156
exploration/env_infos/reward_ctrl Mean              -0.382966
exploration/env_infos/reward_ctrl Std                0.0926578
exploration/env_infos/reward_ctrl Max               -0.054156
exploration/env_infos/reward_ctrl Min               -0.589388
evaluation/num steps total                      565000
evaluation/num paths total                         565
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.985
evaluation/Rewards Std                               1.02901
evaluation/Rewards Max                               6.56102
evaluation/Rewards Min                              -1.17615
evaluation/Returns Mean                           3985
evaluation/Returns Std                              63.8632
evaluation/Returns Max                            4076.81
evaluation/Returns Min                            3899.82
evaluation/Actions Mean                             -0.0204379
evaluation/Actions Std                               0.812965
evaluation/Actions Max                               0.999982
evaluation/Actions Min                              -0.999911
evaluation/Num Paths                                 5
evaluation/Average Returns                        3985
evaluation/env_infos/final/reward_run Mean           4.94978
evaluation/env_infos/final/reward_run Std            0.767846
evaluation/env_infos/final/reward_run Max            5.82302
evaluation/env_infos/final/reward_run Min            4.03325
evaluation/env_infos/initial/reward_run Mean         0.00302001
evaluation/env_infos/initial/reward_run Std          0.106542
evaluation/env_infos/initial/reward_run Max          0.166272
evaluation/env_infos/initial/reward_run Min         -0.118413
evaluation/env_infos/reward_run Mean                 4.3818
evaluation/env_infos/reward_run Std                  1.01466
evaluation/env_infos/reward_run Max                  6.94744
evaluation/env_infos/reward_run Min                 -0.696212
evaluation/env_infos/final/reward_ctrl Mean         -0.363313
evaluation/env_infos/final/reward_ctrl Std           0.0428534
evaluation/env_infos/final/reward_ctrl Max          -0.315675
evaluation/env_infos/final/reward_ctrl Min          -0.426322
evaluation/env_infos/initial/reward_ctrl Mean       -0.0337371
evaluation/env_infos/initial/reward_ctrl Std         0.0207538
evaluation/env_infos/initial/reward_ctrl Max        -0.0111036
evaluation/env_infos/initial/reward_ctrl Min        -0.0713709
evaluation/env_infos/reward_ctrl Mean               -0.396798
evaluation/env_infos/reward_ctrl Std                 0.0894899
evaluation/env_infos/reward_ctrl Max                -0.0111036
evaluation/env_infos/reward_ctrl Min                -0.592235
time/data storing (s)                                0.00682066
time/evaluation sampling (s)                         2.45077
time/exploration sampling (s)                        0.650106
time/logging (s)                                     0.0412637
time/saving (s)                                      0.016485
time/training (s)                                   31.9301
time/epoch (s)                                      35.0955
time/total (s)                                    4798.24
Epoch                                              112
----------------------------------------------  ---------------
2020-07-08 22:26:44.029306 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 113 finished
----------------------------------------------  ---------------
replay_buffer/size                              115000
trainer/QF1 Loss                                     6.66407
trainer/QF2 Loss                                     6.086
trainer/Policy Loss                               -192.236
trainer/Q1 Predictions Mean                        198.169
trainer/Q1 Predictions Std                         102.771
trainer/Q1 Predictions Max                         287.687
trainer/Q1 Predictions Min                           6.53142
trainer/Q2 Predictions Mean                        198.056
trainer/Q2 Predictions Std                         102.868
trainer/Q2 Predictions Max                         290.051
trainer/Q2 Predictions Min                           6.32653
trainer/Q Targets Mean                             197.882
trainer/Q Targets Std                              102.667
trainer/Q Targets Max                              293.136
trainer/Q Targets Min                                5.18134
trainer/Log Pis Mean                                 6.06953
trainer/Log Pis Std                                  5.20927
trainer/Log Pis Max                                 21.2571
trainer/Log Pis Min                                 -5.08822
trainer/Policy mu Mean                               0.0627967
trainer/Policy mu Std                                1.52529
trainer/Policy mu Max                                3.77169
trainer/Policy mu Min                               -6.13398
trainer/Policy log std Mean                         -0.816476
trainer/Policy log std Std                           0.361221
trainer/Policy log std Max                          -0.118637
trainer/Policy log std Min                          -2.4928
trainer/Alpha                                        0.0779783
trainer/Alpha Loss                                   0.177406
exploration/num steps total                     115000
exploration/num paths total                        115
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.59779
exploration/Rewards Std                              1.01378
exploration/Rewards Max                              6.1232
exploration/Rewards Min                             -0.553616
exploration/Returns Mean                          3597.79
exploration/Returns Std                              0
exploration/Returns Max                           3597.79
exploration/Returns Min                           3597.79
exploration/Actions Mean                             0.00293341
exploration/Actions Std                              0.797506
exploration/Actions Max                              0.999972
exploration/Actions Min                             -0.999845
exploration/Num Paths                                1
exploration/Average Returns                       3597.79
exploration/env_infos/final/reward_run Mean          4.38922
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.38922
exploration/env_infos/final/reward_run Min           4.38922
exploration/env_infos/initial/reward_run Mean        0.164575
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.164575
exploration/env_infos/initial/reward_run Min         0.164575
exploration/env_infos/reward_run Mean                3.97941
exploration/env_infos/reward_run Std                 0.99718
exploration/env_infos/reward_run Max                 6.50938
exploration/env_infos/reward_run Min                -0.00141942
exploration/env_infos/final/reward_ctrl Mean        -0.387934
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.387934
exploration/env_infos/final/reward_ctrl Min         -0.387934
exploration/env_infos/initial/reward_ctrl Mean      -0.0326044
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0326044
exploration/env_infos/initial/reward_ctrl Min       -0.0326044
exploration/env_infos/reward_ctrl Mean              -0.381614
exploration/env_infos/reward_ctrl Std                0.0958377
exploration/env_infos/reward_ctrl Max               -0.0326044
exploration/env_infos/reward_ctrl Min               -0.584369
evaluation/num steps total                      570000
evaluation/num paths total                         570
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.96122
evaluation/Rewards Std                               1.03499
evaluation/Rewards Max                               6.4411
evaluation/Rewards Min                              -0.976428
evaluation/Returns Mean                           3961.22
evaluation/Returns Std                              71.1937
evaluation/Returns Max                            4011.28
evaluation/Returns Min                            3820.17
evaluation/Actions Mean                              0.00369039
evaluation/Actions Std                               0.806616
evaluation/Actions Max                               0.999991
evaluation/Actions Min                              -0.99964
evaluation/Num Paths                                 5
evaluation/Average Returns                        3961.22
evaluation/env_infos/final/reward_run Mean           4.18121
evaluation/env_infos/final/reward_run Std            0.724345
evaluation/env_infos/final/reward_run Max            5.47048
evaluation/env_infos/final/reward_run Min            3.26824
evaluation/env_infos/initial/reward_run Mean         0.0505323
evaluation/env_infos/initial/reward_run Std          0.125092
evaluation/env_infos/initial/reward_run Max          0.260697
evaluation/env_infos/initial/reward_run Min         -0.100221
evaluation/env_infos/reward_run Mean                 4.3516
evaluation/env_infos/reward_run Std                  1.01978
evaluation/env_infos/reward_run Max                  6.89363
evaluation/env_infos/reward_run Min                 -0.453278
evaluation/env_infos/final/reward_ctrl Mean         -0.350287
evaluation/env_infos/final/reward_ctrl Std           0.0510727
evaluation/env_infos/final/reward_ctrl Max          -0.292243
evaluation/env_infos/final/reward_ctrl Min          -0.413452
evaluation/env_infos/initial/reward_ctrl Mean       -0.0242857
evaluation/env_infos/initial/reward_ctrl Std         0.00961934
evaluation/env_infos/initial/reward_ctrl Max        -0.0174964
evaluation/env_infos/initial/reward_ctrl Min        -0.0429468
evaluation/env_infos/reward_ctrl Mean               -0.390386
evaluation/env_infos/reward_ctrl Std                 0.088797
evaluation/env_infos/reward_ctrl Max                -0.0174964
evaluation/env_infos/reward_ctrl Min                -0.587001
time/data storing (s)                                0.00679595
time/evaluation sampling (s)                         2.60246
time/exploration sampling (s)                        0.665981
time/logging (s)                                     0.156243
time/saving (s)                                      0.0164157
time/training (s)                                   31.5283
time/epoch (s)                                      34.9762
time/total (s)                                    4833.38
Epoch                                              113
----------------------------------------------  ---------------
2020-07-08 22:27:22.046694 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 114 finished
----------------------------------------------  ---------------
replay_buffer/size                              116000
trainer/QF1 Loss                                    14.2991
trainer/QF2 Loss                                    15.1207
trainer/Policy Loss                               -203.848
trainer/Q1 Predictions Mean                        209.563
trainer/Q1 Predictions Std                          95.0969
trainer/Q1 Predictions Max                         290.479
trainer/Q1 Predictions Min                           7.00888
trainer/Q2 Predictions Mean                        210.015
trainer/Q2 Predictions Std                          95.313
trainer/Q2 Predictions Max                         291.983
trainer/Q2 Predictions Min                           7.42522
trainer/Q Targets Mean                             209.793
trainer/Q Targets Std                               95.1272
trainer/Q Targets Max                              291.781
trainer/Q Targets Min                                6.72597
trainer/Log Pis Mean                                 5.9988
trainer/Log Pis Std                                  5.27214
trainer/Log Pis Max                                 35.2956
trainer/Log Pis Min                                 -5.5393
trainer/Policy mu Mean                               0.0347621
trainer/Policy mu Std                                1.49335
trainer/Policy mu Max                                3.85044
trainer/Policy mu Min                               -5.84552
trainer/Policy log std Mean                         -0.831342
trainer/Policy log std Std                           0.336649
trainer/Policy log std Max                          -0.0933664
trainer/Policy log std Min                          -2.28925
trainer/Alpha                                        0.0793793
trainer/Alpha Loss                                  -0.00304332
exploration/num steps total                     116000
exploration/num paths total                        116
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.77133
exploration/Rewards Std                              1.05887
exploration/Rewards Max                              6.15156
exploration/Rewards Min                             -0.825275
exploration/Returns Mean                          3771.33
exploration/Returns Std                              0
exploration/Returns Max                           3771.33
exploration/Returns Min                           3771.33
exploration/Actions Mean                             0.00832084
exploration/Actions Std                              0.795583
exploration/Actions Max                              0.999978
exploration/Actions Min                             -0.999843
exploration/Num Paths                                1
exploration/Average Returns                       3771.33
exploration/env_infos/final/reward_run Mean          3.38115
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.38115
exploration/env_infos/final/reward_run Min           3.38115
exploration/env_infos/initial/reward_run Mean        0.401252
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.401252
exploration/env_infos/initial/reward_run Min         0.401252
exploration/env_infos/reward_run Mean                4.15114
exploration/env_infos/reward_run Std                 1.05227
exploration/env_infos/reward_run Max                 6.52036
exploration/env_infos/reward_run Min                -0.332025
exploration/env_infos/final/reward_ctrl Mean        -0.3728
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.3728
exploration/env_infos/final/reward_ctrl Min         -0.3728
exploration/env_infos/initial/reward_ctrl Mean      -0.0659631
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0659631
exploration/env_infos/initial/reward_ctrl Min       -0.0659631
exploration/env_infos/reward_ctrl Mean              -0.379813
exploration/env_infos/reward_ctrl Std                0.0921625
exploration/env_infos/reward_ctrl Max               -0.0632915
exploration/env_infos/reward_ctrl Min               -0.578813
evaluation/num steps total                      575000
evaluation/num paths total                         575
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.94606
evaluation/Rewards Std                               1.05675
evaluation/Rewards Max                               6.38674
evaluation/Rewards Min                              -1.10743
evaluation/Returns Mean                           3946.06
evaluation/Returns Std                              68.0343
evaluation/Returns Max                            4050.5
evaluation/Returns Min                            3875.49
evaluation/Actions Mean                             -0.0125887
evaluation/Actions Std                               0.803595
evaluation/Actions Max                               0.999932
evaluation/Actions Min                              -0.999791
evaluation/Num Paths                                 5
evaluation/Average Returns                        3946.06
evaluation/env_infos/final/reward_run Mean           3.90268
evaluation/env_infos/final/reward_run Std            0.591073
evaluation/env_infos/final/reward_run Max            4.96279
evaluation/env_infos/final/reward_run Min            3.33054
evaluation/env_infos/initial/reward_run Mean         0.0648552
evaluation/env_infos/initial/reward_run Std          0.160827
evaluation/env_infos/initial/reward_run Max          0.36948
evaluation/env_infos/initial/reward_run Min         -0.063328
evaluation/env_infos/reward_run Mean                 4.33362
evaluation/env_infos/reward_run Std                  1.05044
evaluation/env_infos/reward_run Max                  6.84482
evaluation/env_infos/reward_run Min                 -0.728849
evaluation/env_infos/final/reward_ctrl Mean         -0.305672
evaluation/env_infos/final/reward_ctrl Std           0.117941
evaluation/env_infos/final/reward_ctrl Max          -0.143959
evaluation/env_infos/final/reward_ctrl Min          -0.457024
evaluation/env_infos/initial/reward_ctrl Mean       -0.0343991
evaluation/env_infos/initial/reward_ctrl Std         0.0147575
evaluation/env_infos/initial/reward_ctrl Max        -0.0121797
evaluation/env_infos/initial/reward_ctrl Min        -0.0533341
evaluation/env_infos/reward_ctrl Mean               -0.387554
evaluation/env_infos/reward_ctrl Std                 0.0921758
evaluation/env_infos/reward_ctrl Max                -0.0121797
evaluation/env_infos/reward_ctrl Min                -0.582589
time/data storing (s)                                0.00706069
time/evaluation sampling (s)                         2.51924
time/exploration sampling (s)                        0.620972
time/logging (s)                                     0.0408348
time/saving (s)                                      0.0174371
time/training (s)                                   34.5843
time/epoch (s)                                      37.7898
time/total (s)                                    4871.26
Epoch                                              114
----------------------------------------------  ---------------
2020-07-08 22:28:09.855262 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 115 finished
----------------------------------------------  ---------------
replay_buffer/size                              117000
trainer/QF1 Loss                                    12.8858
trainer/QF2 Loss                                    12.374
trainer/Policy Loss                               -208.221
trainer/Q1 Predictions Mean                        214.817
trainer/Q1 Predictions Std                          89.2224
trainer/Q1 Predictions Max                         295.845
trainer/Q1 Predictions Min                           7.55711
trainer/Q2 Predictions Mean                        214.786
trainer/Q2 Predictions Std                          89.1923
trainer/Q2 Predictions Max                         296.463
trainer/Q2 Predictions Min                           8.53726
trainer/Q Targets Mean                             214.826
trainer/Q Targets Std                               89.2204
trainer/Q Targets Max                              296.512
trainer/Q Targets Min                                7.22544
trainer/Log Pis Mean                                 6.79523
trainer/Log Pis Std                                  5.60376
trainer/Log Pis Max                                 24.8041
trainer/Log Pis Min                                 -4.76598
trainer/Policy mu Mean                               0.0121232
trainer/Policy mu Std                                1.59432
trainer/Policy mu Max                                4.12775
trainer/Policy mu Min                               -4.16596
trainer/Policy log std Mean                         -0.834381
trainer/Policy log std Std                           0.335848
trainer/Policy log std Max                          -0.122241
trainer/Policy log std Min                          -2.44408
trainer/Alpha                                        0.0792675
trainer/Alpha Loss                                   2.01604
exploration/num steps total                     117000
exploration/num paths total                        117
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.96629
exploration/Rewards Std                              1.05863
exploration/Rewards Max                              6.25025
exploration/Rewards Min                             -0.500337
exploration/Returns Mean                          3966.29
exploration/Returns Std                              0
exploration/Returns Max                           3966.29
exploration/Returns Min                           3966.29
exploration/Actions Mean                            -0.0112878
exploration/Actions Std                              0.822615
exploration/Actions Max                              0.999664
exploration/Actions Min                             -0.999574
exploration/Num Paths                                1
exploration/Average Returns                       3966.29
exploration/env_infos/final/reward_run Mean          4.3778
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.3778
exploration/env_infos/final/reward_run Min           4.3778
exploration/env_infos/initial/reward_run Mean        0.0099709
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0099709
exploration/env_infos/initial/reward_run Min         0.0099709
exploration/env_infos/reward_run Mean                4.37238
exploration/env_infos/reward_run Std                 1.05521
exploration/env_infos/reward_run Max                 6.70072
exploration/env_infos/reward_run Min                -0.0531251
exploration/env_infos/final/reward_ctrl Mean        -0.282695
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.282695
exploration/env_infos/final/reward_ctrl Min         -0.282695
exploration/env_infos/initial/reward_ctrl Mean      -0.0280871
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0280871
exploration/env_infos/initial/reward_ctrl Min       -0.0280871
exploration/env_infos/reward_ctrl Mean              -0.406093
exploration/env_infos/reward_ctrl Std                0.0900646
exploration/env_infos/reward_ctrl Max               -0.0280871
exploration/env_infos/reward_ctrl Min               -0.579935
evaluation/num steps total                      580000
evaluation/num paths total                         580
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.05381
evaluation/Rewards Std                               1.03069
evaluation/Rewards Max                               6.19857
evaluation/Rewards Min                              -1.02218
evaluation/Returns Mean                           4053.81
evaluation/Returns Std                              71.9518
evaluation/Returns Max                            4180.93
evaluation/Returns Min                            3961.73
evaluation/Actions Mean                             -0.0260567
evaluation/Actions Std                               0.836605
evaluation/Actions Max                               0.999827
evaluation/Actions Min                              -0.99942
evaluation/Num Paths                                 5
evaluation/Average Returns                        4053.81
evaluation/env_infos/final/reward_run Mean           3.92595
evaluation/env_infos/final/reward_run Std            0.315196
evaluation/env_infos/final/reward_run Max            4.30253
evaluation/env_infos/final/reward_run Min            3.52124
evaluation/env_infos/initial/reward_run Mean         0.169263
evaluation/env_infos/initial/reward_run Std          0.0374272
evaluation/env_infos/initial/reward_run Max          0.211929
evaluation/env_infos/initial/reward_run Min          0.122284
evaluation/env_infos/reward_run Mean                 4.47416
evaluation/env_infos/reward_run Std                  1.023
evaluation/env_infos/reward_run Max                  6.68931
evaluation/env_infos/reward_run Min                 -0.525782
evaluation/env_infos/final/reward_ctrl Mean         -0.47444
evaluation/env_infos/final/reward_ctrl Std           0.0675409
evaluation/env_infos/final/reward_ctrl Max          -0.362436
evaluation/env_infos/final/reward_ctrl Min          -0.565788
evaluation/env_infos/initial/reward_ctrl Mean       -0.0347283
evaluation/env_infos/initial/reward_ctrl Std         0.0131288
evaluation/env_infos/initial/reward_ctrl Max        -0.0163895
evaluation/env_infos/initial/reward_ctrl Min        -0.0508322
evaluation/env_infos/reward_ctrl Mean               -0.420353
evaluation/env_infos/reward_ctrl Std                 0.0898169
evaluation/env_infos/reward_ctrl Max                -0.0163895
evaluation/env_infos/reward_ctrl Min                -0.595302
time/data storing (s)                                0.00795173
time/evaluation sampling (s)                         4.8681
time/exploration sampling (s)                        0.967125
time/logging (s)                                     0.160889
time/saving (s)                                      0.0290062
time/training (s)                                   41.8359
time/epoch (s)                                      47.869
time/total (s)                                    4919.18
Epoch                                              115
----------------------------------------------  ---------------
2020-07-08 22:28:52.463243 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 116 finished
----------------------------------------------  ----------------
replay_buffer/size                              118000
trainer/QF1 Loss                                     6.85653
trainer/QF2 Loss                                     8.38209
trainer/Policy Loss                               -182.137
trainer/Q1 Predictions Mean                        186.962
trainer/Q1 Predictions Std                         107.93
trainer/Q1 Predictions Max                         289.869
trainer/Q1 Predictions Min                           4.50543
trainer/Q2 Predictions Mean                        186.605
trainer/Q2 Predictions Std                         107.884
trainer/Q2 Predictions Max                         289.485
trainer/Q2 Predictions Min                          -6.76493
trainer/Q Targets Mean                             187.641
trainer/Q Targets Std                              108.192
trainer/Q Targets Max                              291.34
trainer/Q Targets Min                                2.9501
trainer/Log Pis Mean                                 4.85259
trainer/Log Pis Std                                  4.96927
trainer/Log Pis Max                                 25.0846
trainer/Log Pis Min                                 -6.61021
trainer/Policy mu Mean                               0.0792889
trainer/Policy mu Std                                1.41668
trainer/Policy mu Max                                4.28256
trainer/Policy mu Min                               -4.86064
trainer/Policy log std Mean                         -0.776384
trainer/Policy log std Std                           0.344265
trainer/Policy log std Max                           0.3893
trainer/Policy log std Min                          -2.18418
trainer/Alpha                                        0.0814497
trainer/Alpha Loss                                  -2.87707
exploration/num steps total                     118000
exploration/num paths total                        118
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.88113
exploration/Rewards Std                              1.01688
exploration/Rewards Max                              6.16003
exploration/Rewards Min                             -0.696436
exploration/Returns Mean                          3881.13
exploration/Returns Std                              0
exploration/Returns Max                           3881.13
exploration/Returns Min                           3881.13
exploration/Actions Mean                             0.00412444
exploration/Actions Std                              0.806535
exploration/Actions Max                              0.99988
exploration/Actions Min                             -0.999501
exploration/Num Paths                                1
exploration/Average Returns                       3881.13
exploration/env_infos/final/reward_run Mean          4.0014
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.0014
exploration/env_infos/final/reward_run Min           4.0014
exploration/env_infos/initial/reward_run Mean       -0.0644663
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0644663
exploration/env_infos/initial/reward_run Min        -0.0644663
exploration/env_infos/reward_run Mean                4.27144
exploration/env_infos/reward_run Std                 1.00618
exploration/env_infos/reward_run Max                 6.52141
exploration/env_infos/reward_run Min                -0.205444
exploration/env_infos/final/reward_ctrl Mean        -0.387441
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.387441
exploration/env_infos/final/reward_ctrl Min         -0.387441
exploration/env_infos/initial/reward_ctrl Mean      -0.0197234
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0197234
exploration/env_infos/initial/reward_ctrl Min       -0.0197234
exploration/env_infos/reward_ctrl Mean              -0.39031
exploration/env_infos/reward_ctrl Std                0.087121
exploration/env_infos/reward_ctrl Max               -0.0197234
exploration/env_infos/reward_ctrl Min               -0.585651
evaluation/num steps total                      585000
evaluation/num paths total                         585
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.05631
evaluation/Rewards Std                               1.04359
evaluation/Rewards Max                               6.49026
evaluation/Rewards Min                              -0.851295
evaluation/Returns Mean                           4056.31
evaluation/Returns Std                              56.2267
evaluation/Returns Max                            4142.83
evaluation/Returns Min                            4003.4
evaluation/Actions Mean                             -0.000398617
evaluation/Actions Std                               0.816132
evaluation/Actions Max                               0.999808
evaluation/Actions Min                              -0.999696
evaluation/Num Paths                                 5
evaluation/Average Returns                        4056.31
evaluation/env_infos/final/reward_run Mean           4.81636
evaluation/env_infos/final/reward_run Std            0.490552
evaluation/env_infos/final/reward_run Max            5.26374
evaluation/env_infos/final/reward_run Min            3.90675
evaluation/env_infos/initial/reward_run Mean         0.180464
evaluation/env_infos/initial/reward_run Std          0.137941
evaluation/env_infos/initial/reward_run Max          0.356267
evaluation/env_infos/initial/reward_run Min         -0.050427
evaluation/env_infos/reward_run Mean                 4.45595
evaluation/env_infos/reward_run Std                  1.03563
evaluation/env_infos/reward_run Max                  6.85032
evaluation/env_infos/reward_run Min                 -0.459903
evaluation/env_infos/final/reward_ctrl Mean         -0.364248
evaluation/env_infos/final/reward_ctrl Std           0.122775
evaluation/env_infos/final/reward_ctrl Max          -0.165128
evaluation/env_infos/final/reward_ctrl Min          -0.481994
evaluation/env_infos/initial/reward_ctrl Mean       -0.0389067
evaluation/env_infos/initial/reward_ctrl Std         0.0238301
evaluation/env_infos/initial/reward_ctrl Max        -0.0143791
evaluation/env_infos/initial/reward_ctrl Min        -0.0808403
evaluation/env_infos/reward_ctrl Mean               -0.399643
evaluation/env_infos/reward_ctrl Std                 0.0863845
evaluation/env_infos/reward_ctrl Max                -0.0143791
evaluation/env_infos/reward_ctrl Min                -0.583439
time/data storing (s)                                0.00723519
time/evaluation sampling (s)                         2.5748
time/exploration sampling (s)                        0.663264
time/logging (s)                                     0.0405129
time/saving (s)                                      0.016999
time/training (s)                                   39.1478
time/epoch (s)                                      42.4506
time/total (s)                                    4961.66
Epoch                                              116
----------------------------------------------  ----------------
2020-07-08 22:29:35.478562 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 117 finished
----------------------------------------------  ---------------
replay_buffer/size                              119000
trainer/QF1 Loss                                     6.86069
trainer/QF2 Loss                                     6.65029
trainer/Policy Loss                               -201.702
trainer/Q1 Predictions Mean                        207.83
trainer/Q1 Predictions Std                          94.0198
trainer/Q1 Predictions Max                         293.823
trainer/Q1 Predictions Min                           8.14126
trainer/Q2 Predictions Mean                        207.6
trainer/Q2 Predictions Std                          93.9449
trainer/Q2 Predictions Max                         294.594
trainer/Q2 Predictions Min                           7.66026
trainer/Q Targets Mean                             208.143
trainer/Q Targets Std                               94.1741
trainer/Q Targets Max                              297.854
trainer/Q Targets Min                                7.63342
trainer/Log Pis Mean                                 6.25985
trainer/Log Pis Std                                  5.4344
trainer/Log Pis Max                                 22.8061
trainer/Log Pis Min                                 -5.491
trainer/Policy mu Mean                               0.0652174
trainer/Policy mu Std                                1.54993
trainer/Policy mu Max                                6.05364
trainer/Policy mu Min                               -4.65745
trainer/Policy log std Mean                         -0.801033
trainer/Policy log std Std                           0.324363
trainer/Policy log std Max                           0.0132775
trainer/Policy log std Min                          -2.20981
trainer/Alpha                                        0.0815775
trainer/Alpha Loss                                   0.65125
exploration/num steps total                     119000
exploration/num paths total                        119
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.86741
exploration/Rewards Std                              1.10468
exploration/Rewards Max                              6.07188
exploration/Rewards Min                             -1.03703
exploration/Returns Mean                          3867.41
exploration/Returns Std                              0
exploration/Returns Max                           3867.41
exploration/Returns Min                           3867.41
exploration/Actions Mean                            -0.0121277
exploration/Actions Std                              0.819451
exploration/Actions Max                              0.999927
exploration/Actions Min                             -0.999929
exploration/Num Paths                                1
exploration/Average Returns                       3867.41
exploration/env_infos/final/reward_run Mean          3.27255
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.27255
exploration/env_infos/final/reward_run Min           3.27255
exploration/env_infos/initial/reward_run Mean        0.244043
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.244043
exploration/env_infos/initial/reward_run Min         0.244043
exploration/env_infos/reward_run Mean                4.2704
exploration/env_infos/reward_run Std                 1.09353
exploration/env_infos/reward_run Max                 6.48681
exploration/env_infos/reward_run Min                -0.530083
exploration/env_infos/final/reward_ctrl Mean        -0.479653
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.479653
exploration/env_infos/final/reward_ctrl Min         -0.479653
exploration/env_infos/initial/reward_ctrl Mean      -0.0819648
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0819648
exploration/env_infos/initial/reward_ctrl Min       -0.0819648
exploration/env_infos/reward_ctrl Mean              -0.402989
exploration/env_infos/reward_ctrl Std                0.0913397
exploration/env_infos/reward_ctrl Max               -0.0819648
exploration/env_infos/reward_ctrl Min               -0.586924
evaluation/num steps total                      590000
evaluation/num paths total                         590
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.04489
evaluation/Rewards Std                               1.06361
evaluation/Rewards Max                               6.54864
evaluation/Rewards Min                              -1.05476
evaluation/Returns Mean                           4044.89
evaluation/Returns Std                              60.7486
evaluation/Returns Max                            4151.79
evaluation/Returns Min                            3989.34
evaluation/Actions Mean                             -0.0149586
evaluation/Actions Std                               0.824675
evaluation/Actions Max                               0.999921
evaluation/Actions Min                              -0.999185
evaluation/Num Paths                                 5
evaluation/Average Returns                        4044.89
evaluation/env_infos/final/reward_run Mean           4.73563
evaluation/env_infos/final/reward_run Std            0.783708
evaluation/env_infos/final/reward_run Max            5.52991
evaluation/env_infos/final/reward_run Min            3.29314
evaluation/env_infos/initial/reward_run Mean         0.146191
evaluation/env_infos/initial/reward_run Std          0.138535
evaluation/env_infos/initial/reward_run Max          0.335615
evaluation/env_infos/initial/reward_run Min         -0.0918705
evaluation/env_infos/reward_run Mean                 4.45308
evaluation/env_infos/reward_run Std                  1.05693
evaluation/env_infos/reward_run Max                  7.01804
evaluation/env_infos/reward_run Min                 -0.517941
evaluation/env_infos/final/reward_ctrl Mean         -0.438124
evaluation/env_infos/final/reward_ctrl Std           0.0443932
evaluation/env_infos/final/reward_ctrl Max          -0.383353
evaluation/env_infos/final/reward_ctrl Min          -0.482719
evaluation/env_infos/initial/reward_ctrl Mean       -0.0313326
evaluation/env_infos/initial/reward_ctrl Std         0.0231744
evaluation/env_infos/initial/reward_ctrl Max        -0.00993651
evaluation/env_infos/initial/reward_ctrl Min        -0.0706911
evaluation/env_infos/reward_ctrl Mean               -0.408187
evaluation/env_infos/reward_ctrl Std                 0.0927859
evaluation/env_infos/reward_ctrl Max                -0.00993651
evaluation/env_infos/reward_ctrl Min                -0.588826
time/data storing (s)                                0.0131179
time/evaluation sampling (s)                         3.38441
time/exploration sampling (s)                        1.4909
time/logging (s)                                     0.0585026
time/saving (s)                                      0.0250338
time/training (s)                                   37.8109
time/epoch (s)                                      42.7828
time/total (s)                                    5004.69
Epoch                                              117
----------------------------------------------  ---------------
2020-07-08 22:30:13.401801 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 118 finished
----------------------------------------------  ----------------
replay_buffer/size                              120000
trainer/QF1 Loss                                    12.8514
trainer/QF2 Loss                                    16.1151
trainer/Policy Loss                               -207.823
trainer/Q1 Predictions Mean                        213.818
trainer/Q1 Predictions Std                          91.7825
trainer/Q1 Predictions Max                         299.92
trainer/Q1 Predictions Min                           3.92411
trainer/Q2 Predictions Mean                        213.94
trainer/Q2 Predictions Std                          91.7337
trainer/Q2 Predictions Max                         302.701
trainer/Q2 Predictions Min                           4.56356
trainer/Q Targets Mean                             213.976
trainer/Q Targets Std                               92.2336
trainer/Q Targets Max                              300.222
trainer/Q Targets Min                                2.25954
trainer/Log Pis Mean                                 6.16894
trainer/Log Pis Std                                  5.3046
trainer/Log Pis Max                                 26.1266
trainer/Log Pis Min                                 -6.6459
trainer/Policy mu Mean                               0.0150227
trainer/Policy mu Std                                1.51762
trainer/Policy mu Max                                4.53644
trainer/Policy mu Min                               -3.45499
trainer/Policy log std Mean                         -0.850576
trainer/Policy log std Std                           0.349652
trainer/Policy log std Max                          -0.0938557
trainer/Policy log std Min                          -2.66822
trainer/Alpha                                        0.0822536
trainer/Alpha Loss                                   0.422018
exploration/num steps total                     120000
exploration/num paths total                        120
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.96946
exploration/Rewards Std                              0.990903
exploration/Rewards Max                              5.95323
exploration/Rewards Min                             -0.744047
exploration/Returns Mean                          3969.46
exploration/Returns Std                              0
exploration/Returns Max                           3969.46
exploration/Returns Min                           3969.46
exploration/Actions Mean                            -0.0303719
exploration/Actions Std                              0.80519
exploration/Actions Max                              0.999919
exploration/Actions Min                             -0.999796
exploration/Num Paths                                1
exploration/Average Returns                       3969.46
exploration/env_infos/final/reward_run Mean          3.15925
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.15925
exploration/env_infos/final/reward_run Min           3.15925
exploration/env_infos/initial/reward_run Mean        0.218931
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.218931
exploration/env_infos/initial/reward_run Min         0.218931
exploration/env_infos/reward_run Mean                4.35901
exploration/env_infos/reward_run Std                 0.981717
exploration/env_infos/reward_run Max                 6.35195
exploration/env_infos/reward_run Min                -0.302437
exploration/env_infos/final/reward_ctrl Mean        -0.398973
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.398973
exploration/env_infos/final/reward_ctrl Min         -0.398973
exploration/env_infos/initial/reward_ctrl Mean      -0.0354161
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0354161
exploration/env_infos/initial/reward_ctrl Min       -0.0354161
exploration/env_infos/reward_ctrl Mean              -0.389552
exploration/env_infos/reward_ctrl Std                0.0913349
exploration/env_infos/reward_ctrl Max               -0.0354161
exploration/env_infos/reward_ctrl Min               -0.583844
evaluation/num steps total                      595000
evaluation/num paths total                         595
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.98677
evaluation/Rewards Std                               1.03885
evaluation/Rewards Max                               6.52957
evaluation/Rewards Min                              -0.89382
evaluation/Returns Mean                           3986.77
evaluation/Returns Std                              28.8497
evaluation/Returns Max                            4036.52
evaluation/Returns Min                            3946.83
evaluation/Actions Mean                             -0.0289541
evaluation/Actions Std                               0.809853
evaluation/Actions Max                               0.999607
evaluation/Actions Min                              -0.999589
evaluation/Num Paths                                 5
evaluation/Average Returns                        3986.77
evaluation/env_infos/final/reward_run Mean           4.28695
evaluation/env_infos/final/reward_run Std            0.956
evaluation/env_infos/final/reward_run Max            5.14536
evaluation/env_infos/final/reward_run Min            2.55511
evaluation/env_infos/initial/reward_run Mean        -0.0925904
evaluation/env_infos/initial/reward_run Std          0.106328
evaluation/env_infos/initial/reward_run Max          0.000586059
evaluation/env_infos/initial/reward_run Min         -0.268606
evaluation/env_infos/reward_run Mean                 4.38079
evaluation/env_infos/reward_run Std                  1.03045
evaluation/env_infos/reward_run Max                  6.86178
evaluation/env_infos/reward_run Min                 -0.481593
evaluation/env_infos/final/reward_ctrl Mean         -0.36808
evaluation/env_infos/final/reward_ctrl Std           0.0655064
evaluation/env_infos/final/reward_ctrl Max          -0.28001
evaluation/env_infos/final/reward_ctrl Min          -0.472121
evaluation/env_infos/initial/reward_ctrl Mean       -0.0369379
evaluation/env_infos/initial/reward_ctrl Std         0.0158397
evaluation/env_infos/initial/reward_ctrl Max        -0.0154943
evaluation/env_infos/initial/reward_ctrl Min        -0.0595104
evaluation/env_infos/reward_ctrl Mean               -0.39402
evaluation/env_infos/reward_ctrl Std                 0.0900332
evaluation/env_infos/reward_ctrl Max                -0.0154943
evaluation/env_infos/reward_ctrl Min                -0.593504
time/data storing (s)                                0.00677534
time/evaluation sampling (s)                         3.10869
time/exploration sampling (s)                        0.65281
time/logging (s)                                     0.0398723
time/saving (s)                                      0.01684
time/training (s)                                   34.0514
time/epoch (s)                                      37.8763
time/total (s)                                    5042.59
Epoch                                              118
----------------------------------------------  ----------------
2020-07-08 22:30:54.424651 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 119 finished
----------------------------------------------  ---------------
replay_buffer/size                              121000
trainer/QF1 Loss                                     9.80312
trainer/QF2 Loss                                     9.9481
trainer/Policy Loss                               -193.851
trainer/Q1 Predictions Mean                        200.002
trainer/Q1 Predictions Std                         103.132
trainer/Q1 Predictions Max                         296.22
trainer/Q1 Predictions Min                           6.7958
trainer/Q2 Predictions Mean                        199.512
trainer/Q2 Predictions Std                         103.051
trainer/Q2 Predictions Max                         295.671
trainer/Q2 Predictions Min                           7.32761
trainer/Q Targets Mean                             199.657
trainer/Q Targets Std                              103.077
trainer/Q Targets Max                              296.579
trainer/Q Targets Min                                7.16818
trainer/Log Pis Mean                                 6.00061
trainer/Log Pis Std                                  5.45598
trainer/Log Pis Max                                 26.3962
trainer/Log Pis Min                                 -5.72136
trainer/Policy mu Mean                               0.00310027
trainer/Policy mu Std                                1.51943
trainer/Policy mu Max                                4.54104
trainer/Policy mu Min                               -3.95721
trainer/Policy log std Mean                         -0.805381
trainer/Policy log std Std                           0.345124
trainer/Policy log std Max                          -0.104036
trainer/Policy log std Min                          -2.20449
trainer/Alpha                                        0.0819498
trainer/Alpha Loss                                   0.00151886
exploration/num steps total                     121000
exploration/num paths total                        121
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.0773
exploration/Rewards Std                              0.982486
exploration/Rewards Max                              6.20426
exploration/Rewards Min                             -0.616054
exploration/Returns Mean                          4077.3
exploration/Returns Std                              0
exploration/Returns Max                           4077.3
exploration/Returns Min                           4077.3
exploration/Actions Mean                            -0.00576133
exploration/Actions Std                              0.794415
exploration/Actions Max                              0.999815
exploration/Actions Min                             -0.999807
exploration/Num Paths                                1
exploration/Average Returns                       4077.3
exploration/env_infos/final/reward_run Mean          5.97928
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.97928
exploration/env_infos/final/reward_run Min           5.97928
exploration/env_infos/initial/reward_run Mean        0.079116
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.079116
exploration/env_infos/initial/reward_run Min         0.079116
exploration/env_infos/reward_run Mean                4.45598
exploration/env_infos/reward_run Std                 0.96876
exploration/env_infos/reward_run Max                 6.66256
exploration/env_infos/reward_run Min                -0.125409
exploration/env_infos/final/reward_ctrl Mean        -0.336043
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.336043
exploration/env_infos/final/reward_ctrl Min         -0.336043
exploration/env_infos/initial/reward_ctrl Mean      -0.0695188
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0695188
exploration/env_infos/initial/reward_ctrl Min       -0.0695188
exploration/env_infos/reward_ctrl Mean              -0.378677
exploration/env_infos/reward_ctrl Std                0.0937805
exploration/env_infos/reward_ctrl Max               -0.0695188
exploration/env_infos/reward_ctrl Min               -0.579882
evaluation/num steps total                      600000
evaluation/num paths total                         600
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.0701
evaluation/Rewards Std                               1.01924
evaluation/Rewards Max                               6.32215
evaluation/Rewards Min                              -1.03263
evaluation/Returns Mean                           4070.1
evaluation/Returns Std                              65.5013
evaluation/Returns Max                            4138.74
evaluation/Returns Min                            3963.1
evaluation/Actions Mean                             -0.00865015
evaluation/Actions Std                               0.802003
evaluation/Actions Max                               0.999919
evaluation/Actions Min                              -0.99965
evaluation/Num Paths                                 5
evaluation/Average Returns                        4070.1
evaluation/env_infos/final/reward_run Mean           4.54975
evaluation/env_infos/final/reward_run Std            0.720587
evaluation/env_infos/final/reward_run Max            5.28847
evaluation/env_infos/final/reward_run Min            3.18571
evaluation/env_infos/initial/reward_run Mean         0.054176
evaluation/env_infos/initial/reward_run Std          0.122398
evaluation/env_infos/initial/reward_run Max          0.217522
evaluation/env_infos/initial/reward_run Min         -0.151396
evaluation/env_infos/reward_run Mean                 4.45607
evaluation/env_infos/reward_run Std                  1.00672
evaluation/env_infos/reward_run Max                  6.78025
evaluation/env_infos/reward_run Min                 -0.546689
evaluation/env_infos/final/reward_ctrl Mean         -0.333828
evaluation/env_infos/final/reward_ctrl Std           0.0596913
evaluation/env_infos/final/reward_ctrl Max          -0.225549
evaluation/env_infos/final/reward_ctrl Min          -0.401626
evaluation/env_infos/initial/reward_ctrl Mean       -0.0309091
evaluation/env_infos/initial/reward_ctrl Std         0.0198833
evaluation/env_infos/initial/reward_ctrl Max        -0.006037
evaluation/env_infos/initial/reward_ctrl Min        -0.0597317
evaluation/env_infos/reward_ctrl Mean               -0.38597
evaluation/env_infos/reward_ctrl Std                 0.0918778
evaluation/env_infos/reward_ctrl Max                -0.006037
evaluation/env_infos/reward_ctrl Min                -0.583114
time/data storing (s)                                0.00666803
time/evaluation sampling (s)                         2.57869
time/exploration sampling (s)                        0.656395
time/logging (s)                                     0.0430692
time/saving (s)                                      0.0174339
time/training (s)                                   37.5629
time/epoch (s)                                      40.8652
time/total (s)                                    5083.61
Epoch                                              119
----------------------------------------------  ---------------
2020-07-08 22:31:34.779024 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 120 finished
----------------------------------------------  ---------------
replay_buffer/size                              122000
trainer/QF1 Loss                                     9.41484
trainer/QF2 Loss                                    12.7976
trainer/Policy Loss                               -204.733
trainer/Q1 Predictions Mean                        210.419
trainer/Q1 Predictions Std                          95.3465
trainer/Q1 Predictions Max                         294.586
trainer/Q1 Predictions Min                           8.87782
trainer/Q2 Predictions Mean                        210.122
trainer/Q2 Predictions Std                          95.2587
trainer/Q2 Predictions Max                         296.573
trainer/Q2 Predictions Min                           8.96214
trainer/Q Targets Mean                             209.832
trainer/Q Targets Std                               95.1708
trainer/Q Targets Max                              296.562
trainer/Q Targets Min                                8.78976
trainer/Log Pis Mean                                 6.04153
trainer/Log Pis Std                                  5.20928
trainer/Log Pis Max                                 23.2936
trainer/Log Pis Min                                 -5.50485
trainer/Policy mu Mean                               0.0160254
trainer/Policy mu Std                                1.50571
trainer/Policy mu Max                                4.8154
trainer/Policy mu Min                               -3.64782
trainer/Policy log std Mean                         -0.815814
trainer/Policy log std Std                           0.342173
trainer/Policy log std Max                           0.0260582
trainer/Policy log std Min                          -2.2332
trainer/Alpha                                        0.0826508
trainer/Alpha Loss                                   0.103526
exploration/num steps total                     122000
exploration/num paths total                        122
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.91406
exploration/Rewards Std                              1.0829
exploration/Rewards Max                              6.15655
exploration/Rewards Min                             -1.32293
exploration/Returns Mean                          3914.06
exploration/Returns Std                              0
exploration/Returns Max                           3914.06
exploration/Returns Min                           3914.06
exploration/Actions Mean                            -0.0319091
exploration/Actions Std                              0.810794
exploration/Actions Max                              0.999947
exploration/Actions Min                             -0.999827
exploration/Num Paths                                1
exploration/Average Returns                       3914.06
exploration/env_infos/final/reward_run Mean          4.49068
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.49068
exploration/env_infos/final/reward_run Min           4.49068
exploration/env_infos/initial/reward_run Mean       -0.206475
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.206475
exploration/env_infos/initial/reward_run Min        -0.206475
exploration/env_infos/reward_run Mean                4.30911
exploration/env_infos/reward_run Std                 1.07028
exploration/env_infos/reward_run Max                 6.56309
exploration/env_infos/reward_run Min                -0.786759
exploration/env_infos/final/reward_ctrl Mean        -0.480858
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.480858
exploration/env_infos/final/reward_ctrl Min         -0.480858
exploration/env_infos/initial/reward_ctrl Mean      -0.0277883
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0277883
exploration/env_infos/initial/reward_ctrl Min       -0.0277883
exploration/env_infos/reward_ctrl Mean              -0.395043
exploration/env_infos/reward_ctrl Std                0.0943038
exploration/env_infos/reward_ctrl Max               -0.0277883
exploration/env_infos/reward_ctrl Min               -0.593329
evaluation/num steps total                      605000
evaluation/num paths total                         605
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.11401
evaluation/Rewards Std                               1.05547
evaluation/Rewards Max                               6.36536
evaluation/Rewards Min                              -0.8999
evaluation/Returns Mean                           4114.01
evaluation/Returns Std                              95.346
evaluation/Returns Max                            4245.56
evaluation/Returns Min                            3986.47
evaluation/Actions Mean                             -0.0305789
evaluation/Actions Std                               0.823991
evaluation/Actions Max                               0.999969
evaluation/Actions Min                              -0.999267
evaluation/Num Paths                                 5
evaluation/Average Returns                        4114.01
evaluation/env_infos/final/reward_run Mean           5.13846
evaluation/env_infos/final/reward_run Std            0.843766
evaluation/env_infos/final/reward_run Max            6.19114
evaluation/env_infos/final/reward_run Min            3.73389
evaluation/env_infos/initial/reward_run Mean         0.0542012
evaluation/env_infos/initial/reward_run Std          0.0914082
evaluation/env_infos/initial/reward_run Max          0.19004
evaluation/env_infos/initial/reward_run Min         -0.0660795
evaluation/env_infos/reward_run Mean                 4.52195
evaluation/env_infos/reward_run Std                  1.04177
evaluation/env_infos/reward_run Max                  6.64759
evaluation/env_infos/reward_run Min                 -0.40818
evaluation/env_infos/final/reward_ctrl Mean         -0.48189
evaluation/env_infos/final/reward_ctrl Std           0.050566
evaluation/env_infos/final/reward_ctrl Max          -0.405702
evaluation/env_infos/final/reward_ctrl Min          -0.54671
evaluation/env_infos/initial/reward_ctrl Mean       -0.0358372
evaluation/env_infos/initial/reward_ctrl Std         0.0178824
evaluation/env_infos/initial/reward_ctrl Max        -0.0145314
evaluation/env_infos/initial/reward_ctrl Min        -0.067623
evaluation/env_infos/reward_ctrl Mean               -0.407938
evaluation/env_infos/reward_ctrl Std                 0.0905178
evaluation/env_infos/reward_ctrl Max                -0.0145314
evaluation/env_infos/reward_ctrl Min                -0.585011
time/data storing (s)                                0.00666793
time/evaluation sampling (s)                         2.87407
time/exploration sampling (s)                        0.648871
time/logging (s)                                     0.061045
time/saving (s)                                      0.0177896
time/training (s)                                   36.7236
time/epoch (s)                                      40.332
time/total (s)                                    5123.98
Epoch                                              120
----------------------------------------------  ---------------
2020-07-08 22:32:17.665300 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 121 finished
----------------------------------------------  ---------------
replay_buffer/size                              123000
trainer/QF1 Loss                                    12.393
trainer/QF2 Loss                                    10.4221
trainer/Policy Loss                               -196.794
trainer/Q1 Predictions Mean                        202.094
trainer/Q1 Predictions Std                          98.0914
trainer/Q1 Predictions Max                         291.639
trainer/Q1 Predictions Min                           7.97604
trainer/Q2 Predictions Mean                        201.718
trainer/Q2 Predictions Std                          97.9958
trainer/Q2 Predictions Max                         292.697
trainer/Q2 Predictions Min                           8.20686
trainer/Q Targets Mean                             202.15
trainer/Q Targets Std                               98.2996
trainer/Q Targets Max                              291.82
trainer/Q Targets Min                                7.44576
trainer/Log Pis Mean                                 5.68834
trainer/Log Pis Std                                  5.04243
trainer/Log Pis Max                                 35.3548
trainer/Log Pis Min                                 -5.38006
trainer/Policy mu Mean                               0.0675594
trainer/Policy mu Std                                1.49761
trainer/Policy mu Max                                5.7534
trainer/Policy mu Min                               -4.65492
trainer/Policy log std Mean                         -0.794907
trainer/Policy log std Std                           0.339995
trainer/Policy log std Max                           0.0532007
trainer/Policy log std Min                          -2.13291
trainer/Alpha                                        0.0835568
trainer/Alpha Loss                                  -0.773612
exploration/num steps total                     123000
exploration/num paths total                        123
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.946
exploration/Rewards Std                              1.03287
exploration/Rewards Max                              6.24961
exploration/Rewards Min                             -0.699732
exploration/Returns Mean                          3946
exploration/Returns Std                              0
exploration/Returns Max                           3946
exploration/Returns Min                           3946
exploration/Actions Mean                             0.0185862
exploration/Actions Std                              0.810855
exploration/Actions Max                              0.999772
exploration/Actions Min                             -0.9995
exploration/Num Paths                                1
exploration/Average Returns                       3946
exploration/env_infos/final/reward_run Mean          3.31517
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.31517
exploration/env_infos/final/reward_run Min           3.31517
exploration/env_infos/initial/reward_run Mean        0.1628
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.1628
exploration/env_infos/initial/reward_run Min         0.1628
exploration/env_infos/reward_run Mean                4.3407
exploration/env_infos/reward_run Std                 1.01829
exploration/env_infos/reward_run Max                 6.5558
exploration/env_infos/reward_run Min                -0.194804
exploration/env_infos/final/reward_ctrl Mean        -0.271333
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.271333
exploration/env_infos/final/reward_ctrl Min         -0.271333
exploration/env_infos/initial/reward_ctrl Mean      -0.0915149
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0915149
exploration/env_infos/initial/reward_ctrl Min       -0.0915149
exploration/env_infos/reward_ctrl Mean              -0.394699
exploration/env_infos/reward_ctrl Std                0.0900263
exploration/env_infos/reward_ctrl Max               -0.0842603
exploration/env_infos/reward_ctrl Min               -0.588012
evaluation/num steps total                      610000
evaluation/num paths total                         610
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.15982
evaluation/Rewards Std                               1.05236
evaluation/Rewards Max                               6.36462
evaluation/Rewards Min                              -0.805342
evaluation/Returns Mean                           4159.82
evaluation/Returns Std                              49.3484
evaluation/Returns Max                            4215.32
evaluation/Returns Min                            4089.74
evaluation/Actions Mean                              0.00600404
evaluation/Actions Std                               0.82117
evaluation/Actions Max                               0.999334
evaluation/Actions Min                              -0.999723
evaluation/Num Paths                                 5
evaluation/Average Returns                        4159.82
evaluation/env_infos/final/reward_run Mean           4.32655
evaluation/env_infos/final/reward_run Std            0.551388
evaluation/env_infos/final/reward_run Max            5.20953
evaluation/env_infos/final/reward_run Min            3.67812
evaluation/env_infos/initial/reward_run Mean         0.356173
evaluation/env_infos/initial/reward_run Std          0.285663
evaluation/env_infos/initial/reward_run Max          0.78366
evaluation/env_infos/initial/reward_run Min         -0.0680673
evaluation/env_infos/reward_run Mean                 4.56444
evaluation/env_infos/reward_run Std                  1.04287
evaluation/env_infos/reward_run Max                  6.69607
evaluation/env_infos/reward_run Min                 -0.241371
evaluation/env_infos/final/reward_ctrl Mean         -0.415401
evaluation/env_infos/final/reward_ctrl Std           0.100953
evaluation/env_infos/final/reward_ctrl Max          -0.266782
evaluation/env_infos/final/reward_ctrl Min          -0.56593
evaluation/env_infos/initial/reward_ctrl Mean       -0.0499186
evaluation/env_infos/initial/reward_ctrl Std         0.0299238
evaluation/env_infos/initial/reward_ctrl Max        -0.0100119
evaluation/env_infos/initial/reward_ctrl Min        -0.0856031
evaluation/env_infos/reward_ctrl Mean               -0.404614
evaluation/env_infos/reward_ctrl Std                 0.0901825
evaluation/env_infos/reward_ctrl Max                -0.0100119
evaluation/env_infos/reward_ctrl Min                -0.58458
time/data storing (s)                                0.00759416
time/evaluation sampling (s)                         2.99647
time/exploration sampling (s)                        0.860542
time/logging (s)                                     0.0541119
time/saving (s)                                      0.030083
time/training (s)                                   38.9064
time/epoch (s)                                      42.8552
time/total (s)                                    5166.85
Epoch                                              121
----------------------------------------------  ---------------
2020-07-08 22:33:03.145752 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 122 finished
----------------------------------------------  ---------------
replay_buffer/size                              124000
trainer/QF1 Loss                                     8.10121
trainer/QF2 Loss                                     9.7028
trainer/Policy Loss                               -209.969
trainer/Q1 Predictions Mean                        216.076
trainer/Q1 Predictions Std                          93.7035
trainer/Q1 Predictions Max                         297.23
trainer/Q1 Predictions Min                           6.01163
trainer/Q2 Predictions Mean                        215.975
trainer/Q2 Predictions Std                          93.602
trainer/Q2 Predictions Max                         295.779
trainer/Q2 Predictions Min                           7.2534
trainer/Q Targets Mean                             216.466
trainer/Q Targets Std                               93.9164
trainer/Q Targets Max                              300.744
trainer/Q Targets Min                                7.16923
trainer/Log Pis Mean                                 6.17382
trainer/Log Pis Std                                  5.17133
trainer/Log Pis Max                                 22.6366
trainer/Log Pis Min                                 -5.26908
trainer/Policy mu Mean                               0.14326
trainer/Policy mu Std                                1.52675
trainer/Policy mu Max                                4.42055
trainer/Policy mu Min                               -4.74279
trainer/Policy log std Mean                         -0.815338
trainer/Policy log std Std                           0.353553
trainer/Policy log std Max                           0.162236
trainer/Policy log std Min                          -2.40186
trainer/Alpha                                        0.0832415
trainer/Alpha Loss                                   0.432124
exploration/num steps total                     124000
exploration/num paths total                        124
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.09119
exploration/Rewards Std                              1.06573
exploration/Rewards Max                              6.20942
exploration/Rewards Min                             -0.773711
exploration/Returns Mean                          4091.19
exploration/Returns Std                              0
exploration/Returns Max                           4091.19
exploration/Returns Min                           4091.19
exploration/Actions Mean                             0.00489542
exploration/Actions Std                              0.81816
exploration/Actions Max                              0.99969
exploration/Actions Min                             -0.999902
exploration/Num Paths                                1
exploration/Average Returns                       4091.19
exploration/env_infos/final/reward_run Mean          5.69004
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.69004
exploration/env_infos/final/reward_run Min           5.69004
exploration/env_infos/initial/reward_run Mean       -0.274518
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.274518
exploration/env_infos/initial/reward_run Min        -0.274518
exploration/env_infos/reward_run Mean                4.49284
exploration/env_infos/reward_run Std                 1.05792
exploration/env_infos/reward_run Max                 6.45683
exploration/env_infos/reward_run Min                -0.478852
exploration/env_infos/final/reward_ctrl Mean        -0.325891
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.325891
exploration/env_infos/final/reward_ctrl Min         -0.325891
exploration/env_infos/initial/reward_ctrl Mean      -0.0276758
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0276758
exploration/env_infos/initial/reward_ctrl Min       -0.0276758
exploration/env_infos/reward_ctrl Mean              -0.401645
exploration/env_infos/reward_ctrl Std                0.0862488
exploration/env_infos/reward_ctrl Max               -0.0276758
exploration/env_infos/reward_ctrl Min               -0.577484
evaluation/num steps total                      615000
evaluation/num paths total                         615
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.20335
evaluation/Rewards Std                               1.07711
evaluation/Rewards Max                               6.41125
evaluation/Rewards Min                              -0.790804
evaluation/Returns Mean                           4203.35
evaluation/Returns Std                              60.6941
evaluation/Returns Max                            4291.78
evaluation/Returns Min                            4112
evaluation/Actions Mean                              0.00212836
evaluation/Actions Std                               0.829221
evaluation/Actions Max                               0.999545
evaluation/Actions Min                              -0.999141
evaluation/Num Paths                                 5
evaluation/Average Returns                        4203.35
evaluation/env_infos/final/reward_run Mean           4.41733
evaluation/env_infos/final/reward_run Std            0.837585
evaluation/env_infos/final/reward_run Max            5.64046
evaluation/env_infos/final/reward_run Min            3.52569
evaluation/env_infos/initial/reward_run Mean         0.107627
evaluation/env_infos/initial/reward_run Std          0.090334
evaluation/env_infos/initial/reward_run Max          0.22708
evaluation/env_infos/initial/reward_run Min         -0.00195565
evaluation/env_infos/reward_run Mean                 4.61592
evaluation/env_infos/reward_run Std                  1.06574
evaluation/env_infos/reward_run Max                  6.75127
evaluation/env_infos/reward_run Min                 -0.276315
evaluation/env_infos/final/reward_ctrl Mean         -0.378003
evaluation/env_infos/final/reward_ctrl Std           0.0740945
evaluation/env_infos/final/reward_ctrl Max          -0.258141
evaluation/env_infos/final/reward_ctrl Min          -0.491532
evaluation/env_infos/initial/reward_ctrl Mean       -0.0241727
evaluation/env_infos/initial/reward_ctrl Std         0.00729783
evaluation/env_infos/initial/reward_ctrl Max        -0.0163953
evaluation/env_infos/initial/reward_ctrl Min        -0.0377273
evaluation/env_infos/reward_ctrl Mean               -0.412568
evaluation/env_infos/reward_ctrl Std                 0.0860894
evaluation/env_infos/reward_ctrl Max                -0.0163953
evaluation/env_infos/reward_ctrl Min                -0.590435
time/data storing (s)                                0.00674043
time/evaluation sampling (s)                         3.87782
time/exploration sampling (s)                        0.766671
time/logging (s)                                     0.0449417
time/saving (s)                                      0.0192419
time/training (s)                                   40.1705
time/epoch (s)                                      44.886
time/total (s)                                    5211.77
Epoch                                              122
----------------------------------------------  ---------------
2020-07-08 22:33:38.419854 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 123 finished
----------------------------------------------  ---------------
replay_buffer/size                              125000
trainer/QF1 Loss                                     9.31513
trainer/QF2 Loss                                     8.30807
trainer/Policy Loss                               -208.558
trainer/Q1 Predictions Mean                        215.379
trainer/Q1 Predictions Std                          94.5347
trainer/Q1 Predictions Max                         301.416
trainer/Q1 Predictions Min                           8.06324
trainer/Q2 Predictions Mean                        215.012
trainer/Q2 Predictions Std                          94.3771
trainer/Q2 Predictions Max                         299.149
trainer/Q2 Predictions Min                           7.87719
trainer/Q Targets Mean                             214.939
trainer/Q Targets Std                               94.2141
trainer/Q Targets Max                              295.69
trainer/Q Targets Min                                8.59527
trainer/Log Pis Mean                                 6.65782
trainer/Log Pis Std                                  5.48293
trainer/Log Pis Max                                 22.9846
trainer/Log Pis Min                                 -4.74465
trainer/Policy mu Mean                               0.00181695
trainer/Policy mu Std                                1.58157
trainer/Policy mu Max                                4.12627
trainer/Policy mu Min                               -4.31822
trainer/Policy log std Mean                         -0.80603
trainer/Policy log std Std                           0.328419
trainer/Policy log std Max                           0.0190784
trainer/Policy log std Min                          -2.23634
trainer/Alpha                                        0.0840544
trainer/Alpha Loss                                   1.62909
exploration/num steps total                     125000
exploration/num paths total                        125
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.78321
exploration/Rewards Std                              1.03292
exploration/Rewards Max                              5.99237
exploration/Rewards Min                             -1.10853
exploration/Returns Mean                          3783.21
exploration/Returns Std                              0
exploration/Returns Max                           3783.21
exploration/Returns Min                           3783.21
exploration/Actions Mean                             0.00341475
exploration/Actions Std                              0.815937
exploration/Actions Max                              0.999728
exploration/Actions Min                             -0.999879
exploration/Num Paths                                1
exploration/Average Returns                       3783.21
exploration/env_infos/final/reward_run Mean          4.97026
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.97026
exploration/env_infos/final/reward_run Min           4.97026
exploration/env_infos/initial/reward_run Mean        0.0596253
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0596253
exploration/env_infos/initial/reward_run Min         0.0596253
exploration/env_infos/reward_run Mean                4.18267
exploration/env_infos/reward_run Std                 1.02072
exploration/env_infos/reward_run Max                 6.20092
exploration/env_infos/reward_run Min                -0.662149
exploration/env_infos/final/reward_ctrl Mean        -0.411562
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.411562
exploration/env_infos/final/reward_ctrl Min         -0.411562
exploration/env_infos/initial/reward_ctrl Mean      -0.0660964
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0660964
exploration/env_infos/initial/reward_ctrl Min       -0.0660964
exploration/env_infos/reward_ctrl Mean              -0.399459
exploration/env_infos/reward_ctrl Std                0.0945863
exploration/env_infos/reward_ctrl Max               -0.0660964
exploration/env_infos/reward_ctrl Min               -0.588461
evaluation/num steps total                      620000
evaluation/num paths total                         620
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.10805
evaluation/Rewards Std                               1.08045
evaluation/Rewards Max                               6.50733
evaluation/Rewards Min                              -0.881409
evaluation/Returns Mean                           4108.05
evaluation/Returns Std                             134.084
evaluation/Returns Max                            4335.12
evaluation/Returns Min                            3940.34
evaluation/Actions Mean                             -0.00307592
evaluation/Actions Std                               0.829843
evaluation/Actions Max                               0.999704
evaluation/Actions Min                              -0.999541
evaluation/Num Paths                                 5
evaluation/Average Returns                        4108.05
evaluation/env_infos/final/reward_run Mean           5.47706
evaluation/env_infos/final/reward_run Std            1.32078
evaluation/env_infos/final/reward_run Max            6.74572
evaluation/env_infos/final/reward_run Min            3.33531
evaluation/env_infos/initial/reward_run Mean         0.189553
evaluation/env_infos/initial/reward_run Std          0.0525367
evaluation/env_infos/initial/reward_run Max          0.256569
evaluation/env_infos/initial/reward_run Min          0.108143
evaluation/env_infos/reward_run Mean                 4.52124
evaluation/env_infos/reward_run Std                  1.06905
evaluation/env_infos/reward_run Max                  6.90399
evaluation/env_infos/reward_run Min                 -0.336229
evaluation/env_infos/final/reward_ctrl Mean         -0.399598
evaluation/env_infos/final/reward_ctrl Std           0.0674216
evaluation/env_infos/final/reward_ctrl Max          -0.329677
evaluation/env_infos/final/reward_ctrl Min          -0.525701
evaluation/env_infos/initial/reward_ctrl Mean       -0.032262
evaluation/env_infos/initial/reward_ctrl Std         0.0132817
evaluation/env_infos/initial/reward_ctrl Max        -0.016711
evaluation/env_infos/initial/reward_ctrl Min        -0.0490252
evaluation/env_infos/reward_ctrl Mean               -0.41319
evaluation/env_infos/reward_ctrl Std                 0.0911129
evaluation/env_infos/reward_ctrl Max                -0.016711
evaluation/env_infos/reward_ctrl Min                -0.589096
time/data storing (s)                                0.00787469
time/evaluation sampling (s)                         2.94871
time/exploration sampling (s)                        0.715755
time/logging (s)                                     0.0419052
time/saving (s)                                      0.01636
time/training (s)                                   31.521
time/epoch (s)                                      35.2516
time/total (s)                                    5247.03
Epoch                                              123
----------------------------------------------  ---------------
2020-07-08 22:34:13.248036 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 124 finished
----------------------------------------------  ---------------
replay_buffer/size                              126000
trainer/QF1 Loss                                     7.42392
trainer/QF2 Loss                                     8.42546
trainer/Policy Loss                               -210.253
trainer/Q1 Predictions Mean                        216.191
trainer/Q1 Predictions Std                          92.4071
trainer/Q1 Predictions Max                         295.383
trainer/Q1 Predictions Min                           6.09393
trainer/Q2 Predictions Mean                        216.346
trainer/Q2 Predictions Std                          92.4563
trainer/Q2 Predictions Max                         299.51
trainer/Q2 Predictions Min                           5.94902
trainer/Q Targets Mean                             215.885
trainer/Q Targets Std                               92.3552
trainer/Q Targets Max                              294.066
trainer/Q Targets Min                                4.4265
trainer/Log Pis Mean                                 6.27993
trainer/Log Pis Std                                  4.96352
trainer/Log Pis Max                                 21.1195
trainer/Log Pis Min                                 -5.46571
trainer/Policy mu Mean                               0.00102017
trainer/Policy mu Std                                1.55084
trainer/Policy mu Max                                4.02766
trainer/Policy mu Min                               -3.70685
trainer/Policy log std Mean                         -0.817259
trainer/Policy log std Std                           0.34287
trainer/Policy log std Max                          -0.0273346
trainer/Policy log std Min                          -2.31222
trainer/Alpha                                        0.0844005
trainer/Alpha Loss                                   0.692035
exploration/num steps total                     126000
exploration/num paths total                        126
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.87466
exploration/Rewards Std                              1.03501
exploration/Rewards Max                              5.98863
exploration/Rewards Min                             -0.864517
exploration/Returns Mean                          3874.66
exploration/Returns Std                              0
exploration/Returns Max                           3874.66
exploration/Returns Min                           3874.66
exploration/Actions Mean                            -0.00917423
exploration/Actions Std                              0.809853
exploration/Actions Max                              0.999722
exploration/Actions Min                             -0.999847
exploration/Num Paths                                1
exploration/Average Returns                       3874.66
exploration/env_infos/final/reward_run Mean          5.87516
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.87516
exploration/env_infos/final/reward_run Min           5.87516
exploration/env_infos/initial/reward_run Mean       -0.0253321
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0253321
exploration/env_infos/initial/reward_run Min        -0.0253321
exploration/env_infos/reward_run Mean                4.26823
exploration/env_infos/reward_run Std                 1.01999
exploration/env_infos/reward_run Max                 6.31671
exploration/env_infos/reward_run Min                -0.399987
exploration/env_infos/final/reward_ctrl Mean        -0.505121
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.505121
exploration/env_infos/final/reward_ctrl Min         -0.505121
exploration/env_infos/initial/reward_ctrl Mean      -0.03201
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.03201
exploration/env_infos/initial/reward_ctrl Min       -0.03201
exploration/env_infos/reward_ctrl Mean              -0.393568
exploration/env_infos/reward_ctrl Std                0.10041
exploration/env_infos/reward_ctrl Max               -0.03201
exploration/env_infos/reward_ctrl Min               -0.587086
evaluation/num steps total                      625000
evaluation/num paths total                         625
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.16406
evaluation/Rewards Std                               1.02869
evaluation/Rewards Max                               6.14111
evaluation/Rewards Min                              -0.748388
evaluation/Returns Mean                           4164.06
evaluation/Returns Std                              42.2105
evaluation/Returns Max                            4213.1
evaluation/Returns Min                            4107.28
evaluation/Actions Mean                             -0.0107996
evaluation/Actions Std                               0.823136
evaluation/Actions Max                               0.999135
evaluation/Actions Min                              -0.999328
evaluation/Num Paths                                 5
evaluation/Average Returns                        4164.06
evaluation/env_infos/final/reward_run Mean           4.39819
evaluation/env_infos/final/reward_run Std            0.326713
evaluation/env_infos/final/reward_run Max            4.82777
evaluation/env_infos/final/reward_run Min            3.91529
evaluation/env_infos/initial/reward_run Mean         0.0940903
evaluation/env_infos/initial/reward_run Std          0.131017
evaluation/env_infos/initial/reward_run Max          0.287565
evaluation/env_infos/initial/reward_run Min         -0.0730994
evaluation/env_infos/reward_run Mean                 4.57066
evaluation/env_infos/reward_run Std                  1.01382
evaluation/env_infos/reward_run Max                  6.55021
evaluation/env_infos/reward_run Min                 -0.187648
evaluation/env_infos/final/reward_ctrl Mean         -0.49733
evaluation/env_infos/final/reward_ctrl Std           0.068711
evaluation/env_infos/final/reward_ctrl Max          -0.382938
evaluation/env_infos/final/reward_ctrl Min          -0.555281
evaluation/env_infos/initial/reward_ctrl Mean       -0.0316302
evaluation/env_infos/initial/reward_ctrl Std         0.0150975
evaluation/env_infos/initial/reward_ctrl Max        -0.00585966
evaluation/env_infos/initial/reward_ctrl Min        -0.044325
evaluation/env_infos/reward_ctrl Mean               -0.406601
evaluation/env_infos/reward_ctrl Std                 0.0947725
evaluation/env_infos/reward_ctrl Max                -0.00585966
evaluation/env_infos/reward_ctrl Min                -0.58068
time/data storing (s)                                0.00665827
time/evaluation sampling (s)                         2.63528
time/exploration sampling (s)                        0.647392
time/logging (s)                                     0.0402708
time/saving (s)                                      0.0164498
time/training (s)                                   31.4632
time/epoch (s)                                      34.8093
time/total (s)                                    5281.85
Epoch                                              124
----------------------------------------------  ---------------
2020-07-08 22:34:47.946822 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 125 finished
----------------------------------------------  ---------------
replay_buffer/size                              127000
trainer/QF1 Loss                                    15.4525
trainer/QF2 Loss                                    15.6945
trainer/Policy Loss                               -207.227
trainer/Q1 Predictions Mean                        213.94
trainer/Q1 Predictions Std                          98.2975
trainer/Q1 Predictions Max                         298.732
trainer/Q1 Predictions Min                           8.00125
trainer/Q2 Predictions Mean                        213.757
trainer/Q2 Predictions Std                          98.2217
trainer/Q2 Predictions Max                         298.622
trainer/Q2 Predictions Min                           7.72703
trainer/Q Targets Mean                             213.605
trainer/Q Targets Std                               98.0996
trainer/Q Targets Max                              298.618
trainer/Q Targets Min                                7.79965
trainer/Log Pis Mean                                 6.88513
trainer/Log Pis Std                                  5.93553
trainer/Log Pis Max                                 28.5589
trainer/Log Pis Min                                 -6.15513
trainer/Policy mu Mean                               0.00634821
trainer/Policy mu Std                                1.62622
trainer/Policy mu Max                                3.77837
trainer/Policy mu Min                               -4.33453
trainer/Policy log std Mean                         -0.788141
trainer/Policy log std Std                           0.347374
trainer/Policy log std Max                           0.0367413
trainer/Policy log std Min                          -2.3535
trainer/Alpha                                        0.085148
trainer/Alpha Loss                                   2.18066
exploration/num steps total                     127000
exploration/num paths total                        127
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.96843
exploration/Rewards Std                              1.05392
exploration/Rewards Max                              5.99494
exploration/Rewards Min                             -0.578464
exploration/Returns Mean                          3968.43
exploration/Returns Std                              0
exploration/Returns Max                           3968.43
exploration/Returns Min                           3968.43
exploration/Actions Mean                            -0.0146667
exploration/Actions Std                              0.822922
exploration/Actions Max                              0.999855
exploration/Actions Min                             -0.999777
exploration/Num Paths                                1
exploration/Average Returns                       3968.43
exploration/env_infos/final/reward_run Mean          5.05009
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.05009
exploration/env_infos/final/reward_run Min           5.05009
exploration/env_infos/initial/reward_run Mean        0.401239
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.401239
exploration/env_infos/initial/reward_run Min         0.401239
exploration/env_infos/reward_run Mean                4.37488
exploration/env_infos/reward_run Std                 1.03684
exploration/env_infos/reward_run Max                 6.33754
exploration/env_infos/reward_run Min                -0.144727
exploration/env_infos/final/reward_ctrl Mean        -0.195768
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.195768
exploration/env_infos/final/reward_ctrl Min         -0.195768
exploration/env_infos/initial/reward_ctrl Mean      -0.0298366
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0298366
exploration/env_infos/initial/reward_ctrl Min       -0.0298366
exploration/env_infos/reward_ctrl Mean              -0.406449
exploration/env_infos/reward_ctrl Std                0.0936339
exploration/env_infos/reward_ctrl Max               -0.0298366
exploration/env_infos/reward_ctrl Min               -0.578792
evaluation/num steps total                      630000
evaluation/num paths total                         630
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.10399
evaluation/Rewards Std                               1.07844
evaluation/Rewards Max                               6.29013
evaluation/Rewards Min                              -0.954674
evaluation/Returns Mean                           4103.99
evaluation/Returns Std                             111.482
evaluation/Returns Max                            4261.78
evaluation/Returns Min                            3947.17
evaluation/Actions Mean                             -0.0184023
evaluation/Actions Std                               0.834099
evaluation/Actions Max                               0.999392
evaluation/Actions Min                              -0.999284
evaluation/Num Paths                                 5
evaluation/Average Returns                        4103.99
evaluation/env_infos/final/reward_run Mean           4.71102
evaluation/env_infos/final/reward_run Std            0.503279
evaluation/env_infos/final/reward_run Max            5.59001
evaluation/env_infos/final/reward_run Min            4.05557
evaluation/env_infos/initial/reward_run Mean         0.152478
evaluation/env_infos/initial/reward_run Std          0.0689994
evaluation/env_infos/initial/reward_run Max          0.224405
evaluation/env_infos/initial/reward_run Min          0.0307246
evaluation/env_infos/reward_run Mean                 4.52162
evaluation/env_infos/reward_run Std                  1.06286
evaluation/env_infos/reward_run Max                  6.7991
evaluation/env_infos/reward_run Min                 -0.850441
evaluation/env_infos/final/reward_ctrl Mean         -0.395666
evaluation/env_infos/final/reward_ctrl Std           0.0838848
evaluation/env_infos/final/reward_ctrl Max          -0.245105
evaluation/env_infos/final/reward_ctrl Min          -0.494678
evaluation/env_infos/initial/reward_ctrl Mean       -0.0474696
evaluation/env_infos/initial/reward_ctrl Std         0.0153341
evaluation/env_infos/initial/reward_ctrl Max        -0.0301955
evaluation/env_infos/initial/reward_ctrl Min        -0.071484
evaluation/env_infos/reward_ctrl Mean               -0.417636
evaluation/env_infos/reward_ctrl Std                 0.094723
evaluation/env_infos/reward_ctrl Max                -0.0301955
evaluation/env_infos/reward_ctrl Min                -0.589947
time/data storing (s)                                0.00668409
time/evaluation sampling (s)                         2.49153
time/exploration sampling (s)                        0.64989
time/logging (s)                                     0.040657
time/saving (s)                                      0.0155945
time/training (s)                                   31.4746
time/epoch (s)                                      34.6789
time/total (s)                                    5316.55
Epoch                                              125
----------------------------------------------  ---------------
2020-07-08 22:35:23.093344 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 126 finished
----------------------------------------------  ---------------
replay_buffer/size                              128000
trainer/QF1 Loss                                     5.63775
trainer/QF2 Loss                                     9.06406
trainer/Policy Loss                               -206.567
trainer/Q1 Predictions Mean                        212.085
trainer/Q1 Predictions Std                         102.597
trainer/Q1 Predictions Max                         299.243
trainer/Q1 Predictions Min                           7.29045
trainer/Q2 Predictions Mean                        212.64
trainer/Q2 Predictions Std                         102.756
trainer/Q2 Predictions Max                         299.43
trainer/Q2 Predictions Min                           7.42916
trainer/Q Targets Mean                             211.939
trainer/Q Targets Std                              102.576
trainer/Q Targets Max                              299.516
trainer/Q Targets Min                                7.06972
trainer/Log Pis Mean                                 5.90702
trainer/Log Pis Std                                  4.82449
trainer/Log Pis Max                                 17.6058
trainer/Log Pis Min                                 -5.296
trainer/Policy mu Mean                               0.0242908
trainer/Policy mu Std                                1.46117
trainer/Policy mu Max                                3.97244
trainer/Policy mu Min                               -3.62236
trainer/Policy log std Mean                         -0.826074
trainer/Policy log std Std                           0.358064
trainer/Policy log std Max                           0.110455
trainer/Policy log std Min                          -2.30305
trainer/Alpha                                        0.0852494
trainer/Alpha Loss                                  -0.228937
exploration/num steps total                     128000
exploration/num paths total                        128
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.09189
exploration/Rewards Std                              1.10894
exploration/Rewards Max                              6.70629
exploration/Rewards Min                             -0.933962
exploration/Returns Mean                          4091.89
exploration/Returns Std                              0
exploration/Returns Max                           4091.89
exploration/Returns Min                           4091.89
exploration/Actions Mean                            -0.00410532
exploration/Actions Std                              0.793294
exploration/Actions Max                              0.999959
exploration/Actions Min                             -0.999631
exploration/Num Paths                                1
exploration/Average Returns                       4091.89
exploration/env_infos/final/reward_run Mean          6.38329
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.38329
exploration/env_infos/final/reward_run Min           6.38329
exploration/env_infos/initial/reward_run Mean       -0.0606783
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0606783
exploration/env_infos/initial/reward_run Min        -0.0606783
exploration/env_infos/reward_run Mean                4.46949
exploration/env_infos/reward_run Std                 1.10031
exploration/env_infos/reward_run Max                 7.07128
exploration/env_infos/reward_run Min                -0.534996
exploration/env_infos/final/reward_ctrl Mean        -0.347667
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.347667
exploration/env_infos/final/reward_ctrl Min         -0.347667
exploration/env_infos/initial/reward_ctrl Mean      -0.0107397
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0107397
exploration/env_infos/initial/reward_ctrl Min       -0.0107397
exploration/env_infos/reward_ctrl Mean              -0.3776
exploration/env_infos/reward_ctrl Std                0.0901053
exploration/env_infos/reward_ctrl Max               -0.0107397
exploration/env_infos/reward_ctrl Min               -0.574058
evaluation/num steps total                      635000
evaluation/num paths total                         635
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.01512
evaluation/Rewards Std                               1.04814
evaluation/Rewards Max                               6.28769
evaluation/Rewards Min                              -0.932845
evaluation/Returns Mean                           4015.12
evaluation/Returns Std                              58.066
evaluation/Returns Max                            4116.52
evaluation/Returns Min                            3944.35
evaluation/Actions Mean                             -0.00572809
evaluation/Actions Std                               0.806942
evaluation/Actions Max                               0.999774
evaluation/Actions Min                              -0.999612
evaluation/Num Paths                                 5
evaluation/Average Returns                        4015.12
evaluation/env_infos/final/reward_run Mean           4.11089
evaluation/env_infos/final/reward_run Std            0.633913
evaluation/env_infos/final/reward_run Max            5.12291
evaluation/env_infos/final/reward_run Min            3.35802
evaluation/env_infos/initial/reward_run Mean         0.0779976
evaluation/env_infos/initial/reward_run Std          0.0706794
evaluation/env_infos/initial/reward_run Max          0.176243
evaluation/env_infos/initial/reward_run Min         -0.0136487
evaluation/env_infos/reward_run Mean                 4.40584
evaluation/env_infos/reward_run Std                  1.03667
evaluation/env_infos/reward_run Max                  6.77525
evaluation/env_infos/reward_run Min                 -0.446214
evaluation/env_infos/final/reward_ctrl Mean         -0.356081
evaluation/env_infos/final/reward_ctrl Std           0.0852932
evaluation/env_infos/final/reward_ctrl Max          -0.255676
evaluation/env_infos/final/reward_ctrl Min          -0.513553
evaluation/env_infos/initial/reward_ctrl Mean       -0.0387197
evaluation/env_infos/initial/reward_ctrl Std         0.0188546
evaluation/env_infos/initial/reward_ctrl Max        -0.0147592
evaluation/env_infos/initial/reward_ctrl Min        -0.0619154
evaluation/env_infos/reward_ctrl Mean               -0.390713
evaluation/env_infos/reward_ctrl Std                 0.0919806
evaluation/env_infos/reward_ctrl Max                -0.0147592
evaluation/env_infos/reward_ctrl Min                -0.580072
time/data storing (s)                                0.00675123
time/evaluation sampling (s)                         2.82309
time/exploration sampling (s)                        0.665288
time/logging (s)                                     0.0401994
time/saving (s)                                      0.0159311
time/training (s)                                   31.5726
time/epoch (s)                                      35.1239
time/total (s)                                    5351.69
Epoch                                              126
----------------------------------------------  ---------------
2020-07-08 22:35:59.202187 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 127 finished
----------------------------------------------  ---------------
replay_buffer/size                              129000
trainer/QF1 Loss                                     8.25063
trainer/QF2 Loss                                     9.56908
trainer/Policy Loss                               -204.298
trainer/Q1 Predictions Mean                        209.923
trainer/Q1 Predictions Std                         101.039
trainer/Q1 Predictions Max                         303.223
trainer/Q1 Predictions Min                           8.62677
trainer/Q2 Predictions Mean                        209.998
trainer/Q2 Predictions Std                         101.009
trainer/Q2 Predictions Max                         301.646
trainer/Q2 Predictions Min                           8.69349
trainer/Q Targets Mean                             210.184
trainer/Q Targets Std                              101.423
trainer/Q Targets Max                              301.624
trainer/Q Targets Min                                8.78888
trainer/Log Pis Mean                                 6.00699
trainer/Log Pis Std                                  5.32789
trainer/Log Pis Max                                 22.4137
trainer/Log Pis Min                                 -4.87982
trainer/Policy mu Mean                               0.00995596
trainer/Policy mu Std                                1.52712
trainer/Policy mu Max                                3.73567
trainer/Policy mu Min                               -4.67596
trainer/Policy log std Mean                         -0.794837
trainer/Policy log std Std                           0.338885
trainer/Policy log std Max                          -0.0809398
trainer/Policy log std Min                          -2.08183
trainer/Alpha                                        0.0864541
trainer/Alpha Loss                                   0.0171054
exploration/num steps total                     129000
exploration/num paths total                        129
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.87437
exploration/Rewards Std                              1.05616
exploration/Rewards Max                              6.5257
exploration/Rewards Min                             -1.18442
exploration/Returns Mean                          3874.37
exploration/Returns Std                              0
exploration/Returns Max                           3874.37
exploration/Returns Min                           3874.37
exploration/Actions Mean                            -0.00543199
exploration/Actions Std                              0.805928
exploration/Actions Max                              0.999948
exploration/Actions Min                             -0.99982
exploration/Num Paths                                1
exploration/Average Returns                       3874.37
exploration/env_infos/final/reward_run Mean          3.50405
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.50405
exploration/env_infos/final/reward_run Min           3.50405
exploration/env_infos/initial/reward_run Mean        0.14645
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.14645
exploration/env_infos/initial/reward_run Min         0.14645
exploration/env_infos/reward_run Mean                4.2641
exploration/env_infos/reward_run Std                 1.04132
exploration/env_infos/reward_run Max                 6.79608
exploration/env_infos/reward_run Min                -0.895877
exploration/env_infos/final/reward_ctrl Mean        -0.359273
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.359273
exploration/env_infos/final/reward_ctrl Min         -0.359273
exploration/env_infos/initial/reward_ctrl Mean      -0.0846713
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0846713
exploration/env_infos/initial/reward_ctrl Min       -0.0846713
exploration/env_infos/reward_ctrl Mean              -0.38973
exploration/env_infos/reward_ctrl Std                0.0936583
exploration/env_infos/reward_ctrl Max               -0.0846713
exploration/env_infos/reward_ctrl Min               -0.586463
evaluation/num steps total                      640000
evaluation/num paths total                         640
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.04182
evaluation/Rewards Std                               1.08196
evaluation/Rewards Max                               6.54177
evaluation/Rewards Min                              -1.15906
evaluation/Returns Mean                           4041.82
evaluation/Returns Std                              45.5714
evaluation/Returns Max                            4097.58
evaluation/Returns Min                            3990.5
evaluation/Actions Mean                             -0.00219148
evaluation/Actions Std                               0.814746
evaluation/Actions Max                               0.999573
evaluation/Actions Min                              -0.999372
evaluation/Num Paths                                 5
evaluation/Average Returns                        4041.82
evaluation/env_infos/final/reward_run Mean           4.71494
evaluation/env_infos/final/reward_run Std            0.521202
evaluation/env_infos/final/reward_run Max            5.2816
evaluation/env_infos/final/reward_run Min            3.87778
evaluation/env_infos/initial/reward_run Mean        -0.0961909
evaluation/env_infos/initial/reward_run Std          0.119269
evaluation/env_infos/initial/reward_run Max          0.108308
evaluation/env_infos/initial/reward_run Min         -0.200897
evaluation/env_infos/reward_run Mean                 4.44011
evaluation/env_infos/reward_run Std                  1.07116
evaluation/env_infos/reward_run Max                  6.97477
evaluation/env_infos/reward_run Min                 -1.00566
evaluation/env_infos/final/reward_ctrl Mean         -0.39132
evaluation/env_infos/final/reward_ctrl Std           0.0657778
evaluation/env_infos/final/reward_ctrl Max          -0.27337
evaluation/env_infos/final/reward_ctrl Min          -0.475254
evaluation/env_infos/initial/reward_ctrl Mean       -0.020185
evaluation/env_infos/initial/reward_ctrl Std         0.013584
evaluation/env_infos/initial/reward_ctrl Max        -0.0083888
evaluation/env_infos/initial/reward_ctrl Min        -0.045836
evaluation/env_infos/reward_ctrl Mean               -0.39829
evaluation/env_infos/reward_ctrl Std                 0.0916422
evaluation/env_infos/reward_ctrl Max                -0.0083888
evaluation/env_infos/reward_ctrl Min                -0.584229
time/data storing (s)                                0.00664641
time/evaluation sampling (s)                         2.59083
time/exploration sampling (s)                        0.67494
time/logging (s)                                     0.0413234
time/saving (s)                                      0.0170945
time/training (s)                                   32.7525
time/epoch (s)                                      36.0834
time/total (s)                                    5387.8
Epoch                                              127
----------------------------------------------  ---------------
2020-07-08 22:36:35.922950 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 128 finished
----------------------------------------------  ---------------
replay_buffer/size                              130000
trainer/QF1 Loss                                     8.82309
trainer/QF2 Loss                                     9.13285
trainer/Policy Loss                               -199.943
trainer/Q1 Predictions Mean                        205.645
trainer/Q1 Predictions Std                         101.013
trainer/Q1 Predictions Max                         301.983
trainer/Q1 Predictions Min                           8.09784
trainer/Q2 Predictions Mean                        205.734
trainer/Q2 Predictions Std                         101.021
trainer/Q2 Predictions Max                         299.655
trainer/Q2 Predictions Min                           8.42581
trainer/Q Targets Mean                             206.583
trainer/Q Targets Std                              101.162
trainer/Q Targets Max                              303.131
trainer/Q Targets Min                                8.62766
trainer/Log Pis Mean                                 6.20441
trainer/Log Pis Std                                  5.40101
trainer/Log Pis Max                                 24.7914
trainer/Log Pis Min                                 -5.18875
trainer/Policy mu Mean                               0.1467
trainer/Policy mu Std                                1.55343
trainer/Policy mu Max                                4.39103
trainer/Policy mu Min                               -5.37151
trainer/Policy log std Mean                         -0.78617
trainer/Policy log std Std                           0.344261
trainer/Policy log std Max                           0.891906
trainer/Policy log std Min                          -2.4125
trainer/Alpha                                        0.0870815
trainer/Alpha Loss                                   0.498946
exploration/num steps total                     130000
exploration/num paths total                        130
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.89009
exploration/Rewards Std                              1.04436
exploration/Rewards Max                              6.25091
exploration/Rewards Min                             -0.632985
exploration/Returns Mean                          3890.09
exploration/Returns Std                              0
exploration/Returns Max                           3890.09
exploration/Returns Min                           3890.09
exploration/Actions Mean                             0.0325583
exploration/Actions Std                              0.799664
exploration/Actions Max                              0.999914
exploration/Actions Min                             -0.999801
exploration/Num Paths                                1
exploration/Average Returns                       3890.09
exploration/env_infos/final/reward_run Mean          4.57026
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.57026
exploration/env_infos/final/reward_run Min           4.57026
exploration/env_infos/initial/reward_run Mean        0.0323538
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0323538
exploration/env_infos/initial/reward_run Min         0.0323538
exploration/env_infos/reward_run Mean                4.2744
exploration/env_infos/reward_run Std                 1.03173
exploration/env_infos/reward_run Max                 6.65293
exploration/env_infos/reward_run Min                -0.284842
exploration/env_infos/final/reward_ctrl Mean        -0.411007
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.411007
exploration/env_infos/final/reward_ctrl Min         -0.411007
exploration/env_infos/initial/reward_ctrl Mean      -0.0938216
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0938216
exploration/env_infos/initial/reward_ctrl Min       -0.0938216
exploration/env_infos/reward_ctrl Mean              -0.384313
exploration/env_infos/reward_ctrl Std                0.0937888
exploration/env_infos/reward_ctrl Max               -0.0784787
exploration/env_infos/reward_ctrl Min               -0.572106
evaluation/num steps total                      645000
evaluation/num paths total                         645
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.93322
evaluation/Rewards Std                               1.37172
evaluation/Rewards Max                               6.42357
evaluation/Rewards Min                              -1.25164
evaluation/Returns Mean                           3933.22
evaluation/Returns Std                             537.035
evaluation/Returns Max                            4291.76
evaluation/Returns Min                            2866.13
evaluation/Actions Mean                              0.0099448
evaluation/Actions Std                               0.808995
evaluation/Actions Max                               0.999879
evaluation/Actions Min                              -0.999932
evaluation/Num Paths                                 5
evaluation/Average Returns                        3933.22
evaluation/env_infos/final/reward_run Mean           3.94764
evaluation/env_infos/final/reward_run Std            1.41767
evaluation/env_infos/final/reward_run Max            6.134
evaluation/env_infos/final/reward_run Min            1.69999
evaluation/env_infos/initial/reward_run Mean         0.273931
evaluation/env_infos/initial/reward_run Std          0.151339
evaluation/env_infos/initial/reward_run Max          0.485004
evaluation/env_infos/initial/reward_run Min          0.0323332
evaluation/env_infos/reward_run Mean                 4.32596
evaluation/env_infos/reward_run Std                  1.37783
evaluation/env_infos/reward_run Max                  6.85778
evaluation/env_infos/reward_run Min                 -0.921069
evaluation/env_infos/final/reward_ctrl Mean         -0.433723
evaluation/env_infos/final/reward_ctrl Std           0.113023
evaluation/env_infos/final/reward_ctrl Max          -0.223536
evaluation/env_infos/final/reward_ctrl Min          -0.557164
evaluation/env_infos/initial/reward_ctrl Mean       -0.0686312
evaluation/env_infos/initial/reward_ctrl Std         0.0110186
evaluation/env_infos/initial/reward_ctrl Max        -0.0576734
evaluation/env_infos/initial/reward_ctrl Min        -0.087565
evaluation/env_infos/reward_ctrl Mean               -0.392743
evaluation/env_infos/reward_ctrl Std                 0.0971531
evaluation/env_infos/reward_ctrl Max                -0.0343909
evaluation/env_infos/reward_ctrl Min                -0.587569
time/data storing (s)                                0.00710691
time/evaluation sampling (s)                         2.74009
time/exploration sampling (s)                        0.650278
time/logging (s)                                     0.0417094
time/saving (s)                                      0.0182639
time/training (s)                                   33.2452
time/epoch (s)                                      36.7026
time/total (s)                                    5424.51
Epoch                                              128
----------------------------------------------  ---------------
2020-07-08 22:37:18.954300 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 129 finished
----------------------------------------------  ----------------
replay_buffer/size                              131000
trainer/QF1 Loss                                     6.17701
trainer/QF2 Loss                                     6.31975
trainer/Policy Loss                               -210.589
trainer/Q1 Predictions Mean                        215.829
trainer/Q1 Predictions Std                         100.484
trainer/Q1 Predictions Max                         304.178
trainer/Q1 Predictions Min                           7.05579
trainer/Q2 Predictions Mean                        215.875
trainer/Q2 Predictions Std                         100.431
trainer/Q2 Predictions Max                         302.751
trainer/Q2 Predictions Min                           6.95794
trainer/Q Targets Mean                             215.786
trainer/Q Targets Std                              100.398
trainer/Q Targets Max                              301.554
trainer/Q Targets Min                                6.91345
trainer/Log Pis Mean                                 5.1961
trainer/Log Pis Std                                  4.58084
trainer/Log Pis Max                                 17.6078
trainer/Log Pis Min                                 -5.43713
trainer/Policy mu Mean                               0.11236
trainer/Policy mu Std                                1.41569
trainer/Policy mu Max                                3.76712
trainer/Policy mu Min                               -4.49481
trainer/Policy log std Mean                         -0.804464
trainer/Policy log std Std                           0.342361
trainer/Policy log std Max                           0.000473946
trainer/Policy log std Min                          -2.36837
trainer/Alpha                                        0.0863238
trainer/Alpha Loss                                  -1.96905
exploration/num steps total                     131000
exploration/num paths total                        131
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.93412
exploration/Rewards Std                              1.13216
exploration/Rewards Max                              6.31132
exploration/Rewards Min                             -1.01689
exploration/Returns Mean                          3934.12
exploration/Returns Std                              0
exploration/Returns Max                           3934.12
exploration/Returns Min                           3934.12
exploration/Actions Mean                             0.0163642
exploration/Actions Std                              0.79731
exploration/Actions Max                              0.999808
exploration/Actions Min                             -0.99989
exploration/Num Paths                                1
exploration/Average Returns                       3934.12
exploration/env_infos/final/reward_run Mean          4.66837
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.66837
exploration/env_infos/final/reward_run Min           4.66837
exploration/env_infos/initial/reward_run Mean        0.0424763
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0424763
exploration/env_infos/initial/reward_run Min         0.0424763
exploration/env_infos/reward_run Mean                4.3157
exploration/env_infos/reward_run Std                 1.12349
exploration/env_infos/reward_run Max                 6.52248
exploration/env_infos/reward_run Min                -0.521844
exploration/env_infos/final/reward_ctrl Mean        -0.37769
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.37769
exploration/env_infos/final/reward_ctrl Min         -0.37769
exploration/env_infos/initial/reward_ctrl Mean      -0.042996
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.042996
exploration/env_infos/initial/reward_ctrl Min       -0.042996
exploration/env_infos/reward_ctrl Mean              -0.381583
exploration/env_infos/reward_ctrl Std                0.0949423
exploration/env_infos/reward_ctrl Max               -0.042996
exploration/env_infos/reward_ctrl Min               -0.594936
evaluation/num steps total                      650000
evaluation/num paths total                         650
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.08435
evaluation/Rewards Std                               1.12033
evaluation/Rewards Max                               6.53505
evaluation/Rewards Min                              -1.21124
evaluation/Returns Mean                           4084.35
evaluation/Returns Std                              67.0861
evaluation/Returns Max                            4209.27
evaluation/Returns Min                            4007.32
evaluation/Actions Mean                              0.00747103
evaluation/Actions Std                               0.812214
evaluation/Actions Max                               0.999961
evaluation/Actions Min                              -0.999578
evaluation/Num Paths                                 5
evaluation/Average Returns                        4084.35
evaluation/env_infos/final/reward_run Mean           4.9047
evaluation/env_infos/final/reward_run Std            1.02897
evaluation/env_infos/final/reward_run Max            5.85057
evaluation/env_infos/final/reward_run Min            3.13958
evaluation/env_infos/initial/reward_run Mean         0.146674
evaluation/env_infos/initial/reward_run Std          0.144739
evaluation/env_infos/initial/reward_run Max          0.320267
evaluation/env_infos/initial/reward_run Min         -0.016641
evaluation/env_infos/reward_run Mean                 4.48019
evaluation/env_infos/reward_run Std                  1.10883
evaluation/env_infos/reward_run Max                  6.83702
evaluation/env_infos/reward_run Min                 -0.767084
evaluation/env_infos/final/reward_ctrl Mean         -0.382841
evaluation/env_infos/final/reward_ctrl Std           0.0518399
evaluation/env_infos/final/reward_ctrl Max          -0.321838
evaluation/env_infos/final/reward_ctrl Min          -0.45055
evaluation/env_infos/initial/reward_ctrl Mean       -0.0488502
evaluation/env_infos/initial/reward_ctrl Std         0.0178976
evaluation/env_infos/initial/reward_ctrl Max        -0.0294993
evaluation/env_infos/initial/reward_ctrl Min        -0.0717817
evaluation/env_infos/reward_ctrl Mean               -0.395848
evaluation/env_infos/reward_ctrl Std                 0.0934719
evaluation/env_infos/reward_ctrl Max                -0.0294993
evaluation/env_infos/reward_ctrl Min                -0.588999
time/data storing (s)                                0.00784347
time/evaluation sampling (s)                         2.82665
time/exploration sampling (s)                        0.639125
time/logging (s)                                     0.0411127
time/saving (s)                                      0.0158885
time/training (s)                                   39.3483
time/epoch (s)                                      42.8789
time/total (s)                                    5467.54
Epoch                                              129
----------------------------------------------  ----------------
2020-07-08 22:37:59.501539 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 130 finished
----------------------------------------------  ---------------
replay_buffer/size                              132000
trainer/QF1 Loss                                     8.86851
trainer/QF2 Loss                                     7.50897
trainer/Policy Loss                               -208.695
trainer/Q1 Predictions Mean                        214.457
trainer/Q1 Predictions Std                         101.184
trainer/Q1 Predictions Max                         302.726
trainer/Q1 Predictions Min                           7.88919
trainer/Q2 Predictions Mean                        214.522
trainer/Q2 Predictions Std                         101.224
trainer/Q2 Predictions Max                         303.048
trainer/Q2 Predictions Min                           7.50741
trainer/Q Targets Mean                             214.242
trainer/Q Targets Std                              100.918
trainer/Q Targets Max                              300.736
trainer/Q Targets Min                                5.82573
trainer/Log Pis Mean                                 5.86273
trainer/Log Pis Std                                  5.27628
trainer/Log Pis Max                                 20.1324
trainer/Log Pis Min                                 -5.59343
trainer/Policy mu Mean                               0.0443423
trainer/Policy mu Std                                1.5234
trainer/Policy mu Max                                5.51832
trainer/Policy mu Min                               -4.79778
trainer/Policy log std Mean                         -0.787676
trainer/Policy log std Std                           0.337265
trainer/Policy log std Max                           0.24713
trainer/Policy log std Min                          -2.32778
trainer/Alpha                                        0.0870534
trainer/Alpha Loss                                  -0.335117
exploration/num steps total                     132000
exploration/num paths total                        132
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.09657
exploration/Rewards Std                              1.16562
exploration/Rewards Max                              6.36288
exploration/Rewards Min                             -0.823386
exploration/Returns Mean                          4096.57
exploration/Returns Std                              0
exploration/Returns Max                           4096.57
exploration/Returns Min                           4096.57
exploration/Actions Mean                            -0.0202742
exploration/Actions Std                              0.825315
exploration/Actions Max                              0.999895
exploration/Actions Min                             -0.99995
exploration/Num Paths                                1
exploration/Average Returns                       4096.57
exploration/env_infos/final/reward_run Mean          3.9029
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.9029
exploration/env_infos/final/reward_run Min           3.9029
exploration/env_infos/initial/reward_run Mean        0.247054
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.247054
exploration/env_infos/initial/reward_run Min         0.247054
exploration/env_infos/reward_run Mean                4.50551
exploration/env_infos/reward_run Std                 1.1645
exploration/env_infos/reward_run Max                 6.80279
exploration/env_infos/reward_run Min                -0.336696
exploration/env_infos/final/reward_ctrl Mean        -0.501798
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.501798
exploration/env_infos/final/reward_ctrl Min         -0.501798
exploration/env_infos/initial/reward_ctrl Mean      -0.0185037
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0185037
exploration/env_infos/initial/reward_ctrl Min       -0.0185037
exploration/env_infos/reward_ctrl Mean              -0.408934
exploration/env_infos/reward_ctrl Std                0.0964545
exploration/env_infos/reward_ctrl Max               -0.0185037
exploration/env_infos/reward_ctrl Min               -0.582268
evaluation/num steps total                      655000
evaluation/num paths total                         655
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.06353
evaluation/Rewards Std                               1.1141
evaluation/Rewards Max                               6.51083
evaluation/Rewards Min                              -1.27553
evaluation/Returns Mean                           4063.53
evaluation/Returns Std                             105.925
evaluation/Returns Max                            4187.69
evaluation/Returns Min                            3915.67
evaluation/Actions Mean                             -0.0149743
evaluation/Actions Std                               0.840296
evaluation/Actions Max                               0.999504
evaluation/Actions Min                              -0.999451
evaluation/Num Paths                                 5
evaluation/Average Returns                        4063.53
evaluation/env_infos/final/reward_run Mean           4.43464
evaluation/env_infos/final/reward_run Std            0.840517
evaluation/env_infos/final/reward_run Max            5.55661
evaluation/env_infos/final/reward_run Min            3.35296
evaluation/env_infos/initial/reward_run Mean         0.1934
evaluation/env_infos/initial/reward_run Std          0.262709
evaluation/env_infos/initial/reward_run Max          0.379957
evaluation/env_infos/initial/reward_run Min         -0.314376
evaluation/env_infos/reward_run Mean                 4.48732
evaluation/env_infos/reward_run Std                  1.10785
evaluation/env_infos/reward_run Max                  6.82409
evaluation/env_infos/reward_run Min                 -0.993769
evaluation/env_infos/final/reward_ctrl Mean         -0.445749
evaluation/env_infos/final/reward_ctrl Std           0.119437
evaluation/env_infos/final/reward_ctrl Max          -0.211053
evaluation/env_infos/final/reward_ctrl Min          -0.532027
evaluation/env_infos/initial/reward_ctrl Mean       -0.0383593
evaluation/env_infos/initial/reward_ctrl Std         0.00723123
evaluation/env_infos/initial/reward_ctrl Max        -0.0292926
evaluation/env_infos/initial/reward_ctrl Min        -0.0489764
evaluation/env_infos/reward_ctrl Mean               -0.423793
evaluation/env_infos/reward_ctrl Std                 0.0960541
evaluation/env_infos/reward_ctrl Max                -0.0292926
evaluation/env_infos/reward_ctrl Min                -0.593825
time/data storing (s)                                0.0336026
time/evaluation sampling (s)                         2.85032
time/exploration sampling (s)                        1.16848
time/logging (s)                                     0.042904
time/saving (s)                                      0.0158361
time/training (s)                                   36.4176
time/epoch (s)                                      40.5287
time/total (s)                                    5508.08
Epoch                                              130
----------------------------------------------  ---------------
2020-07-08 22:38:40.417404 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 131 finished
----------------------------------------------  ---------------
replay_buffer/size                              133000
trainer/QF1 Loss                                     9.64119
trainer/QF2 Loss                                     8.89259
trainer/Policy Loss                               -217.035
trainer/Q1 Predictions Mean                        223.451
trainer/Q1 Predictions Std                          92.1315
trainer/Q1 Predictions Max                         304.044
trainer/Q1 Predictions Min                           8.84109
trainer/Q2 Predictions Mean                        223.587
trainer/Q2 Predictions Std                          92.1875
trainer/Q2 Predictions Max                         304.832
trainer/Q2 Predictions Min                           8.7455
trainer/Q Targets Mean                             224.396
trainer/Q Targets Std                               92.4584
trainer/Q Targets Max                              305.229
trainer/Q Targets Min                                8.17153
trainer/Log Pis Mean                                 6.57717
trainer/Log Pis Std                                  5.16637
trainer/Log Pis Max                                 20.7734
trainer/Log Pis Min                                 -7.45153
trainer/Policy mu Mean                               0.0253384
trainer/Policy mu Std                                1.57499
trainer/Policy mu Max                                4.28912
trainer/Policy mu Min                               -6.18302
trainer/Policy log std Mean                         -0.814912
trainer/Policy log std Std                           0.349025
trainer/Policy log std Max                           0.150142
trainer/Policy log std Min                          -2.27454
trainer/Alpha                                        0.0871982
trainer/Alpha Loss                                   1.4081
exploration/num steps total                     133000
exploration/num paths total                        133
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.92886
exploration/Rewards Std                              1.06767
exploration/Rewards Max                              6.08846
exploration/Rewards Min                             -1.07408
exploration/Returns Mean                          3928.86
exploration/Returns Std                              0
exploration/Returns Max                           3928.86
exploration/Returns Min                           3928.86
exploration/Actions Mean                            -0.00878258
exploration/Actions Std                              0.811044
exploration/Actions Max                              0.999783
exploration/Actions Min                             -0.999894
exploration/Num Paths                                1
exploration/Average Returns                       3928.86
exploration/env_infos/final/reward_run Mean          4.07992
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.07992
exploration/env_infos/final/reward_run Min           4.07992
exploration/env_infos/initial/reward_run Mean        0.0974253
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0974253
exploration/env_infos/initial/reward_run Min         0.0974253
exploration/env_infos/reward_run Mean                4.32358
exploration/env_infos/reward_run Std                 1.05623
exploration/env_infos/reward_run Max                 6.579
exploration/env_infos/reward_run Min                -0.585668
exploration/env_infos/final/reward_ctrl Mean        -0.269186
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.269186
exploration/env_infos/final/reward_ctrl Min         -0.269186
exploration/env_infos/initial/reward_ctrl Mean      -0.0533278
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0533278
exploration/env_infos/initial/reward_ctrl Min       -0.0533278
exploration/env_infos/reward_ctrl Mean              -0.394722
exploration/env_infos/reward_ctrl Std                0.0994519
exploration/env_infos/reward_ctrl Max               -0.0533278
exploration/env_infos/reward_ctrl Min               -0.59152
evaluation/num steps total                      660000
evaluation/num paths total                         660
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.19127
evaluation/Rewards Std                               1.07144
evaluation/Rewards Max                               6.38797
evaluation/Rewards Min                              -0.913966
evaluation/Returns Mean                           4191.27
evaluation/Returns Std                              58.4067
evaluation/Returns Max                            4295.4
evaluation/Returns Min                            4126.06
evaluation/Actions Mean                             -0.0176143
evaluation/Actions Std                               0.820304
evaluation/Actions Max                               0.999328
evaluation/Actions Min                              -0.999706
evaluation/Num Paths                                 5
evaluation/Average Returns                        4191.27
evaluation/env_infos/final/reward_run Mean           4.86636
evaluation/env_infos/final/reward_run Std            0.594974
evaluation/env_infos/final/reward_run Max            5.62794
evaluation/env_infos/final/reward_run Min            4.03134
evaluation/env_infos/initial/reward_run Mean         0.171741
evaluation/env_infos/initial/reward_run Std          0.129381
evaluation/env_infos/initial/reward_run Max          0.359078
evaluation/env_infos/initial/reward_run Min          0.0330996
evaluation/env_infos/reward_run Mean                 4.5952
evaluation/env_infos/reward_run Std                  1.06204
evaluation/env_infos/reward_run Max                  6.83598
evaluation/env_infos/reward_run Min                 -0.440458
evaluation/env_infos/final/reward_ctrl Mean         -0.452805
evaluation/env_infos/final/reward_ctrl Std           0.0410539
evaluation/env_infos/final/reward_ctrl Max          -0.390781
evaluation/env_infos/final/reward_ctrl Min          -0.511435
evaluation/env_infos/initial/reward_ctrl Mean       -0.0389596
evaluation/env_infos/initial/reward_ctrl Std         0.00767486
evaluation/env_infos/initial/reward_ctrl Max        -0.0279379
evaluation/env_infos/initial/reward_ctrl Min        -0.0507924
evaluation/env_infos/reward_ctrl Mean               -0.403925
evaluation/env_infos/reward_ctrl Std                 0.0967298
evaluation/env_infos/reward_ctrl Max                -0.0279379
evaluation/env_infos/reward_ctrl Min                -0.587258
time/data storing (s)                                0.00666538
time/evaluation sampling (s)                         2.49161
time/exploration sampling (s)                        0.648489
time/logging (s)                                     0.0458543
time/saving (s)                                      0.0177623
time/training (s)                                   37.684
time/epoch (s)                                      40.8944
time/total (s)                                    5548.99
Epoch                                              131
----------------------------------------------  ---------------
2020-07-08 22:39:23.169678 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 132 finished
----------------------------------------------  ---------------
replay_buffer/size                              134000
trainer/QF1 Loss                                     6.12734
trainer/QF2 Loss                                     6.12588
trainer/Policy Loss                               -215.202
trainer/Q1 Predictions Mean                        221.048
trainer/Q1 Predictions Std                          96.8618
trainer/Q1 Predictions Max                         306.893
trainer/Q1 Predictions Min                           8.23016
trainer/Q2 Predictions Mean                        221.267
trainer/Q2 Predictions Std                          96.8975
trainer/Q2 Predictions Max                         307.579
trainer/Q2 Predictions Min                           8.44079
trainer/Q Targets Mean                             220.733
trainer/Q Targets Std                               96.7482
trainer/Q Targets Max                              307.592
trainer/Q Targets Min                                8.85862
trainer/Log Pis Mean                                 6.11737
trainer/Log Pis Std                                  5.21496
trainer/Log Pis Max                                 20.8415
trainer/Log Pis Min                                 -7.7297
trainer/Policy mu Mean                               0.0836281
trainer/Policy mu Std                                1.53344
trainer/Policy mu Max                                4.8065
trainer/Policy mu Min                               -3.86486
trainer/Policy log std Mean                         -0.809542
trainer/Policy log std Std                           0.333606
trainer/Policy log std Max                           0.0844651
trainer/Policy log std Min                          -2.33474
trainer/Alpha                                        0.0894817
trainer/Alpha Loss                                   0.283314
exploration/num steps total                     134000
exploration/num paths total                        134
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.79149
exploration/Rewards Std                              1.05858
exploration/Rewards Max                              6.30987
exploration/Rewards Min                             -0.369397
exploration/Returns Mean                          3791.49
exploration/Returns Std                              0
exploration/Returns Max                           3791.49
exploration/Returns Min                           3791.49
exploration/Actions Mean                             0.0304889
exploration/Actions Std                              0.809596
exploration/Actions Max                              0.999994
exploration/Actions Min                             -0.999811
exploration/Num Paths                                1
exploration/Average Returns                       3791.49
exploration/env_infos/final/reward_run Mean          3.18527
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.18527
exploration/env_infos/final/reward_run Min           3.18527
exploration/env_infos/initial/reward_run Mean       -0.056012
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.056012
exploration/env_infos/initial/reward_run Min        -0.056012
exploration/env_infos/reward_run Mean                4.18532
exploration/env_infos/reward_run Std                 1.04213
exploration/env_infos/reward_run Max                 6.62797
exploration/env_infos/reward_run Min                -0.056012
exploration/env_infos/final/reward_ctrl Mean        -0.400915
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.400915
exploration/env_infos/final/reward_ctrl Min         -0.400915
exploration/env_infos/initial/reward_ctrl Mean      -0.00493069
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.00493069
exploration/env_infos/initial/reward_ctrl Min       -0.00493069
exploration/env_infos/reward_ctrl Mean              -0.393825
exploration/env_infos/reward_ctrl Std                0.0944222
exploration/env_infos/reward_ctrl Max               -0.00493069
exploration/env_infos/reward_ctrl Min               -0.591513
evaluation/num steps total                      665000
evaluation/num paths total                         665
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.15108
evaluation/Rewards Std                               1.07657
evaluation/Rewards Max                               6.54748
evaluation/Rewards Min                              -0.891088
evaluation/Returns Mean                           4151.08
evaluation/Returns Std                              42.6876
evaluation/Returns Max                            4213.42
evaluation/Returns Min                            4106.51
evaluation/Actions Mean                              0.0216799
evaluation/Actions Std                               0.823349
evaluation/Actions Max                               0.99936
evaluation/Actions Min                              -0.999373
evaluation/Num Paths                                 5
evaluation/Average Returns                        4151.08
evaluation/env_infos/final/reward_run Mean           5.34142
evaluation/env_infos/final/reward_run Std            0.734662
evaluation/env_infos/final/reward_run Max            6.28411
evaluation/env_infos/final/reward_run Min            4.16863
evaluation/env_infos/initial/reward_run Mean         0.106349
evaluation/env_infos/initial/reward_run Std          0.107552
evaluation/env_infos/initial/reward_run Max          0.191436
evaluation/env_infos/initial/reward_run Min         -0.104778
evaluation/env_infos/reward_run Mean                 4.55811
evaluation/env_infos/reward_run Std                  1.06474
evaluation/env_infos/reward_run Max                  6.86986
evaluation/env_infos/reward_run Min                 -0.410528
evaluation/env_infos/final/reward_ctrl Mean         -0.448519
evaluation/env_infos/final/reward_ctrl Std           0.0672573
evaluation/env_infos/final/reward_ctrl Max          -0.315842
evaluation/env_infos/final/reward_ctrl Min          -0.500572
evaluation/env_infos/initial/reward_ctrl Mean       -0.0349084
evaluation/env_infos/initial/reward_ctrl Std         0.01842
evaluation/env_infos/initial/reward_ctrl Max        -0.0114262
evaluation/env_infos/initial/reward_ctrl Min        -0.0662461
evaluation/env_infos/reward_ctrl Mean               -0.407025
evaluation/env_infos/reward_ctrl Std                 0.0901095
evaluation/env_infos/reward_ctrl Max                -0.0114262
evaluation/env_infos/reward_ctrl Min                -0.586417
time/data storing (s)                                0.00795812
time/evaluation sampling (s)                         2.59679
time/exploration sampling (s)                        0.933494
time/logging (s)                                     0.040204
time/saving (s)                                      0.0161058
time/training (s)                                   39.0373
time/epoch (s)                                      42.6318
time/total (s)                                    5591.74
Epoch                                              132
----------------------------------------------  ---------------
2020-07-08 22:40:06.950249 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 133 finished
----------------------------------------------  ----------------
replay_buffer/size                              135000
trainer/QF1 Loss                                     8.35788
trainer/QF2 Loss                                     7.20664
trainer/Policy Loss                               -225.237
trainer/Q1 Predictions Mean                        232.384
trainer/Q1 Predictions Std                          89.4928
trainer/Q1 Predictions Max                         308.26
trainer/Q1 Predictions Min                           8.54879
trainer/Q2 Predictions Mean                        231.757
trainer/Q2 Predictions Std                          89.3695
trainer/Q2 Predictions Max                         309.694
trainer/Q2 Predictions Min                           7.08521
trainer/Q Targets Mean                             231.934
trainer/Q Targets Std                               89.4342
trainer/Q Targets Max                              310.377
trainer/Q Targets Min                                8.53386
trainer/Log Pis Mean                                 6.90206
trainer/Log Pis Std                                  5.48045
trainer/Log Pis Max                                 22.7787
trainer/Log Pis Min                                 -4.82326
trainer/Policy mu Mean                               0.0537751
trainer/Policy mu Std                                1.61602
trainer/Policy mu Max                                4.22026
trainer/Policy mu Min                               -3.70261
trainer/Policy log std Mean                         -0.818525
trainer/Policy log std Std                           0.3408
trainer/Policy log std Max                           0.0677925
trainer/Policy log std Min                          -2.67207
trainer/Alpha                                        0.0888397
trainer/Alpha Loss                                   2.1839
exploration/num steps total                     135000
exploration/num paths total                        135
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.10951
exploration/Rewards Std                              1.00825
exploration/Rewards Max                              6.01053
exploration/Rewards Min                             -0.50758
exploration/Returns Mean                          4109.51
exploration/Returns Std                              0
exploration/Returns Max                           4109.51
exploration/Returns Min                           4109.51
exploration/Actions Mean                            -0.000273465
exploration/Actions Std                              0.82034
exploration/Actions Max                              0.999724
exploration/Actions Min                             -0.999695
exploration/Num Paths                                1
exploration/Average Returns                       4109.51
exploration/env_infos/final/reward_run Mean          3.05668
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.05668
exploration/env_infos/final/reward_run Min           3.05668
exploration/env_infos/initial/reward_run Mean       -0.0363705
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0363705
exploration/env_infos/initial/reward_run Min        -0.0363705
exploration/env_infos/reward_run Mean                4.51328
exploration/env_infos/reward_run Std                 0.998729
exploration/env_infos/reward_run Max                 6.38514
exploration/env_infos/reward_run Min                -0.0363705
exploration/env_infos/final/reward_ctrl Mean        -0.384265
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.384265
exploration/env_infos/final/reward_ctrl Min         -0.384265
exploration/env_infos/initial/reward_ctrl Mean      -0.0121882
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0121882
exploration/env_infos/initial/reward_ctrl Min       -0.0121882
exploration/env_infos/reward_ctrl Mean              -0.403775
exploration/env_infos/reward_ctrl Std                0.0929476
exploration/env_infos/reward_ctrl Max               -0.0121882
exploration/env_infos/reward_ctrl Min               -0.579107
evaluation/num steps total                      670000
evaluation/num paths total                         670
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.2235
evaluation/Rewards Std                               1.06092
evaluation/Rewards Max                               6.33804
evaluation/Rewards Min                              -0.910041
evaluation/Returns Mean                           4223.5
evaluation/Returns Std                              67.5057
evaluation/Returns Max                            4295.67
evaluation/Returns Min                            4103.44
evaluation/Actions Mean                             -0.0110668
evaluation/Actions Std                               0.833364
evaluation/Actions Max                               0.999607
evaluation/Actions Min                              -0.999052
evaluation/Num Paths                                 5
evaluation/Average Returns                        4223.5
evaluation/env_infos/final/reward_run Mean           4.41598
evaluation/env_infos/final/reward_run Std            0.991209
evaluation/env_infos/final/reward_run Max            5.65544
evaluation/env_infos/final/reward_run Min            3.1573
evaluation/env_infos/initial/reward_run Mean         0.00393921
evaluation/env_infos/initial/reward_run Std          0.119926
evaluation/env_infos/initial/reward_run Max          0.154524
evaluation/env_infos/initial/reward_run Min         -0.153199
evaluation/env_infos/reward_run Mean                 4.64027
evaluation/env_infos/reward_run Std                  1.05093
evaluation/env_infos/reward_run Max                  6.67149
evaluation/env_infos/reward_run Min                 -0.560392
evaluation/env_infos/final/reward_ctrl Mean         -0.404559
evaluation/env_infos/final/reward_ctrl Std           0.119427
evaluation/env_infos/final/reward_ctrl Max          -0.207821
evaluation/env_infos/final/reward_ctrl Min          -0.538876
evaluation/env_infos/initial/reward_ctrl Mean       -0.0308202
evaluation/env_infos/initial/reward_ctrl Std         0.0167892
evaluation/env_infos/initial/reward_ctrl Max        -0.0161703
evaluation/env_infos/initial/reward_ctrl Min        -0.0636731
evaluation/env_infos/reward_ctrl Mean               -0.416771
evaluation/env_infos/reward_ctrl Std                 0.0952519
evaluation/env_infos/reward_ctrl Max                -0.0161703
evaluation/env_infos/reward_ctrl Min                -0.588729
time/data storing (s)                                0.00668619
time/evaluation sampling (s)                         2.86427
time/exploration sampling (s)                        0.76313
time/logging (s)                                     0.0419749
time/saving (s)                                      0.016142
time/training (s)                                   39.878
time/epoch (s)                                      43.5702
time/total (s)                                    5635.51
Epoch                                              133
----------------------------------------------  ----------------
2020-07-08 22:40:48.714962 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 134 finished
----------------------------------------------  ---------------
replay_buffer/size                              136000
trainer/QF1 Loss                                     7.49998
trainer/QF2 Loss                                     8.50159
trainer/Policy Loss                               -228.21
trainer/Q1 Predictions Mean                        235.09
trainer/Q1 Predictions Std                          84.3499
trainer/Q1 Predictions Max                         307.097
trainer/Q1 Predictions Min                           8.91621
trainer/Q2 Predictions Mean                        234.818
trainer/Q2 Predictions Std                          84.2361
trainer/Q2 Predictions Max                         306.055
trainer/Q2 Predictions Min                           9.05945
trainer/Q Targets Mean                             234.707
trainer/Q Targets Std                               84.3819
trainer/Q Targets Max                              305.07
trainer/Q Targets Min                                8.93794
trainer/Log Pis Mean                                 6.92974
trainer/Log Pis Std                                  4.95946
trainer/Log Pis Max                                 22.0593
trainer/Log Pis Min                                 -4.32074
trainer/Policy mu Mean                               0.11273
trainer/Policy mu Std                                1.57673
trainer/Policy mu Max                                3.77187
trainer/Policy mu Min                               -3.76661
trainer/Policy log std Mean                         -0.834756
trainer/Policy log std Std                           0.332528
trainer/Policy log std Max                          -0.0881971
trainer/Policy log std Min                          -2.39764
trainer/Alpha                                        0.0897803
trainer/Alpha Loss                                   2.24117
exploration/num steps total                     136000
exploration/num paths total                        136
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.02541
exploration/Rewards Std                              1.03611
exploration/Rewards Max                              6.28562
exploration/Rewards Min                             -0.673653
exploration/Returns Mean                          4025.41
exploration/Returns Std                              0
exploration/Returns Max                           4025.41
exploration/Returns Min                           4025.41
exploration/Actions Mean                            -0.0204475
exploration/Actions Std                              0.81757
exploration/Actions Max                              0.999856
exploration/Actions Min                             -0.99973
exploration/Num Paths                                1
exploration/Average Returns                       4025.41
exploration/env_infos/final/reward_run Mean          2.8393
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.8393
exploration/env_infos/final/reward_run Min           2.8393
exploration/env_infos/initial/reward_run Mean        0.371264
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.371264
exploration/env_infos/initial/reward_run Min         0.371264
exploration/env_infos/reward_run Mean                4.42672
exploration/env_infos/reward_run Std                 1.02419
exploration/env_infos/reward_run Max                 6.56823
exploration/env_infos/reward_run Min                -0.0997911
exploration/env_infos/final/reward_ctrl Mean        -0.363477
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.363477
exploration/env_infos/final/reward_ctrl Min         -0.363477
exploration/env_infos/initial/reward_ctrl Mean      -0.0308081
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0308081
exploration/env_infos/initial/reward_ctrl Min       -0.0308081
exploration/env_infos/reward_ctrl Mean              -0.401303
exploration/env_infos/reward_ctrl Std                0.0898996
exploration/env_infos/reward_ctrl Max               -0.0308081
exploration/env_infos/reward_ctrl Min               -0.577598
evaluation/num steps total                      675000
evaluation/num paths total                         675
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.2338
evaluation/Rewards Std                               1.0852
evaluation/Rewards Max                               6.71877
evaluation/Rewards Min                              -0.891637
evaluation/Returns Mean                           4233.8
evaluation/Returns Std                              81.1417
evaluation/Returns Max                            4373.79
evaluation/Returns Min                            4127.08
evaluation/Actions Mean                             -0.0319566
evaluation/Actions Std                               0.836997
evaluation/Actions Max                               0.999587
evaluation/Actions Min                              -0.999294
evaluation/Num Paths                                 5
evaluation/Average Returns                        4233.8
evaluation/env_infos/final/reward_run Mean           4.92398
evaluation/env_infos/final/reward_run Std            0.858752
evaluation/env_infos/final/reward_run Max            6.31363
evaluation/env_infos/final/reward_run Min            3.67781
evaluation/env_infos/initial/reward_run Mean         0.093718
evaluation/env_infos/initial/reward_run Std          0.117333
evaluation/env_infos/initial/reward_run Max          0.226568
evaluation/env_infos/initial/reward_run Min         -0.0830827
evaluation/env_infos/reward_run Mean                 4.65475
evaluation/env_infos/reward_run Std                  1.07605
evaluation/env_infos/reward_run Max                  6.97688
evaluation/env_infos/reward_run Min                 -0.342721
evaluation/env_infos/final/reward_ctrl Mean         -0.411609
evaluation/env_infos/final/reward_ctrl Std           0.0856167
evaluation/env_infos/final/reward_ctrl Max          -0.286976
evaluation/env_infos/final/reward_ctrl Min          -0.516593
evaluation/env_infos/initial/reward_ctrl Mean       -0.0160775
evaluation/env_infos/initial/reward_ctrl Std         0.00672357
evaluation/env_infos/initial/reward_ctrl Max        -0.00684808
evaluation/env_infos/initial/reward_ctrl Min        -0.0264333
evaluation/env_infos/reward_ctrl Mean               -0.420952
evaluation/env_infos/reward_ctrl Std                 0.0887814
evaluation/env_infos/reward_ctrl Max                -0.00684808
evaluation/env_infos/reward_ctrl Min                -0.591584
time/data storing (s)                                0.00667388
time/evaluation sampling (s)                         3.04627
time/exploration sampling (s)                        0.639356
time/logging (s)                                     0.0401835
time/saving (s)                                      0.0202058
time/training (s)                                   37.9834
time/epoch (s)                                      41.7361
time/total (s)                                    5677.27
Epoch                                              134
----------------------------------------------  ---------------
2020-07-08 22:41:27.297433 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 135 finished
----------------------------------------------  --------------
replay_buffer/size                              137000
trainer/QF1 Loss                                    17.6654
trainer/QF2 Loss                                    18.6406
trainer/Policy Loss                               -223.222
trainer/Q1 Predictions Mean                        229.436
trainer/Q1 Predictions Std                          91.6881
trainer/Q1 Predictions Max                         304.561
trainer/Q1 Predictions Min                           9.40287
trainer/Q2 Predictions Mean                        229.123
trainer/Q2 Predictions Std                          91.5794
trainer/Q2 Predictions Max                         307.423
trainer/Q2 Predictions Min                           9.55483
trainer/Q Targets Mean                             229.165
trainer/Q Targets Std                               91.6568
trainer/Q Targets Max                              303.63
trainer/Q Targets Min                                8.57522
trainer/Log Pis Mean                                 6.03869
trainer/Log Pis Std                                  4.94953
trainer/Log Pis Max                                 19.0343
trainer/Log Pis Min                                 -4.57896
trainer/Policy mu Mean                              -0.0115195
trainer/Policy mu Std                                1.50825
trainer/Policy mu Max                                3.80104
trainer/Policy mu Min                               -3.30813
trainer/Policy log std Mean                         -0.823337
trainer/Policy log std Std                           0.329296
trainer/Policy log std Max                          -0.0676073
trainer/Policy log std Min                          -2.25216
trainer/Alpha                                        0.088643
trainer/Alpha Loss                                   0.0937412
exploration/num steps total                     137000
exploration/num paths total                        137
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.886
exploration/Rewards Std                              1.07227
exploration/Rewards Max                              6.06261
exploration/Rewards Min                             -1.01475
exploration/Returns Mean                          3886
exploration/Returns Std                              0
exploration/Returns Max                           3886
exploration/Returns Min                           3886
exploration/Actions Mean                            -0.0118089
exploration/Actions Std                              0.815764
exploration/Actions Max                              0.999699
exploration/Actions Min                             -0.999584
exploration/Num Paths                                1
exploration/Average Returns                       3886
exploration/env_infos/final/reward_run Mean          5.1982
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.1982
exploration/env_infos/final/reward_run Min           5.1982
exploration/env_infos/initial/reward_run Mean       -0.181479
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.181479
exploration/env_infos/initial/reward_run Min        -0.181479
exploration/env_infos/reward_run Mean                4.28537
exploration/env_infos/reward_run Std                 1.05785
exploration/env_infos/reward_run Max                 6.32758
exploration/env_infos/reward_run Min                -0.656008
exploration/env_infos/final/reward_ctrl Mean        -0.379379
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.379379
exploration/env_infos/final/reward_ctrl Min         -0.379379
exploration/env_infos/initial/reward_ctrl Mean      -0.0888408
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0888408
exploration/env_infos/initial/reward_ctrl Min       -0.0888408
exploration/env_infos/reward_ctrl Mean              -0.399366
exploration/env_infos/reward_ctrl Std                0.0945916
exploration/env_infos/reward_ctrl Max               -0.0888408
exploration/env_infos/reward_ctrl Min               -0.587421
evaluation/num steps total                      680000
evaluation/num paths total                         680
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.27131
evaluation/Rewards Std                               1.07192
evaluation/Rewards Max                               6.54404
evaluation/Rewards Min                              -1.41959
evaluation/Returns Mean                           4271.31
evaluation/Returns Std                              84.0619
evaluation/Returns Max                            4398.13
evaluation/Returns Min                            4161.83
evaluation/Actions Mean                             -0.0269647
evaluation/Actions Std                               0.831293
evaluation/Actions Max                               0.999898
evaluation/Actions Min                              -0.999549
evaluation/Num Paths                                 5
evaluation/Average Returns                        4271.31
evaluation/env_infos/final/reward_run Mean           4.24289
evaluation/env_infos/final/reward_run Std            0.675225
evaluation/env_infos/final/reward_run Max            5.24226
evaluation/env_infos/final/reward_run Min            3.36036
evaluation/env_infos/initial/reward_run Mean         0.0475495
evaluation/env_infos/initial/reward_run Std          0.13625
evaluation/env_infos/initial/reward_run Max          0.201275
evaluation/env_infos/initial/reward_run Min         -0.153374
evaluation/env_infos/reward_run Mean                 4.68637
evaluation/env_infos/reward_run Std                  1.06084
evaluation/env_infos/reward_run Max                  6.87749
evaluation/env_infos/reward_run Min                 -0.888879
evaluation/env_infos/final/reward_ctrl Mean         -0.2822
evaluation/env_infos/final/reward_ctrl Std           0.0475061
evaluation/env_infos/final/reward_ctrl Max          -0.225341
evaluation/env_infos/final/reward_ctrl Min          -0.353655
evaluation/env_infos/initial/reward_ctrl Mean       -0.0324179
evaluation/env_infos/initial/reward_ctrl Std         0.0187406
evaluation/env_infos/initial/reward_ctrl Max        -0.0129815
evaluation/env_infos/initial/reward_ctrl Min        -0.0639668
evaluation/env_infos/reward_ctrl Mean               -0.415065
evaluation/env_infos/reward_ctrl Std                 0.0946095
evaluation/env_infos/reward_ctrl Max                -0.0129815
evaluation/env_infos/reward_ctrl Min                -0.584763
time/data storing (s)                                0.0104376
time/evaluation sampling (s)                         3.82994
time/exploration sampling (s)                        0.834449
time/logging (s)                                     0.0419566
time/saving (s)                                      0.0160165
time/training (s)                                   33.8121
time/epoch (s)                                      38.5449
time/total (s)                                    5715.85
Epoch                                              135
----------------------------------------------  --------------
2020-07-08 22:42:01.658760 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 136 finished
----------------------------------------------  ---------------
replay_buffer/size                              138000
trainer/QF1 Loss                                     6.2175
trainer/QF2 Loss                                     7.52284
trainer/Policy Loss                               -209.946
trainer/Q1 Predictions Mean                        216.388
trainer/Q1 Predictions Std                         102.407
trainer/Q1 Predictions Max                         304.4
trainer/Q1 Predictions Min                           9.50927
trainer/Q2 Predictions Mean                        216.019
trainer/Q2 Predictions Std                         102.148
trainer/Q2 Predictions Max                         301.991
trainer/Q2 Predictions Min                           9.74065
trainer/Q Targets Mean                             216.776
trainer/Q Targets Std                              102.487
trainer/Q Targets Max                              307.078
trainer/Q Targets Min                                9.46051
trainer/Log Pis Mean                                 6.16397
trainer/Log Pis Std                                  5.45088
trainer/Log Pis Max                                 23.7642
trainer/Log Pis Min                                 -4.85571
trainer/Policy mu Mean                               0.121636
trainer/Policy mu Std                                1.53357
trainer/Policy mu Max                                5.41714
trainer/Policy mu Min                               -4.39556
trainer/Policy log std Mean                         -0.787941
trainer/Policy log std Std                           0.332816
trainer/Policy log std Max                           0.101698
trainer/Policy log std Min                          -2.34025
trainer/Alpha                                        0.0892215
trainer/Alpha Loss                                   0.396279
exploration/num steps total                     138000
exploration/num paths total                        138
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.07856
exploration/Rewards Std                              1.0382
exploration/Rewards Max                              6.79323
exploration/Rewards Min                             -0.679234
exploration/Returns Mean                          4078.56
exploration/Returns Std                              0
exploration/Returns Max                           4078.56
exploration/Returns Min                           4078.56
exploration/Actions Mean                             0.00635806
exploration/Actions Std                              0.809326
exploration/Actions Max                              0.999701
exploration/Actions Min                             -0.999268
exploration/Num Paths                                1
exploration/Average Returns                       4078.56
exploration/env_infos/final/reward_run Mean          3.67728
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.67728
exploration/env_infos/final/reward_run Min           3.67728
exploration/env_infos/initial/reward_run Mean        0.274802
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.274802
exploration/env_infos/initial/reward_run Min         0.274802
exploration/env_infos/reward_run Mean                4.47159
exploration/env_infos/reward_run Std                 1.02324
exploration/env_infos/reward_run Max                 7.0555
exploration/env_infos/reward_run Min                -0.127707
exploration/env_infos/final/reward_ctrl Mean        -0.451187
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.451187
exploration/env_infos/final/reward_ctrl Min         -0.451187
exploration/env_infos/initial/reward_ctrl Mean      -0.0430068
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0430068
exploration/env_infos/initial/reward_ctrl Min       -0.0430068
exploration/env_infos/reward_ctrl Mean              -0.393029
exploration/env_infos/reward_ctrl Std                0.095669
exploration/env_infos/reward_ctrl Max               -0.0430068
exploration/env_infos/reward_ctrl Min               -0.588108
evaluation/num steps total                      685000
evaluation/num paths total                         685
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.23785
evaluation/Rewards Std                               1.06692
evaluation/Rewards Max                               6.54787
evaluation/Rewards Min                              -0.71909
evaluation/Returns Mean                           4237.85
evaluation/Returns Std                              70.2201
evaluation/Returns Max                            4332.12
evaluation/Returns Min                            4147.47
evaluation/Actions Mean                             -0.0134017
evaluation/Actions Std                               0.835885
evaluation/Actions Max                               0.999651
evaluation/Actions Min                              -0.999155
evaluation/Num Paths                                 5
evaluation/Average Returns                        4237.85
evaluation/env_infos/final/reward_run Mean           5.1909
evaluation/env_infos/final/reward_run Std            1.2195
evaluation/env_infos/final/reward_run Max            6.41936
evaluation/env_infos/final/reward_run Min            3.35465
evaluation/env_infos/initial/reward_run Mean         0.0805639
evaluation/env_infos/initial/reward_run Std          0.088977
evaluation/env_infos/initial/reward_run Max          0.226238
evaluation/env_infos/initial/reward_run Min          0.00773735
evaluation/env_infos/reward_run Mean                 4.65718
evaluation/env_infos/reward_run Std                  1.05512
evaluation/env_infos/reward_run Max                  6.91853
evaluation/env_infos/reward_run Min                 -0.203622
evaluation/env_infos/final/reward_ctrl Mean         -0.41929
evaluation/env_infos/final/reward_ctrl Std           0.0912224
evaluation/env_infos/final/reward_ctrl Max          -0.263682
evaluation/env_infos/final/reward_ctrl Min          -0.519646
evaluation/env_infos/initial/reward_ctrl Mean       -0.0353992
evaluation/env_infos/initial/reward_ctrl Std         0.0127932
evaluation/env_infos/initial/reward_ctrl Max        -0.0143167
evaluation/env_infos/initial/reward_ctrl Min        -0.0537986
evaluation/env_infos/reward_ctrl Mean               -0.41933
evaluation/env_infos/reward_ctrl Std                 0.0951766
evaluation/env_infos/reward_ctrl Max                -0.0143167
evaluation/env_infos/reward_ctrl Min                -0.585984
time/data storing (s)                                0.00674594
time/evaluation sampling (s)                         2.41206
time/exploration sampling (s)                        0.621326
time/logging (s)                                     0.0416311
time/saving (s)                                      0.0181936
time/training (s)                                   31.2411
time/epoch (s)                                      34.3411
time/total (s)                                    5750.2
Epoch                                              136
----------------------------------------------  ---------------
2020-07-08 22:42:42.527466 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 137 finished
----------------------------------------------  ---------------
replay_buffer/size                              139000
trainer/QF1 Loss                                    11.9731
trainer/QF2 Loss                                    11.6018
trainer/Policy Loss                               -208.318
trainer/Q1 Predictions Mean                        213.614
trainer/Q1 Predictions Std                         100.57
trainer/Q1 Predictions Max                         306.046
trainer/Q1 Predictions Min                           9.3269
trainer/Q2 Predictions Mean                        213.545
trainer/Q2 Predictions Std                         100.561
trainer/Q2 Predictions Max                         306.587
trainer/Q2 Predictions Min                           9.06497
trainer/Q Targets Mean                             213.996
trainer/Q Targets Std                              100.592
trainer/Q Targets Max                              308.088
trainer/Q Targets Min                                8.63843
trainer/Log Pis Mean                                 5.59468
trainer/Log Pis Std                                  5.5669
trainer/Log Pis Max                                 23.6842
trainer/Log Pis Min                                 -4.96596
trainer/Policy mu Mean                               0.0355933
trainer/Policy mu Std                                1.49756
trainer/Policy mu Max                                4.58271
trainer/Policy mu Min                               -3.73277
trainer/Policy log std Mean                         -0.804468
trainer/Policy log std Std                           0.339043
trainer/Policy log std Max                           0.0220015
trainer/Policy log std Min                          -2.21779
trainer/Alpha                                        0.0876978
trainer/Alpha Loss                                  -0.986456
exploration/num steps total                     139000
exploration/num paths total                        139
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.97476
exploration/Rewards Std                              1.039
exploration/Rewards Max                              6.145
exploration/Rewards Min                             -0.0478871
exploration/Returns Mean                          3974.76
exploration/Returns Std                              0
exploration/Returns Max                           3974.76
exploration/Returns Min                           3974.76
exploration/Actions Mean                            -0.0144505
exploration/Actions Std                              0.808119
exploration/Actions Max                              0.999879
exploration/Actions Min                             -0.999664
exploration/Num Paths                                1
exploration/Average Returns                       3974.76
exploration/env_infos/final/reward_run Mean          5.13989
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.13989
exploration/env_infos/final/reward_run Min           5.13989
exploration/env_infos/initial/reward_run Mean        0.0568255
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0568255
exploration/env_infos/initial/reward_run Min         0.0568255
exploration/env_infos/reward_run Mean                4.36672
exploration/env_infos/reward_run Std                 1.03132
exploration/env_infos/reward_run Max                 6.53575
exploration/env_infos/reward_run Min                 0.0568255
exploration/env_infos/final/reward_ctrl Mean        -0.486142
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.486142
exploration/env_infos/final/reward_ctrl Min         -0.486142
exploration/env_infos/initial/reward_ctrl Mean      -0.0405056
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0405056
exploration/env_infos/initial/reward_ctrl Min       -0.0405056
exploration/env_infos/reward_ctrl Mean              -0.391959
exploration/env_infos/reward_ctrl Std                0.105945
exploration/env_infos/reward_ctrl Max               -0.0405056
exploration/env_infos/reward_ctrl Min               -0.587662
evaluation/num steps total                      690000
evaluation/num paths total                         690
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.21674
evaluation/Rewards Std                               1.07022
evaluation/Rewards Max                               6.41822
evaluation/Rewards Min                              -1.19184
evaluation/Returns Mean                           4216.74
evaluation/Returns Std                              76.6201
evaluation/Returns Max                            4329.18
evaluation/Returns Min                            4101.39
evaluation/Actions Mean                             -0.0333181
evaluation/Actions Std                               0.831509
evaluation/Actions Max                               0.999495
evaluation/Actions Min                              -0.999805
evaluation/Num Paths                                 5
evaluation/Average Returns                        4216.74
evaluation/env_infos/final/reward_run Mean           4.64993
evaluation/env_infos/final/reward_run Std            0.652734
evaluation/env_infos/final/reward_run Max            5.80905
evaluation/env_infos/final/reward_run Min            3.911
evaluation/env_infos/initial/reward_run Mean         0.118178
evaluation/env_infos/initial/reward_run Std          0.11833
evaluation/env_infos/initial/reward_run Max          0.261031
evaluation/env_infos/initial/reward_run Min         -0.0614463
evaluation/env_infos/reward_run Mean                 4.63225
evaluation/env_infos/reward_run Std                  1.06127
evaluation/env_infos/reward_run Max                  6.84838
evaluation/env_infos/reward_run Min                 -0.746853
evaluation/env_infos/final/reward_ctrl Mean         -0.380729
evaluation/env_infos/final/reward_ctrl Std           0.129721
evaluation/env_infos/final/reward_ctrl Max          -0.213755
evaluation/env_infos/final/reward_ctrl Min          -0.552754
evaluation/env_infos/initial/reward_ctrl Mean       -0.0267048
evaluation/env_infos/initial/reward_ctrl Std         0.00517556
evaluation/env_infos/initial/reward_ctrl Max        -0.0179107
evaluation/env_infos/initial/reward_ctrl Min        -0.033666
evaluation/env_infos/reward_ctrl Mean               -0.41551
evaluation/env_infos/reward_ctrl Std                 0.103377
evaluation/env_infos/reward_ctrl Max                -0.0122723
evaluation/env_infos/reward_ctrl Min                -0.581622
time/data storing (s)                                0.0109167
time/evaluation sampling (s)                         2.53785
time/exploration sampling (s)                        0.765054
time/logging (s)                                     0.0420308
time/saving (s)                                      0.0165607
time/training (s)                                   37.4778
time/epoch (s)                                      40.8503
time/total (s)                                    5791.07
Epoch                                              137
----------------------------------------------  ---------------
2020-07-08 22:43:21.608281 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 138 finished
----------------------------------------------  ---------------
replay_buffer/size                              140000
trainer/QF1 Loss                                     8.1614
trainer/QF2 Loss                                     6.59424
trainer/Policy Loss                               -213.993
trainer/Q1 Predictions Mean                        219.908
trainer/Q1 Predictions Std                         100.678
trainer/Q1 Predictions Max                         302.374
trainer/Q1 Predictions Min                           8.4449
trainer/Q2 Predictions Mean                        219.356
trainer/Q2 Predictions Std                         100.528
trainer/Q2 Predictions Max                         301.874
trainer/Q2 Predictions Min                           7.80597
trainer/Q Targets Mean                             219.575
trainer/Q Targets Std                              100.648
trainer/Q Targets Max                              300.532
trainer/Q Targets Min                                7.81204
trainer/Log Pis Mean                                 5.73324
trainer/Log Pis Std                                  5.16872
trainer/Log Pis Max                                 17.2799
trainer/Log Pis Min                                 -5.91436
trainer/Policy mu Mean                               0.0188807
trainer/Policy mu Std                                1.47763
trainer/Policy mu Max                                4.10821
trainer/Policy mu Min                               -3.91941
trainer/Policy log std Mean                         -0.814925
trainer/Policy log std Std                           0.34809
trainer/Policy log std Max                          -0.0723599
trainer/Policy log std Min                          -2.45382
trainer/Alpha                                        0.0894355
trainer/Alpha Loss                                  -0.64403
exploration/num steps total                     140000
exploration/num paths total                        140
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.92514
exploration/Rewards Std                              1.04188
exploration/Rewards Max                              6.19789
exploration/Rewards Min                             -0.702931
exploration/Returns Mean                          3925.14
exploration/Returns Std                              0
exploration/Returns Max                           3925.14
exploration/Returns Min                           3925.14
exploration/Actions Mean                             0.00714544
exploration/Actions Std                              0.814441
exploration/Actions Max                              0.999514
exploration/Actions Min                             -0.999583
exploration/Num Paths                                1
exploration/Average Returns                       3925.14
exploration/env_infos/final/reward_run Mean          4.52936
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.52936
exploration/env_infos/final/reward_run Min           4.52936
exploration/env_infos/initial/reward_run Mean        0.102323
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.102323
exploration/env_infos/initial/reward_run Min         0.102323
exploration/env_infos/reward_run Mean                4.32316
exploration/env_infos/reward_run Std                 1.0362
exploration/env_infos/reward_run Max                 6.65891
exploration/env_infos/reward_run Min                -0.239188
exploration/env_infos/final/reward_ctrl Mean        -0.333644
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.333644
exploration/env_infos/final/reward_ctrl Min         -0.333644
exploration/env_infos/initial/reward_ctrl Mean      -0.0375307
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0375307
exploration/env_infos/initial/reward_ctrl Min       -0.0375307
exploration/env_infos/reward_ctrl Mean              -0.398019
exploration/env_infos/reward_ctrl Std                0.0918606
exploration/env_infos/reward_ctrl Max               -0.0375307
exploration/env_infos/reward_ctrl Min               -0.576532
evaluation/num steps total                      695000
evaluation/num paths total                         695
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.20391
evaluation/Rewards Std                               1.03432
evaluation/Rewards Max                               6.3637
evaluation/Rewards Min                              -0.737129
evaluation/Returns Mean                           4203.91
evaluation/Returns Std                              47.609
evaluation/Returns Max                            4277.77
evaluation/Returns Min                            4148.65
evaluation/Actions Mean                             -0.0330106
evaluation/Actions Std                               0.840805
evaluation/Actions Max                               0.999748
evaluation/Actions Min                              -0.998548
evaluation/Num Paths                                 5
evaluation/Average Returns                        4203.91
evaluation/env_infos/final/reward_run Mean           3.8571
evaluation/env_infos/final/reward_run Std            0.442498
evaluation/env_infos/final/reward_run Max            4.66771
evaluation/env_infos/final/reward_run Min            3.43089
evaluation/env_infos/initial/reward_run Mean         0.129865
evaluation/env_infos/initial/reward_run Std          0.0857649
evaluation/env_infos/initial/reward_run Max          0.267188
evaluation/env_infos/initial/reward_run Min          0.0344577
evaluation/env_infos/reward_run Mean                 4.62874
evaluation/env_infos/reward_run Std                  1.03004
evaluation/env_infos/reward_run Max                  6.87735
evaluation/env_infos/reward_run Min                 -0.334645
evaluation/env_infos/final/reward_ctrl Mean         -0.452154
evaluation/env_infos/final/reward_ctrl Std           0.0222211
evaluation/env_infos/final/reward_ctrl Max          -0.428104
evaluation/env_infos/final/reward_ctrl Min          -0.490851
evaluation/env_infos/initial/reward_ctrl Mean       -0.0231374
evaluation/env_infos/initial/reward_ctrl Std         0.00593452
evaluation/env_infos/initial/reward_ctrl Max        -0.0164392
evaluation/env_infos/initial/reward_ctrl Min        -0.032344
evaluation/env_infos/reward_ctrl Mean               -0.424826
evaluation/env_infos/reward_ctrl Std                 0.0853931
evaluation/env_infos/reward_ctrl Max                -0.013283
evaluation/env_infos/reward_ctrl Min                -0.578366
time/data storing (s)                                0.00828577
time/evaluation sampling (s)                         2.93082
time/exploration sampling (s)                        0.861477
time/logging (s)                                     0.0413967
time/saving (s)                                      0.0179206
time/training (s)                                   35.1898
time/epoch (s)                                      39.0497
time/total (s)                                    5830.14
Epoch                                              138
----------------------------------------------  ---------------
2020-07-08 22:44:04.154069 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 139 finished
----------------------------------------------  ---------------
replay_buffer/size                              141000
trainer/QF1 Loss                                     8.9199
trainer/QF2 Loss                                     7.92019
trainer/Policy Loss                               -215.667
trainer/Q1 Predictions Mean                        221.481
trainer/Q1 Predictions Std                          99.4522
trainer/Q1 Predictions Max                         303.327
trainer/Q1 Predictions Min                           8.9209
trainer/Q2 Predictions Mean                        221.583
trainer/Q2 Predictions Std                          99.4084
trainer/Q2 Predictions Max                         302.223
trainer/Q2 Predictions Min                           9.95424
trainer/Q Targets Mean                             221.598
trainer/Q Targets Std                               99.4107
trainer/Q Targets Max                              304.83
trainer/Q Targets Min                                8.62628
trainer/Log Pis Mean                                 6.22064
trainer/Log Pis Std                                  5.24585
trainer/Log Pis Max                                 24.28
trainer/Log Pis Min                                 -8.20714
trainer/Policy mu Mean                               0.0332255
trainer/Policy mu Std                                1.54125
trainer/Policy mu Max                                4.70698
trainer/Policy mu Min                               -3.74754
trainer/Policy log std Mean                         -0.792311
trainer/Policy log std Std                           0.344237
trainer/Policy log std Max                          -0.0129791
trainer/Policy log std Min                          -2.37088
trainer/Alpha                                        0.0916234
trainer/Alpha Loss                                   0.527333
exploration/num steps total                     141000
exploration/num paths total                        141
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.134
exploration/Rewards Std                              1.01893
exploration/Rewards Max                              6.3411
exploration/Rewards Min                             -0.592245
exploration/Returns Mean                          4134
exploration/Returns Std                              0
exploration/Returns Max                           4134
exploration/Returns Min                           4134
exploration/Actions Mean                            -0.0160842
exploration/Actions Std                              0.828514
exploration/Actions Max                              0.999368
exploration/Actions Min                             -0.999805
exploration/Num Paths                                1
exploration/Average Returns                       4134
exploration/env_infos/final/reward_run Mean          5.24782
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.24782
exploration/env_infos/final/reward_run Min           5.24782
exploration/env_infos/initial/reward_run Mean        0.233657
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.233657
exploration/env_infos/initial/reward_run Min         0.233657
exploration/env_infos/reward_run Mean                4.54602
exploration/env_infos/reward_run Std                 1.01343
exploration/env_infos/reward_run Max                 6.66599
exploration/env_infos/reward_run Min                -0.133598
exploration/env_infos/final/reward_ctrl Mean        -0.264168
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.264168
exploration/env_infos/final/reward_ctrl Min         -0.264168
exploration/env_infos/initial/reward_ctrl Mean      -0.0532895
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0532895
exploration/env_infos/initial/reward_ctrl Min       -0.0532895
exploration/env_infos/reward_ctrl Mean              -0.412017
exploration/env_infos/reward_ctrl Std                0.0945585
exploration/env_infos/reward_ctrl Max               -0.0532895
exploration/env_infos/reward_ctrl Min               -0.571789
evaluation/num steps total                      700000
evaluation/num paths total                         700
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.24304
evaluation/Rewards Std                               1.05498
evaluation/Rewards Max                               6.60487
evaluation/Rewards Min                              -1.05904
evaluation/Returns Mean                           4243.04
evaluation/Returns Std                              70.3972
evaluation/Returns Max                            4320.09
evaluation/Returns Min                            4120.52
evaluation/Actions Mean                             -0.0213071
evaluation/Actions Std                               0.834628
evaluation/Actions Max                               0.998577
evaluation/Actions Min                              -0.997755
evaluation/Num Paths                                 5
evaluation/Average Returns                        4243.04
evaluation/env_infos/final/reward_run Mean           4.81637
evaluation/env_infos/final/reward_run Std            0.839865
evaluation/env_infos/final/reward_run Max            5.95755
evaluation/env_infos/final/reward_run Min            3.80918
evaluation/env_infos/initial/reward_run Mean         0.170823
evaluation/env_infos/initial/reward_run Std          0.0595638
evaluation/env_infos/initial/reward_run Max          0.237883
evaluation/env_infos/initial/reward_run Min          0.0666751
evaluation/env_infos/reward_run Mean                 4.66127
evaluation/env_infos/reward_run Std                  1.04636
evaluation/env_infos/reward_run Max                  6.87519
evaluation/env_infos/reward_run Min                 -0.632987
evaluation/env_infos/final/reward_ctrl Mean         -0.445153
evaluation/env_infos/final/reward_ctrl Std           0.116691
evaluation/env_infos/final/reward_ctrl Max          -0.227447
evaluation/env_infos/final/reward_ctrl Min          -0.543706
evaluation/env_infos/initial/reward_ctrl Mean       -0.0249224
evaluation/env_infos/initial/reward_ctrl Std         0.00700108
evaluation/env_infos/initial/reward_ctrl Max        -0.0167927
evaluation/env_infos/initial/reward_ctrl Min        -0.0363826
evaluation/env_infos/reward_ctrl Mean               -0.418235
evaluation/env_infos/reward_ctrl Std                 0.0983577
evaluation/env_infos/reward_ctrl Max                -0.0167927
evaluation/env_infos/reward_ctrl Min                -0.580346
time/data storing (s)                                0.0121834
time/evaluation sampling (s)                         4.22689
time/exploration sampling (s)                        0.670257
time/logging (s)                                     0.0412497
time/saving (s)                                      0.021585
time/training (s)                                   37.5469
time/epoch (s)                                      42.5191
time/total (s)                                    5872.68
Epoch                                              139
----------------------------------------------  ---------------
2020-07-08 22:44:44.728726 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 140 finished
----------------------------------------------  ---------------
replay_buffer/size                              142000
trainer/QF1 Loss                                     8.39858
trainer/QF2 Loss                                     8.86046
trainer/Policy Loss                               -221.777
trainer/Q1 Predictions Mean                        227.598
trainer/Q1 Predictions Std                          92.098
trainer/Q1 Predictions Max                         306.988
trainer/Q1 Predictions Min                           8.75787
trainer/Q2 Predictions Mean                        227.209
trainer/Q2 Predictions Std                          91.8316
trainer/Q2 Predictions Max                         306.643
trainer/Q2 Predictions Min                           9.06725
trainer/Q Targets Mean                             227.238
trainer/Q Targets Std                               92.006
trainer/Q Targets Max                              308.428
trainer/Q Targets Min                                7.5179
trainer/Log Pis Mean                                 5.86049
trainer/Log Pis Std                                  5.18439
trainer/Log Pis Max                                 21.6083
trainer/Log Pis Min                                 -5.01452
trainer/Policy mu Mean                               0.02063
trainer/Policy mu Std                                1.49717
trainer/Policy mu Max                                4.99459
trainer/Policy mu Min                               -3.67949
trainer/Policy log std Mean                         -0.823485
trainer/Policy log std Std                           0.323693
trainer/Policy log std Max                          -0.0922292
trainer/Policy log std Min                          -2.2764
trainer/Alpha                                        0.091746
trainer/Alpha Loss                                  -0.333256
exploration/num steps total                     142000
exploration/num paths total                        142
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.19288
exploration/Rewards Std                              1.06596
exploration/Rewards Max                              6.38052
exploration/Rewards Min                             -0.719595
exploration/Returns Mean                          4192.88
exploration/Returns Std                              0
exploration/Returns Max                           4192.88
exploration/Returns Min                           4192.88
exploration/Actions Mean                            -0.0268915
exploration/Actions Std                              0.825114
exploration/Actions Max                              0.999619
exploration/Actions Min                             -0.999666
exploration/Num Paths                                1
exploration/Average Returns                       4192.88
exploration/env_infos/final/reward_run Mean          4.45369
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.45369
exploration/env_infos/final/reward_run Min           4.45369
exploration/env_infos/initial/reward_run Mean       -0.0281384
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0281384
exploration/env_infos/initial/reward_run Min        -0.0281384
exploration/env_infos/reward_run Mean                4.6018
exploration/env_infos/reward_run Std                 1.06379
exploration/env_infos/reward_run Max                 6.79333
exploration/env_infos/reward_run Min                -0.190094
exploration/env_infos/final/reward_ctrl Mean        -0.340587
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.340587
exploration/env_infos/final/reward_ctrl Min         -0.340587
exploration/env_infos/initial/reward_ctrl Mean      -0.0251429
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0251429
exploration/env_infos/initial/reward_ctrl Min       -0.0251429
exploration/env_infos/reward_ctrl Mean              -0.408922
exploration/env_infos/reward_ctrl Std                0.1009
exploration/env_infos/reward_ctrl Max               -0.0251429
exploration/env_infos/reward_ctrl Min               -0.581748
evaluation/num steps total                      705000
evaluation/num paths total                         705
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.27917
evaluation/Rewards Std                               1.06792
evaluation/Rewards Max                               6.56944
evaluation/Rewards Min                              -0.809404
evaluation/Returns Mean                           4279.17
evaluation/Returns Std                             112.043
evaluation/Returns Max                            4444.07
evaluation/Returns Min                            4137.11
evaluation/Actions Mean                             -0.0337728
evaluation/Actions Std                               0.835168
evaluation/Actions Max                               0.999685
evaluation/Actions Min                              -0.999069
evaluation/Num Paths                                 5
evaluation/Average Returns                        4279.17
evaluation/env_infos/final/reward_run Mean           4.59242
evaluation/env_infos/final/reward_run Std            0.749811
evaluation/env_infos/final/reward_run Max            5.87146
evaluation/env_infos/final/reward_run Min            3.5776
evaluation/env_infos/initial/reward_run Mean         0.207549
evaluation/env_infos/initial/reward_run Std          0.112812
evaluation/env_infos/initial/reward_run Max          0.407186
evaluation/env_infos/initial/reward_run Min          0.0879087
evaluation/env_infos/reward_run Mean                 4.69836
evaluation/env_infos/reward_run Std                  1.05954
evaluation/env_infos/reward_run Max                  7.02865
evaluation/env_infos/reward_run Min                 -0.374547
evaluation/env_infos/final/reward_ctrl Mean         -0.433766
evaluation/env_infos/final/reward_ctrl Std           0.0854528
evaluation/env_infos/final/reward_ctrl Max          -0.323091
evaluation/env_infos/final/reward_ctrl Min          -0.530066
evaluation/env_infos/initial/reward_ctrl Mean       -0.0344529
evaluation/env_infos/initial/reward_ctrl Std         0.0197446
evaluation/env_infos/initial/reward_ctrl Max        -0.0143556
evaluation/env_infos/initial/reward_ctrl Min        -0.0712999
evaluation/env_infos/reward_ctrl Mean               -0.419187
evaluation/env_infos/reward_ctrl Std                 0.101392
evaluation/env_infos/reward_ctrl Max                -0.0143556
evaluation/env_infos/reward_ctrl Min                -0.585264
time/data storing (s)                                0.00680696
time/evaluation sampling (s)                         2.45218
time/exploration sampling (s)                        0.669105
time/logging (s)                                     0.0530599
time/saving (s)                                      0.0188275
time/training (s)                                   37.3559
time/epoch (s)                                      40.5559
time/total (s)                                    5913.26
Epoch                                              140
----------------------------------------------  ---------------
2020-07-08 22:45:23.698569 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 141 finished
----------------------------------------------  ---------------
replay_buffer/size                              143000
trainer/QF1 Loss                                     8.22382
trainer/QF2 Loss                                     8.53175
trainer/Policy Loss                               -213.194
trainer/Q1 Predictions Mean                        218.738
trainer/Q1 Predictions Std                         101.188
trainer/Q1 Predictions Max                         308.865
trainer/Q1 Predictions Min                           9.34261
trainer/Q2 Predictions Mean                        218.722
trainer/Q2 Predictions Std                         101.257
trainer/Q2 Predictions Max                         310.439
trainer/Q2 Predictions Min                           8.92617
trainer/Q Targets Mean                             218.733
trainer/Q Targets Std                              101.366
trainer/Q Targets Max                              308.494
trainer/Q Targets Min                                7.04814
trainer/Log Pis Mean                                 5.56512
trainer/Log Pis Std                                  5.16878
trainer/Log Pis Max                                 19.0027
trainer/Log Pis Min                                 -5.79941
trainer/Policy mu Mean                               0.0252421
trainer/Policy mu Std                                1.48788
trainer/Policy mu Max                                4.63836
trainer/Policy mu Min                               -3.93084
trainer/Policy log std Mean                         -0.805202
trainer/Policy log std Std                           0.355273
trainer/Policy log std Max                           0.0318458
trainer/Policy log std Min                          -2.29792
trainer/Alpha                                        0.0916688
trainer/Alpha Loss                                  -1.03913
exploration/num steps total                     143000
exploration/num paths total                        143
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.9701
exploration/Rewards Std                              1.07529
exploration/Rewards Max                              6.01517
exploration/Rewards Min                             -0.745328
exploration/Returns Mean                          3970.1
exploration/Returns Std                              0
exploration/Returns Max                           3970.1
exploration/Returns Min                           3970.1
exploration/Actions Mean                            -0.00139512
exploration/Actions Std                              0.818655
exploration/Actions Max                              0.999955
exploration/Actions Min                             -0.999815
exploration/Num Paths                                1
exploration/Average Returns                       3970.1
exploration/env_infos/final/reward_run Mean          5.47176
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.47176
exploration/env_infos/final/reward_run Min           5.47176
exploration/env_infos/initial/reward_run Mean       -0.22793
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.22793
exploration/env_infos/initial/reward_run Min        -0.22793
exploration/env_infos/reward_run Mean                4.37222
exploration/env_infos/reward_run Std                 1.06367
exploration/env_infos/reward_run Max                 6.36306
exploration/env_infos/reward_run Min                -0.258853
exploration/env_infos/final/reward_ctrl Mean        -0.2669
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.2669
exploration/env_infos/final/reward_ctrl Min         -0.2669
exploration/env_infos/initial/reward_ctrl Mean      -0.00905485
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.00905485
exploration/env_infos/initial/reward_ctrl Min       -0.00905485
exploration/env_infos/reward_ctrl Mean              -0.402119
exploration/env_infos/reward_ctrl Std                0.100123
exploration/env_infos/reward_ctrl Max               -0.00905485
exploration/env_infos/reward_ctrl Min               -0.584985
evaluation/num steps total                      710000
evaluation/num paths total                         710
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.31399
evaluation/Rewards Std                               1.07656
evaluation/Rewards Max                               6.60259
evaluation/Rewards Min                              -0.850349
evaluation/Returns Mean                           4313.99
evaluation/Returns Std                              94.7495
evaluation/Returns Max                            4425.01
evaluation/Returns Min                            4171.98
evaluation/Actions Mean                             -0.0196085
evaluation/Actions Std                               0.837254
evaluation/Actions Max                               0.999761
evaluation/Actions Min                              -0.999397
evaluation/Num Paths                                 5
evaluation/Average Returns                        4313.99
evaluation/env_infos/final/reward_run Mean           4.23488
evaluation/env_infos/final/reward_run Std            1.0112
evaluation/env_infos/final/reward_run Max            6.08134
evaluation/env_infos/final/reward_run Min            3.27402
evaluation/env_infos/initial/reward_run Mean        -0.0958116
evaluation/env_infos/initial/reward_run Std          0.104683
evaluation/env_infos/initial/reward_run Max          0.086925
evaluation/env_infos/initial/reward_run Min         -0.21771
evaluation/env_infos/reward_run Mean                 4.73481
evaluation/env_infos/reward_run Std                  1.06628
evaluation/env_infos/reward_run Max                  6.92394
evaluation/env_infos/reward_run Min                 -0.434152
evaluation/env_infos/final/reward_ctrl Mean         -0.513143
evaluation/env_infos/final/reward_ctrl Std           0.0310755
evaluation/env_infos/final/reward_ctrl Max          -0.467305
evaluation/env_infos/final/reward_ctrl Min          -0.561837
evaluation/env_infos/initial/reward_ctrl Mean       -0.0384396
evaluation/env_infos/initial/reward_ctrl Std         0.022212
evaluation/env_infos/initial/reward_ctrl Max        -0.0116067
evaluation/env_infos/initial/reward_ctrl Min        -0.0776766
evaluation/env_infos/reward_ctrl Mean               -0.420828
evaluation/env_infos/reward_ctrl Std                 0.0954986
evaluation/env_infos/reward_ctrl Max                -0.0116067
evaluation/env_infos/reward_ctrl Min                -0.583059
time/data storing (s)                                0.0149225
time/evaluation sampling (s)                         3.74238
time/exploration sampling (s)                        1.23745
time/logging (s)                                     0.0434325
time/saving (s)                                      0.0174124
time/training (s)                                   33.8581
time/epoch (s)                                      38.9137
time/total (s)                                    5952.22
Epoch                                              141
----------------------------------------------  ---------------
2020-07-08 22:45:58.606545 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 142 finished
----------------------------------------------  ---------------
replay_buffer/size                              144000
trainer/QF1 Loss                                     7.75445
trainer/QF2 Loss                                     9.08574
trainer/Policy Loss                               -210.37
trainer/Q1 Predictions Mean                        215.782
trainer/Q1 Predictions Std                         100.916
trainer/Q1 Predictions Max                         302.799
trainer/Q1 Predictions Min                          10.5011
trainer/Q2 Predictions Mean                        216.37
trainer/Q2 Predictions Std                         101.197
trainer/Q2 Predictions Max                         304.755
trainer/Q2 Predictions Min                          10.7133
trainer/Q Targets Mean                             216.407
trainer/Q Targets Std                              101.394
trainer/Q Targets Max                              304.383
trainer/Q Targets Min                               10.2092
trainer/Log Pis Mean                                 5.71386
trainer/Log Pis Std                                  5.28081
trainer/Log Pis Max                                 22.4132
trainer/Log Pis Min                                 -5.88004
trainer/Policy mu Mean                               0.0102068
trainer/Policy mu Std                                1.47772
trainer/Policy mu Max                                3.30287
trainer/Policy mu Min                               -4.51977
trainer/Policy log std Mean                         -0.781732
trainer/Policy log std Std                           0.345261
trainer/Policy log std Max                           0.614113
trainer/Policy log std Min                          -2.42315
trainer/Alpha                                        0.0919448
trainer/Alpha Loss                                  -0.682862
exploration/num steps total                     144000
exploration/num paths total                        144
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.00569
exploration/Rewards Std                              1.05596
exploration/Rewards Max                              6.30641
exploration/Rewards Min                             -0.869788
exploration/Returns Mean                          4005.69
exploration/Returns Std                              0
exploration/Returns Max                           4005.69
exploration/Returns Min                           4005.69
exploration/Actions Mean                            -0.0235667
exploration/Actions Std                              0.833662
exploration/Actions Max                              0.999944
exploration/Actions Min                             -0.999582
exploration/Num Paths                                1
exploration/Average Returns                       4005.69
exploration/env_infos/final/reward_run Mean          4.15196
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.15196
exploration/env_infos/final/reward_run Min           4.15196
exploration/env_infos/initial/reward_run Mean       -0.0419821
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0419821
exploration/env_infos/initial/reward_run Min        -0.0419821
exploration/env_infos/reward_run Mean                4.42302
exploration/env_infos/reward_run Std                 1.05413
exploration/env_infos/reward_run Max                 6.6603
exploration/env_infos/reward_run Min                -0.414701
exploration/env_infos/final/reward_ctrl Mean        -0.517853
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.517853
exploration/env_infos/final/reward_ctrl Min         -0.517853
exploration/env_infos/initial/reward_ctrl Mean      -0.0678612
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0678612
exploration/env_infos/initial/reward_ctrl Min       -0.0678612
exploration/env_infos/reward_ctrl Mean              -0.417329
exploration/env_infos/reward_ctrl Std                0.0907413
exploration/env_infos/reward_ctrl Max               -0.0639127
exploration/env_infos/reward_ctrl Min               -0.576603
evaluation/num steps total                      715000
evaluation/num paths total                         715
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.1333
evaluation/Rewards Std                               1.07045
evaluation/Rewards Max                               6.35336
evaluation/Rewards Min                              -0.977779
evaluation/Returns Mean                           4133.3
evaluation/Returns Std                              91.21
evaluation/Returns Max                            4290.11
evaluation/Returns Min                            4023.29
evaluation/Actions Mean                             -0.0381076
evaluation/Actions Std                               0.84917
evaluation/Actions Max                               0.999043
evaluation/Actions Min                              -0.999279
evaluation/Num Paths                                 5
evaluation/Average Returns                        4133.3
evaluation/env_infos/final/reward_run Mean           4.1152
evaluation/env_infos/final/reward_run Std            0.738132
evaluation/env_infos/final/reward_run Max            5.25364
evaluation/env_infos/final/reward_run Min            3.16115
evaluation/env_infos/initial/reward_run Mean         0.0196378
evaluation/env_infos/initial/reward_run Std          0.0973497
evaluation/env_infos/initial/reward_run Max          0.10467
evaluation/env_infos/initial/reward_run Min         -0.126051
evaluation/env_infos/reward_run Mean                 4.56683
evaluation/env_infos/reward_run Std                  1.06752
evaluation/env_infos/reward_run Max                  6.78765
evaluation/env_infos/reward_run Min                 -0.503842
evaluation/env_infos/final/reward_ctrl Mean         -0.372029
evaluation/env_infos/final/reward_ctrl Std           0.0702847
evaluation/env_infos/final/reward_ctrl Max          -0.278783
evaluation/env_infos/final/reward_ctrl Min          -0.493099
evaluation/env_infos/initial/reward_ctrl Mean       -0.0169341
evaluation/env_infos/initial/reward_ctrl Std         0.00816393
evaluation/env_infos/initial/reward_ctrl Max        -0.0046675
evaluation/env_infos/initial/reward_ctrl Min        -0.0294671
evaluation/env_infos/reward_ctrl Mean               -0.433525
evaluation/env_infos/reward_ctrl Std                 0.092193
evaluation/env_infos/reward_ctrl Max                -0.0046675
evaluation/env_infos/reward_ctrl Min                -0.591517
time/data storing (s)                                0.00681866
time/evaluation sampling (s)                         2.64877
time/exploration sampling (s)                        0.68243
time/logging (s)                                     0.043318
time/saving (s)                                      0.0162081
time/training (s)                                   31.4926
time/epoch (s)                                      34.8901
time/total (s)                                    5987.12
Epoch                                              142
----------------------------------------------  ---------------
2020-07-08 22:46:33.209257 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 143 finished
----------------------------------------------  ---------------
replay_buffer/size                              145000
trainer/QF1 Loss                                     9.7751
trainer/QF2 Loss                                     8.24441
trainer/Policy Loss                               -229.337
trainer/Q1 Predictions Mean                        235.344
trainer/Q1 Predictions Std                          87.8209
trainer/Q1 Predictions Max                         308.709
trainer/Q1 Predictions Min                           8.78669
trainer/Q2 Predictions Mean                        235.245
trainer/Q2 Predictions Std                          87.7892
trainer/Q2 Predictions Max                         307.663
trainer/Q2 Predictions Min                           9.87589
trainer/Q Targets Mean                             235.104
trainer/Q Targets Std                               87.7337
trainer/Q Targets Max                              308.301
trainer/Q Targets Min                                8.87566
trainer/Log Pis Mean                                 6.01629
trainer/Log Pis Std                                  4.9649
trainer/Log Pis Max                                 23.6945
trainer/Log Pis Min                                 -5.39107
trainer/Policy mu Mean                               0.0309085
trainer/Policy mu Std                                1.49778
trainer/Policy mu Max                                4.83519
trainer/Policy mu Min                               -4.37164
trainer/Policy log std Mean                         -0.832861
trainer/Policy log std Std                           0.359857
trainer/Policy log std Max                           0.102353
trainer/Policy log std Min                          -2.49496
trainer/Alpha                                        0.0923626
trainer/Alpha Loss                                   0.0388093
exploration/num steps total                     145000
exploration/num paths total                        145
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.98146
exploration/Rewards Std                              1.15048
exploration/Rewards Max                              6.31222
exploration/Rewards Min                             -1.87205
exploration/Returns Mean                          3981.46
exploration/Returns Std                              0
exploration/Returns Max                           3981.46
exploration/Returns Min                           3981.46
exploration/Actions Mean                             0.00151283
exploration/Actions Std                              0.806312
exploration/Actions Max                              0.999899
exploration/Actions Min                             -0.999987
exploration/Num Paths                                1
exploration/Average Returns                       3981.46
exploration/env_infos/final/reward_run Mean          6.58595
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.58595
exploration/env_infos/final/reward_run Min           6.58595
exploration/env_infos/initial/reward_run Mean       -0.175489
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.175489
exploration/env_infos/initial/reward_run Min        -0.175489
exploration/env_infos/reward_run Mean                4.37154
exploration/env_infos/reward_run Std                 1.145
exploration/env_infos/reward_run Max                 6.6375
exploration/env_infos/reward_run Min                -1.64761
exploration/env_infos/final/reward_ctrl Mean        -0.273728
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.273728
exploration/env_infos/final/reward_ctrl Min         -0.273728
exploration/env_infos/initial/reward_ctrl Mean      -0.113109
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.113109
exploration/env_infos/initial/reward_ctrl Min       -0.113109
exploration/env_infos/reward_ctrl Mean              -0.390084
exploration/env_infos/reward_ctrl Std                0.0924579
exploration/env_infos/reward_ctrl Max               -0.113109
exploration/env_infos/reward_ctrl Min               -0.593201
evaluation/num steps total                      720000
evaluation/num paths total                         720
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.24395
evaluation/Rewards Std                               1.08016
evaluation/Rewards Max                               6.44745
evaluation/Rewards Min                              -1.01869
evaluation/Returns Mean                           4243.95
evaluation/Returns Std                              38.2387
evaluation/Returns Max                            4297.15
evaluation/Returns Min                            4209
evaluation/Actions Mean                             -0.00915094
evaluation/Actions Std                               0.816132
evaluation/Actions Max                               0.99975
evaluation/Actions Min                              -0.99902
evaluation/Num Paths                                 5
evaluation/Average Returns                        4243.95
evaluation/env_infos/final/reward_run Mean           4.77432
evaluation/env_infos/final/reward_run Std            0.534348
evaluation/env_infos/final/reward_run Max            5.47367
evaluation/env_infos/final/reward_run Min            4.14133
evaluation/env_infos/initial/reward_run Mean        -0.0267217
evaluation/env_infos/initial/reward_run Std          0.182175
evaluation/env_infos/initial/reward_run Max          0.285842
evaluation/env_infos/initial/reward_run Min         -0.240498
evaluation/env_infos/reward_run Mean                 4.64364
evaluation/env_infos/reward_run Std                  1.0721
evaluation/env_infos/reward_run Max                  6.90981
evaluation/env_infos/reward_run Min                 -0.532541
evaluation/env_infos/final/reward_ctrl Mean         -0.363088
evaluation/env_infos/final/reward_ctrl Std           0.056919
evaluation/env_infos/final/reward_ctrl Max          -0.301362
evaluation/env_infos/final/reward_ctrl Min          -0.463381
evaluation/env_infos/initial/reward_ctrl Mean       -0.0245084
evaluation/env_infos/initial/reward_ctrl Std         0.0164023
evaluation/env_infos/initial/reward_ctrl Max        -0.0044712
evaluation/env_infos/initial/reward_ctrl Min        -0.0537699
evaluation/env_infos/reward_ctrl Mean               -0.399693
evaluation/env_infos/reward_ctrl Std                 0.0962064
evaluation/env_infos/reward_ctrl Max                -0.0044712
evaluation/env_infos/reward_ctrl Min                -0.587657
time/data storing (s)                                0.00706971
time/evaluation sampling (s)                         2.53459
time/exploration sampling (s)                        0.640484
time/logging (s)                                     0.0421621
time/saving (s)                                      0.0187984
time/training (s)                                   31.341
time/epoch (s)                                      34.5841
time/total (s)                                    6021.72
Epoch                                              143
----------------------------------------------  ---------------
2020-07-08 22:47:07.911112 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 144 finished
----------------------------------------------  ---------------
replay_buffer/size                              146000
trainer/QF1 Loss                                     8.84952
trainer/QF2 Loss                                     7.61769
trainer/Policy Loss                               -218.518
trainer/Q1 Predictions Mean                        223.93
trainer/Q1 Predictions Std                          98.224
trainer/Q1 Predictions Max                         310.721
trainer/Q1 Predictions Min                          10.6352
trainer/Q2 Predictions Mean                        224.278
trainer/Q2 Predictions Std                          98.1722
trainer/Q2 Predictions Max                         311.697
trainer/Q2 Predictions Min                          10.9767
trainer/Q Targets Mean                             223.598
trainer/Q Targets Std                               97.9325
trainer/Q Targets Max                              309.745
trainer/Q Targets Min                                9.75703
trainer/Log Pis Mean                                 5.95762
trainer/Log Pis Std                                  5.25052
trainer/Log Pis Max                                 20.5131
trainer/Log Pis Min                                 -6.35119
trainer/Policy mu Mean                               0.107344
trainer/Policy mu Std                                1.49616
trainer/Policy mu Max                                4.15951
trainer/Policy mu Min                               -3.99586
trainer/Policy log std Mean                         -0.811895
trainer/Policy log std Std                           0.35276
trainer/Policy log std Max                          -0.0155842
trainer/Policy log std Min                          -2.38859
trainer/Alpha                                        0.0937016
trainer/Alpha Loss                                  -0.100344
exploration/num steps total                     146000
exploration/num paths total                        146
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.1436
exploration/Rewards Std                              1.09409
exploration/Rewards Max                              6.39015
exploration/Rewards Min                             -0.721528
exploration/Returns Mean                          4143.6
exploration/Returns Std                              0
exploration/Returns Max                           4143.6
exploration/Returns Min                           4143.6
exploration/Actions Mean                            -0.00499879
exploration/Actions Std                              0.835011
exploration/Actions Max                              0.999942
exploration/Actions Min                             -0.999933
exploration/Num Paths                                1
exploration/Average Returns                       4143.6
exploration/env_infos/final/reward_run Mean          5.43115
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.43115
exploration/env_infos/final/reward_run Min           5.43115
exploration/env_infos/initial/reward_run Mean        0.0651893
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0651893
exploration/env_infos/initial/reward_run Min         0.0651893
exploration/env_infos/reward_run Mean                4.56196
exploration/env_infos/reward_run Std                 1.08627
exploration/env_infos/reward_run Max                 6.69629
exploration/env_infos/reward_run Min                -0.283488
exploration/env_infos/final/reward_ctrl Mean        -0.43256
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.43256
exploration/env_infos/final/reward_ctrl Min         -0.43256
exploration/env_infos/initial/reward_ctrl Mean      -0.023737
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.023737
exploration/env_infos/initial/reward_ctrl Min       -0.023737
exploration/env_infos/reward_ctrl Mean              -0.418361
exploration/env_infos/reward_ctrl Std                0.0963395
exploration/env_infos/reward_ctrl Max               -0.023737
exploration/env_infos/reward_ctrl Min               -0.583927
evaluation/num steps total                      725000
evaluation/num paths total                         725
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.12965
evaluation/Rewards Std                               1.12873
evaluation/Rewards Max                               6.5576
evaluation/Rewards Min                              -0.850863
evaluation/Returns Mean                           4129.65
evaluation/Returns Std                             147.889
evaluation/Returns Max                            4289.63
evaluation/Returns Min                            3874.63
evaluation/Actions Mean                             -0.028852
evaluation/Actions Std                               0.849104
evaluation/Actions Max                               0.999252
evaluation/Actions Min                              -0.999097
evaluation/Num Paths                                 5
evaluation/Average Returns                        4129.65
evaluation/env_infos/final/reward_run Mean           4.29506
evaluation/env_infos/final/reward_run Std            0.759304
evaluation/env_infos/final/reward_run Max            5.23099
evaluation/env_infos/final/reward_run Min            3.15054
evaluation/env_infos/initial/reward_run Mean         0.0873752
evaluation/env_infos/initial/reward_run Std          0.105473
evaluation/env_infos/initial/reward_run Max          0.272419
evaluation/env_infos/initial/reward_run Min         -0.0511491
evaluation/env_infos/reward_run Mean                 4.56273
evaluation/env_infos/reward_run Std                  1.12167
evaluation/env_infos/reward_run Max                  6.88596
evaluation/env_infos/reward_run Min                 -0.37055
evaluation/env_infos/final/reward_ctrl Mean         -0.446599
evaluation/env_infos/final/reward_ctrl Std           0.0750117
evaluation/env_infos/final/reward_ctrl Max          -0.347137
evaluation/env_infos/final/reward_ctrl Min          -0.555222
evaluation/env_infos/initial/reward_ctrl Mean       -0.0419663
evaluation/env_infos/initial/reward_ctrl Std         0.0182999
evaluation/env_infos/initial/reward_ctrl Max        -0.0150803
evaluation/env_infos/initial/reward_ctrl Min        -0.0689057
evaluation/env_infos/reward_ctrl Mean               -0.433086
evaluation/env_infos/reward_ctrl Std                 0.095382
evaluation/env_infos/reward_ctrl Max                -0.0150803
evaluation/env_infos/reward_ctrl Min                -0.585325
time/data storing (s)                                0.00666568
time/evaluation sampling (s)                         2.53522
time/exploration sampling (s)                        0.689318
time/logging (s)                                     0.0407819
time/saving (s)                                      0.0159367
time/training (s)                                   31.3935
time/epoch (s)                                      34.6815
time/total (s)                                    6056.41
Epoch                                              144
----------------------------------------------  ---------------
2020-07-08 22:47:44.151851 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 145 finished
----------------------------------------------  ---------------
replay_buffer/size                              147000
trainer/QF1 Loss                                     7.52234
trainer/QF2 Loss                                     9.12755
trainer/Policy Loss                               -220.386
trainer/Q1 Predictions Mean                        226.536
trainer/Q1 Predictions Std                          96.6552
trainer/Q1 Predictions Max                         307.725
trainer/Q1 Predictions Min                          11.0682
trainer/Q2 Predictions Mean                        226.035
trainer/Q2 Predictions Std                          96.3247
trainer/Q2 Predictions Max                         305.675
trainer/Q2 Predictions Min                          11.2466
trainer/Q Targets Mean                             226.133
trainer/Q Targets Std                               96.4677
trainer/Q Targets Max                              308.52
trainer/Q Targets Min                               11.042
trainer/Log Pis Mean                                 5.91388
trainer/Log Pis Std                                  5.12673
trainer/Log Pis Max                                 21.3791
trainer/Log Pis Min                                 -4.99944
trainer/Policy mu Mean                               0.0216877
trainer/Policy mu Std                                1.53423
trainer/Policy mu Max                                5.09754
trainer/Policy mu Min                               -3.87849
trainer/Policy log std Mean                         -0.793121
trainer/Policy log std Std                           0.324432
trainer/Policy log std Max                           0.166052
trainer/Policy log std Min                          -2.53117
trainer/Alpha                                        0.0950113
trainer/Alpha Loss                                  -0.202706
exploration/num steps total                     147000
exploration/num paths total                        147
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.34806
exploration/Rewards Std                              1.05896
exploration/Rewards Max                              6.48954
exploration/Rewards Min                             -0.466009
exploration/Returns Mean                          4348.06
exploration/Returns Std                              0
exploration/Returns Max                           4348.06
exploration/Returns Min                           4348.06
exploration/Actions Mean                             0.0113632
exploration/Actions Std                              0.817201
exploration/Actions Max                              0.999927
exploration/Actions Min                             -0.999535
exploration/Num Paths                                1
exploration/Average Returns                       4348.06
exploration/env_infos/final/reward_run Mean          3.23274
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.23274
exploration/env_infos/final/reward_run Min           3.23274
exploration/env_infos/initial/reward_run Mean       -0.0913678
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0913678
exploration/env_infos/initial/reward_run Min        -0.0913678
exploration/env_infos/reward_run Mean                4.74883
exploration/env_infos/reward_run Std                 1.05169
exploration/env_infos/reward_run Max                 6.88377
exploration/env_infos/reward_run Min                -0.0913678
exploration/env_infos/final/reward_ctrl Mean        -0.460364
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.460364
exploration/env_infos/final/reward_ctrl Min         -0.460364
exploration/env_infos/initial/reward_ctrl Mean      -0.0634513
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0634513
exploration/env_infos/initial/reward_ctrl Min       -0.0634513
exploration/env_infos/reward_ctrl Mean              -0.400768
exploration/env_infos/reward_ctrl Std                0.0963426
exploration/env_infos/reward_ctrl Max               -0.0634513
exploration/env_infos/reward_ctrl Min               -0.569348
evaluation/num steps total                      730000
evaluation/num paths total                         730
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.35637
evaluation/Rewards Std                               1.09663
evaluation/Rewards Max                               6.69442
evaluation/Rewards Min                              -0.915108
evaluation/Returns Mean                           4356.37
evaluation/Returns Std                              82.485
evaluation/Returns Max                            4416.22
evaluation/Returns Min                            4197.15
evaluation/Actions Mean                             -0.00461869
evaluation/Actions Std                               0.843734
evaluation/Actions Max                               0.999877
evaluation/Actions Min                              -0.999741
evaluation/Num Paths                                 5
evaluation/Average Returns                        4356.37
evaluation/env_infos/final/reward_run Mean           4.32561
evaluation/env_infos/final/reward_run Std            0.563693
evaluation/env_infos/final/reward_run Max            5.22569
evaluation/env_infos/final/reward_run Min            3.71464
evaluation/env_infos/initial/reward_run Mean         0.25129
evaluation/env_infos/initial/reward_run Std          0.261547
evaluation/env_infos/initial/reward_run Max          0.69804
evaluation/env_infos/initial/reward_run Min         -0.0332301
evaluation/env_infos/reward_run Mean                 4.78351
evaluation/env_infos/reward_run Std                  1.08733
evaluation/env_infos/reward_run Max                  7.11344
evaluation/env_infos/reward_run Min                 -0.403584
evaluation/env_infos/final/reward_ctrl Mean         -0.430866
evaluation/env_infos/final/reward_ctrl Std           0.0790237
evaluation/env_infos/final/reward_ctrl Max          -0.335878
evaluation/env_infos/final/reward_ctrl Min          -0.529618
evaluation/env_infos/initial/reward_ctrl Mean       -0.0440612
evaluation/env_infos/initial/reward_ctrl Std         0.0237875
evaluation/env_infos/initial/reward_ctrl Max        -0.0185864
evaluation/env_infos/initial/reward_ctrl Min        -0.0889323
evaluation/env_infos/reward_ctrl Mean               -0.427145
evaluation/env_infos/reward_ctrl Std                 0.0912853
evaluation/env_infos/reward_ctrl Max                -0.0185864
evaluation/env_infos/reward_ctrl Min                -0.581331
time/data storing (s)                                0.00653017
time/evaluation sampling (s)                         2.52915
time/exploration sampling (s)                        0.659965
time/logging (s)                                     0.0490377
time/saving (s)                                      0.0164595
time/training (s)                                   32.9649
time/epoch (s)                                      36.226
time/total (s)                                    6092.66
Epoch                                              145
----------------------------------------------  ---------------
2020-07-08 22:48:23.066038 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 146 finished
----------------------------------------------  ---------------
replay_buffer/size                              148000
trainer/QF1 Loss                                     9.47431
trainer/QF2 Loss                                     9.17954
trainer/Policy Loss                               -219.049
trainer/Q1 Predictions Mean                        224.383
trainer/Q1 Predictions Std                          97.9934
trainer/Q1 Predictions Max                         309.222
trainer/Q1 Predictions Min                          10.9397
trainer/Q2 Predictions Mean                        225.096
trainer/Q2 Predictions Std                          97.9589
trainer/Q2 Predictions Max                         309.852
trainer/Q2 Predictions Min                          10.1485
trainer/Q Targets Mean                             224.588
trainer/Q Targets Std                               97.7777
trainer/Q Targets Max                              308.923
trainer/Q Targets Min                               11.361
trainer/Log Pis Mean                                 6.15505
trainer/Log Pis Std                                  5.62381
trainer/Log Pis Max                                 23.4824
trainer/Log Pis Min                                 -6.13633
trainer/Policy mu Mean                              -0.0250043
trainer/Policy mu Std                                1.53363
trainer/Policy mu Max                                4.6696
trainer/Policy mu Min                               -3.92656
trainer/Policy log std Mean                         -0.821383
trainer/Policy log std Std                           0.341402
trainer/Policy log std Max                          -0.0309405
trainer/Policy log std Min                          -2.32232
trainer/Alpha                                        0.0938485
trainer/Alpha Loss                                   0.366859
exploration/num steps total                     148000
exploration/num paths total                        148
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.96393
exploration/Rewards Std                              1.00642
exploration/Rewards Max                              6.13676
exploration/Rewards Min                             -0.0238418
exploration/Returns Mean                          3963.93
exploration/Returns Std                              0
exploration/Returns Max                           3963.93
exploration/Returns Min                           3963.93
exploration/Actions Mean                            -0.060233
exploration/Actions Std                              0.835249
exploration/Actions Max                              0.999254
exploration/Actions Min                             -0.999899
exploration/Num Paths                                1
exploration/Average Returns                       3963.93
exploration/env_infos/final/reward_run Mean          2.70078
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.70078
exploration/env_infos/final/reward_run Min           2.70078
exploration/env_infos/initial/reward_run Mean        0.378814
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.378814
exploration/env_infos/initial/reward_run Min         0.378814
exploration/env_infos/reward_run Mean                4.38469
exploration/env_infos/reward_run Std                 0.994882
exploration/env_infos/reward_run Max                 6.48005
exploration/env_infos/reward_run Min                 0.243388
exploration/env_infos/final/reward_ctrl Mean        -0.411741
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.411741
exploration/env_infos/final/reward_ctrl Min         -0.411741
exploration/env_infos/initial/reward_ctrl Mean      -0.0832164
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0832164
exploration/env_infos/initial/reward_ctrl Min       -0.0832164
exploration/env_infos/reward_ctrl Mean              -0.420761
exploration/env_infos/reward_ctrl Std                0.0932558
exploration/env_infos/reward_ctrl Max               -0.0832164
exploration/env_infos/reward_ctrl Min               -0.582426
evaluation/num steps total                      735000
evaluation/num paths total                         735
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.17751
evaluation/Rewards Std                               1.0194
evaluation/Rewards Max                               6.34368
evaluation/Rewards Min                              -0.650299
evaluation/Returns Mean                           4177.51
evaluation/Returns Std                              30.5199
evaluation/Returns Max                            4221.49
evaluation/Returns Min                            4142.34
evaluation/Actions Mean                             -0.0657942
evaluation/Actions Std                               0.845987
evaluation/Actions Max                               0.998461
evaluation/Actions Min                              -0.999542
evaluation/Num Paths                                 5
evaluation/Average Returns                        4177.51
evaluation/env_infos/final/reward_run Mean           4.88152
evaluation/env_infos/final/reward_run Std            0.925236
evaluation/env_infos/final/reward_run Max            6.17523
evaluation/env_infos/final/reward_run Min            3.75036
evaluation/env_infos/initial/reward_run Mean        -0.0831742
evaluation/env_infos/initial/reward_run Std          0.10064
evaluation/env_infos/initial/reward_run Max          0.098202
evaluation/env_infos/initial/reward_run Min         -0.189767
evaluation/env_infos/reward_run Mean                 4.60952
evaluation/env_infos/reward_run Std                  1.00614
evaluation/env_infos/reward_run Max                  6.65099
evaluation/env_infos/reward_run Min                 -0.411503
evaluation/env_infos/final/reward_ctrl Mean         -0.447108
evaluation/env_infos/final/reward_ctrl Std           0.089826
evaluation/env_infos/final/reward_ctrl Max          -0.33839
evaluation/env_infos/final/reward_ctrl Min          -0.543812
evaluation/env_infos/initial/reward_ctrl Mean       -0.0519825
evaluation/env_infos/initial/reward_ctrl Std         0.0262547
evaluation/env_infos/initial/reward_ctrl Max        -0.0184366
evaluation/env_infos/initial/reward_ctrl Min        -0.0902233
evaluation/env_infos/reward_ctrl Mean               -0.432013
evaluation/env_infos/reward_ctrl Std                 0.0922761
evaluation/env_infos/reward_ctrl Max                -0.0184366
evaluation/env_infos/reward_ctrl Min                -0.579579
time/data storing (s)                                0.00692409
time/evaluation sampling (s)                         2.80615
time/exploration sampling (s)                        0.652459
time/logging (s)                                     0.0445485
time/saving (s)                                      0.0171608
time/training (s)                                   35.3636
time/epoch (s)                                      38.8908
time/total (s)                                    6131.56
Epoch                                              146
----------------------------------------------  ---------------
2020-07-08 22:49:04.629299 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 147 finished
----------------------------------------------  ---------------
replay_buffer/size                              149000
trainer/QF1 Loss                                     7.36868
trainer/QF2 Loss                                     9.06679
trainer/Policy Loss                               -226.757
trainer/Q1 Predictions Mean                        233.128
trainer/Q1 Predictions Std                          93.854
trainer/Q1 Predictions Max                         311.115
trainer/Q1 Predictions Min                          12.6983
trainer/Q2 Predictions Mean                        232.71
trainer/Q2 Predictions Std                          93.6208
trainer/Q2 Predictions Max                         309.768
trainer/Q2 Predictions Min                          11.6656
trainer/Q Targets Mean                             232.75
trainer/Q Targets Std                               93.6389
trainer/Q Targets Max                              314.578
trainer/Q Targets Min                               12.0152
trainer/Log Pis Mean                                 6.27017
trainer/Log Pis Std                                  5.28398
trainer/Log Pis Max                                 25.0113
trainer/Log Pis Min                                 -5.09883
trainer/Policy mu Mean                              -0.0140456
trainer/Policy mu Std                                1.52663
trainer/Policy mu Max                                4.15764
trainer/Policy mu Min                               -3.73619
trainer/Policy log std Mean                         -0.82579
trainer/Policy log std Std                           0.341374
trainer/Policy log std Max                          -0.0257653
trainer/Policy log std Min                          -2.36432
trainer/Alpha                                        0.0949739
trainer/Alpha Loss                                   0.636023
exploration/num steps total                     149000
exploration/num paths total                        149
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.99553
exploration/Rewards Std                              1.05721
exploration/Rewards Max                              6.15976
exploration/Rewards Min                             -0.721206
exploration/Returns Mean                          3995.53
exploration/Returns Std                              0
exploration/Returns Max                           3995.53
exploration/Returns Min                           3995.53
exploration/Actions Mean                            -0.0235148
exploration/Actions Std                              0.816853
exploration/Actions Max                              0.999979
exploration/Actions Min                             -0.999375
exploration/Num Paths                                1
exploration/Average Returns                       3995.53
exploration/env_infos/final/reward_run Mean          4.6995
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.6995
exploration/env_infos/final/reward_run Min           4.6995
exploration/env_infos/initial/reward_run Mean       -0.017355
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.017355
exploration/env_infos/initial/reward_run Min        -0.017355
exploration/env_infos/reward_run Mean                4.39621
exploration/env_infos/reward_run Std                 1.05055
exploration/env_infos/reward_run Max                 6.51962
exploration/env_infos/reward_run Min                -0.471718
exploration/env_infos/final/reward_ctrl Mean        -0.51452
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.51452
exploration/env_infos/final/reward_ctrl Min         -0.51452
exploration/env_infos/initial/reward_ctrl Mean      -0.0743835
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0743835
exploration/env_infos/initial/reward_ctrl Min       -0.0743835
exploration/env_infos/reward_ctrl Mean              -0.400681
exploration/env_infos/reward_ctrl Std                0.100084
exploration/env_infos/reward_ctrl Max               -0.0743835
exploration/env_infos/reward_ctrl Min               -0.583779
evaluation/num steps total                      740000
evaluation/num paths total                         740
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.28803
evaluation/Rewards Std                               1.06231
evaluation/Rewards Max                               6.47027
evaluation/Rewards Min                              -0.857849
evaluation/Returns Mean                           4288.03
evaluation/Returns Std                              84.2212
evaluation/Returns Max                            4407.45
evaluation/Returns Min                            4179.08
evaluation/Actions Mean                             -0.029807
evaluation/Actions Std                               0.837248
evaluation/Actions Max                               0.998253
evaluation/Actions Min                              -0.999063
evaluation/Num Paths                                 5
evaluation/Average Returns                        4288.03
evaluation/env_infos/final/reward_run Mean           4.67932
evaluation/env_infos/final/reward_run Std            1.03867
evaluation/env_infos/final/reward_run Max            6.0049
evaluation/env_infos/final/reward_run Min            3.5035
evaluation/env_infos/initial/reward_run Mean         0.133466
evaluation/env_infos/initial/reward_run Std          0.168575
evaluation/env_infos/initial/reward_run Max          0.382552
evaluation/env_infos/initial/reward_run Min         -0.0118135
evaluation/env_infos/reward_run Mean                 4.70915
evaluation/env_infos/reward_run Std                  1.05108
evaluation/env_infos/reward_run Max                  6.80244
evaluation/env_infos/reward_run Min                 -0.334655
evaluation/env_infos/final/reward_ctrl Mean         -0.513401
evaluation/env_infos/final/reward_ctrl Std           0.0471987
evaluation/env_infos/final/reward_ctrl Max          -0.42325
evaluation/env_infos/final/reward_ctrl Min          -0.560268
evaluation/env_infos/initial/reward_ctrl Mean       -0.0216543
evaluation/env_infos/initial/reward_ctrl Std         0.00981956
evaluation/env_infos/initial/reward_ctrl Max        -0.0064427
evaluation/env_infos/initial/reward_ctrl Min        -0.0322024
evaluation/env_infos/reward_ctrl Mean               -0.421123
evaluation/env_infos/reward_ctrl Std                 0.097802
evaluation/env_infos/reward_ctrl Max                -0.0064427
evaluation/env_infos/reward_ctrl Min                -0.589652
time/data storing (s)                                0.00718915
time/evaluation sampling (s)                         2.77533
time/exploration sampling (s)                        0.698137
time/logging (s)                                     0.0410772
time/saving (s)                                      0.0165801
time/training (s)                                   38.0032
time/epoch (s)                                      41.5415
time/total (s)                                    6173.11
Epoch                                              147
----------------------------------------------  ---------------
2020-07-08 22:49:48.978639 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 148 finished
----------------------------------------------  ---------------
replay_buffer/size                              150000
trainer/QF1 Loss                                     5.99326
trainer/QF2 Loss                                     8.32441
trainer/Policy Loss                               -212.44
trainer/Q1 Predictions Mean                        218.612
trainer/Q1 Predictions Std                         104.468
trainer/Q1 Predictions Max                         314.539
trainer/Q1 Predictions Min                          10.7173
trainer/Q2 Predictions Mean                        218.331
trainer/Q2 Predictions Std                         104.353
trainer/Q2 Predictions Max                         311.539
trainer/Q2 Predictions Min                          10.9411
trainer/Q Targets Mean                             218.116
trainer/Q Targets Std                              104.357
trainer/Q Targets Max                              313.701
trainer/Q Targets Min                                9.88484
trainer/Log Pis Mean                                 5.98255
trainer/Log Pis Std                                  4.99149
trainer/Log Pis Max                                 21.4865
trainer/Log Pis Min                                 -6.4721
trainer/Policy mu Mean                               0.0654151
trainer/Policy mu Std                                1.49574
trainer/Policy mu Max                                4.54742
trainer/Policy mu Min                               -3.898
trainer/Policy log std Mean                         -0.7891
trainer/Policy log std Std                           0.332765
trainer/Policy log std Max                           0.131562
trainer/Policy log std Min                          -2.27981
trainer/Alpha                                        0.0957435
trainer/Alpha Loss                                  -0.0409333
exploration/num steps total                     150000
exploration/num paths total                        150
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.10555
exploration/Rewards Std                              1.09174
exploration/Rewards Max                              6.33639
exploration/Rewards Min                             -0.96068
exploration/Returns Mean                          4105.55
exploration/Returns Std                              0
exploration/Returns Max                           4105.55
exploration/Returns Min                           4105.55
exploration/Actions Mean                            -0.0102826
exploration/Actions Std                              0.828119
exploration/Actions Max                              0.999725
exploration/Actions Min                             -0.999848
exploration/Num Paths                                1
exploration/Average Returns                       4105.55
exploration/env_infos/final/reward_run Mean          3.84312
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.84312
exploration/env_infos/final/reward_run Min           3.84312
exploration/env_infos/initial/reward_run Mean       -0.0660636
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0660636
exploration/env_infos/initial/reward_run Min        -0.0660636
exploration/env_infos/reward_run Mean                4.51708
exploration/env_infos/reward_run Std                 1.08469
exploration/env_infos/reward_run Max                 6.86757
exploration/env_infos/reward_run Min                -0.489551
exploration/env_infos/final/reward_ctrl Mean        -0.488756
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.488756
exploration/env_infos/final/reward_ctrl Min         -0.488756
exploration/env_infos/initial/reward_ctrl Mean      -0.0360135
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0360135
exploration/env_infos/initial/reward_ctrl Min       -0.0360135
exploration/env_infos/reward_ctrl Mean              -0.411532
exploration/env_infos/reward_ctrl Std                0.0922526
exploration/env_infos/reward_ctrl Max               -0.0360135
exploration/env_infos/reward_ctrl Min               -0.581401
evaluation/num steps total                      745000
evaluation/num paths total                         745
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.25054
evaluation/Rewards Std                               1.08363
evaluation/Rewards Max                               6.51885
evaluation/Rewards Min                              -0.977293
evaluation/Returns Mean                           4250.54
evaluation/Returns Std                              67.4881
evaluation/Returns Max                            4336.15
evaluation/Returns Min                            4128.16
evaluation/Actions Mean                             -0.0312616
evaluation/Actions Std                               0.853983
evaluation/Actions Max                               0.999884
evaluation/Actions Min                              -0.99971
evaluation/Num Paths                                 5
evaluation/Average Returns                        4250.54
evaluation/env_infos/final/reward_run Mean           4.8819
evaluation/env_infos/final/reward_run Std            0.919627
evaluation/env_infos/final/reward_run Max            6.06521
evaluation/env_infos/final/reward_run Min            3.84074
evaluation/env_infos/initial/reward_run Mean        -0.104284
evaluation/env_infos/initial/reward_run Std          0.089914
evaluation/env_infos/initial/reward_run Max         -0.0221922
evaluation/env_infos/initial/reward_run Min         -0.253267
evaluation/env_infos/reward_run Mean                 4.68869
evaluation/env_infos/reward_run Std                  1.0809
evaluation/env_infos/reward_run Max                  6.87042
evaluation/env_infos/reward_run Min                 -0.533928
evaluation/env_infos/final/reward_ctrl Mean         -0.459639
evaluation/env_infos/final/reward_ctrl Std           0.0905054
evaluation/env_infos/final/reward_ctrl Max          -0.317253
evaluation/env_infos/final/reward_ctrl Min          -0.561527
evaluation/env_infos/initial/reward_ctrl Mean       -0.0390385
evaluation/env_infos/initial/reward_ctrl Std         0.0170486
evaluation/env_infos/initial/reward_ctrl Max        -0.0197387
evaluation/env_infos/initial/reward_ctrl Min        -0.0662719
evaluation/env_infos/reward_ctrl Mean               -0.438159
evaluation/env_infos/reward_ctrl Std                 0.0874746
evaluation/env_infos/reward_ctrl Max                -0.0194701
evaluation/env_infos/reward_ctrl Min                -0.592984
time/data storing (s)                                0.00680099
time/evaluation sampling (s)                         2.8757
time/exploration sampling (s)                        1.7557
time/logging (s)                                     0.0414588
time/saving (s)                                      0.0175628
time/training (s)                                   39.6341
time/epoch (s)                                      44.3314
time/total (s)                                    6217.46
Epoch                                              148
----------------------------------------------  ---------------
2020-07-08 22:50:25.917079 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 149 finished
----------------------------------------------  ---------------
replay_buffer/size                              151000
trainer/QF1 Loss                                     6.08011
trainer/QF2 Loss                                     6.3383
trainer/Policy Loss                               -226.417
trainer/Q1 Predictions Mean                        231.893
trainer/Q1 Predictions Std                          97.5416
trainer/Q1 Predictions Max                         309.915
trainer/Q1 Predictions Min                          11.1188
trainer/Q2 Predictions Mean                        232.274
trainer/Q2 Predictions Std                          97.5379
trainer/Q2 Predictions Max                         312.919
trainer/Q2 Predictions Min                          12.1935
trainer/Q Targets Mean                             232.706
trainer/Q Targets Std                               97.8781
trainer/Q Targets Max                              316.699
trainer/Q Targets Min                               11.5195
trainer/Log Pis Mean                                 5.75016
trainer/Log Pis Std                                  4.75782
trainer/Log Pis Max                                 24.0531
trainer/Log Pis Min                                 -5.63851
trainer/Policy mu Mean                               0.106254
trainer/Policy mu Std                                1.44825
trainer/Policy mu Max                                3.78243
trainer/Policy mu Min                               -3.66369
trainer/Policy log std Mean                         -0.839531
trainer/Policy log std Std                           0.34769
trainer/Policy log std Max                           0.125507
trainer/Policy log std Min                          -2.38562
trainer/Alpha                                        0.0932087
trainer/Alpha Loss                                  -0.592842
exploration/num steps total                     151000
exploration/num paths total                        151
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.09641
exploration/Rewards Std                              1.09
exploration/Rewards Max                              6.25849
exploration/Rewards Min                             -0.723839
exploration/Returns Mean                          4096.41
exploration/Returns Std                              0
exploration/Returns Max                           4096.41
exploration/Returns Min                           4096.41
exploration/Actions Mean                             0.00120064
exploration/Actions Std                              0.826885
exploration/Actions Max                              0.999482
exploration/Actions Min                             -0.999633
exploration/Num Paths                                1
exploration/Average Returns                       4096.41
exploration/env_infos/final/reward_run Mean          5.32663
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.32663
exploration/env_infos/final/reward_run Min           5.32663
exploration/env_infos/initial/reward_run Mean        0.273825
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.273825
exploration/env_infos/initial/reward_run Min         0.273825
exploration/env_infos/reward_run Mean                4.50666
exploration/env_infos/reward_run Std                 1.08819
exploration/env_infos/reward_run Max                 6.6019
exploration/env_infos/reward_run Min                -0.316294
exploration/env_infos/final/reward_ctrl Mean        -0.46164
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.46164
exploration/env_infos/final/reward_ctrl Min         -0.46164
exploration/env_infos/initial/reward_ctrl Mean      -0.0593249
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0593249
exploration/env_infos/initial/reward_ctrl Min       -0.0593249
exploration/env_infos/reward_ctrl Mean              -0.410244
exploration/env_infos/reward_ctrl Std                0.0907166
exploration/env_infos/reward_ctrl Max               -0.0593249
exploration/env_infos/reward_ctrl Min               -0.573161
evaluation/num steps total                      750000
evaluation/num paths total                         750
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.35978
evaluation/Rewards Std                               1.08528
evaluation/Rewards Max                               6.6368
evaluation/Rewards Min                              -0.755377
evaluation/Returns Mean                           4359.78
evaluation/Returns Std                              65.5755
evaluation/Returns Max                            4479.4
evaluation/Returns Min                            4293.48
evaluation/Actions Mean                             -0.0158154
evaluation/Actions Std                               0.843432
evaluation/Actions Max                               0.999719
evaluation/Actions Min                              -0.999809
evaluation/Num Paths                                 5
evaluation/Average Returns                        4359.78
evaluation/env_infos/final/reward_run Mean           5.20572
evaluation/env_infos/final/reward_run Std            1.15046
evaluation/env_infos/final/reward_run Max            6.78873
evaluation/env_infos/final/reward_run Min            3.29921
evaluation/env_infos/initial/reward_run Mean        -0.0319664
evaluation/env_infos/initial/reward_run Std          0.137009
evaluation/env_infos/initial/reward_run Max          0.119565
evaluation/env_infos/initial/reward_run Min         -0.266486
evaluation/env_infos/reward_run Mean                 4.78676
evaluation/env_infos/reward_run Std                  1.08114
evaluation/env_infos/reward_run Max                  6.94673
evaluation/env_infos/reward_run Min                 -0.449561
evaluation/env_infos/final/reward_ctrl Mean         -0.410807
evaluation/env_infos/final/reward_ctrl Std           0.0788885
evaluation/env_infos/final/reward_ctrl Max          -0.310856
evaluation/env_infos/final/reward_ctrl Min          -0.504527
evaluation/env_infos/initial/reward_ctrl Mean       -0.0616043
evaluation/env_infos/initial/reward_ctrl Std         0.0166667
evaluation/env_infos/initial/reward_ctrl Max        -0.0357166
evaluation/env_infos/initial/reward_ctrl Min        -0.0883653
evaluation/env_infos/reward_ctrl Mean               -0.426977
evaluation/env_infos/reward_ctrl Std                 0.0896941
evaluation/env_infos/reward_ctrl Max                -0.0357166
evaluation/env_infos/reward_ctrl Min                -0.58483
time/data storing (s)                                0.00668748
time/evaluation sampling (s)                         2.58955
time/exploration sampling (s)                        0.739274
time/logging (s)                                     0.0432272
time/saving (s)                                      0.0160654
time/training (s)                                   33.5269
time/epoch (s)                                      36.9217
time/total (s)                                    6254.39
Epoch                                              149
----------------------------------------------  ---------------
2020-07-08 22:51:05.825864 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 150 finished
----------------------------------------------  ---------------
replay_buffer/size                              152000
trainer/QF1 Loss                                     5.14266
trainer/QF2 Loss                                     6.4772
trainer/Policy Loss                               -228.761
trainer/Q1 Predictions Mean                        234.768
trainer/Q1 Predictions Std                          97.447
trainer/Q1 Predictions Max                         312.265
trainer/Q1 Predictions Min                          11.3139
trainer/Q2 Predictions Mean                        234.424
trainer/Q2 Predictions Std                          97.3272
trainer/Q2 Predictions Max                         311.236
trainer/Q2 Predictions Min                          11.1428
trainer/Q Targets Mean                             234.884
trainer/Q Targets Std                               97.6552
trainer/Q Targets Max                              311.587
trainer/Q Targets Min                               10.0764
trainer/Log Pis Mean                                 5.8977
trainer/Log Pis Std                                  4.96785
trainer/Log Pis Max                                 19.4869
trainer/Log Pis Min                                 -5.98084
trainer/Policy mu Mean                               0.112511
trainer/Policy mu Std                                1.51421
trainer/Policy mu Max                                4.18641
trainer/Policy mu Min                               -2.94728
trainer/Policy log std Mean                         -0.822284
trainer/Policy log std Std                           0.347277
trainer/Policy log std Max                           0.47779
trainer/Policy log std Min                          -2.48309
trainer/Alpha                                        0.0939473
trainer/Alpha Loss                                  -0.241914
exploration/num steps total                     152000
exploration/num paths total                        152
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.19199
exploration/Rewards Std                              1.10388
exploration/Rewards Max                              6.27646
exploration/Rewards Min                             -1.29335
exploration/Returns Mean                          4191.99
exploration/Returns Std                              0
exploration/Returns Max                           4191.99
exploration/Returns Min                           4191.99
exploration/Actions Mean                            -0.00608763
exploration/Actions Std                              0.824584
exploration/Actions Max                              0.999797
exploration/Actions Min                             -0.999089
exploration/Num Paths                                1
exploration/Average Returns                       4191.99
exploration/env_infos/final/reward_run Mean          6.34961
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.34961
exploration/env_infos/final/reward_run Min           6.34961
exploration/env_infos/initial/reward_run Mean       -0.240489
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.240489
exploration/env_infos/initial/reward_run Min        -0.240489
exploration/env_infos/reward_run Mean                4.59997
exploration/env_infos/reward_run Std                 1.09674
exploration/env_infos/reward_run Max                 6.61845
exploration/env_infos/reward_run Min                -0.827047
exploration/env_infos/final/reward_ctrl Mean        -0.35739
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.35739
exploration/env_infos/final/reward_ctrl Min         -0.35739
exploration/env_infos/initial/reward_ctrl Mean      -0.136024
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.136024
exploration/env_infos/initial/reward_ctrl Min       -0.136024
exploration/env_infos/reward_ctrl Mean              -0.407985
exploration/env_infos/reward_ctrl Std                0.0902092
exploration/env_infos/reward_ctrl Max               -0.130179
exploration/env_infos/reward_ctrl Min               -0.583504
evaluation/num steps total                      755000
evaluation/num paths total                         755
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.39779
evaluation/Rewards Std                               1.05542
evaluation/Rewards Max                               6.41369
evaluation/Rewards Min                              -0.545691
evaluation/Returns Mean                           4397.79
evaluation/Returns Std                              88.6528
evaluation/Returns Max                            4512.95
evaluation/Returns Min                            4251.76
evaluation/Actions Mean                             -0.0192891
evaluation/Actions Std                               0.840024
evaluation/Actions Max                               0.998289
evaluation/Actions Min                              -0.998483
evaluation/Num Paths                                 5
evaluation/Average Returns                        4397.79
evaluation/env_infos/final/reward_run Mean           5.57555
evaluation/env_infos/final/reward_run Std            0.621124
evaluation/env_infos/final/reward_run Max            6.399
evaluation/env_infos/final/reward_run Min            4.48282
evaluation/env_infos/initial/reward_run Mean         0.0805217
evaluation/env_infos/initial/reward_run Std          0.0819149
evaluation/env_infos/initial/reward_run Max          0.229382
evaluation/env_infos/initial/reward_run Min         -0.00524259
evaluation/env_infos/reward_run Mean                 4.8214
evaluation/env_infos/reward_run Std                  1.0443
evaluation/env_infos/reward_run Max                  6.80223
evaluation/env_infos/reward_run Min                 -0.0666416
evaluation/env_infos/final/reward_ctrl Mean         -0.392791
evaluation/env_infos/final/reward_ctrl Std           0.0456812
evaluation/env_infos/final/reward_ctrl Max          -0.341724
evaluation/env_infos/final/reward_ctrl Min          -0.453923
evaluation/env_infos/initial/reward_ctrl Mean       -0.0606315
evaluation/env_infos/initial/reward_ctrl Std         0.0113491
evaluation/env_infos/initial/reward_ctrl Max        -0.0465893
evaluation/env_infos/initial/reward_ctrl Min        -0.0794391
evaluation/env_infos/reward_ctrl Mean               -0.423607
evaluation/env_infos/reward_ctrl Std                 0.0902335
evaluation/env_infos/reward_ctrl Max                -0.0465893
evaluation/env_infos/reward_ctrl Min                -0.56808
time/data storing (s)                                0.0110739
time/evaluation sampling (s)                         3.94093
time/exploration sampling (s)                        0.726863
time/logging (s)                                     0.0441979
time/saving (s)                                      0.0174992
time/training (s)                                   35.1373
time/epoch (s)                                      39.8779
time/total (s)                                    6294.3
Epoch                                              150
----------------------------------------------  ---------------
2020-07-08 22:51:47.824157 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 151 finished
----------------------------------------------  ---------------
replay_buffer/size                              153000
trainer/QF1 Loss                                     6.99596
trainer/QF2 Loss                                     6.84685
trainer/Policy Loss                               -212.686
trainer/Q1 Predictions Mean                        218.448
trainer/Q1 Predictions Std                         109.558
trainer/Q1 Predictions Max                         313.124
trainer/Q1 Predictions Min                          11.7377
trainer/Q2 Predictions Mean                        218.092
trainer/Q2 Predictions Std                         109.317
trainer/Q2 Predictions Max                         314.399
trainer/Q2 Predictions Min                          12.3422
trainer/Q Targets Mean                             217.991
trainer/Q Targets Std                              109.369
trainer/Q Targets Max                              312.353
trainer/Q Targets Min                               11.8494
trainer/Log Pis Mean                                 5.75705
trainer/Log Pis Std                                  5.46922
trainer/Log Pis Max                                 20.7442
trainer/Log Pis Min                                 -6.27165
trainer/Policy mu Mean                               0.0870411
trainer/Policy mu Std                                1.50625
trainer/Policy mu Max                                3.65698
trainer/Policy mu Min                               -4.17699
trainer/Policy log std Mean                         -0.77095
trainer/Policy log std Std                           0.329802
trainer/Policy log std Max                          -0.0128994
trainer/Policy log std Min                          -2.2918
trainer/Alpha                                        0.0960761
trainer/Alpha Loss                                  -0.569125
exploration/num steps total                     153000
exploration/num paths total                        153
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.05125
exploration/Rewards Std                              1.01828
exploration/Rewards Max                              6.0402
exploration/Rewards Min                             -0.735389
exploration/Returns Mean                          4051.25
exploration/Returns Std                              0
exploration/Returns Max                           4051.25
exploration/Returns Min                           4051.25
exploration/Actions Mean                            -0.0413717
exploration/Actions Std                              0.840324
exploration/Actions Max                              0.999548
exploration/Actions Min                             -0.999645
exploration/Num Paths                                1
exploration/Average Returns                       4051.25
exploration/env_infos/final/reward_run Mean          5.93904
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.93904
exploration/env_infos/final/reward_run Min           5.93904
exploration/env_infos/initial/reward_run Mean        0.11569
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.11569
exploration/env_infos/initial/reward_run Min         0.11569
exploration/env_infos/reward_run Mean                4.47597
exploration/env_infos/reward_run Std                 1.01323
exploration/env_infos/reward_run Max                 6.4178
exploration/env_infos/reward_run Min                -0.321972
exploration/env_infos/final/reward_ctrl Mean        -0.2811
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.2811
exploration/env_infos/final/reward_ctrl Min         -0.2811
exploration/env_infos/initial/reward_ctrl Mean      -0.0105694
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0105694
exploration/env_infos/initial/reward_ctrl Min       -0.0105694
exploration/env_infos/reward_ctrl Mean              -0.424714
exploration/env_infos/reward_ctrl Std                0.0932618
exploration/env_infos/reward_ctrl Max               -0.0105694
exploration/env_infos/reward_ctrl Min               -0.588606
evaluation/num steps total                      760000
evaluation/num paths total                         760
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.17462
evaluation/Rewards Std                               1.04746
evaluation/Rewards Max                               6.42055
evaluation/Rewards Min                              -0.856871
evaluation/Returns Mean                           4174.62
evaluation/Returns Std                              74.5106
evaluation/Returns Max                            4303.53
evaluation/Returns Min                            4087.22
evaluation/Actions Mean                             -0.0506702
evaluation/Actions Std                               0.85848
evaluation/Actions Max                               0.999811
evaluation/Actions Min                              -0.999961
evaluation/Num Paths                                 5
evaluation/Average Returns                        4174.62
evaluation/env_infos/final/reward_run Mean           4.81546
evaluation/env_infos/final/reward_run Std            0.49688
evaluation/env_infos/final/reward_run Max            5.4794
evaluation/env_infos/final/reward_run Min            4.03488
evaluation/env_infos/initial/reward_run Mean         0.0215694
evaluation/env_infos/initial/reward_run Std          0.175033
evaluation/env_infos/initial/reward_run Max          0.302442
evaluation/env_infos/initial/reward_run Min         -0.19857
evaluation/env_infos/reward_run Mean                 4.61835
evaluation/env_infos/reward_run Std                  1.03995
evaluation/env_infos/reward_run Max                  6.89554
evaluation/env_infos/reward_run Min                 -0.471092
evaluation/env_infos/final/reward_ctrl Mean         -0.466234
evaluation/env_infos/final/reward_ctrl Std           0.0902843
evaluation/env_infos/final/reward_ctrl Max          -0.351744
evaluation/env_infos/final/reward_ctrl Min          -0.556148
evaluation/env_infos/initial/reward_ctrl Mean       -0.0426304
evaluation/env_infos/initial/reward_ctrl Std         0.012169
evaluation/env_infos/initial/reward_ctrl Max        -0.0272634
evaluation/env_infos/initial/reward_ctrl Min        -0.0568725
evaluation/env_infos/reward_ctrl Mean               -0.443734
evaluation/env_infos/reward_ctrl Std                 0.0912687
evaluation/env_infos/reward_ctrl Max                -0.0272634
evaluation/env_infos/reward_ctrl Min                -0.587269
time/data storing (s)                                0.00719516
time/evaluation sampling (s)                         3.46015
time/exploration sampling (s)                        0.659076
time/logging (s)                                     0.0465808
time/saving (s)                                      0.0157155
time/training (s)                                   37.794
time/epoch (s)                                      41.9827
time/total (s)                                    6336.29
Epoch                                              151
----------------------------------------------  ---------------
2020-07-08 22:52:34.358847 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 152 finished
----------------------------------------------  --------------
replay_buffer/size                              154000
trainer/QF1 Loss                                     7.86733
trainer/QF2 Loss                                     7.57448
trainer/Policy Loss                               -228.605
trainer/Q1 Predictions Mean                        235.089
trainer/Q1 Predictions Std                          93.1203
trainer/Q1 Predictions Max                         311.402
trainer/Q1 Predictions Min                          11.2291
trainer/Q2 Predictions Mean                        235.566
trainer/Q2 Predictions Std                          93.3622
trainer/Q2 Predictions Max                         310.639
trainer/Q2 Predictions Min                          11.8073
trainer/Q Targets Mean                             235.569
trainer/Q Targets Std                               93.1845
trainer/Q Targets Max                              311.406
trainer/Q Targets Min                               11.2915
trainer/Log Pis Mean                                 6.7706
trainer/Log Pis Std                                  5.05611
trainer/Log Pis Max                                 17.1895
trainer/Log Pis Min                                 -4.73462
trainer/Policy mu Mean                              -0.0013507
trainer/Policy mu Std                                1.56504
trainer/Policy mu Max                                3.95235
trainer/Policy mu Min                               -3.43842
trainer/Policy log std Mean                         -0.832201
trainer/Policy log std Std                           0.348568
trainer/Policy log std Max                          -0.0928173
trainer/Policy log std Min                          -2.42565
trainer/Alpha                                        0.0946942
trainer/Alpha Loss                                   1.81649
exploration/num steps total                     154000
exploration/num paths total                        154
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.20928
exploration/Rewards Std                              1.07321
exploration/Rewards Max                              6.29855
exploration/Rewards Min                             -0.709996
exploration/Returns Mean                          4209.28
exploration/Returns Std                              0
exploration/Returns Max                           4209.28
exploration/Returns Min                           4209.28
exploration/Actions Mean                            -0.0153705
exploration/Actions Std                              0.820093
exploration/Actions Max                              0.999643
exploration/Actions Min                             -0.999484
exploration/Num Paths                                1
exploration/Average Returns                       4209.28
exploration/env_infos/final/reward_run Mean          5.93195
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.93195
exploration/env_infos/final/reward_run Min           5.93195
exploration/env_infos/initial/reward_run Mean        0.218015
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.218015
exploration/env_infos/initial/reward_run Min         0.218015
exploration/env_infos/reward_run Mean                4.61295
exploration/env_infos/reward_run Std                 1.06293
exploration/env_infos/reward_run Max                 6.67913
exploration/env_infos/reward_run Min                -0.213127
exploration/env_infos/final/reward_ctrl Mean        -0.219656
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.219656
exploration/env_infos/final/reward_ctrl Min         -0.219656
exploration/env_infos/initial/reward_ctrl Mean      -0.0769843
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0769843
exploration/env_infos/initial/reward_ctrl Min       -0.0769843
exploration/env_infos/reward_ctrl Mean              -0.403674
exploration/env_infos/reward_ctrl Std                0.102321
exploration/env_infos/reward_ctrl Max               -0.066232
exploration/env_infos/reward_ctrl Min               -0.57865
evaluation/num steps total                      765000
evaluation/num paths total                         765
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.25766
evaluation/Rewards Std                               1.06057
evaluation/Rewards Max                               6.41658
evaluation/Rewards Min                              -0.521534
evaluation/Returns Mean                           4257.66
evaluation/Returns Std                              69.4948
evaluation/Returns Max                            4381.12
evaluation/Returns Min                            4169.91
evaluation/Actions Mean                             -0.0151524
evaluation/Actions Std                               0.832126
evaluation/Actions Max                               0.999315
evaluation/Actions Min                              -0.999797
evaluation/Num Paths                                 5
evaluation/Average Returns                        4257.66
evaluation/env_infos/final/reward_run Mean           4.42827
evaluation/env_infos/final/reward_run Std            1.01507
evaluation/env_infos/final/reward_run Max            5.77622
evaluation/env_infos/final/reward_run Min            3.36086
evaluation/env_infos/initial/reward_run Mean         0.0125336
evaluation/env_infos/initial/reward_run Std          0.0901762
evaluation/env_infos/initial/reward_run Max          0.185627
evaluation/env_infos/initial/reward_run Min         -0.0788824
evaluation/env_infos/reward_run Mean                 4.67326
evaluation/env_infos/reward_run Std                  1.04928
evaluation/env_infos/reward_run Max                  6.72021
evaluation/env_infos/reward_run Min                 -0.0788824
evaluation/env_infos/final/reward_ctrl Mean         -0.465567
evaluation/env_infos/final/reward_ctrl Std           0.0565149
evaluation/env_infos/final/reward_ctrl Max          -0.357155
evaluation/env_infos/final/reward_ctrl Min          -0.517487
evaluation/env_infos/initial/reward_ctrl Mean       -0.0337209
evaluation/env_infos/initial/reward_ctrl Std         0.0131498
evaluation/env_infos/initial/reward_ctrl Max        -0.0216736
evaluation/env_infos/initial/reward_ctrl Min        -0.052042
evaluation/env_infos/reward_ctrl Mean               -0.415598
evaluation/env_infos/reward_ctrl Std                 0.101379
evaluation/env_infos/reward_ctrl Max                -0.0216736
evaluation/env_infos/reward_ctrl Min                -0.5849
time/data storing (s)                                0.0102806
time/evaluation sampling (s)                         2.81153
time/exploration sampling (s)                        0.717192
time/logging (s)                                     0.0456981
time/saving (s)                                      0.0183794
time/training (s)                                   42.7946
time/epoch (s)                                      46.3976
time/total (s)                                    6382.82
Epoch                                              152
----------------------------------------------  --------------
2020-07-08 22:53:22.600203 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 153 finished
----------------------------------------------  ---------------
replay_buffer/size                              155000
trainer/QF1 Loss                                     7.04632
trainer/QF2 Loss                                     8.90012
trainer/Policy Loss                               -231.174
trainer/Q1 Predictions Mean                        237.042
trainer/Q1 Predictions Std                          89.1526
trainer/Q1 Predictions Max                         312.242
trainer/Q1 Predictions Min                          11.6287
trainer/Q2 Predictions Mean                        236.671
trainer/Q2 Predictions Std                          89.1608
trainer/Q2 Predictions Max                         312.682
trainer/Q2 Predictions Min                          11.8088
trainer/Q Targets Mean                             236.817
trainer/Q Targets Std                               89.1167
trainer/Q Targets Max                              312.274
trainer/Q Targets Min                               10.9985
trainer/Log Pis Mean                                 6.02929
trainer/Log Pis Std                                  4.58474
trainer/Log Pis Max                                 19.6117
trainer/Log Pis Min                                 -7.97113
trainer/Policy mu Mean                               0.0347794
trainer/Policy mu Std                                1.49188
trainer/Policy mu Max                                4.12669
trainer/Policy mu Min                               -4.00471
trainer/Policy log std Mean                         -0.8258
trainer/Policy log std Std                           0.33747
trainer/Policy log std Max                           0.178206
trainer/Policy log std Min                          -2.32489
trainer/Alpha                                        0.0964521
trainer/Alpha Loss                                   0.068491
exploration/num steps total                     155000
exploration/num paths total                        155
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.23205
exploration/Rewards Std                              1.07728
exploration/Rewards Max                              6.2777
exploration/Rewards Min                             -0.573649
exploration/Returns Mean                          4232.05
exploration/Returns Std                              0
exploration/Returns Max                           4232.05
exploration/Returns Min                           4232.05
exploration/Actions Mean                            -0.0165213
exploration/Actions Std                              0.827254
exploration/Actions Max                              0.999616
exploration/Actions Min                             -0.999653
exploration/Num Paths                                1
exploration/Average Returns                       4232.05
exploration/env_infos/final/reward_run Mean          4.74564
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.74564
exploration/env_infos/final/reward_run Min           4.74564
exploration/env_infos/initial/reward_run Mean        0.275436
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.275436
exploration/env_infos/initial/reward_run Min         0.275436
exploration/env_infos/reward_run Mean                4.64282
exploration/env_infos/reward_run Std                 1.07388
exploration/env_infos/reward_run Max                 6.76393
exploration/env_infos/reward_run Min                -0.0889271
exploration/env_infos/final/reward_ctrl Mean        -0.358904
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.358904
exploration/env_infos/final/reward_ctrl Min         -0.358904
exploration/env_infos/initial/reward_ctrl Mean      -0.109372
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.109372
exploration/env_infos/initial/reward_ctrl Min       -0.109372
exploration/env_infos/reward_ctrl Mean              -0.410774
exploration/env_infos/reward_ctrl Std                0.091629
exploration/env_infos/reward_ctrl Max               -0.109372
exploration/env_infos/reward_ctrl Min               -0.581862
evaluation/num steps total                      770000
evaluation/num paths total                         770
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.31683
evaluation/Rewards Std                               1.10878
evaluation/Rewards Max                               6.72803
evaluation/Rewards Min                              -0.847989
evaluation/Returns Mean                           4316.83
evaluation/Returns Std                              57.9973
evaluation/Returns Max                            4386.93
evaluation/Returns Min                            4253.22
evaluation/Actions Mean                             -0.0212331
evaluation/Actions Std                               0.84675
evaluation/Actions Max                               0.998828
evaluation/Actions Min                              -0.998017
evaluation/Num Paths                                 5
evaluation/Average Returns                        4316.83
evaluation/env_infos/final/reward_run Mean           4.70635
evaluation/env_infos/final/reward_run Std            0.675679
evaluation/env_infos/final/reward_run Max            5.66138
evaluation/env_infos/final/reward_run Min            3.71056
evaluation/env_infos/initial/reward_run Mean        -0.00661394
evaluation/env_infos/initial/reward_run Std          0.154911
evaluation/env_infos/initial/reward_run Max          0.173022
evaluation/env_infos/initial/reward_run Min         -0.273329
evaluation/env_infos/reward_run Mean                 4.74729
evaluation/env_infos/reward_run Std                  1.10676
evaluation/env_infos/reward_run Max                  7.04413
evaluation/env_infos/reward_run Min                 -0.370393
evaluation/env_infos/final/reward_ctrl Mean         -0.381414
evaluation/env_infos/final/reward_ctrl Std           0.11738
evaluation/env_infos/final/reward_ctrl Max          -0.259769
evaluation/env_infos/final/reward_ctrl Min          -0.557759
evaluation/env_infos/initial/reward_ctrl Mean       -0.0239909
evaluation/env_infos/initial/reward_ctrl Std         0.0173873
evaluation/env_infos/initial/reward_ctrl Max        -0.0071514
evaluation/env_infos/initial/reward_ctrl Min        -0.0516902
evaluation/env_infos/reward_ctrl Mean               -0.430462
evaluation/env_infos/reward_ctrl Std                 0.0915123
evaluation/env_infos/reward_ctrl Max                -0.0071514
evaluation/env_infos/reward_ctrl Min                -0.581056
time/data storing (s)                                0.0069149
time/evaluation sampling (s)                         3.72658
time/exploration sampling (s)                        0.874714
time/logging (s)                                     0.0424276
time/saving (s)                                      0.0202189
time/training (s)                                   43.522
time/epoch (s)                                      48.1929
time/total (s)                                    6431.05
Epoch                                              153
----------------------------------------------  ---------------
2020-07-08 22:54:04.248380 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 154 finished
----------------------------------------------  --------------
replay_buffer/size                              156000
trainer/QF1 Loss                                     8.74034
trainer/QF2 Loss                                     9.33917
trainer/Policy Loss                               -225.902
trainer/Q1 Predictions Mean                        232.275
trainer/Q1 Predictions Std                          99.2653
trainer/Q1 Predictions Max                         307.887
trainer/Q1 Predictions Min                          12.0746
trainer/Q2 Predictions Mean                        231.9
trainer/Q2 Predictions Std                          98.9857
trainer/Q2 Predictions Max                         309.935
trainer/Q2 Predictions Min                          12.4174
trainer/Q Targets Mean                             232.291
trainer/Q Targets Std                               99.2264
trainer/Q Targets Max                              311.608
trainer/Q Targets Min                               11.4495
trainer/Log Pis Mean                                 6.23082
trainer/Log Pis Std                                  5.1385
trainer/Log Pis Max                                 18.4594
trainer/Log Pis Min                                 -7.27515
trainer/Policy mu Mean                               0.0684179
trainer/Policy mu Std                                1.52583
trainer/Policy mu Max                                3.94964
trainer/Policy mu Min                               -3.64832
trainer/Policy log std Mean                         -0.794726
trainer/Policy log std Std                           0.340268
trainer/Policy log std Max                          -0.0140905
trainer/Policy log std Min                          -2.42536
trainer/Alpha                                        0.0956504
trainer/Alpha Loss                                   0.541752
exploration/num steps total                     156000
exploration/num paths total                        156
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.07809
exploration/Rewards Std                              1.06076
exploration/Rewards Max                              6.17239
exploration/Rewards Min                             -0.880976
exploration/Returns Mean                          4078.09
exploration/Returns Std                              0
exploration/Returns Max                           4078.09
exploration/Returns Min                           4078.09
exploration/Actions Mean                            -0.0221107
exploration/Actions Std                              0.832111
exploration/Actions Max                              0.999831
exploration/Actions Min                             -0.999505
exploration/Num Paths                                1
exploration/Average Returns                       4078.09
exploration/env_infos/final/reward_run Mean          4.91494
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.91494
exploration/env_infos/final/reward_run Min           4.91494
exploration/env_infos/initial/reward_run Mean       -0.370637
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.370637
exploration/env_infos/initial/reward_run Min        -0.370637
exploration/env_infos/reward_run Mean                4.49383
exploration/env_infos/reward_run Std                 1.06
exploration/env_infos/reward_run Max                 6.57713
exploration/env_infos/reward_run Min                -0.41877
exploration/env_infos/final/reward_ctrl Mean        -0.372821
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.372821
exploration/env_infos/final/reward_ctrl Min         -0.372821
exploration/env_infos/initial/reward_ctrl Mean      -0.0261078
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0261078
exploration/env_infos/initial/reward_ctrl Min       -0.0261078
exploration/env_infos/reward_ctrl Mean              -0.415739
exploration/env_infos/reward_ctrl Std                0.0948188
exploration/env_infos/reward_ctrl Max               -0.0261078
exploration/env_infos/reward_ctrl Min               -0.591028
evaluation/num steps total                      775000
evaluation/num paths total                         775
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.25745
evaluation/Rewards Std                               1.07501
evaluation/Rewards Max                               6.65444
evaluation/Rewards Min                              -0.999238
evaluation/Returns Mean                           4257.45
evaluation/Returns Std                              46.2285
evaluation/Returns Max                            4305.45
evaluation/Returns Min                            4192.39
evaluation/Actions Mean                             -0.0319465
evaluation/Actions Std                               0.846032
evaluation/Actions Max                               0.999504
evaluation/Actions Min                              -0.9981
evaluation/Num Paths                                 5
evaluation/Average Returns                        4257.45
evaluation/env_infos/final/reward_run Mean           3.81627
evaluation/env_infos/final/reward_run Std            0.616139
evaluation/env_infos/final/reward_run Max            4.72863
evaluation/env_infos/final/reward_run Min            3.05442
evaluation/env_infos/initial/reward_run Mean        -0.153436
evaluation/env_infos/initial/reward_run Std          0.180793
evaluation/env_infos/initial/reward_run Max          0.0867052
evaluation/env_infos/initial/reward_run Min         -0.373777
evaluation/env_infos/reward_run Mean                 4.68753
evaluation/env_infos/reward_run Std                  1.07059
evaluation/env_infos/reward_run Max                  7.05339
evaluation/env_infos/reward_run Min                 -0.721194
evaluation/env_infos/final/reward_ctrl Mean         -0.469071
evaluation/env_infos/final/reward_ctrl Std           0.0769494
evaluation/env_infos/final/reward_ctrl Max          -0.336299
evaluation/env_infos/final/reward_ctrl Min          -0.54358
evaluation/env_infos/initial/reward_ctrl Mean       -0.0385991
evaluation/env_infos/initial/reward_ctrl Std         0.0156404
evaluation/env_infos/initial/reward_ctrl Max        -0.0237266
evaluation/env_infos/initial/reward_ctrl Min        -0.0673245
evaluation/env_infos/reward_ctrl Mean               -0.430075
evaluation/env_infos/reward_ctrl Std                 0.0929541
evaluation/env_infos/reward_ctrl Max                -0.0237266
evaluation/env_infos/reward_ctrl Min                -0.575816
time/data storing (s)                                0.0084944
time/evaluation sampling (s)                         4.17873
time/exploration sampling (s)                        0.860961
time/logging (s)                                     0.0408933
time/saving (s)                                      0.0163951
time/training (s)                                   36.4901
time/epoch (s)                                      41.5956
time/total (s)                                    6472.69
Epoch                                              154
----------------------------------------------  --------------
2020-07-08 22:54:50.895424 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 155 finished
----------------------------------------------  ---------------
replay_buffer/size                              157000
trainer/QF1 Loss                                     7.47872
trainer/QF2 Loss                                     7.77702
trainer/Policy Loss                               -232.405
trainer/Q1 Predictions Mean                        237.817
trainer/Q1 Predictions Std                          92.3527
trainer/Q1 Predictions Max                         314.341
trainer/Q1 Predictions Min                          11.1418
trainer/Q2 Predictions Mean                        238.376
trainer/Q2 Predictions Std                          92.4112
trainer/Q2 Predictions Max                         315.081
trainer/Q2 Predictions Min                          10.9596
trainer/Q Targets Mean                             238.471
trainer/Q Targets Std                               92.5519
trainer/Q Targets Max                              315
trainer/Q Targets Min                               10.0114
trainer/Log Pis Mean                                 6.041
trainer/Log Pis Std                                  5.22058
trainer/Log Pis Max                                 22.2511
trainer/Log Pis Min                                 -6.44061
trainer/Policy mu Mean                               0.0528364
trainer/Policy mu Std                                1.52802
trainer/Policy mu Max                                5.36515
trainer/Policy mu Min                               -5.37719
trainer/Policy log std Mean                         -0.797088
trainer/Policy log std Std                           0.329611
trainer/Policy log std Max                           0.0175635
trainer/Policy log std Min                          -2.35391
trainer/Alpha                                        0.0973013
trainer/Alpha Loss                                   0.0955297
exploration/num steps total                     157000
exploration/num paths total                        157
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.32156
exploration/Rewards Std                              1.00107
exploration/Rewards Max                              6.20671
exploration/Rewards Min                             -0.36042
exploration/Returns Mean                          4321.56
exploration/Returns Std                              0
exploration/Returns Max                           4321.56
exploration/Returns Min                           4321.56
exploration/Actions Mean                            -0.0143157
exploration/Actions Std                              0.826854
exploration/Actions Max                              0.999419
exploration/Actions Min                             -0.999722
exploration/Num Paths                                1
exploration/Average Returns                       4321.56
exploration/env_infos/final/reward_run Mean          5.25906
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.25906
exploration/env_infos/final/reward_run Min           5.25906
exploration/env_infos/initial/reward_run Mean        0.0741038
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0741038
exploration/env_infos/initial/reward_run Min         0.0741038
exploration/env_infos/reward_run Mean                4.7319
exploration/env_infos/reward_run Std                 0.98864
exploration/env_infos/reward_run Max                 6.50382
exploration/env_infos/reward_run Min                 0.0741038
exploration/env_infos/final/reward_ctrl Mean        -0.222602
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.222602
exploration/env_infos/final/reward_ctrl Min         -0.222602
exploration/env_infos/initial/reward_ctrl Mean      -0.0315106
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0315106
exploration/env_infos/initial/reward_ctrl Min       -0.0315106
exploration/env_infos/reward_ctrl Mean              -0.410336
exploration/env_infos/reward_ctrl Std                0.0973587
exploration/env_infos/reward_ctrl Max               -0.0315106
exploration/env_infos/reward_ctrl Min               -0.57659
evaluation/num steps total                      780000
evaluation/num paths total                         780
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.35757
evaluation/Rewards Std                               1.13729
evaluation/Rewards Max                               6.58829
evaluation/Rewards Min                              -1.12622
evaluation/Returns Mean                           4357.57
evaluation/Returns Std                              86.262
evaluation/Returns Max                            4509.86
evaluation/Returns Min                            4265.32
evaluation/Actions Mean                             -0.0256637
evaluation/Actions Std                               0.842185
evaluation/Actions Max                               0.99913
evaluation/Actions Min                              -0.99888
evaluation/Num Paths                                 5
evaluation/Average Returns                        4357.57
evaluation/env_infos/final/reward_run Mean           5.6714
evaluation/env_infos/final/reward_run Std            0.521454
evaluation/env_infos/final/reward_run Max            6.36483
evaluation/env_infos/final/reward_run Min            4.99707
evaluation/env_infos/initial/reward_run Mean        -0.0821788
evaluation/env_infos/initial/reward_run Std          0.170658
evaluation/env_infos/initial/reward_run Max          0.203428
evaluation/env_infos/initial/reward_run Min         -0.298932
evaluation/env_infos/reward_run Mean                 4.78353
evaluation/env_infos/reward_run Std                  1.13514
evaluation/env_infos/reward_run Max                  6.92704
evaluation/env_infos/reward_run Min                 -0.767496
evaluation/env_infos/final/reward_ctrl Mean         -0.417712
evaluation/env_infos/final/reward_ctrl Std           0.0783585
evaluation/env_infos/final/reward_ctrl Max          -0.30283
evaluation/env_infos/final/reward_ctrl Min          -0.508578
evaluation/env_infos/initial/reward_ctrl Mean       -0.0308485
evaluation/env_infos/initial/reward_ctrl Std         0.00712459
evaluation/env_infos/initial/reward_ctrl Max        -0.0189853
evaluation/env_infos/initial/reward_ctrl Min        -0.0413368
evaluation/env_infos/reward_ctrl Mean               -0.42596
evaluation/env_infos/reward_ctrl Std                 0.0965346
evaluation/env_infos/reward_ctrl Max                -0.0189853
evaluation/env_infos/reward_ctrl Min                -0.584392
time/data storing (s)                                0.00739669
time/evaluation sampling (s)                         4.9007
time/exploration sampling (s)                        1.13257
time/logging (s)                                     0.0407744
time/saving (s)                                      0.019309
time/training (s)                                   40.528
time/epoch (s)                                      46.6287
time/total (s)                                    6519.33
Epoch                                              155
----------------------------------------------  ---------------
2020-07-08 22:55:41.112460 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 156 finished
----------------------------------------------  ----------------
replay_buffer/size                              158000
trainer/QF1 Loss                                     7.69256
trainer/QF2 Loss                                    10.3173
trainer/Policy Loss                               -225.923
trainer/Q1 Predictions Mean                        231.438
trainer/Q1 Predictions Std                          98.4077
trainer/Q1 Predictions Max                         314.11
trainer/Q1 Predictions Min                          10.8769
trainer/Q2 Predictions Mean                        231.654
trainer/Q2 Predictions Std                          98.3668
trainer/Q2 Predictions Max                         314.355
trainer/Q2 Predictions Min                          11.5383
trainer/Q Targets Mean                             231.639
trainer/Q Targets Std                               98.5033
trainer/Q Targets Max                              315.721
trainer/Q Targets Min                               10.4315
trainer/Log Pis Mean                                 5.73296
trainer/Log Pis Std                                  5.05523
trainer/Log Pis Max                                 20.1692
trainer/Log Pis Min                                 -6.15655
trainer/Policy mu Mean                               0.0856701
trainer/Policy mu Std                                1.50242
trainer/Policy mu Max                                4.00051
trainer/Policy mu Min                               -3.63992
trainer/Policy log std Mean                         -0.811516
trainer/Policy log std Std                           0.357901
trainer/Policy log std Max                          -0.0122956
trainer/Policy log std Min                          -2.43073
trainer/Alpha                                        0.0973791
trainer/Alpha Loss                                  -0.622018
exploration/num steps total                     158000
exploration/num paths total                        158
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.2309
exploration/Rewards Std                              1.10105
exploration/Rewards Max                              6.63701
exploration/Rewards Min                             -0.616166
exploration/Returns Mean                          4230.9
exploration/Returns Std                              0
exploration/Returns Max                           4230.9
exploration/Returns Min                           4230.9
exploration/Actions Mean                             0.000563455
exploration/Actions Std                              0.817282
exploration/Actions Max                              0.999758
exploration/Actions Min                             -0.999763
exploration/Num Paths                                1
exploration/Average Returns                       4230.9
exploration/env_infos/final/reward_run Mean          6.02431
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.02431
exploration/env_infos/final/reward_run Min           6.02431
exploration/env_infos/initial/reward_run Mean        0.00738281
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.00738281
exploration/env_infos/initial/reward_run Min         0.00738281
exploration/env_infos/reward_run Mean                4.63167
exploration/env_infos/reward_run Std                 1.09452
exploration/env_infos/reward_run Max                 6.91897
exploration/env_infos/reward_run Min                -0.232356
exploration/env_infos/final/reward_ctrl Mean        -0.399717
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.399717
exploration/env_infos/final/reward_ctrl Min         -0.399717
exploration/env_infos/initial/reward_ctrl Mean      -0.0528355
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0528355
exploration/env_infos/initial/reward_ctrl Min       -0.0528355
exploration/env_infos/reward_ctrl Mean              -0.40077
exploration/env_infos/reward_ctrl Std                0.0963733
exploration/env_infos/reward_ctrl Max               -0.0528355
exploration/env_infos/reward_ctrl Min               -0.590998
evaluation/num steps total                      785000
evaluation/num paths total                         785
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.31868
evaluation/Rewards Std                               1.10891
evaluation/Rewards Max                               6.68473
evaluation/Rewards Min                              -1.21905
evaluation/Returns Mean                           4318.68
evaluation/Returns Std                              82.1963
evaluation/Returns Max                            4410.42
evaluation/Returns Min                            4208.29
evaluation/Actions Mean                             -0.00828566
evaluation/Actions Std                               0.829479
evaluation/Actions Max                               0.998894
evaluation/Actions Min                              -0.999303
evaluation/Num Paths                                 5
evaluation/Average Returns                        4318.68
evaluation/env_infos/final/reward_run Mean           5.12884
evaluation/env_infos/final/reward_run Std            0.714718
evaluation/env_infos/final/reward_run Max            6.03688
evaluation/env_infos/final/reward_run Min            4.01353
evaluation/env_infos/initial/reward_run Mean        -0.101875
evaluation/env_infos/initial/reward_run Std          0.127044
evaluation/env_infos/initial/reward_run Max          0.0770746
evaluation/env_infos/initial/reward_run Min         -0.27493
evaluation/env_infos/reward_run Mean                 4.73154
evaluation/env_infos/reward_run Std                  1.09775
evaluation/env_infos/reward_run Max                  7.01756
evaluation/env_infos/reward_run Min                 -0.726877
evaluation/env_infos/final/reward_ctrl Mean         -0.417985
evaluation/env_infos/final/reward_ctrl Std           0.0681604
evaluation/env_infos/final/reward_ctrl Max          -0.302277
evaluation/env_infos/final/reward_ctrl Min          -0.490295
evaluation/env_infos/initial/reward_ctrl Mean       -0.0513132
evaluation/env_infos/initial/reward_ctrl Std         0.0124486
evaluation/env_infos/initial/reward_ctrl Max        -0.0343732
evaluation/env_infos/initial/reward_ctrl Min        -0.0724545
evaluation/env_infos/reward_ctrl Mean               -0.412863
evaluation/env_infos/reward_ctrl Std                 0.0960814
evaluation/env_infos/reward_ctrl Max                -0.0343732
evaluation/env_infos/reward_ctrl Min                -0.583408
time/data storing (s)                                0.00881544
time/evaluation sampling (s)                         3.39032
time/exploration sampling (s)                        0.778236
time/logging (s)                                     0.0454563
time/saving (s)                                      0.038219
time/training (s)                                   45.9423
time/epoch (s)                                      50.2033
time/total (s)                                    6569.55
Epoch                                              156
----------------------------------------------  ----------------
2020-07-08 22:56:24.864147 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 157 finished
----------------------------------------------  ---------------
replay_buffer/size                              159000
trainer/QF1 Loss                                    14.9392
trainer/QF2 Loss                                    18.2392
trainer/Policy Loss                               -231.757
trainer/Q1 Predictions Mean                        237.686
trainer/Q1 Predictions Std                          93.223
trainer/Q1 Predictions Max                         317.123
trainer/Q1 Predictions Min                          11.8684
trainer/Q2 Predictions Mean                        237.324
trainer/Q2 Predictions Std                          93.1616
trainer/Q2 Predictions Max                         318.005
trainer/Q2 Predictions Min                          11.789
trainer/Q Targets Mean                             237.969
trainer/Q Targets Std                               93.074
trainer/Q Targets Max                              314.779
trainer/Q Targets Min                               11.5349
trainer/Log Pis Mean                                 5.86706
trainer/Log Pis Std                                  5.02008
trainer/Log Pis Max                                 26.4261
trainer/Log Pis Min                                 -5.62005
trainer/Policy mu Mean                               0.0567006
trainer/Policy mu Std                                1.52119
trainer/Policy mu Max                                3.47601
trainer/Policy mu Min                               -4.9425
trainer/Policy log std Mean                         -0.813076
trainer/Policy log std Std                           0.33311
trainer/Policy log std Max                           0.11095
trainer/Policy log std Min                          -2.54032
trainer/Alpha                                        0.098086
trainer/Alpha Loss                                  -0.308703
exploration/num steps total                     159000
exploration/num paths total                        159
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.05149
exploration/Rewards Std                              1.09533
exploration/Rewards Max                              6.29906
exploration/Rewards Min                             -0.833312
exploration/Returns Mean                          4051.49
exploration/Returns Std                              0
exploration/Returns Max                           4051.49
exploration/Returns Min                           4051.49
exploration/Actions Mean                             0.00895846
exploration/Actions Std                              0.815241
exploration/Actions Max                              0.999638
exploration/Actions Min                             -0.999555
exploration/Num Paths                                1
exploration/Average Returns                       4051.49
exploration/env_infos/final/reward_run Mean          3.54452
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.54452
exploration/env_infos/final/reward_run Min           3.54452
exploration/env_infos/initial/reward_run Mean        0.374811
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.374811
exploration/env_infos/initial/reward_run Min         0.374811
exploration/env_infos/reward_run Mean                4.45031
exploration/env_infos/reward_run Std                 1.09349
exploration/env_infos/reward_run Max                 6.73433
exploration/env_infos/reward_run Min                -0.330295
exploration/env_infos/final/reward_ctrl Mean        -0.483798
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.483798
exploration/env_infos/final/reward_ctrl Min         -0.483798
exploration/env_infos/initial/reward_ctrl Mean      -0.0566826
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0566826
exploration/env_infos/initial/reward_ctrl Min       -0.0566826
exploration/env_infos/reward_ctrl Mean              -0.398819
exploration/env_infos/reward_ctrl Std                0.0934492
exploration/env_infos/reward_ctrl Max               -0.0566826
exploration/env_infos/reward_ctrl Min               -0.574554
evaluation/num steps total                      790000
evaluation/num paths total                         790
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.34602
evaluation/Rewards Std                               1.10203
evaluation/Rewards Max                               6.62435
evaluation/Rewards Min                              -1.08322
evaluation/Returns Mean                           4346.02
evaluation/Returns Std                              90.7743
evaluation/Returns Max                            4468.6
evaluation/Returns Min                            4199.44
evaluation/Actions Mean                             -0.00869675
evaluation/Actions Std                               0.830608
evaluation/Actions Max                               0.998485
evaluation/Actions Min                              -0.999151
evaluation/Num Paths                                 5
evaluation/Average Returns                        4346.02
evaluation/env_infos/final/reward_run Mean           5.20108
evaluation/env_infos/final/reward_run Std            0.917349
evaluation/env_infos/final/reward_run Max            6.29622
evaluation/env_infos/final/reward_run Min            3.51157
evaluation/env_infos/initial/reward_run Mean         0.24548
evaluation/env_infos/initial/reward_run Std          0.317956
evaluation/env_infos/initial/reward_run Max          0.802795
evaluation/env_infos/initial/reward_run Min         -0.128063
evaluation/env_infos/reward_run Mean                 4.76002
evaluation/env_infos/reward_run Std                  1.0989
evaluation/env_infos/reward_run Max                  6.91851
evaluation/env_infos/reward_run Min                 -0.610874
evaluation/env_infos/final/reward_ctrl Mean         -0.436247
evaluation/env_infos/final/reward_ctrl Std           0.0903566
evaluation/env_infos/final/reward_ctrl Max          -0.314758
evaluation/env_infos/final/reward_ctrl Min          -0.523253
evaluation/env_infos/initial/reward_ctrl Mean       -0.0651124
evaluation/env_infos/initial/reward_ctrl Std         0.0509722
evaluation/env_infos/initial/reward_ctrl Max        -0.0215328
evaluation/env_infos/initial/reward_ctrl Min        -0.164177
evaluation/env_infos/reward_ctrl Mean               -0.413991
evaluation/env_infos/reward_ctrl Std                 0.0962603
evaluation/env_infos/reward_ctrl Max                -0.0215328
evaluation/env_infos/reward_ctrl Min                -0.579091
time/data storing (s)                                0.0093811
time/evaluation sampling (s)                         2.96087
time/exploration sampling (s)                        0.886468
time/logging (s)                                     0.0408956
time/saving (s)                                      0.0164091
time/training (s)                                   39.7895
time/epoch (s)                                      43.7035
time/total (s)                                    6613.29
Epoch                                              157
----------------------------------------------  ---------------
2020-07-08 22:57:22.152888 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 158 finished
----------------------------------------------  ----------------
replay_buffer/size                              160000
trainer/QF1 Loss                                     7.50148
trainer/QF2 Loss                                     8.0805
trainer/Policy Loss                               -228.742
trainer/Q1 Predictions Mean                        234.315
trainer/Q1 Predictions Std                          94.9465
trainer/Q1 Predictions Max                         316.703
trainer/Q1 Predictions Min                          13.1244
trainer/Q2 Predictions Mean                        234.767
trainer/Q2 Predictions Std                          95.1825
trainer/Q2 Predictions Max                         317.664
trainer/Q2 Predictions Min                          13.2601
trainer/Q Targets Mean                             234.219
trainer/Q Targets Std                               95.0978
trainer/Q Targets Max                              318.573
trainer/Q Targets Min                               12.9007
trainer/Log Pis Mean                                 5.93024
trainer/Log Pis Std                                  5.25482
trainer/Log Pis Max                                 18.3231
trainer/Log Pis Min                                 -5.74495
trainer/Policy mu Mean                               0.0864566
trainer/Policy mu Std                                1.50262
trainer/Policy mu Max                                4.74219
trainer/Policy mu Min                               -3.63431
trainer/Policy log std Mean                         -0.802439
trainer/Policy log std Std                           0.339913
trainer/Policy log std Max                          -0.0404351
trainer/Policy log std Min                          -2.49379
trainer/Alpha                                        0.0992207
trainer/Alpha Loss                                  -0.161175
exploration/num steps total                     160000
exploration/num paths total                        160
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.14345
exploration/Rewards Std                              1.10759
exploration/Rewards Max                              6.37369
exploration/Rewards Min                             -0.748271
exploration/Returns Mean                          4143.45
exploration/Returns Std                              0
exploration/Returns Max                           4143.45
exploration/Returns Min                           4143.45
exploration/Actions Mean                            -0.000527158
exploration/Actions Std                              0.814402
exploration/Actions Max                              0.999852
exploration/Actions Min                             -0.999719
exploration/Num Paths                                1
exploration/Average Returns                       4143.45
exploration/env_infos/final/reward_run Mean          4.35213
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.35213
exploration/env_infos/final/reward_run Min           4.35213
exploration/env_infos/initial/reward_run Mean       -0.16921
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.16921
exploration/env_infos/initial/reward_run Min        -0.16921
exploration/env_infos/reward_run Mean                4.5414
exploration/env_infos/reward_run Std                 1.10199
exploration/env_infos/reward_run Max                 6.79639
exploration/env_infos/reward_run Min                -0.2521
exploration/env_infos/final/reward_ctrl Mean        -0.491042
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.491042
exploration/env_infos/final/reward_ctrl Min         -0.491042
exploration/env_infos/initial/reward_ctrl Mean      -0.0115947
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0115947
exploration/env_infos/initial/reward_ctrl Min       -0.0115947
exploration/env_infos/reward_ctrl Mean              -0.39795
exploration/env_infos/reward_ctrl Std                0.0963863
exploration/env_infos/reward_ctrl Max               -0.0115947
exploration/env_infos/reward_ctrl Min               -0.578621
evaluation/num steps total                      795000
evaluation/num paths total                         795
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.09519
evaluation/Rewards Std                               1.49543
evaluation/Rewards Max                               6.56063
evaluation/Rewards Min                              -1.55271
evaluation/Returns Mean                           4095.19
evaluation/Returns Std                             420.72
evaluation/Returns Max                            4493.14
evaluation/Returns Min                            3419.82
evaluation/Actions Mean                             -0.0321223
evaluation/Actions Std                               0.823892
evaluation/Actions Max                               0.999911
evaluation/Actions Min                              -0.999942
evaluation/Num Paths                                 5
evaluation/Average Returns                        4095.19
evaluation/env_infos/final/reward_run Mean           3.69401
evaluation/env_infos/final/reward_run Std            2.1829
evaluation/env_infos/final/reward_run Max            5.82062
evaluation/env_infos/final/reward_run Min            0.95179
evaluation/env_infos/initial/reward_run Mean        -0.128712
evaluation/env_infos/initial/reward_run Std          0.178569
evaluation/env_infos/initial/reward_run Max          0.195386
evaluation/env_infos/initial/reward_run Min         -0.349638
evaluation/env_infos/reward_run Mean                 4.50309
evaluation/env_infos/reward_run Std                  1.51631
evaluation/env_infos/reward_run Max                  6.87572
evaluation/env_infos/reward_run Min                 -1.26493
evaluation/env_infos/final/reward_ctrl Mean         -0.407825
evaluation/env_infos/final/reward_ctrl Std           0.0823058
evaluation/env_infos/final/reward_ctrl Max          -0.259011
evaluation/env_infos/final/reward_ctrl Min          -0.485602
evaluation/env_infos/initial/reward_ctrl Mean       -0.0581211
evaluation/env_infos/initial/reward_ctrl Std         0.0188193
evaluation/env_infos/initial/reward_ctrl Max        -0.0361093
evaluation/env_infos/initial/reward_ctrl Min        -0.0835147
evaluation/env_infos/reward_ctrl Mean               -0.407898
evaluation/env_infos/reward_ctrl Std                 0.101447
evaluation/env_infos/reward_ctrl Max                -0.0361093
evaluation/env_infos/reward_ctrl Min                -0.590108
time/data storing (s)                                0.0161702
time/evaluation sampling (s)                         3.87994
time/exploration sampling (s)                        0.871932
time/logging (s)                                     0.041389
time/saving (s)                                      0.016203
time/training (s)                                   52.4185
time/epoch (s)                                      57.2441
time/total (s)                                    6670.57
Epoch                                              158
----------------------------------------------  ----------------
2020-07-08 22:58:08.957057 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 159 finished
----------------------------------------------  ---------------
replay_buffer/size                              161000
trainer/QF1 Loss                                     9.31862
trainer/QF2 Loss                                     7.19565
trainer/Policy Loss                               -234.928
trainer/Q1 Predictions Mean                        241.459
trainer/Q1 Predictions Std                          89.8512
trainer/Q1 Predictions Max                         312.052
trainer/Q1 Predictions Min                          11.6853
trainer/Q2 Predictions Mean                        241.231
trainer/Q2 Predictions Std                          89.8205
trainer/Q2 Predictions Max                         312.219
trainer/Q2 Predictions Min                          12.7469
trainer/Q Targets Mean                             240.728
trainer/Q Targets Std                               89.7895
trainer/Q Targets Max                              314.015
trainer/Q Targets Min                               11.5304
trainer/Log Pis Mean                                 6.30774
trainer/Log Pis Std                                  4.72741
trainer/Log Pis Max                                 17.8848
trainer/Log Pis Min                                 -6.50669
trainer/Policy mu Mean                               0.0917591
trainer/Policy mu Std                                1.53855
trainer/Policy mu Max                                3.76271
trainer/Policy mu Min                               -3.59034
trainer/Policy log std Mean                         -0.803618
trainer/Policy log std Std                           0.325912
trainer/Policy log std Max                          -0.0693784
trainer/Policy log std Min                          -2.45174
trainer/Alpha                                        0.0986829
trainer/Alpha Loss                                   0.712702
exploration/num steps total                     161000
exploration/num paths total                        161
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.93032
exploration/Rewards Std                              1.1368
exploration/Rewards Max                              6.1554
exploration/Rewards Min                             -1.28248
exploration/Returns Mean                          3930.32
exploration/Returns Std                              0
exploration/Returns Max                           3930.32
exploration/Returns Min                           3930.32
exploration/Actions Mean                            -0.00834229
exploration/Actions Std                              0.826897
exploration/Actions Max                              0.99977
exploration/Actions Min                             -0.999703
exploration/Num Paths                                1
exploration/Average Returns                       3930.32
exploration/env_infos/final/reward_run Mean          5.34134
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.34134
exploration/env_infos/final/reward_run Min           5.34134
exploration/env_infos/initial/reward_run Mean       -0.018083
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.018083
exploration/env_infos/initial/reward_run Min        -0.018083
exploration/env_infos/reward_run Mean                4.34062
exploration/env_infos/reward_run Std                 1.13864
exploration/env_infos/reward_run Max                 6.48757
exploration/env_infos/reward_run Min                -0.902923
exploration/env_infos/final/reward_ctrl Mean        -0.505322
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.505322
exploration/env_infos/final/reward_ctrl Min         -0.505322
exploration/env_infos/initial/reward_ctrl Mean      -0.0726718
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0726718
exploration/env_infos/initial/reward_ctrl Min       -0.0726718
exploration/env_infos/reward_ctrl Mean              -0.410296
exploration/env_infos/reward_ctrl Std                0.0927318
exploration/env_infos/reward_ctrl Max               -0.0726718
exploration/env_infos/reward_ctrl Min               -0.57887
evaluation/num steps total                      800000
evaluation/num paths total                         800
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.29893
evaluation/Rewards Std                               1.07231
evaluation/Rewards Max                               6.56326
evaluation/Rewards Min                              -0.713315
evaluation/Returns Mean                           4298.93
evaluation/Returns Std                              76.9497
evaluation/Returns Max                            4377.11
evaluation/Returns Min                            4159.72
evaluation/Actions Mean                             -0.0240316
evaluation/Actions Std                               0.848262
evaluation/Actions Max                               0.99808
evaluation/Actions Min                              -0.997633
evaluation/Num Paths                                 5
evaluation/Average Returns                        4298.93
evaluation/env_infos/final/reward_run Mean           6.04834
evaluation/env_infos/final/reward_run Std            0.455271
evaluation/env_infos/final/reward_run Max            6.74623
evaluation/env_infos/final/reward_run Min            5.34629
evaluation/env_infos/initial/reward_run Mean         0.0679207
evaluation/env_infos/initial/reward_run Std          0.139809
evaluation/env_infos/initial/reward_run Max          0.255156
evaluation/env_infos/initial/reward_run Min         -0.17454
evaluation/env_infos/reward_run Mean                 4.731
evaluation/env_infos/reward_run Std                  1.06971
evaluation/env_infos/reward_run Max                  7.03677
evaluation/env_infos/reward_run Min                 -0.393451
evaluation/env_infos/final/reward_ctrl Mean         -0.489045
evaluation/env_infos/final/reward_ctrl Std           0.0482824
evaluation/env_infos/final/reward_ctrl Max          -0.41215
evaluation/env_infos/final/reward_ctrl Min          -0.554653
evaluation/env_infos/initial/reward_ctrl Mean       -0.029899
evaluation/env_infos/initial/reward_ctrl Std         0.0195553
evaluation/env_infos/initial/reward_ctrl Max        -0.00648785
evaluation/env_infos/initial/reward_ctrl Min        -0.060362
evaluation/env_infos/reward_ctrl Mean               -0.432076
evaluation/env_infos/reward_ctrl Std                 0.0912439
evaluation/env_infos/reward_ctrl Max                -0.00648785
evaluation/env_infos/reward_ctrl Min                -0.584382
time/data storing (s)                                0.0123254
time/evaluation sampling (s)                         2.51974
time/exploration sampling (s)                        0.966302
time/logging (s)                                     0.0413446
time/saving (s)                                      0.0159672
time/training (s)                                   43.0433
time/epoch (s)                                      46.599
time/total (s)                                    6717.36
Epoch                                              159
----------------------------------------------  ---------------
2020-07-08 22:59:00.423454 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 160 finished
----------------------------------------------  ---------------
replay_buffer/size                              162000
trainer/QF1 Loss                                     5.37566
trainer/QF2 Loss                                     7.872
trainer/Policy Loss                               -239.29
trainer/Q1 Predictions Mean                        245.012
trainer/Q1 Predictions Std                          86.1456
trainer/Q1 Predictions Max                         311.716
trainer/Q1 Predictions Min                          10.073
trainer/Q2 Predictions Mean                        245.137
trainer/Q2 Predictions Std                          86.1838
trainer/Q2 Predictions Max                         312.833
trainer/Q2 Predictions Min                          11.649
trainer/Q Targets Mean                             244.991
trainer/Q Targets Std                               86.1104
trainer/Q Targets Max                              312.011
trainer/Q Targets Min                               10.6356
trainer/Log Pis Mean                                 5.8606
trainer/Log Pis Std                                  4.96538
trainer/Log Pis Max                                 20.8977
trainer/Log Pis Min                                 -5.9154
trainer/Policy mu Mean                               0.0554817
trainer/Policy mu Std                                1.48889
trainer/Policy mu Max                                4.02366
trainer/Policy mu Min                               -3.58654
trainer/Policy log std Mean                         -0.811208
trainer/Policy log std Std                           0.323552
trainer/Policy log std Max                          -0.0415725
trainer/Policy log std Min                          -2.51667
trainer/Alpha                                        0.0990157
trainer/Alpha Loss                                  -0.322373
exploration/num steps total                     162000
exploration/num paths total                        162
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.27615
exploration/Rewards Std                              1.07929
exploration/Rewards Max                              6.42836
exploration/Rewards Min                             -0.75085
exploration/Returns Mean                          4276.15
exploration/Returns Std                              0
exploration/Returns Max                           4276.15
exploration/Returns Min                           4276.15
exploration/Actions Mean                            -0.0086554
exploration/Actions Std                              0.820904
exploration/Actions Max                              0.999422
exploration/Actions Min                             -0.999436
exploration/Num Paths                                1
exploration/Average Returns                       4276.15
exploration/env_infos/final/reward_run Mean          4.94887
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.94887
exploration/env_infos/final/reward_run Min           4.94887
exploration/env_infos/initial/reward_run Mean        0.240885
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.240885
exploration/env_infos/initial/reward_run Min         0.240885
exploration/env_infos/reward_run Mean                4.68053
exploration/env_infos/reward_run Std                 1.06408
exploration/env_infos/reward_run Max                 6.89228
exploration/env_infos/reward_run Min                -0.17807
exploration/env_infos/final/reward_ctrl Mean        -0.299547
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.299547
exploration/env_infos/final/reward_ctrl Min         -0.299547
exploration/env_infos/initial/reward_ctrl Mean      -0.118439
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.118439
exploration/env_infos/initial/reward_ctrl Min       -0.118439
exploration/env_infos/reward_ctrl Mean              -0.404375
exploration/env_infos/reward_ctrl Std                0.0981834
exploration/env_infos/reward_ctrl Max               -0.114088
exploration/env_infos/reward_ctrl Min               -0.580411
evaluation/num steps total                      805000
evaluation/num paths total                         805
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.35268
evaluation/Rewards Std                               1.09048
evaluation/Rewards Max                               6.75312
evaluation/Rewards Min                              -1.03752
evaluation/Returns Mean                           4352.68
evaluation/Returns Std                              78.1161
evaluation/Returns Max                            4481.33
evaluation/Returns Min                            4285.47
evaluation/Actions Mean                             -0.0146213
evaluation/Actions Std                               0.838177
evaluation/Actions Max                               0.999365
evaluation/Actions Min                              -0.998987
evaluation/Num Paths                                 5
evaluation/Average Returns                        4352.68
evaluation/env_infos/final/reward_run Mean           4.30288
evaluation/env_infos/final/reward_run Std            0.443182
evaluation/env_infos/final/reward_run Max            5.12536
evaluation/env_infos/final/reward_run Min            3.81728
evaluation/env_infos/initial/reward_run Mean         0.023744
evaluation/env_infos/initial/reward_run Std          0.160446
evaluation/env_infos/initial/reward_run Max          0.336928
evaluation/env_infos/initial/reward_run Min         -0.107199
evaluation/env_infos/reward_run Mean                 4.77434
evaluation/env_infos/reward_run Std                  1.08167
evaluation/env_infos/reward_run Max                  7.06122
evaluation/env_infos/reward_run Min                 -0.517678
evaluation/env_infos/final/reward_ctrl Mean         -0.475836
evaluation/env_infos/final/reward_ctrl Std           0.0595576
evaluation/env_infos/final/reward_ctrl Max          -0.368061
evaluation/env_infos/final/reward_ctrl Min          -0.536814
evaluation/env_infos/initial/reward_ctrl Mean       -0.0579283
evaluation/env_infos/initial/reward_ctrl Std         0.0238054
evaluation/env_infos/initial/reward_ctrl Max        -0.0153183
evaluation/env_infos/initial/reward_ctrl Min        -0.0857744
evaluation/env_infos/reward_ctrl Mean               -0.421653
evaluation/env_infos/reward_ctrl Std                 0.0973528
evaluation/env_infos/reward_ctrl Max                -0.0153183
evaluation/env_infos/reward_ctrl Min                -0.585187
time/data storing (s)                                0.00692781
time/evaluation sampling (s)                         3.05187
time/exploration sampling (s)                        0.689869
time/logging (s)                                     0.0426175
time/saving (s)                                      0.0185669
time/training (s)                                   47.6059
time/epoch (s)                                      51.4158
time/total (s)                                    6768.83
Epoch                                              160
----------------------------------------------  ---------------
2020-07-08 22:59:41.483317 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 161 finished
----------------------------------------------  ----------------
replay_buffer/size                              163000
trainer/QF1 Loss                                     8.17775
trainer/QF2 Loss                                     9.18279
trainer/Policy Loss                               -231.406
trainer/Q1 Predictions Mean                        238.19
trainer/Q1 Predictions Std                          91.4483
trainer/Q1 Predictions Max                         311.067
trainer/Q1 Predictions Min                          13.5408
trainer/Q2 Predictions Mean                        237.543
trainer/Q2 Predictions Std                          91.1125
trainer/Q2 Predictions Max                         310.54
trainer/Q2 Predictions Min                          13.7779
trainer/Q Targets Mean                             236.946
trainer/Q Targets Std                               91.1234
trainer/Q Targets Max                              312.403
trainer/Q Targets Min                               12.606
trainer/Log Pis Mean                                 6.57881
trainer/Log Pis Std                                  5.20137
trainer/Log Pis Max                                 21.8951
trainer/Log Pis Min                                 -5.2924
trainer/Policy mu Mean                               0.135496
trainer/Policy mu Std                                1.56906
trainer/Policy mu Max                                4.70543
trainer/Policy mu Min                               -4.07794
trainer/Policy log std Mean                         -0.791451
trainer/Policy log std Std                           0.327049
trainer/Policy log std Max                           0.155409
trainer/Policy log std Min                          -2.43525
trainer/Alpha                                        0.0969389
trainer/Alpha Loss                                   1.35088
exploration/num steps total                     163000
exploration/num paths total                        163
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             3.98371
exploration/Rewards Std                              1.07573
exploration/Rewards Max                              6.09813
exploration/Rewards Min                             -0.752928
exploration/Returns Mean                          3983.71
exploration/Returns Std                              0
exploration/Returns Max                           3983.71
exploration/Returns Min                           3983.71
exploration/Actions Mean                            -0.0015445
exploration/Actions Std                              0.823649
exploration/Actions Max                              0.999871
exploration/Actions Min                             -0.99971
exploration/Num Paths                                1
exploration/Average Returns                       3983.71
exploration/env_infos/final/reward_run Mean          2.96353
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           2.96353
exploration/env_infos/final/reward_run Min           2.96353
exploration/env_infos/initial/reward_run Mean       -0.000257874
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.000257874
exploration/env_infos/initial/reward_run Min        -0.000257874
exploration/env_infos/reward_run Mean                4.39075
exploration/env_infos/reward_run Std                 1.06825
exploration/env_infos/reward_run Max                 6.37789
exploration/env_infos/reward_run Min                -0.293442
exploration/env_infos/final/reward_ctrl Mean        -0.425879
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.425879
exploration/env_infos/final/reward_ctrl Min         -0.425879
exploration/env_infos/initial/reward_ctrl Mean      -0.0951119
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0951119
exploration/env_infos/initial/reward_ctrl Min       -0.0951119
exploration/env_infos/reward_ctrl Mean              -0.40704
exploration/env_infos/reward_ctrl Std                0.0966548
exploration/env_infos/reward_ctrl Max               -0.0862622
exploration/env_infos/reward_ctrl Min               -0.580846
evaluation/num steps total                      810000
evaluation/num paths total                         810
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.29164
evaluation/Rewards Std                               1.0504
evaluation/Rewards Max                               6.93162
evaluation/Rewards Min                              -0.804643
evaluation/Returns Mean                           4291.64
evaluation/Returns Std                              58.5143
evaluation/Returns Max                            4347.5
evaluation/Returns Min                            4178.16
evaluation/Actions Mean                             -0.0227531
evaluation/Actions Std                               0.849151
evaluation/Actions Max                               0.998763
evaluation/Actions Min                              -0.998619
evaluation/Num Paths                                 5
evaluation/Average Returns                        4291.64
evaluation/env_infos/final/reward_run Mean           5.39181
evaluation/env_infos/final/reward_run Std            0.442275
evaluation/env_infos/final/reward_run Max            6.01321
evaluation/env_infos/final/reward_run Min            4.79323
evaluation/env_infos/initial/reward_run Mean         0.159526
evaluation/env_infos/initial/reward_run Std          0.157623
evaluation/env_infos/initial/reward_run Max          0.354174
evaluation/env_infos/initial/reward_run Min         -0.0343594
evaluation/env_infos/reward_run Mean                 4.72458
evaluation/env_infos/reward_run Std                  1.04324
evaluation/env_infos/reward_run Max                  7.26945
evaluation/env_infos/reward_run Min                 -0.292542
evaluation/env_infos/final/reward_ctrl Mean         -0.521875
evaluation/env_infos/final/reward_ctrl Std           0.0489557
evaluation/env_infos/final/reward_ctrl Max          -0.428237
evaluation/env_infos/final/reward_ctrl Min          -0.56494
evaluation/env_infos/initial/reward_ctrl Mean       -0.05183
evaluation/env_infos/initial/reward_ctrl Std         0.0189436
evaluation/env_infos/initial/reward_ctrl Max        -0.0259422
evaluation/env_infos/initial/reward_ctrl Min        -0.0799648
evaluation/env_infos/reward_ctrl Mean               -0.432945
evaluation/env_infos/reward_ctrl Std                 0.0929442
evaluation/env_infos/reward_ctrl Max                -0.0259422
evaluation/env_infos/reward_ctrl Min                -0.581111
time/data storing (s)                                0.00693984
time/evaluation sampling (s)                         3.14651
time/exploration sampling (s)                        0.670453
time/logging (s)                                     0.0437056
time/saving (s)                                      0.0162731
time/training (s)                                   37.1335
time/epoch (s)                                      41.0174
time/total (s)                                    6809.88
Epoch                                              161
----------------------------------------------  ----------------
2020-07-08 23:00:29.467895 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 162 finished
----------------------------------------------  ---------------
replay_buffer/size                              164000
trainer/QF1 Loss                                     6.48504
trainer/QF2 Loss                                     6.85823
trainer/Policy Loss                               -238.9
trainer/Q1 Predictions Mean                        244.784
trainer/Q1 Predictions Std                          89.1266
trainer/Q1 Predictions Max                         316.128
trainer/Q1 Predictions Min                          13.1669
trainer/Q2 Predictions Mean                        244.957
trainer/Q2 Predictions Std                          89.0826
trainer/Q2 Predictions Max                         315.926
trainer/Q2 Predictions Min                          13.2221
trainer/Q Targets Mean                             245.264
trainer/Q Targets Std                               89.1697
trainer/Q Targets Max                              318.612
trainer/Q Targets Min                               13.0933
trainer/Log Pis Mean                                 6.15411
trainer/Log Pis Std                                  4.90862
trainer/Log Pis Max                                 23.4971
trainer/Log Pis Min                                 -6.27188
trainer/Policy mu Mean                               0.0515113
trainer/Policy mu Std                                1.51762
trainer/Policy mu Max                                3.70238
trainer/Policy mu Min                               -3.49148
trainer/Policy log std Mean                         -0.817403
trainer/Policy log std Std                           0.340499
trainer/Policy log std Max                           0.0656535
trainer/Policy log std Min                          -2.55703
trainer/Alpha                                        0.0986484
trainer/Alpha Loss                                   0.356961
exploration/num steps total                     164000
exploration/num paths total                        164
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.23923
exploration/Rewards Std                              1.04004
exploration/Rewards Max                              6.38157
exploration/Rewards Min                             -0.588502
exploration/Returns Mean                          4239.23
exploration/Returns Std                              0
exploration/Returns Max                           4239.23
exploration/Returns Min                           4239.23
exploration/Actions Mean                            -0.0155983
exploration/Actions Std                              0.818056
exploration/Actions Max                              0.999654
exploration/Actions Min                             -0.999351
exploration/Num Paths                                1
exploration/Average Returns                       4239.23
exploration/env_infos/final/reward_run Mean          4.02729
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.02729
exploration/env_infos/final/reward_run Min           4.02729
exploration/env_infos/initial/reward_run Mean       -0.0400871
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0400871
exploration/env_infos/initial/reward_run Min        -0.0400871
exploration/env_infos/reward_run Mean                4.6409
exploration/env_infos/reward_run Std                 1.03561
exploration/env_infos/reward_run Max                 6.64538
exploration/env_infos/reward_run Min                -0.165417
exploration/env_infos/final/reward_ctrl Mean        -0.564061
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.564061
exploration/env_infos/final/reward_ctrl Min         -0.564061
exploration/env_infos/initial/reward_ctrl Mean      -0.0976425
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0976425
exploration/env_infos/initial/reward_ctrl Min       -0.0976425
exploration/env_infos/reward_ctrl Mean              -0.401676
exploration/env_infos/reward_ctrl Std                0.0951812
exploration/env_infos/reward_ctrl Max               -0.0818773
exploration/env_infos/reward_ctrl Min               -0.574842
evaluation/num steps total                      815000
evaluation/num paths total                         815
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.30527
evaluation/Rewards Std                               1.0355
evaluation/Rewards Max                               6.50921
evaluation/Rewards Min                              -0.650561
evaluation/Returns Mean                           4305.27
evaluation/Returns Std                              70.6407
evaluation/Returns Max                            4375.66
evaluation/Returns Min                            4196.3
evaluation/Actions Mean                             -0.0174718
evaluation/Actions Std                               0.828895
evaluation/Actions Max                               0.998122
evaluation/Actions Min                              -0.997751
evaluation/Num Paths                                 5
evaluation/Average Returns                        4305.27
evaluation/env_infos/final/reward_run Mean           5.11115
evaluation/env_infos/final/reward_run Std            0.820617
evaluation/env_infos/final/reward_run Max            6.19445
evaluation/env_infos/final/reward_run Min            3.97108
evaluation/env_infos/initial/reward_run Mean         0.118811
evaluation/env_infos/initial/reward_run Std          0.281393
evaluation/env_infos/initial/reward_run Max          0.570386
evaluation/env_infos/initial/reward_run Min         -0.296564
evaluation/env_infos/reward_run Mean                 4.71769
evaluation/env_infos/reward_run Std                  1.02789
evaluation/env_infos/reward_run Max                  6.88323
evaluation/env_infos/reward_run Min                 -0.318405
evaluation/env_infos/final/reward_ctrl Mean         -0.412095
evaluation/env_infos/final/reward_ctrl Std           0.0448093
evaluation/env_infos/final/reward_ctrl Max          -0.328458
evaluation/env_infos/final/reward_ctrl Min          -0.462288
evaluation/env_infos/initial/reward_ctrl Mean       -0.0495852
evaluation/env_infos/initial/reward_ctrl Std         0.0287281
evaluation/env_infos/initial/reward_ctrl Max        -0.02622
evaluation/env_infos/initial/reward_ctrl Min        -0.101708
evaluation/env_infos/reward_ctrl Mean               -0.412424
evaluation/env_infos/reward_ctrl Std                 0.0971977
evaluation/env_infos/reward_ctrl Max                -0.02622
evaluation/env_infos/reward_ctrl Min                -0.577715
time/data storing (s)                                0.00671924
time/evaluation sampling (s)                         2.58142
time/exploration sampling (s)                        0.648202
time/logging (s)                                     0.0497663
time/saving (s)                                      0.017715
time/training (s)                                   44.6566
time/epoch (s)                                      47.9604
time/total (s)                                    6857.86
Epoch                                              162
----------------------------------------------  ---------------
2020-07-08 23:01:27.874197 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 163 finished
----------------------------------------------  ---------------
replay_buffer/size                              165000
trainer/QF1 Loss                                     5.60659
trainer/QF2 Loss                                     7.27662
trainer/Policy Loss                               -234.102
trainer/Q1 Predictions Mean                        239.73
trainer/Q1 Predictions Std                          92.6728
trainer/Q1 Predictions Max                         311.237
trainer/Q1 Predictions Min                          14.5051
trainer/Q2 Predictions Mean                        240.44
trainer/Q2 Predictions Std                          92.8696
trainer/Q2 Predictions Max                         314.579
trainer/Q2 Predictions Min                          15.197
trainer/Q Targets Mean                             239.784
trainer/Q Targets Std                               92.6033
trainer/Q Targets Max                              314.323
trainer/Q Targets Min                               13.7617
trainer/Log Pis Mean                                 5.96722
trainer/Log Pis Std                                  4.83179
trainer/Log Pis Max                                 23.058
trainer/Log Pis Min                                 -4.71502
trainer/Policy mu Mean                               0.0953754
trainer/Policy mu Std                                1.50334
trainer/Policy mu Max                                3.83653
trainer/Policy mu Min                               -4.14623
trainer/Policy log std Mean                         -0.821256
trainer/Policy log std Std                           0.3553
trainer/Policy log std Max                          -0.0583797
trainer/Policy log std Min                          -2.69134
trainer/Alpha                                        0.0990356
trainer/Alpha Loss                                  -0.0757828
exploration/num steps total                     165000
exploration/num paths total                        165
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.28845
exploration/Rewards Std                              1.04088
exploration/Rewards Max                              6.53026
exploration/Rewards Min                             -0.613426
exploration/Returns Mean                          4288.45
exploration/Returns Std                              0
exploration/Returns Max                           4288.45
exploration/Returns Min                           4288.45
exploration/Actions Mean                             0.0106777
exploration/Actions Std                              0.812346
exploration/Actions Max                              0.999665
exploration/Actions Min                             -0.999395
exploration/Num Paths                                1
exploration/Average Returns                       4288.45
exploration/env_infos/final/reward_run Mean          4.49744
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.49744
exploration/env_infos/final/reward_run Min           4.49744
exploration/env_infos/initial/reward_run Mean        0.305749
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.305749
exploration/env_infos/initial/reward_run Min         0.305749
exploration/env_infos/reward_run Mean                4.68446
exploration/env_infos/reward_run Std                 1.02671
exploration/env_infos/reward_run Max                 6.86637
exploration/env_infos/reward_run Min                -0.12973
exploration/env_infos/final/reward_ctrl Mean        -0.484921
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.484921
exploration/env_infos/final/reward_ctrl Min         -0.484921
exploration/env_infos/initial/reward_ctrl Mean      -0.0808666
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0808666
exploration/env_infos/initial/reward_ctrl Min       -0.0808666
exploration/env_infos/reward_ctrl Mean              -0.396012
exploration/env_infos/reward_ctrl Std                0.0960666
exploration/env_infos/reward_ctrl Max               -0.0808666
exploration/env_infos/reward_ctrl Min               -0.591446
evaluation/num steps total                      820000
evaluation/num paths total                         820
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.36201
evaluation/Rewards Std                               1.10555
evaluation/Rewards Max                               6.57149
evaluation/Rewards Min                              -1.04834
evaluation/Returns Mean                           4362.01
evaluation/Returns Std                              42.9501
evaluation/Returns Max                            4426.41
evaluation/Returns Min                            4300.82
evaluation/Actions Mean                              0.0120526
evaluation/Actions Std                               0.826498
evaluation/Actions Max                               0.998668
evaluation/Actions Min                              -0.998553
evaluation/Num Paths                                 5
evaluation/Average Returns                        4362.01
evaluation/env_infos/final/reward_run Mean           5.2918
evaluation/env_infos/final/reward_run Std            1.00726
evaluation/env_infos/final/reward_run Max            6.56165
evaluation/env_infos/final/reward_run Min            3.78134
evaluation/env_infos/initial/reward_run Mean         0.0544854
evaluation/env_infos/initial/reward_run Std          0.248426
evaluation/env_infos/initial/reward_run Max          0.29172
evaluation/env_infos/initial/reward_run Min         -0.372845
evaluation/env_infos/reward_run Mean                 4.77196
evaluation/env_infos/reward_run Std                  1.09764
evaluation/env_infos/reward_run Max                  6.92449
evaluation/env_infos/reward_run Min                 -0.619722
evaluation/env_infos/final/reward_ctrl Mean         -0.437392
evaluation/env_infos/final/reward_ctrl Std           0.099478
evaluation/env_infos/final/reward_ctrl Max          -0.31474
evaluation/env_infos/final/reward_ctrl Min          -0.546057
evaluation/env_infos/initial/reward_ctrl Mean       -0.0685717
evaluation/env_infos/initial/reward_ctrl Std         0.02073
evaluation/env_infos/initial/reward_ctrl Max        -0.0465239
evaluation/env_infos/initial/reward_ctrl Min        -0.102317
evaluation/env_infos/reward_ctrl Mean               -0.409946
evaluation/env_infos/reward_ctrl Std                 0.0958884
evaluation/env_infos/reward_ctrl Max                -0.0465239
evaluation/env_infos/reward_ctrl Min                -0.582171
time/data storing (s)                                0.00723814
time/evaluation sampling (s)                         3.3648
time/exploration sampling (s)                        0.761262
time/logging (s)                                     0.0465048
time/saving (s)                                      0.0188735
time/training (s)                                   54.1699
time/epoch (s)                                      58.3686
time/total (s)                                    6916.26
Epoch                                              163
----------------------------------------------  ---------------
2020-07-08 23:02:10.862293 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 164 finished
----------------------------------------------  ---------------
replay_buffer/size                              166000
trainer/QF1 Loss                                     8.08982
trainer/QF2 Loss                                    12.2415
trainer/Policy Loss                               -241.978
trainer/Q1 Predictions Mean                        248.34
trainer/Q1 Predictions Std                          86.4423
trainer/Q1 Predictions Max                         312.934
trainer/Q1 Predictions Min                          12.1661
trainer/Q2 Predictions Mean                        248.295
trainer/Q2 Predictions Std                          86.1797
trainer/Q2 Predictions Max                         312.998
trainer/Q2 Predictions Min                          11.668
trainer/Q Targets Mean                             248.559
trainer/Q Targets Std                               86.3531
trainer/Q Targets Max                              315.729
trainer/Q Targets Min                               12.1025
trainer/Log Pis Mean                                 6.2226
trainer/Log Pis Std                                  4.72703
trainer/Log Pis Max                                 17.8738
trainer/Log Pis Min                                 -6.52591
trainer/Policy mu Mean                               0.136338
trainer/Policy mu Std                                1.50997
trainer/Policy mu Max                                3.62789
trainer/Policy mu Min                               -3.82705
trainer/Policy log std Mean                         -0.837676
trainer/Policy log std Std                           0.352577
trainer/Policy log std Max                          -0.0679533
trainer/Policy log std Min                          -2.8308
trainer/Alpha                                        0.0980708
trainer/Alpha Loss                                   0.516872
exploration/num steps total                     166000
exploration/num paths total                        166
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.21165
exploration/Rewards Std                              1.04729
exploration/Rewards Max                              6.26906
exploration/Rewards Min                             -0.441514
exploration/Returns Mean                          4211.65
exploration/Returns Std                              0
exploration/Returns Max                           4211.65
exploration/Returns Min                           4211.65
exploration/Actions Mean                             0.00631838
exploration/Actions Std                              0.829029
exploration/Actions Max                              0.999527
exploration/Actions Min                             -0.99957
exploration/Num Paths                                1
exploration/Average Returns                       4211.65
exploration/env_infos/final/reward_run Mean          6.21138
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.21138
exploration/env_infos/final/reward_run Min           6.21138
exploration/env_infos/initial/reward_run Mean       -0.0578567
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0578567
exploration/env_infos/initial/reward_run Min        -0.0578567
exploration/env_infos/reward_run Mean                4.62405
exploration/env_infos/reward_run Std                 1.0366
exploration/env_infos/reward_run Max                 6.65259
exploration/env_infos/reward_run Min                -0.140198
exploration/env_infos/final/reward_ctrl Mean        -0.366324
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.366324
exploration/env_infos/final/reward_ctrl Min         -0.366324
exploration/env_infos/initial/reward_ctrl Mean      -0.0606432
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0606432
exploration/env_infos/initial/reward_ctrl Min       -0.0606432
exploration/env_infos/reward_ctrl Mean              -0.412397
exploration/env_infos/reward_ctrl Std                0.0955394
exploration/env_infos/reward_ctrl Max               -0.0606432
exploration/env_infos/reward_ctrl Min               -0.584096
evaluation/num steps total                      825000
evaluation/num paths total                         825
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.38558
evaluation/Rewards Std                               1.0802
evaluation/Rewards Max                               6.74719
evaluation/Rewards Min                              -1.03199
evaluation/Returns Mean                           4385.58
evaluation/Returns Std                              91.1078
evaluation/Returns Max                            4535.64
evaluation/Returns Min                            4283.89
evaluation/Actions Mean                             -0.00129773
evaluation/Actions Std                               0.83562
evaluation/Actions Max                               0.999516
evaluation/Actions Min                              -0.99899
evaluation/Num Paths                                 5
evaluation/Average Returns                        4385.58
evaluation/env_infos/final/reward_run Mean           5.03497
evaluation/env_infos/final/reward_run Std            0.335212
evaluation/env_infos/final/reward_run Max            5.52376
evaluation/env_infos/final/reward_run Min            4.65052
evaluation/env_infos/initial/reward_run Mean         0.0398015
evaluation/env_infos/initial/reward_run Std          0.0806113
evaluation/env_infos/initial/reward_run Max          0.132796
evaluation/env_infos/initial/reward_run Min         -0.101947
evaluation/env_infos/reward_run Mean                 4.80453
evaluation/env_infos/reward_run Std                  1.07308
evaluation/env_infos/reward_run Max                  7.07823
evaluation/env_infos/reward_run Min                 -0.536001
evaluation/env_infos/final/reward_ctrl Mean         -0.370365
evaluation/env_infos/final/reward_ctrl Std           0.03196
evaluation/env_infos/final/reward_ctrl Max          -0.331943
evaluation/env_infos/final/reward_ctrl Min          -0.418473
evaluation/env_infos/initial/reward_ctrl Mean       -0.0722935
evaluation/env_infos/initial/reward_ctrl Std         0.0222886
evaluation/env_infos/initial/reward_ctrl Max        -0.0417947
evaluation/env_infos/initial/reward_ctrl Min        -0.0953556
evaluation/env_infos/reward_ctrl Mean               -0.418957
evaluation/env_infos/reward_ctrl Std                 0.0973107
evaluation/env_infos/reward_ctrl Max                -0.0417947
evaluation/env_infos/reward_ctrl Min                -0.586264
time/data storing (s)                                0.00668331
time/evaluation sampling (s)                         3.29385
time/exploration sampling (s)                        0.655259
time/logging (s)                                     0.0464831
time/saving (s)                                      0.0207329
time/training (s)                                   38.6713
time/epoch (s)                                      42.6943
time/total (s)                                    6959.24
Epoch                                              164
----------------------------------------------  ---------------
2020-07-08 23:03:00.920846 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 165 finished
----------------------------------------------  ---------------
replay_buffer/size                              167000
trainer/QF1 Loss                                     9.52684
trainer/QF2 Loss                                     9.34669
trainer/Policy Loss                               -229.252
trainer/Q1 Predictions Mean                        236.014
trainer/Q1 Predictions Std                          97.328
trainer/Q1 Predictions Max                         317.044
trainer/Q1 Predictions Min                          13.8243
trainer/Q2 Predictions Mean                        235.473
trainer/Q2 Predictions Std                          97.0775
trainer/Q2 Predictions Max                         316.96
trainer/Q2 Predictions Min                          13.2419
trainer/Q Targets Mean                             235.99
trainer/Q Targets Std                               97.3792
trainer/Q Targets Max                              320.789
trainer/Q Targets Min                               12.8162
trainer/Log Pis Mean                                 6.59416
trainer/Log Pis Std                                  5.15344
trainer/Log Pis Max                                 18.4955
trainer/Log Pis Min                                 -4.70146
trainer/Policy mu Mean                               0.0381684
trainer/Policy mu Std                                1.56442
trainer/Policy mu Max                                3.93662
trainer/Policy mu Min                               -3.75082
trainer/Policy log std Mean                         -0.817254
trainer/Policy log std Std                           0.343495
trainer/Policy log std Max                          -0.0766856
trainer/Policy log std Min                          -2.75483
trainer/Alpha                                        0.0991254
trainer/Alpha Loss                                   1.37329
exploration/num steps total                     167000
exploration/num paths total                        167
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.29751
exploration/Rewards Std                              1.03587
exploration/Rewards Max                              6.32083
exploration/Rewards Min                             -0.926587
exploration/Returns Mean                          4297.51
exploration/Returns Std                              0
exploration/Returns Max                           4297.51
exploration/Returns Min                           4297.51
exploration/Actions Mean                             0.00675519
exploration/Actions Std                              0.814612
exploration/Actions Max                              0.999711
exploration/Actions Min                             -0.99972
exploration/Num Paths                                1
exploration/Average Returns                       4297.51
exploration/env_infos/final/reward_run Mean          4.68548
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.68548
exploration/env_infos/final/reward_run Min           4.68548
exploration/env_infos/initial/reward_run Mean        0.0672527
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0672527
exploration/env_infos/initial/reward_run Min         0.0672527
exploration/env_infos/reward_run Mean                4.6957
exploration/env_infos/reward_run Std                 1.02831
exploration/env_infos/reward_run Max                 6.70589
exploration/env_infos/reward_run Min                -0.469204
exploration/env_infos/final/reward_ctrl Mean        -0.330164
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.330164
exploration/env_infos/final/reward_ctrl Min         -0.330164
exploration/env_infos/initial/reward_ctrl Mean      -0.0621838
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0621838
exploration/env_infos/initial/reward_ctrl Min       -0.0621838
exploration/env_infos/reward_ctrl Mean              -0.398182
exploration/env_infos/reward_ctrl Std                0.0997759
exploration/env_infos/reward_ctrl Max               -0.0621838
exploration/env_infos/reward_ctrl Min               -0.570553
evaluation/num steps total                      830000
evaluation/num paths total                         830
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.46684
evaluation/Rewards Std                               1.08753
evaluation/Rewards Max                               6.54831
evaluation/Rewards Min                              -0.946687
evaluation/Returns Mean                           4466.84
evaluation/Returns Std                              91.0152
evaluation/Returns Max                            4550.79
evaluation/Returns Min                            4299.96
evaluation/Actions Mean                             -0.00236925
evaluation/Actions Std                               0.835398
evaluation/Actions Max                               0.999038
evaluation/Actions Min                              -0.998854
evaluation/Num Paths                                 5
evaluation/Average Returns                        4466.84
evaluation/env_infos/final/reward_run Mean           5.24294
evaluation/env_infos/final/reward_run Std            0.750044
evaluation/env_infos/final/reward_run Max            5.76975
evaluation/env_infos/final/reward_run Min            3.81411
evaluation/env_infos/initial/reward_run Mean        -0.0882848
evaluation/env_infos/initial/reward_run Std          0.141856
evaluation/env_infos/initial/reward_run Max          0.176637
evaluation/env_infos/initial/reward_run Min         -0.226456
evaluation/env_infos/reward_run Mean                 4.88558
evaluation/env_infos/reward_run Std                  1.08056
evaluation/env_infos/reward_run Max                  7.0163
evaluation/env_infos/reward_run Min                 -0.548767
evaluation/env_infos/final/reward_ctrl Mean         -0.350034
evaluation/env_infos/final/reward_ctrl Std           0.107808
evaluation/env_infos/final/reward_ctrl Max          -0.207296
evaluation/env_infos/final/reward_ctrl Min          -0.53134
evaluation/env_infos/initial/reward_ctrl Mean       -0.0655295
evaluation/env_infos/initial/reward_ctrl Std         0.0276527
evaluation/env_infos/initial/reward_ctrl Max        -0.0330536
evaluation/env_infos/initial/reward_ctrl Min        -0.102402
evaluation/env_infos/reward_ctrl Mean               -0.418738
evaluation/env_infos/reward_ctrl Std                 0.100171
evaluation/env_infos/reward_ctrl Max                -0.0330536
evaluation/env_infos/reward_ctrl Min                -0.578638
time/data storing (s)                                0.00722692
time/evaluation sampling (s)                         6.15807
time/exploration sampling (s)                        0.75167
time/logging (s)                                     0.0448638
time/saving (s)                                      0.0223168
time/training (s)                                   43.0215
time/epoch (s)                                      50.0057
time/total (s)                                    7009.29
Epoch                                              165
----------------------------------------------  ---------------
2020-07-08 23:03:40.264925 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 166 finished
----------------------------------------------  ---------------
replay_buffer/size                              168000
trainer/QF1 Loss                                     6.58706
trainer/QF2 Loss                                     6.98423
trainer/Policy Loss                               -241.524
trainer/Q1 Predictions Mean                        247.521
trainer/Q1 Predictions Std                          87.4053
trainer/Q1 Predictions Max                         320.202
trainer/Q1 Predictions Min                          13.2804
trainer/Q2 Predictions Mean                        247.754
trainer/Q2 Predictions Std                          87.3335
trainer/Q2 Predictions Max                         318.708
trainer/Q2 Predictions Min                          13.5597
trainer/Q Targets Mean                             248.076
trainer/Q Targets Std                               87.6636
trainer/Q Targets Max                              319.262
trainer/Q Targets Min                               12.8968
trainer/Log Pis Mean                                 6.1746
trainer/Log Pis Std                                  5.0347
trainer/Log Pis Max                                 18.3252
trainer/Log Pis Min                                 -7.22645
trainer/Policy mu Mean                               0.0224929
trainer/Policy mu Std                                1.52882
trainer/Policy mu Max                                4.45859
trainer/Policy mu Min                               -3.26757
trainer/Policy log std Mean                         -0.8338
trainer/Policy log std Std                           0.337604
trainer/Policy log std Max                          -0.0610089
trainer/Policy log std Min                          -2.43293
trainer/Alpha                                        0.0985189
trainer/Alpha Loss                                   0.404662
exploration/num steps total                     168000
exploration/num paths total                        168
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.31551
exploration/Rewards Std                              1.08287
exploration/Rewards Max                              6.39374
exploration/Rewards Min                             -0.819343
exploration/Returns Mean                          4315.51
exploration/Returns Std                              0
exploration/Returns Max                           4315.51
exploration/Returns Min                           4315.51
exploration/Actions Mean                            -0.0259439
exploration/Actions Std                              0.830042
exploration/Actions Max                              0.999767
exploration/Actions Min                             -0.999736
exploration/Num Paths                                1
exploration/Average Returns                       4315.51
exploration/env_infos/final/reward_run Mean          4.53251
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.53251
exploration/env_infos/final/reward_run Min           4.53251
exploration/env_infos/initial/reward_run Mean        0.491892
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.491892
exploration/env_infos/initial/reward_run Min         0.491892
exploration/env_infos/reward_run Mean                4.72929
exploration/env_infos/reward_run Std                 1.07143
exploration/env_infos/reward_run Max                 6.79934
exploration/env_infos/reward_run Min                -0.307841
exploration/env_infos/final/reward_ctrl Mean        -0.34357
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.34357
exploration/env_infos/final/reward_ctrl Min         -0.34357
exploration/env_infos/initial/reward_ctrl Mean      -0.0793122
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0793122
exploration/env_infos/initial/reward_ctrl Min       -0.0793122
exploration/env_infos/reward_ctrl Mean              -0.413786
exploration/env_infos/reward_ctrl Std                0.0957209
exploration/env_infos/reward_ctrl Max               -0.0793122
exploration/env_infos/reward_ctrl Min               -0.577478
evaluation/num steps total                      835000
evaluation/num paths total                         835
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.48285
evaluation/Rewards Std                               1.13375
evaluation/Rewards Max                               6.66285
evaluation/Rewards Min                              -0.997184
evaluation/Returns Mean                           4482.85
evaluation/Returns Std                              72.022
evaluation/Returns Max                            4580.62
evaluation/Returns Min                            4410.01
evaluation/Actions Mean                             -0.0315964
evaluation/Actions Std                               0.834456
evaluation/Actions Max                               0.999839
evaluation/Actions Min                              -0.999987
evaluation/Num Paths                                 5
evaluation/Average Returns                        4482.85
evaluation/env_infos/final/reward_run Mean           4.59839
evaluation/env_infos/final/reward_run Std            1.81413
evaluation/env_infos/final/reward_run Max            6.1695
evaluation/env_infos/final/reward_run Min            1.20013
evaluation/env_infos/initial/reward_run Mean        -0.0775893
evaluation/env_infos/initial/reward_run Std          0.0789472
evaluation/env_infos/initial/reward_run Max          0.0759143
evaluation/env_infos/initial/reward_run Min         -0.138474
evaluation/env_infos/reward_run Mean                 4.90124
evaluation/env_infos/reward_run Std                  1.12907
evaluation/env_infos/reward_run Max                  7.02336
evaluation/env_infos/reward_run Min                 -0.484388
evaluation/env_infos/final/reward_ctrl Mean         -0.392615
evaluation/env_infos/final/reward_ctrl Std           0.0891571
evaluation/env_infos/final/reward_ctrl Max          -0.312607
evaluation/env_infos/final/reward_ctrl Min          -0.561281
evaluation/env_infos/initial/reward_ctrl Mean       -0.0685155
evaluation/env_infos/initial/reward_ctrl Std         0.0293713
evaluation/env_infos/initial/reward_ctrl Max        -0.0469328
evaluation/env_infos/initial/reward_ctrl Min        -0.126271
evaluation/env_infos/reward_ctrl Mean               -0.418389
evaluation/env_infos/reward_ctrl Std                 0.101832
evaluation/env_infos/reward_ctrl Max                -0.0469328
evaluation/env_infos/reward_ctrl Min                -0.596694
time/data storing (s)                                0.00713755
time/evaluation sampling (s)                         3.68796
time/exploration sampling (s)                        0.900417
time/logging (s)                                     0.046399
time/saving (s)                                      0.0196701
time/training (s)                                   34.6599
time/epoch (s)                                      39.3215
time/total (s)                                    7048.63
Epoch                                              166
----------------------------------------------  ---------------
2020-07-08 23:04:19.599832 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 167 finished
----------------------------------------------  ----------------
replay_buffer/size                              169000
trainer/QF1 Loss                                     6.73006
trainer/QF2 Loss                                     7.31807
trainer/Policy Loss                               -244.107
trainer/Q1 Predictions Mean                        250.371
trainer/Q1 Predictions Std                          84.2665
trainer/Q1 Predictions Max                         317.261
trainer/Q1 Predictions Min                          14.2026
trainer/Q2 Predictions Mean                        250.46
trainer/Q2 Predictions Std                          84.2015
trainer/Q2 Predictions Max                         316.326
trainer/Q2 Predictions Min                          13.8473
trainer/Q Targets Mean                             251.086
trainer/Q Targets Std                               84.545
trainer/Q Targets Max                              317.325
trainer/Q Targets Min                               13.2517
trainer/Log Pis Mean                                 6.2632
trainer/Log Pis Std                                  4.96274
trainer/Log Pis Max                                 18.464
trainer/Log Pis Min                                 -6.43017
trainer/Policy mu Mean                               0.0859663
trainer/Policy mu Std                                1.52734
trainer/Policy mu Max                                3.82648
trainer/Policy mu Min                               -4.18726
trainer/Policy log std Mean                         -0.828665
trainer/Policy log std Std                           0.341254
trainer/Policy log std Max                           0.306699
trainer/Policy log std Min                          -2.47489
trainer/Alpha                                        0.0997768
trainer/Alpha Loss                                   0.606662
exploration/num steps total                     169000
exploration/num paths total                        169
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.22023
exploration/Rewards Std                              1.10304
exploration/Rewards Max                              6.6787
exploration/Rewards Min                             -0.673195
exploration/Returns Mean                          4220.23
exploration/Returns Std                              0
exploration/Returns Max                           4220.23
exploration/Returns Min                           4220.23
exploration/Actions Mean                            -0.0126724
exploration/Actions Std                              0.81757
exploration/Actions Max                              0.999658
exploration/Actions Min                             -0.99974
exploration/Num Paths                                1
exploration/Average Returns                       4220.23
exploration/env_infos/final/reward_run Mean          3.46182
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.46182
exploration/env_infos/final/reward_run Min           3.46182
exploration/env_infos/initial/reward_run Mean        0.000270018
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.000270018
exploration/env_infos/initial/reward_run Min         0.000270018
exploration/env_infos/reward_run Mean                4.62138
exploration/env_infos/reward_run Std                 1.10261
exploration/env_infos/reward_run Max                 7.07075
exploration/env_infos/reward_run Min                -0.31611
exploration/env_infos/final/reward_ctrl Mean        -0.439906
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.439906
exploration/env_infos/final/reward_ctrl Min         -0.439906
exploration/env_infos/initial/reward_ctrl Mean      -0.132574
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.132574
exploration/env_infos/initial/reward_ctrl Min       -0.132574
exploration/env_infos/reward_ctrl Mean              -0.401149
exploration/env_infos/reward_ctrl Std                0.0979963
exploration/env_infos/reward_ctrl Max               -0.121393
exploration/env_infos/reward_ctrl Min               -0.584837
evaluation/num steps total                      840000
evaluation/num paths total                         840
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.45208
evaluation/Rewards Std                               1.10448
evaluation/Rewards Max                               6.66359
evaluation/Rewards Min                              -0.90365
evaluation/Returns Mean                           4452.08
evaluation/Returns Std                              74.9873
evaluation/Returns Max                            4535.42
evaluation/Returns Min                            4339.72
evaluation/Actions Mean                             -0.0299188
evaluation/Actions Std                               0.841495
evaluation/Actions Max                               0.999456
evaluation/Actions Min                              -0.998725
evaluation/Num Paths                                 5
evaluation/Average Returns                        4452.08
evaluation/env_infos/final/reward_run Mean           4.78844
evaluation/env_infos/final/reward_run Std            0.776924
evaluation/env_infos/final/reward_run Max            5.74138
evaluation/env_infos/final/reward_run Min            3.64706
evaluation/env_infos/initial/reward_run Mean        -0.0917188
evaluation/env_infos/initial/reward_run Std          0.194851
evaluation/env_infos/initial/reward_run Max          0.280765
evaluation/env_infos/initial/reward_run Min         -0.281045
evaluation/env_infos/reward_run Mean                 4.87748
evaluation/env_infos/reward_run Std                  1.09743
evaluation/env_infos/reward_run Max                  6.98068
evaluation/env_infos/reward_run Min                 -0.533736
evaluation/env_infos/final/reward_ctrl Mean         -0.416977
evaluation/env_infos/final/reward_ctrl Std           0.111751
evaluation/env_infos/final/reward_ctrl Max          -0.249949
evaluation/env_infos/final/reward_ctrl Min          -0.534887
evaluation/env_infos/initial/reward_ctrl Mean       -0.0399238
evaluation/env_infos/initial/reward_ctrl Std         0.0102745
evaluation/env_infos/initial/reward_ctrl Max        -0.0246356
evaluation/env_infos/initial/reward_ctrl Min        -0.0543668
evaluation/env_infos/reward_ctrl Mean               -0.425406
evaluation/env_infos/reward_ctrl Std                 0.0950828
evaluation/env_infos/reward_ctrl Max                -0.0229106
evaluation/env_infos/reward_ctrl Min                -0.581282
time/data storing (s)                                0.00696748
time/evaluation sampling (s)                         2.94076
time/exploration sampling (s)                        0.704503
time/logging (s)                                     0.0519508
time/saving (s)                                      0.0205204
time/training (s)                                   35.5953
time/epoch (s)                                      39.32
time/total (s)                                    7087.96
Epoch                                              167
----------------------------------------------  ----------------
2020-07-08 23:05:11.775647 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 168 finished
----------------------------------------------  ---------------
replay_buffer/size                              170000
trainer/QF1 Loss                                     9.34429
trainer/QF2 Loss                                     8.33802
trainer/Policy Loss                               -222.474
trainer/Q1 Predictions Mean                        228.04
trainer/Q1 Predictions Std                         104.889
trainer/Q1 Predictions Max                         319.67
trainer/Q1 Predictions Min                          15.0673
trainer/Q2 Predictions Mean                        227.807
trainer/Q2 Predictions Std                         104.652
trainer/Q2 Predictions Max                         318.005
trainer/Q2 Predictions Min                          15.0921
trainer/Q Targets Mean                             228.015
trainer/Q Targets Std                              104.585
trainer/Q Targets Max                              318.368
trainer/Q Targets Min                               15.3805
trainer/Log Pis Mean                                 5.49311
trainer/Log Pis Std                                  5.3882
trainer/Log Pis Max                                 23.5611
trainer/Log Pis Min                                 -6.51289
trainer/Policy mu Mean                               0.0236341
trainer/Policy mu Std                                1.48019
trainer/Policy mu Max                                5.57
trainer/Policy mu Min                               -4.31895
trainer/Policy log std Mean                         -0.790815
trainer/Policy log std Std                           0.33508
trainer/Policy log std Max                          -0.0432774
trainer/Policy log std Min                          -2.53392
trainer/Alpha                                        0.100492
trainer/Alpha Loss                                  -1.16463
exploration/num steps total                     170000
exploration/num paths total                        170
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.30386
exploration/Rewards Std                              1.03611
exploration/Rewards Max                              6.41661
exploration/Rewards Min                             -0.738616
exploration/Returns Mean                          4303.86
exploration/Returns Std                              0
exploration/Returns Max                           4303.86
exploration/Returns Min                           4303.86
exploration/Actions Mean                            -0.0143117
exploration/Actions Std                              0.810805
exploration/Actions Max                              0.999448
exploration/Actions Min                             -0.999524
exploration/Num Paths                                1
exploration/Average Returns                       4303.86
exploration/env_infos/final/reward_run Mean          5.93077
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.93077
exploration/env_infos/final/reward_run Min           5.93077
exploration/env_infos/initial/reward_run Mean        0.219469
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.219469
exploration/env_infos/initial/reward_run Min         0.219469
exploration/env_infos/reward_run Mean                4.69843
exploration/env_infos/reward_run Std                 1.02486
exploration/env_infos/reward_run Max                 6.76898
exploration/env_infos/reward_run Min                -0.209254
exploration/env_infos/final/reward_ctrl Mean        -0.242815
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.242815
exploration/env_infos/final/reward_ctrl Min         -0.242815
exploration/env_infos/initial/reward_ctrl Mean      -0.0570633
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0570633
exploration/env_infos/initial/reward_ctrl Min       -0.0570633
exploration/env_infos/reward_ctrl Mean              -0.394566
exploration/env_infos/reward_ctrl Std                0.0989682
exploration/env_infos/reward_ctrl Max               -0.0570145
exploration/env_infos/reward_ctrl Min               -0.578895
evaluation/num steps total                      845000
evaluation/num paths total                         845
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.41304
evaluation/Rewards Std                               1.05725
evaluation/Rewards Max                               6.68496
evaluation/Rewards Min                              -0.976886
evaluation/Returns Mean                           4413.04
evaluation/Returns Std                              81.5885
evaluation/Returns Max                            4546.17
evaluation/Returns Min                            4305.56
evaluation/Actions Mean                             -0.0293542
evaluation/Actions Std                               0.832647
evaluation/Actions Max                               0.998871
evaluation/Actions Min                              -0.999594
evaluation/Num Paths                                 5
evaluation/Average Returns                        4413.04
evaluation/env_infos/final/reward_run Mean           4.24523
evaluation/env_infos/final/reward_run Std            0.963106
evaluation/env_infos/final/reward_run Max            5.75588
evaluation/env_infos/final/reward_run Min            2.94091
evaluation/env_infos/initial/reward_run Mean         0.0761648
evaluation/env_infos/initial/reward_run Std          0.189825
evaluation/env_infos/initial/reward_run Max          0.387987
evaluation/env_infos/initial/reward_run Min         -0.194371
evaluation/env_infos/reward_run Mean                 4.82954
evaluation/env_infos/reward_run Std                  1.0458
evaluation/env_infos/reward_run Max                  7.06288
evaluation/env_infos/reward_run Min                 -0.520996
evaluation/env_infos/final/reward_ctrl Mean         -0.47982
evaluation/env_infos/final/reward_ctrl Std           0.0546322
evaluation/env_infos/final/reward_ctrl Max          -0.381007
evaluation/env_infos/final/reward_ctrl Min          -0.539375
evaluation/env_infos/initial/reward_ctrl Mean       -0.0700374
evaluation/env_infos/initial/reward_ctrl Std         0.0115809
evaluation/env_infos/initial/reward_ctrl Max        -0.0485058
evaluation/env_infos/initial/reward_ctrl Min        -0.0809362
evaluation/env_infos/reward_ctrl Mean               -0.416498
evaluation/env_infos/reward_ctrl Std                 0.0982737
evaluation/env_infos/reward_ctrl Max                -0.0485058
evaluation/env_infos/reward_ctrl Min                -0.586225
time/data storing (s)                                0.00898329
time/evaluation sampling (s)                         4.44516
time/exploration sampling (s)                        2.51759
time/logging (s)                                     0.0484342
time/saving (s)                                      0.0180406
time/training (s)                                   45.0974
time/epoch (s)                                      52.1356
time/total (s)                                    7140.13
Epoch                                              168
----------------------------------------------  ---------------
2020-07-08 23:05:46.876292 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 169 finished
----------------------------------------------  ---------------
replay_buffer/size                              171000
trainer/QF1 Loss                                     6.66577
trainer/QF2 Loss                                     7.78557
trainer/Policy Loss                               -227.506
trainer/Q1 Predictions Mean                        233.401
trainer/Q1 Predictions Std                          98.1789
trainer/Q1 Predictions Max                         320.647
trainer/Q1 Predictions Min                          13.9042
trainer/Q2 Predictions Mean                        233.021
trainer/Q2 Predictions Std                          98.0367
trainer/Q2 Predictions Max                         319.24
trainer/Q2 Predictions Min                          13.495
trainer/Q Targets Mean                             233.382
trainer/Q Targets Std                               98.2439
trainer/Q Targets Max                              320.877
trainer/Q Targets Min                               12.8319
trainer/Log Pis Mean                                 5.95074
trainer/Log Pis Std                                  4.96976
trainer/Log Pis Max                                 19.63
trainer/Log Pis Min                                 -6.54728
trainer/Policy mu Mean                               0.0757038
trainer/Policy mu Std                                1.50014
trainer/Policy mu Max                                3.92161
trainer/Policy mu Min                               -4.14147
trainer/Policy log std Mean                         -0.779956
trainer/Policy log std Std                           0.349887
trainer/Policy log std Max                          -0.00381702
trainer/Policy log std Min                          -2.50743
trainer/Alpha                                        0.100451
trainer/Alpha Loss                                  -0.113192
exploration/num steps total                     171000
exploration/num paths total                        171
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.14472
exploration/Rewards Std                              1.07483
exploration/Rewards Max                              6.3167
exploration/Rewards Min                             -0.719997
exploration/Returns Mean                          4144.72
exploration/Returns Std                              0
exploration/Returns Max                           4144.72
exploration/Returns Min                           4144.72
exploration/Actions Mean                            -0.00970801
exploration/Actions Std                              0.822031
exploration/Actions Max                              0.999678
exploration/Actions Min                             -0.999606
exploration/Num Paths                                1
exploration/Average Returns                       4144.72
exploration/env_infos/final/reward_run Mean          6.05882
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.05882
exploration/env_infos/final/reward_run Min           6.05882
exploration/env_infos/initial/reward_run Mean        0.64483
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.64483
exploration/env_infos/initial/reward_run Min         0.64483
exploration/env_infos/reward_run Mean                4.55021
exploration/env_infos/reward_run Std                 1.07128
exploration/env_infos/reward_run Max                 6.65271
exploration/env_infos/reward_run Min                -0.214725
exploration/env_infos/final/reward_ctrl Mean        -0.529866
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.529866
exploration/env_infos/final/reward_ctrl Min         -0.529866
exploration/env_infos/initial/reward_ctrl Mean      -0.11306
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.11306
exploration/env_infos/initial/reward_ctrl Min       -0.11306
exploration/env_infos/reward_ctrl Mean              -0.405498
exploration/env_infos/reward_ctrl Std                0.096815
exploration/env_infos/reward_ctrl Max               -0.0909874
exploration/env_infos/reward_ctrl Min               -0.575754
evaluation/num steps total                      850000
evaluation/num paths total                         850
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.34762
evaluation/Rewards Std                               1.07854
evaluation/Rewards Max                               6.56694
evaluation/Rewards Min                              -0.773682
evaluation/Returns Mean                           4347.62
evaluation/Returns Std                              40.9471
evaluation/Returns Max                            4398.75
evaluation/Returns Min                            4277.18
evaluation/Actions Mean                             -0.012454
evaluation/Actions Std                               0.83859
evaluation/Actions Max                               0.998938
evaluation/Actions Min                              -0.9986
evaluation/Num Paths                                 5
evaluation/Average Returns                        4347.62
evaluation/env_infos/final/reward_run Mean           5.57136
evaluation/env_infos/final/reward_run Std            0.966393
evaluation/env_infos/final/reward_run Max            6.39763
evaluation/env_infos/final/reward_run Min            3.77647
evaluation/env_infos/initial/reward_run Mean         0.0622651
evaluation/env_infos/initial/reward_run Std          0.145362
evaluation/env_infos/initial/reward_run Max          0.302464
evaluation/env_infos/initial/reward_run Min         -0.151237
evaluation/env_infos/reward_run Mean                 4.76965
evaluation/env_infos/reward_run Std                  1.07357
evaluation/env_infos/reward_run Max                  6.92212
evaluation/env_infos/reward_run Min                 -0.321726
evaluation/env_infos/final/reward_ctrl Mean         -0.436604
evaluation/env_infos/final/reward_ctrl Std           0.0627159
evaluation/env_infos/final/reward_ctrl Max          -0.329182
evaluation/env_infos/final/reward_ctrl Min          -0.518864
evaluation/env_infos/initial/reward_ctrl Mean       -0.0601533
evaluation/env_infos/initial/reward_ctrl Std         0.0159639
evaluation/env_infos/initial/reward_ctrl Max        -0.0330987
evaluation/env_infos/initial/reward_ctrl Min        -0.0770673
evaluation/env_infos/reward_ctrl Mean               -0.422033
evaluation/env_infos/reward_ctrl Std                 0.0980008
evaluation/env_infos/reward_ctrl Max                -0.0330987
evaluation/env_infos/reward_ctrl Min                -0.584039
time/data storing (s)                                0.00674829
time/evaluation sampling (s)                         2.54896
time/exploration sampling (s)                        0.680697
time/logging (s)                                     0.0451036
time/saving (s)                                      0.0185304
time/training (s)                                   31.779
time/epoch (s)                                      35.0791
time/total (s)                                    7175.22
Epoch                                              169
----------------------------------------------  ---------------
2020-07-08 23:06:22.017446 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 170 finished
----------------------------------------------  ---------------
replay_buffer/size                              172000
trainer/QF1 Loss                                     8.939
trainer/QF2 Loss                                     6.61443
trainer/Policy Loss                               -240.317
trainer/Q1 Predictions Mean                        246.76
trainer/Q1 Predictions Std                          90.0568
trainer/Q1 Predictions Max                         320.025
trainer/Q1 Predictions Min                          14.2841
trainer/Q2 Predictions Mean                        246.623
trainer/Q2 Predictions Std                          90.2903
trainer/Q2 Predictions Max                         318.084
trainer/Q2 Predictions Min                          -6.9447
trainer/Q Targets Mean                             246.291
trainer/Q Targets Std                               90.3461
trainer/Q Targets Max                              319.984
trainer/Q Targets Min                              -12.2256
trainer/Log Pis Mean                                 6.32698
trainer/Log Pis Std                                  5.69181
trainer/Log Pis Max                                 31.6861
trainer/Log Pis Min                                 -5.57758
trainer/Policy mu Mean                               0.0233505
trainer/Policy mu Std                                1.53731
trainer/Policy mu Max                                4.82371
trainer/Policy mu Min                               -5.48809
trainer/Policy log std Mean                         -0.814333
trainer/Policy log std Std                           0.326098
trainer/Policy log std Max                           0.212958
trainer/Policy log std Min                          -2.55392
trainer/Alpha                                        0.101135
trainer/Alpha Loss                                   0.749224
exploration/num steps total                     172000
exploration/num paths total                        172
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.17942
exploration/Rewards Std                              1.09593
exploration/Rewards Max                              6.4474
exploration/Rewards Min                             -0.684002
exploration/Returns Mean                          4179.42
exploration/Returns Std                              0
exploration/Returns Max                           4179.42
exploration/Returns Min                           4179.42
exploration/Actions Mean                             0.00109234
exploration/Actions Std                              0.818653
exploration/Actions Max                              0.999739
exploration/Actions Min                             -0.999479
exploration/Num Paths                                1
exploration/Average Returns                       4179.42
exploration/env_infos/final/reward_run Mean          3.59684
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.59684
exploration/env_infos/final/reward_run Min           3.59684
exploration/env_infos/initial/reward_run Mean        0.151125
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.151125
exploration/env_infos/initial/reward_run Min         0.151125
exploration/env_infos/reward_run Mean                4.58153
exploration/env_infos/reward_run Std                 1.09519
exploration/env_infos/reward_run Max                 6.89963
exploration/env_infos/reward_run Min                -0.347621
exploration/env_infos/final/reward_ctrl Mean        -0.524112
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.524112
exploration/env_infos/final/reward_ctrl Min         -0.524112
exploration/env_infos/initial/reward_ctrl Mean      -0.0955999
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0955999
exploration/env_infos/initial/reward_ctrl Min       -0.0955999
exploration/env_infos/reward_ctrl Mean              -0.402116
exploration/env_infos/reward_ctrl Std                0.102403
exploration/env_infos/reward_ctrl Max               -0.0945589
exploration/env_infos/reward_ctrl Min               -0.57023
evaluation/num steps total                      855000
evaluation/num paths total                         855
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.25446
evaluation/Rewards Std                               1.08216
evaluation/Rewards Max                               6.59375
evaluation/Rewards Min                              -0.95799
evaluation/Returns Mean                           4254.46
evaluation/Returns Std                              97.6688
evaluation/Returns Max                            4378.02
evaluation/Returns Min                            4080.5
evaluation/Actions Mean                             -0.00157028
evaluation/Actions Std                               0.82839
evaluation/Actions Max                               0.999012
evaluation/Actions Min                              -0.99793
evaluation/Num Paths                                 5
evaluation/Average Returns                        4254.46
evaluation/env_infos/final/reward_run Mean           4.45553
evaluation/env_infos/final/reward_run Std            0.516356
evaluation/env_infos/final/reward_run Max            5.33377
evaluation/env_infos/final/reward_run Min            3.77191
evaluation/env_infos/initial/reward_run Mean         0.113457
evaluation/env_infos/initial/reward_run Std          0.0377844
evaluation/env_infos/initial/reward_run Max          0.187845
evaluation/env_infos/initial/reward_run Min          0.0846843
evaluation/env_infos/reward_run Mean                 4.6662
evaluation/env_infos/reward_run Std                  1.07722
evaluation/env_infos/reward_run Max                  6.91569
evaluation/env_infos/reward_run Min                 -0.470766
evaluation/env_infos/final/reward_ctrl Mean         -0.425342
evaluation/env_infos/final/reward_ctrl Std           0.0433367
evaluation/env_infos/final/reward_ctrl Max          -0.363434
evaluation/env_infos/final/reward_ctrl Min          -0.473969
evaluation/env_infos/initial/reward_ctrl Mean       -0.0643128
evaluation/env_infos/initial/reward_ctrl Std         0.0271637
evaluation/env_infos/initial/reward_ctrl Max        -0.0264055
evaluation/env_infos/initial/reward_ctrl Min        -0.0951152
evaluation/env_infos/reward_ctrl Mean               -0.41174
evaluation/env_infos/reward_ctrl Std                 0.107033
evaluation/env_infos/reward_ctrl Max                -0.0264055
evaluation/env_infos/reward_ctrl Min                -0.586129
time/data storing (s)                                0.00659844
time/evaluation sampling (s)                         2.49121
time/exploration sampling (s)                        0.628126
time/logging (s)                                     0.0456376
time/saving (s)                                      0.0160223
time/training (s)                                   31.8545
time/epoch (s)                                      35.0421
time/total (s)                                    7210.36
Epoch                                              170
----------------------------------------------  ---------------
2020-07-08 23:06:57.445392 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 171 finished
----------------------------------------------  ---------------
replay_buffer/size                              173000
trainer/QF1 Loss                                     7.44581
trainer/QF2 Loss                                     6.7667
trainer/Policy Loss                               -229.271
trainer/Q1 Predictions Mean                        235.032
trainer/Q1 Predictions Std                         101.654
trainer/Q1 Predictions Max                         321.989
trainer/Q1 Predictions Min                          14.266
trainer/Q2 Predictions Mean                        234.596
trainer/Q2 Predictions Std                         101.551
trainer/Q2 Predictions Max                         320.697
trainer/Q2 Predictions Min                          13.5654
trainer/Q Targets Mean                             235.205
trainer/Q Targets Std                              101.826
trainer/Q Targets Max                              320.28
trainer/Q Targets Min                               14.0267
trainer/Log Pis Mean                                 5.66299
trainer/Log Pis Std                                  5.05097
trainer/Log Pis Max                                 17.4687
trainer/Log Pis Min                                 -6.89891
trainer/Policy mu Mean                               0.0458493
trainer/Policy mu Std                                1.46602
trainer/Policy mu Max                                3.52275
trainer/Policy mu Min                               -3.52487
trainer/Policy log std Mean                         -0.803246
trainer/Policy log std Std                           0.361092
trainer/Policy log std Max                           0.0108608
trainer/Policy log std Min                          -2.72578
trainer/Alpha                                        0.101407
trainer/Alpha Loss                                  -0.771262
exploration/num steps total                     173000
exploration/num paths total                        173
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.18102
exploration/Rewards Std                              1.0829
exploration/Rewards Max                              6.44011
exploration/Rewards Min                             -0.792454
exploration/Returns Mean                          4181.02
exploration/Returns Std                              0
exploration/Returns Max                           4181.02
exploration/Returns Min                           4181.02
exploration/Actions Mean                             0.00276434
exploration/Actions Std                              0.805119
exploration/Actions Max                              0.999969
exploration/Actions Min                             -0.999789
exploration/Num Paths                                1
exploration/Average Returns                       4181.02
exploration/env_infos/final/reward_run Mean          4.82053
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.82053
exploration/env_infos/final/reward_run Min           4.82053
exploration/env_infos/initial/reward_run Mean       -0.191498
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.191498
exploration/env_infos/initial/reward_run Min        -0.191498
exploration/env_infos/reward_run Mean                4.56995
exploration/env_infos/reward_run Std                 1.07487
exploration/env_infos/reward_run Max                 6.71036
exploration/env_infos/reward_run Min                -0.619052
exploration/env_infos/final/reward_ctrl Mean        -0.278561
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.278561
exploration/env_infos/final/reward_ctrl Min         -0.278561
exploration/env_infos/initial/reward_ctrl Mean      -0.104768
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.104768
exploration/env_infos/initial/reward_ctrl Min       -0.104768
exploration/env_infos/reward_ctrl Mean              -0.388934
exploration/env_infos/reward_ctrl Std                0.100699
exploration/env_infos/reward_ctrl Max               -0.0929308
exploration/env_infos/reward_ctrl Min               -0.588059
evaluation/num steps total                      860000
evaluation/num paths total                         860
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.34169
evaluation/Rewards Std                               1.10968
evaluation/Rewards Max                               6.56958
evaluation/Rewards Min                              -1.0876
evaluation/Returns Mean                           4341.69
evaluation/Returns Std                              75.3649
evaluation/Returns Max                            4462.9
evaluation/Returns Min                            4234.77
evaluation/Actions Mean                             -0.00515017
evaluation/Actions Std                               0.823722
evaluation/Actions Max                               0.99942
evaluation/Actions Min                              -0.999077
evaluation/Num Paths                                 5
evaluation/Average Returns                        4341.69
evaluation/env_infos/final/reward_run Mean           5.25317
evaluation/env_infos/final/reward_run Std            0.956494
evaluation/env_infos/final/reward_run Max            6.20897
evaluation/env_infos/final/reward_run Min            3.50595
evaluation/env_infos/initial/reward_run Mean        -0.0917737
evaluation/env_infos/initial/reward_run Std          0.0978855
evaluation/env_infos/initial/reward_run Max          0.0855625
evaluation/env_infos/initial/reward_run Min         -0.197198
evaluation/env_infos/reward_run Mean                 4.74882
evaluation/env_infos/reward_run Std                  1.09968
evaluation/env_infos/reward_run Max                  6.84358
evaluation/env_infos/reward_run Min                 -0.773324
evaluation/env_infos/final/reward_ctrl Mean         -0.383041
evaluation/env_infos/final/reward_ctrl Std           0.0810155
evaluation/env_infos/final/reward_ctrl Max          -0.233009
evaluation/env_infos/final/reward_ctrl Min          -0.470891
evaluation/env_infos/initial/reward_ctrl Mean       -0.0326103
evaluation/env_infos/initial/reward_ctrl Std         0.0073491
evaluation/env_infos/initial/reward_ctrl Max        -0.0242232
evaluation/env_infos/initial/reward_ctrl Min        -0.0456227
evaluation/env_infos/reward_ctrl Mean               -0.407127
evaluation/env_infos/reward_ctrl Std                 0.101075
evaluation/env_infos/reward_ctrl Max                -0.0242232
evaluation/env_infos/reward_ctrl Min                -0.583721
time/data storing (s)                                0.00693187
time/evaluation sampling (s)                         2.38234
time/exploration sampling (s)                        0.631295
time/logging (s)                                     0.0438234
time/saving (s)                                      0.015905
time/training (s)                                   32.293
time/epoch (s)                                      35.3733
time/total (s)                                    7245.77
Epoch                                              171
----------------------------------------------  ---------------
2020-07-08 23:07:33.052364 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 172 finished
----------------------------------------------  ---------------
replay_buffer/size                              174000
trainer/QF1 Loss                                     6.82561
trainer/QF2 Loss                                     8.79705
trainer/Policy Loss                               -242.283
trainer/Q1 Predictions Mean                        248.463
trainer/Q1 Predictions Std                          88.8228
trainer/Q1 Predictions Max                         315.845
trainer/Q1 Predictions Min                          15.321
trainer/Q2 Predictions Mean                        248.473
trainer/Q2 Predictions Std                          88.9155
trainer/Q2 Predictions Max                         317.625
trainer/Q2 Predictions Min                          15.946
trainer/Q Targets Mean                             248.34
trainer/Q Targets Std                               88.8059
trainer/Q Targets Max                              318.181
trainer/Q Targets Min                               15.4647
trainer/Log Pis Mean                                 6.37718
trainer/Log Pis Std                                  4.97979
trainer/Log Pis Max                                 19.2901
trainer/Log Pis Min                                 -5.852
trainer/Policy mu Mean                              -0.0275029
trainer/Policy mu Std                                1.52572
trainer/Policy mu Max                                3.29723
trainer/Policy mu Min                               -4.50996
trainer/Policy log std Mean                         -0.809867
trainer/Policy log std Std                           0.33168
trainer/Policy log std Max                          -0.074371
trainer/Policy log std Min                          -2.37203
trainer/Alpha                                        0.103275
trainer/Alpha Loss                                   0.856379
exploration/num steps total                     174000
exploration/num paths total                        174
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.09265
exploration/Rewards Std                              1.08869
exploration/Rewards Max                              6.20085
exploration/Rewards Min                             -0.848763
exploration/Returns Mean                          4092.65
exploration/Returns Std                              0
exploration/Returns Max                           4092.65
exploration/Returns Min                           4092.65
exploration/Actions Mean                            -0.010762
exploration/Actions Std                              0.808248
exploration/Actions Max                              0.999782
exploration/Actions Min                             -0.999778
exploration/Num Paths                                1
exploration/Average Returns                       4092.65
exploration/env_infos/final/reward_run Mean          4.15367
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.15367
exploration/env_infos/final/reward_run Min           4.15367
exploration/env_infos/initial/reward_run Mean       -0.268265
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.268265
exploration/env_infos/initial/reward_run Min        -0.268265
exploration/env_infos/reward_run Mean                4.48468
exploration/env_infos/reward_run Std                 1.08028
exploration/env_infos/reward_run Max                 6.64513
exploration/env_infos/reward_run Min                -0.680508
exploration/env_infos/final/reward_ctrl Mean        -0.236569
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.236569
exploration/env_infos/final/reward_ctrl Min         -0.236569
exploration/env_infos/initial/reward_ctrl Mean      -0.0923486
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0923486
exploration/env_infos/initial/reward_ctrl Min       -0.0923486
exploration/env_infos/reward_ctrl Mean              -0.392029
exploration/env_infos/reward_ctrl Std                0.0986641
exploration/env_infos/reward_ctrl Max               -0.0923486
exploration/env_infos/reward_ctrl Min               -0.585403
evaluation/num steps total                      865000
evaluation/num paths total                         865
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.32217
evaluation/Rewards Std                               1.07446
evaluation/Rewards Max                               6.52284
evaluation/Rewards Min                              -0.846773
evaluation/Returns Mean                           4322.17
evaluation/Returns Std                              42.3486
evaluation/Returns Max                            4367.46
evaluation/Returns Min                            4250.62
evaluation/Actions Mean                             -0.0102421
evaluation/Actions Std                               0.823711
evaluation/Actions Max                               0.998688
evaluation/Actions Min                              -0.998977
evaluation/Num Paths                                 5
evaluation/Average Returns                        4322.17
evaluation/env_infos/final/reward_run Mean           4.75213
evaluation/env_infos/final/reward_run Std            0.71418
evaluation/env_infos/final/reward_run Max            5.76233
evaluation/env_infos/final/reward_run Min            3.85575
evaluation/env_infos/initial/reward_run Mean        -0.0897609
evaluation/env_infos/initial/reward_run Std          0.202304
evaluation/env_infos/initial/reward_run Max          0.115918
evaluation/env_infos/initial/reward_run Min         -0.450024
evaluation/env_infos/reward_run Mean                 4.72933
evaluation/env_infos/reward_run Std                  1.06342
evaluation/env_infos/reward_run Max                  6.83164
evaluation/env_infos/reward_run Min                 -0.450024
evaluation/env_infos/final/reward_ctrl Mean         -0.349198
evaluation/env_infos/final/reward_ctrl Std           0.067699
evaluation/env_infos/final/reward_ctrl Max          -0.255004
evaluation/env_infos/final/reward_ctrl Min          -0.442044
evaluation/env_infos/initial/reward_ctrl Mean       -0.0442142
evaluation/env_infos/initial/reward_ctrl Std         0.0261953
evaluation/env_infos/initial/reward_ctrl Max        -0.0147302
evaluation/env_infos/initial/reward_ctrl Min        -0.0864303
evaluation/env_infos/reward_ctrl Mean               -0.407163
evaluation/env_infos/reward_ctrl Std                 0.0968503
evaluation/env_infos/reward_ctrl Max                -0.0147302
evaluation/env_infos/reward_ctrl Min                -0.584037
time/data storing (s)                                0.00698273
time/evaluation sampling (s)                         2.60456
time/exploration sampling (s)                        0.639046
time/logging (s)                                     0.0410403
time/saving (s)                                      0.0177585
time/training (s)                                   32.2666
time/epoch (s)                                      35.576
time/total (s)                                    7281.37
Epoch                                              172
----------------------------------------------  ---------------
2020-07-08 23:08:08.021794 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 173 finished
----------------------------------------------  ---------------
replay_buffer/size                              175000
trainer/QF1 Loss                                     5.12947
trainer/QF2 Loss                                     8.46886
trainer/Policy Loss                               -235.367
trainer/Q1 Predictions Mean                        242
trainer/Q1 Predictions Std                          98.2592
trainer/Q1 Predictions Max                         321.4
trainer/Q1 Predictions Min                          14.2719
trainer/Q2 Predictions Mean                        240.999
trainer/Q2 Predictions Std                          97.82
trainer/Q2 Predictions Max                         320.159
trainer/Q2 Predictions Min                          14.8073
trainer/Q Targets Mean                             241.654
trainer/Q Targets Std                               98.2061
trainer/Q Targets Max                              323.212
trainer/Q Targets Min                               14.6368
trainer/Log Pis Mean                                 5.92515
trainer/Log Pis Std                                  5.06823
trainer/Log Pis Max                                 17.0038
trainer/Log Pis Min                                 -6.85861
trainer/Policy mu Mean                               0.0719548
trainer/Policy mu Std                                1.50827
trainer/Policy mu Max                                3.41633
trainer/Policy mu Min                               -3.54422
trainer/Policy log std Mean                         -0.793929
trainer/Policy log std Std                           0.338813
trainer/Policy log std Max                           0.0914414
trainer/Policy log std Min                          -2.37649
trainer/Alpha                                        0.102593
trainer/Alpha Loss                                  -0.170429
exploration/num steps total                     175000
exploration/num paths total                        175
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.13299
exploration/Rewards Std                              1.01491
exploration/Rewards Max                              6.19608
exploration/Rewards Min                             -0.304975
exploration/Returns Mean                          4132.99
exploration/Returns Std                              0
exploration/Returns Max                           4132.99
exploration/Returns Min                           4132.99
exploration/Actions Mean                            -0.0082431
exploration/Actions Std                              0.827603
exploration/Actions Max                              0.999647
exploration/Actions Min                             -0.999772
exploration/Num Paths                                1
exploration/Average Returns                       4132.99
exploration/env_infos/final/reward_run Mean          4.05687
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.05687
exploration/env_infos/final/reward_run Min           4.05687
exploration/env_infos/initial/reward_run Mean        0.216663
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.216663
exploration/env_infos/initial/reward_run Min         0.216663
exploration/env_infos/reward_run Mean                4.54398
exploration/env_infos/reward_run Std                 1.00599
exploration/env_infos/reward_run Max                 6.62829
exploration/env_infos/reward_run Min                 0.164045
exploration/env_infos/final/reward_ctrl Mean        -0.531331
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.531331
exploration/env_infos/final/reward_ctrl Min         -0.531331
exploration/env_infos/initial/reward_ctrl Mean      -0.0426006
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0426006
exploration/env_infos/initial/reward_ctrl Min       -0.0426006
exploration/env_infos/reward_ctrl Mean              -0.410997
exploration/env_infos/reward_ctrl Std                0.0941477
exploration/env_infos/reward_ctrl Max               -0.0426006
exploration/env_infos/reward_ctrl Min               -0.580097
evaluation/num steps total                      870000
evaluation/num paths total                         870
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.43778
evaluation/Rewards Std                               1.06245
evaluation/Rewards Max                               6.67021
evaluation/Rewards Min                              -0.566327
evaluation/Returns Mean                           4437.78
evaluation/Returns Std                              43.6243
evaluation/Returns Max                            4493.01
evaluation/Returns Min                            4367.96
evaluation/Actions Mean                             -0.0183895
evaluation/Actions Std                               0.841173
evaluation/Actions Max                               0.998279
evaluation/Actions Min                              -0.996922
evaluation/Num Paths                                 5
evaluation/Average Returns                        4437.78
evaluation/env_infos/final/reward_run Mean           5.27414
evaluation/env_infos/final/reward_run Std            0.640361
evaluation/env_infos/final/reward_run Max            6.20219
evaluation/env_infos/final/reward_run Min            4.28384
evaluation/env_infos/initial/reward_run Mean         0.135089
evaluation/env_infos/initial/reward_run Std          0.12388
evaluation/env_infos/initial/reward_run Max          0.33103
evaluation/env_infos/initial/reward_run Min         -0.0141474
evaluation/env_infos/reward_run Mean                 4.86253
evaluation/env_infos/reward_run Std                  1.05199
evaluation/env_infos/reward_run Max                  7.0179
evaluation/env_infos/reward_run Min                 -0.132737
evaluation/env_infos/final/reward_ctrl Mean         -0.431639
evaluation/env_infos/final/reward_ctrl Std           0.11477
evaluation/env_infos/final/reward_ctrl Max          -0.281345
evaluation/env_infos/final/reward_ctrl Min          -0.55934
evaluation/env_infos/initial/reward_ctrl Mean       -0.0757056
evaluation/env_infos/initial/reward_ctrl Std         0.0104948
evaluation/env_infos/initial/reward_ctrl Max        -0.0639044
evaluation/env_infos/initial/reward_ctrl Min        -0.0921713
evaluation/env_infos/reward_ctrl Mean               -0.424746
evaluation/env_infos/reward_ctrl Std                 0.095348
evaluation/env_infos/reward_ctrl Max                -0.0639044
evaluation/env_infos/reward_ctrl Min                -0.578241
time/data storing (s)                                0.00672549
time/evaluation sampling (s)                         2.55894
time/exploration sampling (s)                        0.63937
time/logging (s)                                     0.0810875
time/saving (s)                                      0.115096
time/training (s)                                   31.5836
time/epoch (s)                                      34.9848
time/total (s)                                    7316.37
Epoch                                              173
----------------------------------------------  ---------------
2020-07-08 23:08:42.937562 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 174 finished
----------------------------------------------  ---------------
replay_buffer/size                              176000
trainer/QF1 Loss                                     6.33613
trainer/QF2 Loss                                     9.64658
trainer/Policy Loss                               -235.69
trainer/Q1 Predictions Mean                        241.439
trainer/Q1 Predictions Std                          93.9641
trainer/Q1 Predictions Max                         317.933
trainer/Q1 Predictions Min                          14.0561
trainer/Q2 Predictions Mean                        241.589
trainer/Q2 Predictions Std                          93.8036
trainer/Q2 Predictions Max                         316.918
trainer/Q2 Predictions Min                          14.5732
trainer/Q Targets Mean                             241.599
trainer/Q Targets Std                               93.9728
trainer/Q Targets Max                              320.21
trainer/Q Targets Min                               13.4981
trainer/Log Pis Mean                                 5.99006
trainer/Log Pis Std                                  5.10505
trainer/Log Pis Max                                 20.978
trainer/Log Pis Min                                 -5.18441
trainer/Policy mu Mean                               0.0191909
trainer/Policy mu Std                                1.52195
trainer/Policy mu Max                                3.69966
trainer/Policy mu Min                               -3.32369
trainer/Policy log std Mean                         -0.808895
trainer/Policy log std Std                           0.346649
trainer/Policy log std Max                          -0.00507927
trainer/Policy log std Min                          -2.42963
trainer/Alpha                                        0.1029
trainer/Alpha Loss                                  -0.0226043
exploration/num steps total                     176000
exploration/num paths total                        176
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.23555
exploration/Rewards Std                              1.11165
exploration/Rewards Max                              6.29019
exploration/Rewards Min                             -1.08071
exploration/Returns Mean                          4235.55
exploration/Returns Std                              0
exploration/Returns Max                           4235.55
exploration/Returns Min                           4235.55
exploration/Actions Mean                             0.00327006
exploration/Actions Std                              0.812705
exploration/Actions Max                              0.999994
exploration/Actions Min                             -0.999995
exploration/Num Paths                                1
exploration/Average Returns                       4235.55
exploration/env_infos/final/reward_run Mean          5.9585
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.9585
exploration/env_infos/final/reward_run Min           5.9585
exploration/env_infos/initial/reward_run Mean       -0.0275874
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0275874
exploration/env_infos/initial/reward_run Min        -0.0275874
exploration/env_infos/reward_run Mean                4.63185
exploration/env_infos/reward_run Std                 1.09736
exploration/env_infos/reward_run Max                 6.67764
exploration/env_infos/reward_run Min                -0.546335
exploration/env_infos/final/reward_ctrl Mean        -0.358644
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.358644
exploration/env_infos/final/reward_ctrl Min         -0.358644
exploration/env_infos/initial/reward_ctrl Mean      -0.139319
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.139319
exploration/env_infos/initial/reward_ctrl Min       -0.139319
exploration/env_infos/reward_ctrl Mean              -0.3963
exploration/env_infos/reward_ctrl Std                0.0999376
exploration/env_infos/reward_ctrl Max               -0.125924
exploration/env_infos/reward_ctrl Min               -0.586988
evaluation/num steps total                      875000
evaluation/num paths total                         875
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.49177
evaluation/Rewards Std                               1.08753
evaluation/Rewards Max                               6.68699
evaluation/Rewards Min                              -0.812636
evaluation/Returns Mean                           4491.77
evaluation/Returns Std                              55.4556
evaluation/Returns Max                            4593.34
evaluation/Returns Min                            4445.73
evaluation/Actions Mean                             -0.014053
evaluation/Actions Std                               0.834971
evaluation/Actions Max                               0.999168
evaluation/Actions Min                              -0.998192
evaluation/Num Paths                                 5
evaluation/Average Returns                        4491.77
evaluation/env_infos/final/reward_run Mean           5.46968
evaluation/env_infos/final/reward_run Std            0.587527
evaluation/env_infos/final/reward_run Max            6.23894
evaluation/env_infos/final/reward_run Min            4.53289
evaluation/env_infos/initial/reward_run Mean         0.210444
evaluation/env_infos/initial/reward_run Std          0.253629
evaluation/env_infos/initial/reward_run Max          0.490617
evaluation/env_infos/initial/reward_run Min         -0.252328
evaluation/env_infos/reward_run Mean                 4.9102
evaluation/env_infos/reward_run Std                  1.07729
evaluation/env_infos/reward_run Max                  6.99756
evaluation/env_infos/reward_run Min                 -0.646552
evaluation/env_infos/final/reward_ctrl Mean         -0.373983
evaluation/env_infos/final/reward_ctrl Std           0.0694754
evaluation/env_infos/final/reward_ctrl Max          -0.265506
evaluation/env_infos/final/reward_ctrl Min          -0.441849
evaluation/env_infos/initial/reward_ctrl Mean       -0.0770043
evaluation/env_infos/initial/reward_ctrl Std         0.0199728
evaluation/env_infos/initial/reward_ctrl Max        -0.0446279
evaluation/env_infos/initial/reward_ctrl Min        -0.0934847
evaluation/env_infos/reward_ctrl Mean               -0.418424
evaluation/env_infos/reward_ctrl Std                 0.0998132
evaluation/env_infos/reward_ctrl Max                -0.0437709
evaluation/env_infos/reward_ctrl Min                -0.584681
time/data storing (s)                                0.00697066
time/evaluation sampling (s)                         2.4674
time/exploration sampling (s)                        0.626937
time/logging (s)                                     0.0414306
time/saving (s)                                      0.0166459
time/training (s)                                   31.6978
time/epoch (s)                                      34.8572
time/total (s)                                    7351.25
Epoch                                              174
----------------------------------------------  ---------------
2020-07-08 23:09:17.698620 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 175 finished
----------------------------------------------  ---------------
replay_buffer/size                              177000
trainer/QF1 Loss                                     5.84976
trainer/QF2 Loss                                     6.83122
trainer/Policy Loss                               -246.723
trainer/Q1 Predictions Mean                        252.313
trainer/Q1 Predictions Std                          85.6278
trainer/Q1 Predictions Max                         321.814
trainer/Q1 Predictions Min                          14.0268
trainer/Q2 Predictions Mean                        252.735
trainer/Q2 Predictions Std                          85.8019
trainer/Q2 Predictions Max                         322.856
trainer/Q2 Predictions Min                          14.6125
trainer/Q Targets Mean                             252.412
trainer/Q Targets Std                               85.6085
trainer/Q Targets Max                              323.292
trainer/Q Targets Min                               14.3406
trainer/Log Pis Mean                                 6.00026
trainer/Log Pis Std                                  4.69088
trainer/Log Pis Max                                 17.726
trainer/Log Pis Min                                 -5.12621
trainer/Policy mu Mean                               0.0873756
trainer/Policy mu Std                                1.49203
trainer/Policy mu Max                                3.32377
trainer/Policy mu Min                               -3.44733
trainer/Policy log std Mean                         -0.831238
trainer/Policy log std Std                           0.331787
trainer/Policy log std Max                           0.0807118
trainer/Policy log std Min                          -2.42139
trainer/Alpha                                        0.103555
trainer/Alpha Loss                                   0.00059852
exploration/num steps total                     177000
exploration/num paths total                        177
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.37499
exploration/Rewards Std                              1.09992
exploration/Rewards Max                              6.43834
exploration/Rewards Min                             -0.354291
exploration/Returns Mean                          4374.99
exploration/Returns Std                              0
exploration/Returns Max                           4374.99
exploration/Returns Min                           4374.99
exploration/Actions Mean                            -0.0101754
exploration/Actions Std                              0.821333
exploration/Actions Max                              0.999681
exploration/Actions Min                             -0.999373
exploration/Num Paths                                1
exploration/Average Returns                       4374.99
exploration/env_infos/final/reward_run Mean          6.32797
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.32797
exploration/env_infos/final/reward_run Min           6.32797
exploration/env_infos/initial/reward_run Mean       -0.0540316
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0540316
exploration/env_infos/initial/reward_run Min        -0.0540316
exploration/env_infos/reward_run Mean                4.77981
exploration/env_infos/reward_run Std                 1.09526
exploration/env_infos/reward_run Max                 6.79376
exploration/env_infos/reward_run Min                -0.136401
exploration/env_infos/final/reward_ctrl Mean        -0.292942
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.292942
exploration/env_infos/final/reward_ctrl Min         -0.292942
exploration/env_infos/initial/reward_ctrl Mean      -0.0260198
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0260198
exploration/env_infos/initial/reward_ctrl Min       -0.0260198
exploration/env_infos/reward_ctrl Mean              -0.404815
exploration/env_infos/reward_ctrl Std                0.0906455
exploration/env_infos/reward_ctrl Max               -0.0260198
exploration/env_infos/reward_ctrl Min               -0.566855
evaluation/num steps total                      880000
evaluation/num paths total                         880
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.54536
evaluation/Rewards Std                               1.08284
evaluation/Rewards Max                               6.76235
evaluation/Rewards Min                              -0.992815
evaluation/Returns Mean                           4545.36
evaluation/Returns Std                              27.4598
evaluation/Returns Max                            4587.96
evaluation/Returns Min                            4501.66
evaluation/Actions Mean                             -0.024211
evaluation/Actions Std                               0.837901
evaluation/Actions Max                               0.999
evaluation/Actions Min                              -0.998409
evaluation/Num Paths                                 5
evaluation/Average Returns                        4545.36
evaluation/env_infos/final/reward_run Mean           5.17472
evaluation/env_infos/final/reward_run Std            0.758022
evaluation/env_infos/final/reward_run Max            6.51818
evaluation/env_infos/final/reward_run Min            4.44584
evaluation/env_infos/initial/reward_run Mean         0.201135
evaluation/env_infos/initial/reward_run Std          0.161881
evaluation/env_infos/initial/reward_run Max          0.414326
evaluation/env_infos/initial/reward_run Min         -0.0478588
evaluation/env_infos/reward_run Mean                 4.96696
evaluation/env_infos/reward_run Std                  1.07663
evaluation/env_infos/reward_run Max                  7.11746
evaluation/env_infos/reward_run Min                 -0.468842
evaluation/env_infos/final/reward_ctrl Mean         -0.431628
evaluation/env_infos/final/reward_ctrl Std           0.0549339
evaluation/env_infos/final/reward_ctrl Max          -0.365741
evaluation/env_infos/final/reward_ctrl Min          -0.501833
evaluation/env_infos/initial/reward_ctrl Mean       -0.066384
evaluation/env_infos/initial/reward_ctrl Std         0.031714
evaluation/env_infos/initial/reward_ctrl Max        -0.0192429
evaluation/env_infos/initial/reward_ctrl Min        -0.108565
evaluation/env_infos/reward_ctrl Mean               -0.421598
evaluation/env_infos/reward_ctrl Std                 0.0885686
evaluation/env_infos/reward_ctrl Max                -0.0192429
evaluation/env_infos/reward_ctrl Min                -0.574104
time/data storing (s)                                0.00696072
time/evaluation sampling (s)                         2.45791
time/exploration sampling (s)                        0.631865
time/logging (s)                                     0.03987
time/saving (s)                                      0.0160624
time/training (s)                                   31.5649
time/epoch (s)                                      34.7176
time/total (s)                                    7386
Epoch                                              175
----------------------------------------------  ---------------
2020-07-08 23:09:52.567269 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 176 finished
----------------------------------------------  ---------------
replay_buffer/size                              178000
trainer/QF1 Loss                                     7.875
trainer/QF2 Loss                                     7.98727
trainer/Policy Loss                               -229.94
trainer/Q1 Predictions Mean                        235.442
trainer/Q1 Predictions Std                          98.4412
trainer/Q1 Predictions Max                         315.415
trainer/Q1 Predictions Min                          15.1285
trainer/Q2 Predictions Mean                        236.222
trainer/Q2 Predictions Std                          98.699
trainer/Q2 Predictions Max                         316.067
trainer/Q2 Predictions Min                          14.869
trainer/Q Targets Mean                             236.11
trainer/Q Targets Std                               98.5973
trainer/Q Targets Max                              314.897
trainer/Q Targets Min                               14.0859
trainer/Log Pis Mean                                 6.08742
trainer/Log Pis Std                                  5.65032
trainer/Log Pis Max                                 21.1435
trainer/Log Pis Min                                 -8.58595
trainer/Policy mu Mean                               0.118219
trainer/Policy mu Std                                1.52177
trainer/Policy mu Max                                4.3418
trainer/Policy mu Min                               -4.29068
trainer/Policy log std Mean                         -0.784869
trainer/Policy log std Std                           0.34209
trainer/Policy log std Max                           0.432325
trainer/Policy log std Min                          -2.48742
trainer/Alpha                                        0.10226
trainer/Alpha Loss                                   0.199342
exploration/num steps total                     178000
exploration/num paths total                        178
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.21077
exploration/Rewards Std                              1.12751
exploration/Rewards Max                              6.63766
exploration/Rewards Min                             -0.703745
exploration/Returns Mean                          4210.77
exploration/Returns Std                              0
exploration/Returns Max                           4210.77
exploration/Returns Min                           4210.77
exploration/Actions Mean                             0.028003
exploration/Actions Std                              0.815067
exploration/Actions Max                              0.999795
exploration/Actions Min                             -0.999821
exploration/Num Paths                                1
exploration/Average Returns                       4210.77
exploration/env_infos/final/reward_run Mean          3.75439
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.75439
exploration/env_infos/final/reward_run Min           3.75439
exploration/env_infos/initial/reward_run Mean        0.0853562
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0853562
exploration/env_infos/initial/reward_run Min         0.0853562
exploration/env_infos/reward_run Mean                4.60984
exploration/env_infos/reward_run Std                 1.12472
exploration/env_infos/reward_run Max                 6.95896
exploration/env_infos/reward_run Min                -0.268005
exploration/env_infos/final/reward_ctrl Mean        -0.419282
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.419282
exploration/env_infos/final/reward_ctrl Min         -0.419282
exploration/env_infos/initial/reward_ctrl Mean      -0.039599
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.039599
exploration/env_infos/initial/reward_ctrl Min       -0.039599
exploration/env_infos/reward_ctrl Mean              -0.399071
exploration/env_infos/reward_ctrl Std                0.100811
exploration/env_infos/reward_ctrl Max               -0.039599
exploration/env_infos/reward_ctrl Min               -0.586733
evaluation/num steps total                      885000
evaluation/num paths total                         885
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.41515
evaluation/Rewards Std                               1.10401
evaluation/Rewards Max                               6.63216
evaluation/Rewards Min                              -0.998053
evaluation/Returns Mean                           4415.15
evaluation/Returns Std                              70.7883
evaluation/Returns Max                            4536.77
evaluation/Returns Min                            4332.36
evaluation/Actions Mean                              0.0147715
evaluation/Actions Std                               0.833416
evaluation/Actions Max                               0.999779
evaluation/Actions Min                              -0.999054
evaluation/Num Paths                                 5
evaluation/Average Returns                        4415.15
evaluation/env_infos/final/reward_run Mean           5.38729
evaluation/env_infos/final/reward_run Std            1.1428
evaluation/env_infos/final/reward_run Max            6.40924
evaluation/env_infos/final/reward_run Min            3.49513
evaluation/env_infos/initial/reward_run Mean         0.0221479
evaluation/env_infos/initial/reward_run Std          0.0624328
evaluation/env_infos/initial/reward_run Max          0.124252
evaluation/env_infos/initial/reward_run Min         -0.0518355
evaluation/env_infos/reward_run Mean                 4.83203
evaluation/env_infos/reward_run Std                  1.09471
evaluation/env_infos/reward_run Max                  7.02162
evaluation/env_infos/reward_run Min                 -0.551944
evaluation/env_infos/final/reward_ctrl Mean         -0.379221
evaluation/env_infos/final/reward_ctrl Std           0.0976887
evaluation/env_infos/final/reward_ctrl Max          -0.259223
evaluation/env_infos/final/reward_ctrl Min          -0.494318
evaluation/env_infos/initial/reward_ctrl Mean       -0.0374931
evaluation/env_infos/initial/reward_ctrl Std         0.0130518
evaluation/env_infos/initial/reward_ctrl Max        -0.0190332
evaluation/env_infos/initial/reward_ctrl Min        -0.0535729
evaluation/env_infos/reward_ctrl Mean               -0.41688
evaluation/env_infos/reward_ctrl Std                 0.100576
evaluation/env_infos/reward_ctrl Max                -0.0190332
evaluation/env_infos/reward_ctrl Min                -0.588464
time/data storing (s)                                0.00664783
time/evaluation sampling (s)                         2.47019
time/exploration sampling (s)                        0.636888
time/logging (s)                                     0.0406895
time/saving (s)                                      0.016256
time/training (s)                                   31.6655
time/epoch (s)                                      34.8362
time/total (s)                                    7420.86
Epoch                                              176
----------------------------------------------  ---------------
2020-07-08 23:10:28.152581 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 177 finished
----------------------------------------------  ---------------
replay_buffer/size                              179000
trainer/QF1 Loss                                    10.3487
trainer/QF2 Loss                                    12.3904
trainer/Policy Loss                               -237.432
trainer/Q1 Predictions Mean                        243.659
trainer/Q1 Predictions Std                          92.3467
trainer/Q1 Predictions Max                         325.046
trainer/Q1 Predictions Min                          15.3249
trainer/Q2 Predictions Mean                        244.07
trainer/Q2 Predictions Std                          92.6309
trainer/Q2 Predictions Max                         327.462
trainer/Q2 Predictions Min                           7.65589
trainer/Q Targets Mean                             243.847
trainer/Q Targets Std                               92.2875
trainer/Q Targets Max                              327.505
trainer/Q Targets Min                               15.5107
trainer/Log Pis Mean                                 6.42909
trainer/Log Pis Std                                  4.97744
trainer/Log Pis Max                                 18.4551
trainer/Log Pis Min                                 -4.80365
trainer/Policy mu Mean                              -0.0265029
trainer/Policy mu Std                                1.53259
trainer/Policy mu Max                                3.59861
trainer/Policy mu Min                               -3.71028
trainer/Policy log std Mean                         -0.813425
trainer/Policy log std Std                           0.350651
trainer/Policy log std Max                           0.231567
trainer/Policy log std Min                          -2.75538
trainer/Alpha                                        0.104108
trainer/Alpha Loss                                   0.970808
exploration/num steps total                     179000
exploration/num paths total                        179
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.32045
exploration/Rewards Std                              1.05806
exploration/Rewards Max                              6.26588
exploration/Rewards Min                             -0.498497
exploration/Returns Mean                          4320.45
exploration/Returns Std                              0
exploration/Returns Max                           4320.45
exploration/Returns Min                           4320.45
exploration/Actions Mean                            -0.02104
exploration/Actions Std                              0.820626
exploration/Actions Max                              0.999806
exploration/Actions Min                             -0.99963
exploration/Num Paths                                1
exploration/Average Returns                       4320.45
exploration/env_infos/final/reward_run Mean          4.13403
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.13403
exploration/env_infos/final/reward_run Min           4.13403
exploration/env_infos/initial/reward_run Mean        0.0407248
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0407248
exploration/env_infos/initial/reward_run Min         0.0407248
exploration/env_infos/reward_run Mean                4.72477
exploration/env_infos/reward_run Std                 1.05018
exploration/env_infos/reward_run Max                 6.70024
exploration/env_infos/reward_run Min                -0.114228
exploration/env_infos/final/reward_ctrl Mean        -0.46055
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.46055
exploration/env_infos/final/reward_ctrl Min         -0.46055
exploration/env_infos/initial/reward_ctrl Mean      -0.109729
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.109729
exploration/env_infos/initial/reward_ctrl Min       -0.109729
exploration/env_infos/reward_ctrl Mean              -0.404321
exploration/env_infos/reward_ctrl Std                0.101163
exploration/env_infos/reward_ctrl Max               -0.0957091
exploration/env_infos/reward_ctrl Min               -0.575849
evaluation/num steps total                      890000
evaluation/num paths total                         890
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.4713
evaluation/Rewards Std                               1.10705
evaluation/Rewards Max                               6.78537
evaluation/Rewards Min                              -0.89209
evaluation/Returns Mean                           4471.3
evaluation/Returns Std                              37.6369
evaluation/Returns Max                            4507.52
evaluation/Returns Min                            4402.94
evaluation/Actions Mean                             -0.0305266
evaluation/Actions Std                               0.831749
evaluation/Actions Max                               0.998955
evaluation/Actions Min                              -0.999063
evaluation/Num Paths                                 5
evaluation/Average Returns                        4471.3
evaluation/env_infos/final/reward_run Mean           5.03004
evaluation/env_infos/final/reward_run Std            0.963423
evaluation/env_infos/final/reward_run Max            6.09002
evaluation/env_infos/final/reward_run Min            3.44924
evaluation/env_infos/initial/reward_run Mean         0.221124
evaluation/env_infos/initial/reward_run Std          0.0553171
evaluation/env_infos/initial/reward_run Max          0.289311
evaluation/env_infos/initial/reward_run Min          0.144883
evaluation/env_infos/reward_run Mean                 4.88694
evaluation/env_infos/reward_run Std                  1.10055
evaluation/env_infos/reward_run Max                  7.19162
evaluation/env_infos/reward_run Min                 -0.379776
evaluation/env_infos/final/reward_ctrl Mean         -0.424777
evaluation/env_infos/final/reward_ctrl Std           0.10843
evaluation/env_infos/final/reward_ctrl Max          -0.221018
evaluation/env_infos/final/reward_ctrl Min          -0.540801
evaluation/env_infos/initial/reward_ctrl Mean       -0.0664187
evaluation/env_infos/initial/reward_ctrl Std         0.0101761
evaluation/env_infos/initial/reward_ctrl Max        -0.0506024
evaluation/env_infos/initial/reward_ctrl Min        -0.0777888
evaluation/env_infos/reward_ctrl Mean               -0.415643
evaluation/env_infos/reward_ctrl Std                 0.102552
evaluation/env_infos/reward_ctrl Max                -0.0506024
evaluation/env_infos/reward_ctrl Min                -0.584249
time/data storing (s)                                0.00690045
time/evaluation sampling (s)                         2.514
time/exploration sampling (s)                        0.624452
time/logging (s)                                     0.0408713
time/saving (s)                                      0.0160838
time/training (s)                                   32.2554
time/epoch (s)                                      35.4577
time/total (s)                                    7456.44
Epoch                                              177
----------------------------------------------  ---------------
2020-07-08 23:11:03.097135 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 178 finished
----------------------------------------------  ---------------
replay_buffer/size                              180000
trainer/QF1 Loss                                     4.86183
trainer/QF2 Loss                                     6.34768
trainer/Policy Loss                               -249.127
trainer/Q1 Predictions Mean                        254.667
trainer/Q1 Predictions Std                          90.3775
trainer/Q1 Predictions Max                         323.781
trainer/Q1 Predictions Min                          17.3542
trainer/Q2 Predictions Mean                        255.513
trainer/Q2 Predictions Std                          90.623
trainer/Q2 Predictions Max                         323.679
trainer/Q2 Predictions Min                          17.1669
trainer/Q Targets Mean                             254.743
trainer/Q Targets Std                               90.4866
trainer/Q Targets Max                              323.607
trainer/Q Targets Min                               17.1636
trainer/Log Pis Mean                                 5.91145
trainer/Log Pis Std                                  4.69977
trainer/Log Pis Max                                 17.0935
trainer/Log Pis Min                                 -5.77226
trainer/Policy mu Mean                               0.0656157
trainer/Policy mu Std                                1.49269
trainer/Policy mu Max                                3.96506
trainer/Policy mu Min                               -3.5753
trainer/Policy log std Mean                         -0.822838
trainer/Policy log std Std                           0.353471
trainer/Policy log std Max                           0.028485
trainer/Policy log std Min                          -2.60753
trainer/Alpha                                        0.103726
trainer/Alpha Loss                                  -0.20063
exploration/num steps total                     180000
exploration/num paths total                        180
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.36802
exploration/Rewards Std                              1.08776
exploration/Rewards Max                              6.47912
exploration/Rewards Min                             -0.816731
exploration/Returns Mean                          4368.02
exploration/Returns Std                              0
exploration/Returns Max                           4368.02
exploration/Returns Min                           4368.02
exploration/Actions Mean                            -0.0116328
exploration/Actions Std                              0.826319
exploration/Actions Max                              0.999589
exploration/Actions Min                             -0.999387
exploration/Num Paths                                1
exploration/Average Returns                       4368.02
exploration/env_infos/final/reward_run Mean          5.02395
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.02395
exploration/env_infos/final/reward_run Min           5.02395
exploration/env_infos/initial/reward_run Mean        0.532795
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.532795
exploration/env_infos/initial/reward_run Min         0.532795
exploration/env_infos/reward_run Mean                4.77778
exploration/env_infos/reward_run Std                 1.07478
exploration/env_infos/reward_run Max                 6.78381
exploration/env_infos/reward_run Min                -0.332251
exploration/env_infos/final/reward_ctrl Mean        -0.47691
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.47691
exploration/env_infos/final/reward_ctrl Min         -0.47691
exploration/env_infos/initial/reward_ctrl Mean      -0.140743
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.140743
exploration/env_infos/initial/reward_ctrl Min       -0.140743
exploration/env_infos/reward_ctrl Mean              -0.409763
exploration/env_infos/reward_ctrl Std                0.0967649
exploration/env_infos/reward_ctrl Max               -0.105926
exploration/env_infos/reward_ctrl Min               -0.583361
evaluation/num steps total                      895000
evaluation/num paths total                         895
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.52876
evaluation/Rewards Std                               1.11027
evaluation/Rewards Max                               6.93844
evaluation/Rewards Min                              -0.871437
evaluation/Returns Mean                           4528.76
evaluation/Returns Std                              57.9308
evaluation/Returns Max                            4633.6
evaluation/Returns Min                            4463.84
evaluation/Actions Mean                             -0.0165102
evaluation/Actions Std                               0.831789
evaluation/Actions Max                               0.9986
evaluation/Actions Min                              -0.997567
evaluation/Num Paths                                 5
evaluation/Average Returns                        4528.76
evaluation/env_infos/final/reward_run Mean           4.89926
evaluation/env_infos/final/reward_run Std            0.613452
evaluation/env_infos/final/reward_run Max            5.5777
evaluation/env_infos/final/reward_run Min            4.07575
evaluation/env_infos/initial/reward_run Mean        -0.00369278
evaluation/env_infos/initial/reward_run Std          0.205966
evaluation/env_infos/initial/reward_run Max          0.382828
evaluation/env_infos/initial/reward_run Min         -0.233932
evaluation/env_infos/reward_run Mean                 4.94404
evaluation/env_infos/reward_run Std                  1.10161
evaluation/env_infos/reward_run Max                  7.21813
evaluation/env_infos/reward_run Min                 -0.39499
evaluation/env_infos/final/reward_ctrl Mean         -0.427026
evaluation/env_infos/final/reward_ctrl Std           0.0848807
evaluation/env_infos/final/reward_ctrl Max          -0.294151
evaluation/env_infos/final/reward_ctrl Min          -0.504322
evaluation/env_infos/initial/reward_ctrl Mean       -0.102155
evaluation/env_infos/initial/reward_ctrl Std         0.0295003
evaluation/env_infos/initial/reward_ctrl Max        -0.0563018
evaluation/env_infos/initial/reward_ctrl Min        -0.144706
evaluation/env_infos/reward_ctrl Mean               -0.415287
evaluation/env_infos/reward_ctrl Std                 0.100747
evaluation/env_infos/reward_ctrl Max                -0.0563018
evaluation/env_infos/reward_ctrl Min                -0.571568
time/data storing (s)                                0.00664479
time/evaluation sampling (s)                         2.54284
time/exploration sampling (s)                        0.63213
time/logging (s)                                     0.04041
time/saving (s)                                      0.0189641
time/training (s)                                   31.6729
time/epoch (s)                                      34.9139
time/total (s)                                    7491.38
Epoch                                              178
----------------------------------------------  ---------------
2020-07-08 23:11:38.325862 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 179 finished
----------------------------------------------  ---------------
replay_buffer/size                              181000
trainer/QF1 Loss                                     7.91955
trainer/QF2 Loss                                     6.47855
trainer/Policy Loss                               -236.408
trainer/Q1 Predictions Mean                        242.401
trainer/Q1 Predictions Std                          93.4552
trainer/Q1 Predictions Max                         320.012
trainer/Q1 Predictions Min                          15.3736
trainer/Q2 Predictions Mean                        242.512
trainer/Q2 Predictions Std                          93.3887
trainer/Q2 Predictions Max                         321.464
trainer/Q2 Predictions Min                          15.0592
trainer/Q Targets Mean                             242.609
trainer/Q Targets Std                               93.6609
trainer/Q Targets Max                              324.305
trainer/Q Targets Min                               14.6398
trainer/Log Pis Mean                                 6.21592
trainer/Log Pis Std                                  5.12026
trainer/Log Pis Max                                 21.479
trainer/Log Pis Min                                 -4.61208
trainer/Policy mu Mean                              -0.0344831
trainer/Policy mu Std                                1.52232
trainer/Policy mu Max                                3.24995
trainer/Policy mu Min                               -4.19825
trainer/Policy log std Mean                         -0.815068
trainer/Policy log std Std                           0.34569
trainer/Policy log std Max                          -0.0488517
trainer/Policy log std Min                          -2.55788
trainer/Alpha                                        0.10351
trainer/Alpha Loss                                   0.489747
exploration/num steps total                     181000
exploration/num paths total                        181
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.15944
exploration/Rewards Std                              1.09483
exploration/Rewards Max                              6.52384
exploration/Rewards Min                             -0.766284
exploration/Returns Mean                          4159.44
exploration/Returns Std                              0
exploration/Returns Max                           4159.44
exploration/Returns Min                           4159.44
exploration/Actions Mean                            -0.0193182
exploration/Actions Std                              0.815712
exploration/Actions Max                              0.999918
exploration/Actions Min                             -0.999704
exploration/Num Paths                                1
exploration/Average Returns                       4159.44
exploration/env_infos/final/reward_run Mean          5.42466
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.42466
exploration/env_infos/final/reward_run Min           5.42466
exploration/env_infos/initial/reward_run Mean        0.0413447
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0413447
exploration/env_infos/initial/reward_run Min         0.0413447
exploration/env_infos/reward_run Mean                4.55889
exploration/env_infos/reward_run Std                 1.09336
exploration/env_infos/reward_run Max                 6.89357
exploration/env_infos/reward_run Min                -0.359449
exploration/env_infos/final/reward_ctrl Mean        -0.286895
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.286895
exploration/env_infos/final/reward_ctrl Min         -0.286895
exploration/env_infos/initial/reward_ctrl Mean      -0.0451036
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0451036
exploration/env_infos/initial/reward_ctrl Min       -0.0451036
exploration/env_infos/reward_ctrl Mean              -0.399455
exploration/env_infos/reward_ctrl Std                0.101093
exploration/env_infos/reward_ctrl Max               -0.0451036
exploration/env_infos/reward_ctrl Min               -0.582473
evaluation/num steps total                      900000
evaluation/num paths total                         900
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.49066
evaluation/Rewards Std                               1.07606
evaluation/Rewards Max                               6.53176
evaluation/Rewards Min                              -0.776083
evaluation/Returns Mean                           4490.66
evaluation/Returns Std                              39.7984
evaluation/Returns Max                            4550.84
evaluation/Returns Min                            4443.52
evaluation/Actions Mean                             -0.0358344
evaluation/Actions Std                               0.829381
evaluation/Actions Max                               0.998937
evaluation/Actions Min                              -0.998743
evaluation/Num Paths                                 5
evaluation/Average Returns                        4490.66
evaluation/env_infos/final/reward_run Mean           5.58677
evaluation/env_infos/final/reward_run Std            1.09616
evaluation/env_infos/final/reward_run Max            6.44857
evaluation/env_infos/final/reward_run Min            3.47451
evaluation/env_infos/initial/reward_run Mean        -0.00901388
evaluation/env_infos/initial/reward_run Std          0.149077
evaluation/env_infos/initial/reward_run Max          0.279563
evaluation/env_infos/initial/reward_run Min         -0.15351
evaluation/env_infos/reward_run Mean                 4.90416
evaluation/env_infos/reward_run Std                  1.0672
evaluation/env_infos/reward_run Max                  6.83592
evaluation/env_infos/reward_run Min                 -0.363
evaluation/env_infos/final/reward_ctrl Mean         -0.381304
evaluation/env_infos/final/reward_ctrl Std           0.0774202
evaluation/env_infos/final/reward_ctrl Max          -0.296826
evaluation/env_infos/final/reward_ctrl Min          -0.507772
evaluation/env_infos/initial/reward_ctrl Mean       -0.0749432
evaluation/env_infos/initial/reward_ctrl Std         0.0217141
evaluation/env_infos/initial/reward_ctrl Max        -0.0352429
evaluation/env_infos/initial/reward_ctrl Min        -0.0981373
evaluation/env_infos/reward_ctrl Mean               -0.413494
evaluation/env_infos/reward_ctrl Std                 0.101413
evaluation/env_infos/reward_ctrl Max                -0.0352429
evaluation/env_infos/reward_ctrl Min                -0.586709
time/data storing (s)                                0.00654805
time/evaluation sampling (s)                         2.49506
time/exploration sampling (s)                        0.631237
time/logging (s)                                     0.0416433
time/saving (s)                                      0.0159538
time/training (s)                                   32.0024
time/epoch (s)                                      35.1929
time/total (s)                                    7526.61
Epoch                                              179
----------------------------------------------  ---------------
2020-07-08 23:12:14.114725 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 180 finished
----------------------------------------------  ---------------
replay_buffer/size                              182000
trainer/QF1 Loss                                    12.4922
trainer/QF2 Loss                                    13.2171
trainer/Policy Loss                               -237.244
trainer/Q1 Predictions Mean                        243.462
trainer/Q1 Predictions Std                          96.7554
trainer/Q1 Predictions Max                         319.178
trainer/Q1 Predictions Min                          16.3141
trainer/Q2 Predictions Mean                        242.874
trainer/Q2 Predictions Std                          96.5448
trainer/Q2 Predictions Max                         318.218
trainer/Q2 Predictions Min                          16.524
trainer/Q Targets Mean                             242.683
trainer/Q Targets Std                               96.4839
trainer/Q Targets Max                              318.549
trainer/Q Targets Min                               15.0156
trainer/Log Pis Mean                                 6.04199
trainer/Log Pis Std                                  4.93354
trainer/Log Pis Max                                 17.274
trainer/Log Pis Min                                 -5.34475
trainer/Policy mu Mean                              -0.0319476
trainer/Policy mu Std                                1.49225
trainer/Policy mu Max                                3.38004
trainer/Policy mu Min                               -3.64824
trainer/Policy log std Mean                         -0.829968
trainer/Policy log std Std                           0.342836
trainer/Policy log std Max                          -0.0152314
trainer/Policy log std Min                          -2.39962
trainer/Alpha                                        0.10347
trainer/Alpha Loss                                   0.0952658
exploration/num steps total                     182000
exploration/num paths total                        182
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.38888
exploration/Rewards Std                              1.08388
exploration/Rewards Max                              6.59208
exploration/Rewards Min                             -0.632525
exploration/Returns Mean                          4388.88
exploration/Returns Std                              0
exploration/Returns Max                           4388.88
exploration/Returns Min                           4388.88
exploration/Actions Mean                            -0.0135776
exploration/Actions Std                              0.828182
exploration/Actions Max                              0.999222
exploration/Actions Min                             -0.999712
exploration/Num Paths                                1
exploration/Average Returns                       4388.88
exploration/env_infos/final/reward_run Mean          4.19574
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.19574
exploration/env_infos/final/reward_run Min           4.19574
exploration/env_infos/initial/reward_run Mean       -0.33935
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.33935
exploration/env_infos/initial/reward_run Min        -0.33935
exploration/env_infos/reward_run Mean                4.80052
exploration/env_infos/reward_run Std                 1.07757
exploration/env_infos/reward_run Max                 6.94038
exploration/env_infos/reward_run Min                -0.33935
exploration/env_infos/final/reward_ctrl Mean        -0.409514
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.409514
exploration/env_infos/final/reward_ctrl Min         -0.409514
exploration/env_infos/initial/reward_ctrl Mean      -0.089045
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.089045
exploration/env_infos/initial/reward_ctrl Min       -0.089045
exploration/env_infos/reward_ctrl Mean              -0.411642
exploration/env_infos/reward_ctrl Std                0.0949659
exploration/env_infos/reward_ctrl Max               -0.089045
exploration/env_infos/reward_ctrl Min               -0.579444
evaluation/num steps total                      905000
evaluation/num paths total                         905
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.48418
evaluation/Rewards Std                               1.10685
evaluation/Rewards Max                               6.84561
evaluation/Rewards Min                              -0.904063
evaluation/Returns Mean                           4484.18
evaluation/Returns Std                              51.0927
evaluation/Returns Max                            4566.78
evaluation/Returns Min                            4424.9
evaluation/Actions Mean                             -0.0259348
evaluation/Actions Std                               0.842294
evaluation/Actions Max                               0.999104
evaluation/Actions Min                              -0.99961
evaluation/Num Paths                                 5
evaluation/Average Returns                        4484.18
evaluation/env_infos/final/reward_run Mean           5.07786
evaluation/env_infos/final/reward_run Std            1.03512
evaluation/env_infos/final/reward_run Max            6.47811
evaluation/env_infos/final/reward_run Min            3.48293
evaluation/env_infos/initial/reward_run Mean        -0.1153
evaluation/env_infos/initial/reward_run Std          0.0837514
evaluation/env_infos/initial/reward_run Max         -0.0189015
evaluation/env_infos/initial/reward_run Min         -0.255237
evaluation/env_infos/reward_run Mean                 4.91026
evaluation/env_infos/reward_run Std                  1.10218
evaluation/env_infos/reward_run Max                  7.17486
evaluation/env_infos/reward_run Min                 -0.438734
evaluation/env_infos/final/reward_ctrl Mean         -0.434539
evaluation/env_infos/final/reward_ctrl Std           0.0759335
evaluation/env_infos/final/reward_ctrl Max          -0.337
evaluation/env_infos/final/reward_ctrl Min          -0.515263
evaluation/env_infos/initial/reward_ctrl Mean       -0.0525615
evaluation/env_infos/initial/reward_ctrl Std         0.0182717
evaluation/env_infos/initial/reward_ctrl Max        -0.0334118
evaluation/env_infos/initial/reward_ctrl Min        -0.0790799
evaluation/env_infos/reward_ctrl Mean               -0.426079
evaluation/env_infos/reward_ctrl Std                 0.09177
evaluation/env_infos/reward_ctrl Max                -0.0334118
evaluation/env_infos/reward_ctrl Min                -0.588513
time/data storing (s)                                0.00672575
time/evaluation sampling (s)                         2.60254
time/exploration sampling (s)                        0.658898
time/logging (s)                                     0.0469048
time/saving (s)                                      0.0160427
time/training (s)                                   32.1976
time/epoch (s)                                      35.5287
time/total (s)                                    7562.39
Epoch                                              180
----------------------------------------------  ---------------
2020-07-08 23:12:49.845140 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 181 finished
----------------------------------------------  ---------------
replay_buffer/size                              183000
trainer/QF1 Loss                                     7.23175
trainer/QF2 Loss                                     9.12015
trainer/Policy Loss                               -238.666
trainer/Q1 Predictions Mean                        244.511
trainer/Q1 Predictions Std                          95.7983
trainer/Q1 Predictions Max                         325.568
trainer/Q1 Predictions Min                          15.1533
trainer/Q2 Predictions Mean                        244.27
trainer/Q2 Predictions Std                          95.6505
trainer/Q2 Predictions Max                         325.691
trainer/Q2 Predictions Min                          14.6844
trainer/Q Targets Mean                             245.425
trainer/Q Targets Std                               96.1777
trainer/Q Targets Max                              328.57
trainer/Q Targets Min                               14.5935
trainer/Log Pis Mean                                 5.89815
trainer/Log Pis Std                                  5.29867
trainer/Log Pis Max                                 21.3919
trainer/Log Pis Min                                 -5.845
trainer/Policy mu Mean                               0.0553336
trainer/Policy mu Std                                1.48762
trainer/Policy mu Max                                4.00945
trainer/Policy mu Min                               -4.2345
trainer/Policy log std Mean                         -0.820415
trainer/Policy log std Std                           0.336649
trainer/Policy log std Max                          -0.0845889
trainer/Policy log std Min                          -2.4361
trainer/Alpha                                        0.105935
trainer/Alpha Loss                                  -0.228652
exploration/num steps total                     183000
exploration/num paths total                        183
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.45852
exploration/Rewards Std                              1.07096
exploration/Rewards Max                              6.56438
exploration/Rewards Min                             -0.748946
exploration/Returns Mean                          4458.52
exploration/Returns Std                              0
exploration/Returns Max                           4458.52
exploration/Returns Min                           4458.52
exploration/Actions Mean                            -0.00327446
exploration/Actions Std                              0.830497
exploration/Actions Max                              0.999569
exploration/Actions Min                             -0.9996
exploration/Num Paths                                1
exploration/Average Returns                       4458.52
exploration/env_infos/final/reward_run Mean          4.27477
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.27477
exploration/env_infos/final/reward_run Min           4.27477
exploration/env_infos/initial/reward_run Mean       -0.0424668
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0424668
exploration/env_infos/initial/reward_run Min        -0.0424668
exploration/env_infos/reward_run Mean                4.87237
exploration/env_infos/reward_run Std                 1.06373
exploration/env_infos/reward_run Max                 6.95205
exploration/env_infos/reward_run Min                -0.334666
exploration/env_infos/final/reward_ctrl Mean        -0.500354
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.500354
exploration/env_infos/final/reward_ctrl Min         -0.500354
exploration/env_infos/initial/reward_ctrl Mean      -0.0338442
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0338442
exploration/env_infos/initial/reward_ctrl Min       -0.0338442
exploration/env_infos/reward_ctrl Mean              -0.413842
exploration/env_infos/reward_ctrl Std                0.092668
exploration/env_infos/reward_ctrl Max               -0.0281057
exploration/env_infos/reward_ctrl Min               -0.576145
evaluation/num steps total                      910000
evaluation/num paths total                         910
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.44624
evaluation/Rewards Std                               1.09428
evaluation/Rewards Max                               6.54762
evaluation/Rewards Min                              -0.882746
evaluation/Returns Mean                           4446.24
evaluation/Returns Std                              26.9502
evaluation/Returns Max                            4486.55
evaluation/Returns Min                            4401.9
evaluation/Actions Mean                             -0.0117061
evaluation/Actions Std                               0.841202
evaluation/Actions Max                               0.997438
evaluation/Actions Min                              -0.998759
evaluation/Num Paths                                 5
evaluation/Average Returns                        4446.24
evaluation/env_infos/final/reward_run Mean           4.71713
evaluation/env_infos/final/reward_run Std            1.11584
evaluation/env_infos/final/reward_run Max            5.81672
evaluation/env_infos/final/reward_run Min            2.7705
evaluation/env_infos/initial/reward_run Mean        -0.0417571
evaluation/env_infos/initial/reward_run Std          0.189772
evaluation/env_infos/initial/reward_run Max          0.315103
evaluation/env_infos/initial/reward_run Min         -0.250833
evaluation/env_infos/reward_run Mean                 4.87089
evaluation/env_infos/reward_run Std                  1.08854
evaluation/env_infos/reward_run Max                  6.90722
evaluation/env_infos/reward_run Min                 -0.396932
evaluation/env_infos/final/reward_ctrl Mean         -0.427542
evaluation/env_infos/final/reward_ctrl Std           0.0922915
evaluation/env_infos/final/reward_ctrl Max          -0.269577
evaluation/env_infos/final/reward_ctrl Min          -0.546372
evaluation/env_infos/initial/reward_ctrl Mean       -0.0708744
evaluation/env_infos/initial/reward_ctrl Std         0.0493307
evaluation/env_infos/initial/reward_ctrl Max        -0.0126111
evaluation/env_infos/initial/reward_ctrl Min        -0.161189
evaluation/env_infos/reward_ctrl Mean               -0.424654
evaluation/env_infos/reward_ctrl Std                 0.0936845
evaluation/env_infos/reward_ctrl Max                -0.0126111
evaluation/env_infos/reward_ctrl Min                -0.57678
time/data storing (s)                                0.00864263
time/evaluation sampling (s)                         2.63299
time/exploration sampling (s)                        0.66022
time/logging (s)                                     0.0474767
time/saving (s)                                      0.0170175
time/training (s)                                   32.3423
time/epoch (s)                                      35.7087
time/total (s)                                    7598.12
Epoch                                              181
----------------------------------------------  ---------------
2020-07-08 23:13:24.882068 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 182 finished
----------------------------------------------  ---------------
replay_buffer/size                              184000
trainer/QF1 Loss                                     9.81719
trainer/QF2 Loss                                     9.95988
trainer/Policy Loss                               -247.466
trainer/Q1 Predictions Mean                        253.629
trainer/Q1 Predictions Std                          89.0309
trainer/Q1 Predictions Max                         320.689
trainer/Q1 Predictions Min                          15.7152
trainer/Q2 Predictions Mean                        253.252
trainer/Q2 Predictions Std                          89.0617
trainer/Q2 Predictions Max                         320.638
trainer/Q2 Predictions Min                          15.1981
trainer/Q Targets Mean                             253.319
trainer/Q Targets Std                               89.0124
trainer/Q Targets Max                              320.575
trainer/Q Targets Min                               15.1325
trainer/Log Pis Mean                                 5.9195
trainer/Log Pis Std                                  4.77712
trainer/Log Pis Max                                 22.097
trainer/Log Pis Min                                 -6.67255
trainer/Policy mu Mean                               0.0645682
trainer/Policy mu Std                                1.50552
trainer/Policy mu Max                                3.65626
trainer/Policy mu Min                               -3.55693
trainer/Policy log std Mean                         -0.820373
trainer/Policy log std Std                           0.345942
trainer/Policy log std Max                          -0.100006
trainer/Policy log std Min                          -2.46542
trainer/Alpha                                        0.105534
trainer/Alpha Loss                                  -0.181012
exploration/num steps total                     184000
exploration/num paths total                        184
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.18265
exploration/Rewards Std                              1.05097
exploration/Rewards Max                              6.24189
exploration/Rewards Min                             -0.305606
exploration/Returns Mean                          4182.65
exploration/Returns Std                              0
exploration/Returns Max                           4182.65
exploration/Returns Min                           4182.65
exploration/Actions Mean                            -0.00531882
exploration/Actions Std                              0.819993
exploration/Actions Max                              0.999807
exploration/Actions Min                             -0.999402
exploration/Num Paths                                1
exploration/Average Returns                       4182.65
exploration/env_infos/final/reward_run Mean          5.72704
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.72704
exploration/env_infos/final/reward_run Min           5.72704
exploration/env_infos/initial/reward_run Mean       -0.186443
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.186443
exploration/env_infos/initial/reward_run Min        -0.186443
exploration/env_infos/reward_run Mean                4.5861
exploration/env_infos/reward_run Std                 1.04263
exploration/env_infos/reward_run Max                 6.58952
exploration/env_infos/reward_run Min                -0.186443
exploration/env_infos/final/reward_ctrl Mean        -0.477716
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.477716
exploration/env_infos/final/reward_ctrl Min         -0.477716
exploration/env_infos/initial/reward_ctrl Mean      -0.0549916
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0549916
exploration/env_infos/initial/reward_ctrl Min       -0.0549916
exploration/env_infos/reward_ctrl Mean              -0.40345
exploration/env_infos/reward_ctrl Std                0.0943431
exploration/env_infos/reward_ctrl Max               -0.0549916
exploration/env_infos/reward_ctrl Min               -0.570953
evaluation/num steps total                      915000
evaluation/num paths total                         915
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.5349
evaluation/Rewards Std                               1.06822
evaluation/Rewards Max                               6.57103
evaluation/Rewards Min                              -0.663698
evaluation/Returns Mean                           4534.9
evaluation/Returns Std                              45.3314
evaluation/Returns Max                            4580.39
evaluation/Returns Min                            4458.79
evaluation/Actions Mean                             -0.0278777
evaluation/Actions Std                               0.837872
evaluation/Actions Max                               0.997809
evaluation/Actions Min                              -0.998726
evaluation/Num Paths                                 5
evaluation/Average Returns                        4534.9
evaluation/env_infos/final/reward_run Mean           5.27896
evaluation/env_infos/final/reward_run Std            0.611629
evaluation/env_infos/final/reward_run Max            6.34874
evaluation/env_infos/final/reward_run Min            4.63376
evaluation/env_infos/initial/reward_run Mean        -0.0898586
evaluation/env_infos/initial/reward_run Std          0.130695
evaluation/env_infos/initial/reward_run Max          0.021154
evaluation/env_infos/initial/reward_run Min         -0.321428
evaluation/env_infos/reward_run Mean                 4.95659
evaluation/env_infos/reward_run Std                  1.05851
evaluation/env_infos/reward_run Max                  6.90361
evaluation/env_infos/reward_run Min                 -0.321428
evaluation/env_infos/final/reward_ctrl Mean         -0.396976
evaluation/env_infos/final/reward_ctrl Std           0.0845509
evaluation/env_infos/final/reward_ctrl Max          -0.329337
evaluation/env_infos/final/reward_ctrl Min          -0.540165
evaluation/env_infos/initial/reward_ctrl Mean       -0.0425485
evaluation/env_infos/initial/reward_ctrl Std         0.0195721
evaluation/env_infos/initial/reward_ctrl Max        -0.0154578
evaluation/env_infos/initial/reward_ctrl Min        -0.0744712
evaluation/env_infos/reward_ctrl Mean               -0.421684
evaluation/env_infos/reward_ctrl Std                 0.0952764
evaluation/env_infos/reward_ctrl Max                -0.0154578
evaluation/env_infos/reward_ctrl Min                -0.576966
time/data storing (s)                                0.0067789
time/evaluation sampling (s)                         2.58745
time/exploration sampling (s)                        0.63551
time/logging (s)                                     0.0421103
time/saving (s)                                      0.0161545
time/training (s)                                   31.7072
time/epoch (s)                                      34.9952
time/total (s)                                    7633.14
Epoch                                              182
----------------------------------------------  ---------------
2020-07-08 23:13:59.944752 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 183 finished
----------------------------------------------  ---------------
replay_buffer/size                              185000
trainer/QF1 Loss                                     3.99987
trainer/QF2 Loss                                     5.99236
trainer/Policy Loss                               -255.669
trainer/Q1 Predictions Mean                        261.839
trainer/Q1 Predictions Std                          77.7916
trainer/Q1 Predictions Max                         325.3
trainer/Q1 Predictions Min                          13.7083
trainer/Q2 Predictions Mean                        261.657
trainer/Q2 Predictions Std                          77.7825
trainer/Q2 Predictions Max                         328.298
trainer/Q2 Predictions Min                          14.8728
trainer/Q Targets Mean                             261.951
trainer/Q Targets Std                               77.8993
trainer/Q Targets Max                              327.487
trainer/Q Targets Min                               13.3459
trainer/Log Pis Mean                                 6.13169
trainer/Log Pis Std                                  4.75066
trainer/Log Pis Max                                 20.1522
trainer/Log Pis Min                                 -5.42513
trainer/Policy mu Mean                               0.0837799
trainer/Policy mu Std                                1.52559
trainer/Policy mu Max                                5.55011
trainer/Policy mu Min                               -3.33297
trainer/Policy log std Mean                         -0.832503
trainer/Policy log std Std                           0.326572
trainer/Policy log std Max                           0.520241
trainer/Policy log std Min                          -2.55243
trainer/Alpha                                        0.105135
trainer/Alpha Loss                                   0.296657
exploration/num steps total                     185000
exploration/num paths total                        185
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.2722
exploration/Rewards Std                              1.08493
exploration/Rewards Max                              6.43035
exploration/Rewards Min                             -0.465703
exploration/Returns Mean                          4272.2
exploration/Returns Std                              0
exploration/Returns Max                           4272.2
exploration/Returns Min                           4272.2
exploration/Actions Mean                             0.0243713
exploration/Actions Std                              0.811526
exploration/Actions Max                              0.999468
exploration/Actions Min                             -0.999617
exploration/Num Paths                                1
exploration/Average Returns                       4272.2
exploration/env_infos/final/reward_run Mean          5.9011
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.9011
exploration/env_infos/final/reward_run Min           5.9011
exploration/env_infos/initial/reward_run Mean        0.30094
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.30094
exploration/env_infos/initial/reward_run Min         0.30094
exploration/env_infos/reward_run Mean                4.66771
exploration/env_infos/reward_run Std                 1.07615
exploration/env_infos/reward_run Max                 6.82744
exploration/env_infos/reward_run Min                -0.0315724
exploration/env_infos/final/reward_ctrl Mean        -0.362111
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.362111
exploration/env_infos/final/reward_ctrl Min         -0.362111
exploration/env_infos/initial/reward_ctrl Mean      -0.133678
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.133678
exploration/env_infos/initial/reward_ctrl Min       -0.133678
exploration/env_infos/reward_ctrl Mean              -0.395501
exploration/env_infos/reward_ctrl Std                0.0952666
exploration/env_infos/reward_ctrl Max               -0.0916448
exploration/env_infos/reward_ctrl Min               -0.579692
evaluation/num steps total                      920000
evaluation/num paths total                         920
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.42147
evaluation/Rewards Std                               1.11979
evaluation/Rewards Max                               6.65806
evaluation/Rewards Min                              -0.935022
evaluation/Returns Mean                           4421.47
evaluation/Returns Std                              68.8753
evaluation/Returns Max                            4491.99
evaluation/Returns Min                            4318.8
evaluation/Actions Mean                              0.00850188
evaluation/Actions Std                               0.835828
evaluation/Actions Max                               0.998214
evaluation/Actions Min                              -0.999175
evaluation/Num Paths                                 5
evaluation/Average Returns                        4421.47
evaluation/env_infos/final/reward_run Mean           5.17968
evaluation/env_infos/final/reward_run Std            1.03302
evaluation/env_infos/final/reward_run Max            6.44132
evaluation/env_infos/final/reward_run Min            3.74325
evaluation/env_infos/initial/reward_run Mean         0.111633
evaluation/env_infos/initial/reward_run Std          0.0954572
evaluation/env_infos/initial/reward_run Max          0.256094
evaluation/env_infos/initial/reward_run Min         -0.0360969
evaluation/env_infos/reward_run Mean                 4.84068
evaluation/env_infos/reward_run Std                  1.11136
evaluation/env_infos/reward_run Max                  7.10485
evaluation/env_infos/reward_run Min                 -0.440519
evaluation/env_infos/final/reward_ctrl Mean         -0.448741
evaluation/env_infos/final/reward_ctrl Std           0.0483029
evaluation/env_infos/final/reward_ctrl Max          -0.35676
evaluation/env_infos/final/reward_ctrl Min          -0.490683
evaluation/env_infos/initial/reward_ctrl Mean       -0.0860903
evaluation/env_infos/initial/reward_ctrl Std         0.0265823
evaluation/env_infos/initial/reward_ctrl Max        -0.054808
evaluation/env_infos/initial/reward_ctrl Min        -0.11981
evaluation/env_infos/reward_ctrl Mean               -0.419209
evaluation/env_infos/reward_ctrl Std                 0.0987359
evaluation/env_infos/reward_ctrl Max                -0.054808
evaluation/env_infos/reward_ctrl Min                -0.583051
time/data storing (s)                                0.006909
time/evaluation sampling (s)                         2.52784
time/exploration sampling (s)                        0.625124
time/logging (s)                                     0.0411899
time/saving (s)                                      0.0164139
time/training (s)                                   31.6386
time/epoch (s)                                      34.8561
time/total (s)                                    7668.2
Epoch                                              183
----------------------------------------------  ---------------
2020-07-08 23:14:35.064482 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 184 finished
----------------------------------------------  ----------------
replay_buffer/size                              186000
trainer/QF1 Loss                                     7.65187
trainer/QF2 Loss                                     8.89314
trainer/Policy Loss                               -247.418
trainer/Q1 Predictions Mean                        253.738
trainer/Q1 Predictions Std                          87.8249
trainer/Q1 Predictions Max                         321.685
trainer/Q1 Predictions Min                          16.5997
trainer/Q2 Predictions Mean                        253.492
trainer/Q2 Predictions Std                          87.7806
trainer/Q2 Predictions Max                         321.269
trainer/Q2 Predictions Min                          16.919
trainer/Q Targets Mean                             253.151
trainer/Q Targets Std                               87.5654
trainer/Q Targets Max                              321.063
trainer/Q Targets Min                               16.4488
trainer/Log Pis Mean                                 6.53065
trainer/Log Pis Std                                  5.3276
trainer/Log Pis Max                                 30.8432
trainer/Log Pis Min                                 -6.04584
trainer/Policy mu Mean                               0.0711953
trainer/Policy mu Std                                1.55467
trainer/Policy mu Max                                7.038
trainer/Policy mu Min                               -7.10794
trainer/Policy log std Mean                         -0.817337
trainer/Policy log std Std                           0.341483
trainer/Policy log std Max                           0.0308917
trainer/Policy log std Min                          -2.51869
trainer/Alpha                                        0.105608
trainer/Alpha Loss                                   1.19291
exploration/num steps total                     186000
exploration/num paths total                        186
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.39784
exploration/Rewards Std                              1.07227
exploration/Rewards Max                              6.77067
exploration/Rewards Min                             -0.457765
exploration/Returns Mean                          4397.84
exploration/Returns Std                              0
exploration/Returns Max                           4397.84
exploration/Returns Min                           4397.84
exploration/Actions Mean                             0.00279195
exploration/Actions Std                              0.820596
exploration/Actions Max                              0.999586
exploration/Actions Min                             -0.999814
exploration/Num Paths                                1
exploration/Average Returns                       4397.84
exploration/env_infos/final/reward_run Mean          5.93626
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.93626
exploration/env_infos/final/reward_run Min           5.93626
exploration/env_infos/initial/reward_run Mean        0.318494
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.318494
exploration/env_infos/initial/reward_run Min         0.318494
exploration/env_infos/reward_run Mean                4.80188
exploration/env_infos/reward_run Std                 1.06422
exploration/env_infos/reward_run Max                 7.1061
exploration/env_infos/reward_run Min                 0.0645228
exploration/env_infos/final/reward_ctrl Mean        -0.315667
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.315667
exploration/env_infos/final/reward_ctrl Min         -0.315667
exploration/env_infos/initial/reward_ctrl Mean      -0.115207
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.115207
exploration/env_infos/initial/reward_ctrl Min       -0.115207
exploration/env_infos/reward_ctrl Mean              -0.404031
exploration/env_infos/reward_ctrl Std                0.0979826
exploration/env_infos/reward_ctrl Max               -0.115207
exploration/env_infos/reward_ctrl Min               -0.578095
evaluation/num steps total                      925000
evaluation/num paths total                         925
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.52141
evaluation/Rewards Std                               1.09987
evaluation/Rewards Max                               6.70651
evaluation/Rewards Min                              -1.16579
evaluation/Returns Mean                           4521.41
evaluation/Returns Std                              68.8069
evaluation/Returns Max                            4584.77
evaluation/Returns Min                            4403.7
evaluation/Actions Mean                             -0.000794072
evaluation/Actions Std                               0.833469
evaluation/Actions Max                               0.998516
evaluation/Actions Min                              -0.998533
evaluation/Num Paths                                 5
evaluation/Average Returns                        4521.41
evaluation/env_infos/final/reward_run Mean           4.01535
evaluation/env_infos/final/reward_run Std            0.331747
evaluation/env_infos/final/reward_run Max            4.58916
evaluation/env_infos/final/reward_run Min            3.63055
evaluation/env_infos/initial/reward_run Mean         0.153074
evaluation/env_infos/initial/reward_run Std          0.196477
evaluation/env_infos/initial/reward_run Max          0.515057
evaluation/env_infos/initial/reward_run Min         -0.00428121
evaluation/env_infos/reward_run Mean                 4.93822
evaluation/env_infos/reward_run Std                  1.08967
evaluation/env_infos/reward_run Max                  7.10909
evaluation/env_infos/reward_run Min                 -0.63562
evaluation/env_infos/final/reward_ctrl Mean         -0.389565
evaluation/env_infos/final/reward_ctrl Std           0.112203
evaluation/env_infos/final/reward_ctrl Max          -0.225839
evaluation/env_infos/final/reward_ctrl Min          -0.519048
evaluation/env_infos/initial/reward_ctrl Mean       -0.0684241
evaluation/env_infos/initial/reward_ctrl Std         0.00903723
evaluation/env_infos/initial/reward_ctrl Max        -0.0524663
evaluation/env_infos/initial/reward_ctrl Min        -0.0803998
evaluation/env_infos/reward_ctrl Mean               -0.416803
evaluation/env_infos/reward_ctrl Std                 0.103568
evaluation/env_infos/reward_ctrl Max                -0.0524663
evaluation/env_infos/reward_ctrl Min                -0.580436
time/data storing (s)                                0.00685404
time/evaluation sampling (s)                         2.55032
time/exploration sampling (s)                        0.624238
time/logging (s)                                     0.0407462
time/saving (s)                                      0.0198484
time/training (s)                                   31.8334
time/epoch (s)                                      35.0754
time/total (s)                                    7703.31
Epoch                                              184
----------------------------------------------  ----------------
2020-07-08 23:15:22.366718 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 185 finished
----------------------------------------------  --------------
replay_buffer/size                              187000
trainer/QF1 Loss                                     8.62783
trainer/QF2 Loss                                     7.26104
trainer/Policy Loss                               -237.144
trainer/Q1 Predictions Mean                        242.372
trainer/Q1 Predictions Std                         100.968
trainer/Q1 Predictions Max                         320.564
trainer/Q1 Predictions Min                          14.7221
trainer/Q2 Predictions Mean                        242.664
trainer/Q2 Predictions Std                         101.013
trainer/Q2 Predictions Max                         320.376
trainer/Q2 Predictions Min                          15.1071
trainer/Q Targets Mean                             242.867
trainer/Q Targets Std                              100.946
trainer/Q Targets Max                              322.499
trainer/Q Targets Min                               14.6567
trainer/Log Pis Mean                                 5.38872
trainer/Log Pis Std                                  4.71013
trainer/Log Pis Max                                 16.2541
trainer/Log Pis Min                                 -5.97297
trainer/Policy mu Mean                               0.117014
trainer/Policy mu Std                                1.45059
trainer/Policy mu Max                                3.41396
trainer/Policy mu Min                               -2.98259
trainer/Policy log std Mean                         -0.796601
trainer/Policy log std Std                           0.332884
trainer/Policy log std Max                           0.0658274
trainer/Policy log std Min                          -2.26602
trainer/Alpha                                        0.103273
trainer/Alpha Loss                                  -1.38774
exploration/num steps total                     187000
exploration/num paths total                        187
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.35967
exploration/Rewards Std                              1.04303
exploration/Rewards Max                              6.51655
exploration/Rewards Min                             -0.656332
exploration/Returns Mean                          4359.67
exploration/Returns Std                              0
exploration/Returns Max                           4359.67
exploration/Returns Min                           4359.67
exploration/Actions Mean                            -0.0057841
exploration/Actions Std                              0.825701
exploration/Actions Max                              0.999126
exploration/Actions Min                             -0.999668
exploration/Num Paths                                1
exploration/Average Returns                       4359.67
exploration/env_infos/final/reward_run Mean          6.17312
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.17312
exploration/env_infos/final/reward_run Min           6.17312
exploration/env_infos/initial/reward_run Mean        0.0686102
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0686102
exploration/env_infos/initial/reward_run Min         0.0686102
exploration/env_infos/reward_run Mean                4.76876
exploration/env_infos/reward_run Std                 1.03814
exploration/env_infos/reward_run Max                 6.83561
exploration/env_infos/reward_run Min                -0.195432
exploration/env_infos/final/reward_ctrl Mean        -0.47159
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.47159
exploration/env_infos/final/reward_ctrl Min         -0.47159
exploration/env_infos/initial/reward_ctrl Mean      -0.0888883
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0888883
exploration/env_infos/initial/reward_ctrl Min       -0.0888883
exploration/env_infos/reward_ctrl Mean              -0.40909
exploration/env_infos/reward_ctrl Std                0.0959484
exploration/env_infos/reward_ctrl Max               -0.0851683
exploration/env_infos/reward_ctrl Min               -0.577868
evaluation/num steps total                      930000
evaluation/num paths total                         930
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.39817
evaluation/Rewards Std                               1.08868
evaluation/Rewards Max                               6.77827
evaluation/Rewards Min                              -0.858718
evaluation/Returns Mean                           4398.17
evaluation/Returns Std                              57.2196
evaluation/Returns Max                            4511.17
evaluation/Returns Min                            4355.22
evaluation/Actions Mean                             -0.0178241
evaluation/Actions Std                               0.840038
evaluation/Actions Max                               0.997918
evaluation/Actions Min                              -0.997438
evaluation/Num Paths                                 5
evaluation/Average Returns                        4398.17
evaluation/env_infos/final/reward_run Mean           4.40032
evaluation/env_infos/final/reward_run Std            0.652299
evaluation/env_infos/final/reward_run Max            5.00668
evaluation/env_infos/final/reward_run Min            3.14095
evaluation/env_infos/initial/reward_run Mean        -0.0689367
evaluation/env_infos/initial/reward_run Std          0.196032
evaluation/env_infos/initial/reward_run Max          0.277916
evaluation/env_infos/initial/reward_run Min         -0.268751
evaluation/env_infos/reward_run Mean                 4.82176
evaluation/env_infos/reward_run Std                  1.08399
evaluation/env_infos/reward_run Max                  7.08892
evaluation/env_infos/reward_run Min                 -0.338932
evaluation/env_infos/final/reward_ctrl Mean         -0.405552
evaluation/env_infos/final/reward_ctrl Std           0.112701
evaluation/env_infos/final/reward_ctrl Max          -0.273422
evaluation/env_infos/final/reward_ctrl Min          -0.556644
evaluation/env_infos/initial/reward_ctrl Mean       -0.0584829
evaluation/env_infos/initial/reward_ctrl Std         0.0169741
evaluation/env_infos/initial/reward_ctrl Max        -0.0280858
evaluation/env_infos/initial/reward_ctrl Min        -0.0793334
evaluation/env_infos/reward_ctrl Mean               -0.423589
evaluation/env_infos/reward_ctrl Std                 0.0951463
evaluation/env_infos/reward_ctrl Max                -0.0280858
evaluation/env_infos/reward_ctrl Min                -0.576342
time/data storing (s)                                0.0071793
time/evaluation sampling (s)                         3.6789
time/exploration sampling (s)                        1.22787
time/logging (s)                                     0.0464804
time/saving (s)                                      0.016944
time/training (s)                                   42.3045
time/epoch (s)                                      47.2818
time/total (s)                                    7750.61
Epoch                                              185
----------------------------------------------  --------------
2020-07-08 23:16:17.097996 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 186 finished
----------------------------------------------  ----------------
replay_buffer/size                              188000
trainer/QF1 Loss                                     8.17741
trainer/QF2 Loss                                     9.57102
trainer/Policy Loss                               -236.563
trainer/Q1 Predictions Mean                        241.651
trainer/Q1 Predictions Std                          95.4526
trainer/Q1 Predictions Max                         321.789
trainer/Q1 Predictions Min                          15.0891
trainer/Q2 Predictions Mean                        242.192
trainer/Q2 Predictions Std                          95.4835
trainer/Q2 Predictions Max                         324.433
trainer/Q2 Predictions Min                          14.8768
trainer/Q Targets Mean                             242.283
trainer/Q Targets Std                               95.7841
trainer/Q Targets Max                              323.697
trainer/Q Targets Min                               14.5024
trainer/Log Pis Mean                                 5.49649
trainer/Log Pis Std                                  5.1061
trainer/Log Pis Max                                 22.7753
trainer/Log Pis Min                                 -6.64345
trainer/Policy mu Mean                               0.00807358
trainer/Policy mu Std                                1.49212
trainer/Policy mu Max                                3.6615
trainer/Policy mu Min                               -3.38052
trainer/Policy log std Mean                         -0.808589
trainer/Policy log std Std                           0.351139
trainer/Policy log std Max                          -0.0739549
trainer/Policy log std Min                          -2.50848
trainer/Alpha                                        0.105598
trainer/Alpha Loss                                  -1.13195
exploration/num steps total                     188000
exploration/num paths total                        188
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.35357
exploration/Rewards Std                              1.03995
exploration/Rewards Max                              6.40023
exploration/Rewards Min                             -0.355406
exploration/Returns Mean                          4353.57
exploration/Returns Std                              0
exploration/Returns Max                           4353.57
exploration/Returns Min                           4353.57
exploration/Actions Mean                            -0.000823087
exploration/Actions Std                              0.808264
exploration/Actions Max                              0.99939
exploration/Actions Min                             -0.999649
exploration/Num Paths                                1
exploration/Average Returns                       4353.57
exploration/env_infos/final/reward_run Mean          3.51056
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.51056
exploration/env_infos/final/reward_run Min           3.51056
exploration/env_infos/initial/reward_run Mean       -0.0247191
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0247191
exploration/env_infos/initial/reward_run Min        -0.0247191
exploration/env_infos/reward_run Mean                4.74555
exploration/env_infos/reward_run Std                 1.02525
exploration/env_infos/reward_run Max                 6.88992
exploration/env_infos/reward_run Min                -0.0247191
exploration/env_infos/final/reward_ctrl Mean        -0.33085
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.33085
exploration/env_infos/final/reward_ctrl Min         -0.33085
exploration/env_infos/initial/reward_ctrl Mean      -0.122897
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.122897
exploration/env_infos/initial/reward_ctrl Min       -0.122897
exploration/env_infos/reward_ctrl Mean              -0.391975
exploration/env_infos/reward_ctrl Std                0.104511
exploration/env_infos/reward_ctrl Max               -0.0995058
exploration/env_infos/reward_ctrl Min               -0.587691
evaluation/num steps total                      935000
evaluation/num paths total                         935
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.47189
evaluation/Rewards Std                               1.09981
evaluation/Rewards Max                               6.59591
evaluation/Rewards Min                              -1.00447
evaluation/Returns Mean                           4471.89
evaluation/Returns Std                             100.55
evaluation/Returns Max                            4607.59
evaluation/Returns Min                            4334.83
evaluation/Actions Mean                             -0.0157743
evaluation/Actions Std                               0.83077
evaluation/Actions Max                               0.998736
evaluation/Actions Min                              -0.998496
evaluation/Num Paths                                 5
evaluation/Average Returns                        4471.89
evaluation/env_infos/final/reward_run Mean           5.13545
evaluation/env_infos/final/reward_run Std            0.950753
evaluation/env_infos/final/reward_run Max            6.5946
evaluation/env_infos/final/reward_run Min            3.9007
evaluation/env_infos/initial/reward_run Mean        -0.0102221
evaluation/env_infos/initial/reward_run Std          0.272066
evaluation/env_infos/initial/reward_run Max          0.465258
evaluation/env_infos/initial/reward_run Min         -0.303626
evaluation/env_infos/reward_run Mean                 4.88615
evaluation/env_infos/reward_run Std                  1.08986
evaluation/env_infos/reward_run Max                  6.94306
evaluation/env_infos/reward_run Min                 -0.65501
evaluation/env_infos/final/reward_ctrl Mean         -0.449286
evaluation/env_infos/final/reward_ctrl Std           0.103161
evaluation/env_infos/final/reward_ctrl Max          -0.311921
evaluation/env_infos/final/reward_ctrl Min          -0.558514
evaluation/env_infos/initial/reward_ctrl Mean       -0.0754027
evaluation/env_infos/initial/reward_ctrl Std         0.0249875
evaluation/env_infos/initial/reward_ctrl Max        -0.0502957
evaluation/env_infos/initial/reward_ctrl Min        -0.121389
evaluation/env_infos/reward_ctrl Mean               -0.414256
evaluation/env_infos/reward_ctrl Std                 0.10557
evaluation/env_infos/reward_ctrl Max                -0.0493343
evaluation/env_infos/reward_ctrl Min                -0.575546
time/data storing (s)                                0.0096493
time/evaluation sampling (s)                         4.35519
time/exploration sampling (s)                        1.03744
time/logging (s)                                     0.0457008
time/saving (s)                                      0.0205421
time/training (s)                                   49.2407
time/epoch (s)                                      54.7092
time/total (s)                                    7805.34
Epoch                                              186
----------------------------------------------  ----------------
2020-07-08 23:17:06.970415 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 187 finished
----------------------------------------------  ---------------
replay_buffer/size                              189000
trainer/QF1 Loss                                     6.43559
trainer/QF2 Loss                                     6.33411
trainer/Policy Loss                               -253.974
trainer/Q1 Predictions Mean                        259.589
trainer/Q1 Predictions Std                          86.3019
trainer/Q1 Predictions Max                         321.027
trainer/Q1 Predictions Min                          13.9106
trainer/Q2 Predictions Mean                        259.647
trainer/Q2 Predictions Std                          86.3237
trainer/Q2 Predictions Max                         321.67
trainer/Q2 Predictions Min                          14.1054
trainer/Q Targets Mean                             259.488
trainer/Q Targets Std                               86.1294
trainer/Q Targets Max                              324.333
trainer/Q Targets Min                               14.3879
trainer/Log Pis Mean                                 5.61763
trainer/Log Pis Std                                  5.11383
trainer/Log Pis Max                                 18.5796
trainer/Log Pis Min                                 -7.97324
trainer/Policy mu Mean                               0.062851
trainer/Policy mu Std                                1.4822
trainer/Policy mu Max                                3.53705
trainer/Policy mu Min                               -3.16109
trainer/Policy log std Mean                         -0.822257
trainer/Policy log std Std                           0.33147
trainer/Policy log std Max                           0.0192491
trainer/Policy log std Min                          -2.7036
trainer/Alpha                                        0.105976
trainer/Alpha Loss                                  -0.8582
exploration/num steps total                     189000
exploration/num paths total                        189
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.36654
exploration/Rewards Std                              1.06194
exploration/Rewards Max                              6.33851
exploration/Rewards Min                             -0.777818
exploration/Returns Mean                          4366.54
exploration/Returns Std                              0
exploration/Returns Max                           4366.54
exploration/Returns Min                           4366.54
exploration/Actions Mean                            -0.0223144
exploration/Actions Std                              0.831413
exploration/Actions Max                              0.99957
exploration/Actions Min                             -0.999364
exploration/Num Paths                                1
exploration/Average Returns                       4366.54
exploration/env_infos/final/reward_run Mean          5.01573
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.01573
exploration/env_infos/final/reward_run Min           5.01573
exploration/env_infos/initial/reward_run Mean       -0.00887447
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.00887447
exploration/env_infos/initial/reward_run Min        -0.00887447
exploration/env_infos/reward_run Mean                4.78158
exploration/env_infos/reward_run Std                 1.05759
exploration/env_infos/reward_run Max                 6.72418
exploration/env_infos/reward_run Min                -0.313365
exploration/env_infos/final/reward_ctrl Mean        -0.238898
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.238898
exploration/env_infos/final/reward_ctrl Min         -0.238898
exploration/env_infos/initial/reward_ctrl Mean      -0.108939
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.108939
exploration/env_infos/initial/reward_ctrl Min       -0.108939
exploration/env_infos/reward_ctrl Mean              -0.415048
exploration/env_infos/reward_ctrl Std                0.0944961
exploration/env_infos/reward_ctrl Max               -0.0758178
exploration/env_infos/reward_ctrl Min               -0.588499
evaluation/num steps total                      940000
evaluation/num paths total                         940
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.41883
evaluation/Rewards Std                               1.05613
evaluation/Rewards Max                               6.73445
evaluation/Rewards Min                              -0.956773
evaluation/Returns Mean                           4418.83
evaluation/Returns Std                              44.349
evaluation/Returns Max                            4486.82
evaluation/Returns Min                            4359.53
evaluation/Actions Mean                             -0.0382789
evaluation/Actions Std                               0.842956
evaluation/Actions Max                               0.99851
evaluation/Actions Min                              -0.998174
evaluation/Num Paths                                 5
evaluation/Average Returns                        4418.83
evaluation/env_infos/final/reward_run Mean           5.10469
evaluation/env_infos/final/reward_run Std            0.635025
evaluation/env_infos/final/reward_run Max            6.05692
evaluation/env_infos/final/reward_run Min            4.10872
evaluation/env_infos/initial/reward_run Mean         0.182878
evaluation/env_infos/initial/reward_run Std          0.250525
evaluation/env_infos/initial/reward_run Max          0.596023
evaluation/env_infos/initial/reward_run Min         -0.106346
evaluation/env_infos/reward_run Mean                 4.84605
evaluation/env_infos/reward_run Std                  1.04939
evaluation/env_infos/reward_run Max                  7.02658
evaluation/env_infos/reward_run Min                 -0.434788
evaluation/env_infos/final/reward_ctrl Mean         -0.432199
evaluation/env_infos/final/reward_ctrl Std           0.0931171
evaluation/env_infos/final/reward_ctrl Max          -0.282803
evaluation/env_infos/final/reward_ctrl Min          -0.527793
evaluation/env_infos/initial/reward_ctrl Mean       -0.0764985
evaluation/env_infos/initial/reward_ctrl Std         0.0336388
evaluation/env_infos/initial/reward_ctrl Max        -0.0204331
evaluation/env_infos/initial/reward_ctrl Min        -0.108925
evaluation/env_infos/reward_ctrl Mean               -0.427224
evaluation/env_infos/reward_ctrl Std                 0.0945039
evaluation/env_infos/reward_ctrl Max                -0.0204331
evaluation/env_infos/reward_ctrl Min                -0.573503
time/data storing (s)                                0.0130008
time/evaluation sampling (s)                         4.02808
time/exploration sampling (s)                        2.1367
time/logging (s)                                     0.0450876
time/saving (s)                                      0.0205386
time/training (s)                                   43.583
time/epoch (s)                                      49.8264
time/total (s)                                    7855.2
Epoch                                              187
----------------------------------------------  ---------------
2020-07-08 23:17:54.686929 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 188 finished
----------------------------------------------  ---------------
replay_buffer/size                              190000
trainer/QF1 Loss                                     6.25239
trainer/QF2 Loss                                     5.54997
trainer/Policy Loss                               -243.456
trainer/Q1 Predictions Mean                        249.496
trainer/Q1 Predictions Std                          92.2895
trainer/Q1 Predictions Max                         322.697
trainer/Q1 Predictions Min                          15.9391
trainer/Q2 Predictions Mean                        249.614
trainer/Q2 Predictions Std                          92.2616
trainer/Q2 Predictions Max                         323.934
trainer/Q2 Predictions Min                          15.2514
trainer/Q Targets Mean                             249.697
trainer/Q Targets Std                               92.316
trainer/Q Targets Max                              324.224
trainer/Q Targets Min                               16.2991
trainer/Log Pis Mean                                 6.17007
trainer/Log Pis Std                                  4.97024
trainer/Log Pis Max                                 17.8845
trainer/Log Pis Min                                 -6.20183
trainer/Policy mu Mean                               0.0543561
trainer/Policy mu Std                                1.51132
trainer/Policy mu Max                                3.39289
trainer/Policy mu Min                               -3.36808
trainer/Policy log std Mean                         -0.812665
trainer/Policy log std Std                           0.32439
trainer/Policy log std Max                           0.232782
trainer/Policy log std Min                          -2.39753
trainer/Alpha                                        0.106701
trainer/Alpha Loss                                   0.380548
exploration/num steps total                     190000
exploration/num paths total                        190
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.25364
exploration/Rewards Std                              1.06813
exploration/Rewards Max                              6.48013
exploration/Rewards Min                             -0.2488
exploration/Returns Mean                          4253.64
exploration/Returns Std                              0
exploration/Returns Max                           4253.64
exploration/Returns Min                           4253.64
exploration/Actions Mean                            -0.00848215
exploration/Actions Std                              0.824948
exploration/Actions Max                              0.999623
exploration/Actions Min                             -0.999551
exploration/Num Paths                                1
exploration/Average Returns                       4253.64
exploration/env_infos/final/reward_run Mean          5.11384
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.11384
exploration/env_infos/final/reward_run Min           5.11384
exploration/env_infos/initial/reward_run Mean        0.0496048
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0496048
exploration/env_infos/initial/reward_run Min         0.0496048
exploration/env_infos/reward_run Mean                4.66201
exploration/env_infos/reward_run Std                 1.06341
exploration/env_infos/reward_run Max                 6.83313
exploration/env_infos/reward_run Min                 0.0361228
exploration/env_infos/final/reward_ctrl Mean        -0.497685
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.497685
exploration/env_infos/final/reward_ctrl Min         -0.497685
exploration/env_infos/initial/reward_ctrl Mean      -0.0317868
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0317868
exploration/env_infos/initial/reward_ctrl Min       -0.0317868
exploration/env_infos/reward_ctrl Mean              -0.408367
exploration/env_infos/reward_ctrl Std                0.0983873
exploration/env_infos/reward_ctrl Max               -0.0317868
exploration/env_infos/reward_ctrl Min               -0.575044
evaluation/num steps total                      945000
evaluation/num paths total                         945
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.40095
evaluation/Rewards Std                               1.08981
evaluation/Rewards Max                               6.70113
evaluation/Rewards Min                              -0.817615
evaluation/Returns Mean                           4400.95
evaluation/Returns Std                              90.887
evaluation/Returns Max                            4565.09
evaluation/Returns Min                            4330.34
evaluation/Actions Mean                             -0.0255953
evaluation/Actions Std                               0.843504
evaluation/Actions Max                               0.99809
evaluation/Actions Min                              -0.998603
evaluation/Num Paths                                 5
evaluation/Average Returns                        4400.95
evaluation/env_infos/final/reward_run Mean           5.02356
evaluation/env_infos/final/reward_run Std            1.0066
evaluation/env_infos/final/reward_run Max            6.33793
evaluation/env_infos/final/reward_run Min            3.6182
evaluation/env_infos/initial/reward_run Mean        -0.0126524
evaluation/env_infos/initial/reward_run Std          0.0681417
evaluation/env_infos/initial/reward_run Max          0.095677
evaluation/env_infos/initial/reward_run Min         -0.0965027
evaluation/env_infos/reward_run Mean                 4.82824
evaluation/env_infos/reward_run Std                  1.08324
evaluation/env_infos/reward_run Max                  7.11533
evaluation/env_infos/reward_run Min                 -0.315232
evaluation/env_infos/final/reward_ctrl Mean         -0.446431
evaluation/env_infos/final/reward_ctrl Std           0.0818837
evaluation/env_infos/final/reward_ctrl Max          -0.339673
evaluation/env_infos/final/reward_ctrl Min          -0.531195
evaluation/env_infos/initial/reward_ctrl Mean       -0.088999
evaluation/env_infos/initial/reward_ctrl Std         0.0280184
evaluation/env_infos/initial/reward_ctrl Max        -0.0491035
evaluation/env_infos/initial/reward_ctrl Min        -0.129431
evaluation/env_infos/reward_ctrl Mean               -0.427292
evaluation/env_infos/reward_ctrl Std                 0.0975828
evaluation/env_infos/reward_ctrl Max                -0.0491035
evaluation/env_infos/reward_ctrl Min                -0.578698
time/data storing (s)                                0.00695465
time/evaluation sampling (s)                         3.05219
time/exploration sampling (s)                        1.22971
time/logging (s)                                     0.0414247
time/saving (s)                                      0.016293
time/training (s)                                   43.3397
time/epoch (s)                                      47.6862
time/total (s)                                    7902.91
Epoch                                              188
----------------------------------------------  ---------------
2020-07-08 23:18:47.409479 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 189 finished
----------------------------------------------  ---------------
replay_buffer/size                              191000
trainer/QF1 Loss                                     6.56361
trainer/QF2 Loss                                     9.70859
trainer/Policy Loss                               -233.57
trainer/Q1 Predictions Mean                        238.916
trainer/Q1 Predictions Std                         104.246
trainer/Q1 Predictions Max                         326.304
trainer/Q1 Predictions Min                          15.0653
trainer/Q2 Predictions Mean                        238.713
trainer/Q2 Predictions Std                         104.184
trainer/Q2 Predictions Max                         324.698
trainer/Q2 Predictions Min                          15.1004
trainer/Q Targets Mean                             238.788
trainer/Q Targets Std                              104.332
trainer/Q Targets Max                              324.045
trainer/Q Targets Min                               14.7956
trainer/Log Pis Mean                                 5.09981
trainer/Log Pis Std                                  5.337
trainer/Log Pis Max                                 18.6797
trainer/Log Pis Min                                 -6.56825
trainer/Policy mu Mean                               0.06037
trainer/Policy mu Std                                1.4366
trainer/Policy mu Max                                3.84083
trainer/Policy mu Min                               -3.78573
trainer/Policy log std Mean                         -0.807165
trainer/Policy log std Std                           0.348451
trainer/Policy log std Max                          -0.0779205
trainer/Policy log std Min                          -2.59913
trainer/Alpha                                        0.105606
trainer/Alpha Loss                                  -2.02359
exploration/num steps total                     191000
exploration/num paths total                        191
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.2777
exploration/Rewards Std                              1.03886
exploration/Rewards Max                              6.40477
exploration/Rewards Min                             -0.47457
exploration/Returns Mean                          4277.7
exploration/Returns Std                              0
exploration/Returns Max                           4277.7
exploration/Returns Min                           4277.7
exploration/Actions Mean                            -0.00381214
exploration/Actions Std                              0.832719
exploration/Actions Max                              0.999599
exploration/Actions Min                             -0.999369
exploration/Num Paths                                1
exploration/Average Returns                       4277.7
exploration/env_infos/final/reward_run Mean          5.82073
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.82073
exploration/env_infos/final/reward_run Min           5.82073
exploration/env_infos/initial/reward_run Mean       -0.199079
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.199079
exploration/env_infos/initial/reward_run Min        -0.199079
exploration/env_infos/reward_run Mean                4.69376
exploration/env_infos/reward_run Std                 1.02698
exploration/env_infos/reward_run Max                 6.70169
exploration/env_infos/reward_run Min                -0.343699
exploration/env_infos/final/reward_ctrl Mean        -0.429849
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.429849
exploration/env_infos/final/reward_ctrl Min         -0.429849
exploration/env_infos/initial/reward_ctrl Mean      -0.121217
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.121217
exploration/env_infos/initial/reward_ctrl Min       -0.121217
exploration/env_infos/reward_ctrl Mean              -0.416061
exploration/env_infos/reward_ctrl Std                0.0988786
exploration/env_infos/reward_ctrl Max               -0.112765
exploration/env_infos/reward_ctrl Min               -0.581783
evaluation/num steps total                      950000
evaluation/num paths total                         950
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.38592
evaluation/Rewards Std                               1.0771
evaluation/Rewards Max                               6.47472
evaluation/Rewards Min                              -1.09122
evaluation/Returns Mean                           4385.92
evaluation/Returns Std                              50.175
evaluation/Returns Max                            4443.84
evaluation/Returns Min                            4305.45
evaluation/Actions Mean                             -0.0213535
evaluation/Actions Std                               0.847279
evaluation/Actions Max                               0.997761
evaluation/Actions Min                              -0.998889
evaluation/Num Paths                                 5
evaluation/Average Returns                        4385.92
evaluation/env_infos/final/reward_run Mean           5.7774
evaluation/env_infos/final/reward_run Std            0.571684
evaluation/env_infos/final/reward_run Max            6.46777
evaluation/env_infos/final/reward_run Min            4.78357
evaluation/env_infos/initial/reward_run Mean         0.102241
evaluation/env_infos/initial/reward_run Std          0.130382
evaluation/env_infos/initial/reward_run Max          0.313312
evaluation/env_infos/initial/reward_run Min         -0.0186731
evaluation/env_infos/reward_run Mean                 4.81692
evaluation/env_infos/reward_run Std                  1.07124
evaluation/env_infos/reward_run Max                  6.90553
evaluation/env_infos/reward_run Min                 -0.541743
evaluation/env_infos/final/reward_ctrl Mean         -0.451593
evaluation/env_infos/final/reward_ctrl Std           0.0555508
evaluation/env_infos/final/reward_ctrl Max          -0.366533
evaluation/env_infos/final/reward_ctrl Min          -0.506741
evaluation/env_infos/initial/reward_ctrl Mean       -0.0777514
evaluation/env_infos/initial/reward_ctrl Std         0.0281619
evaluation/env_infos/initial/reward_ctrl Max        -0.0311213
evaluation/env_infos/initial/reward_ctrl Min        -0.111769
evaluation/env_infos/reward_ctrl Mean               -0.431002
evaluation/env_infos/reward_ctrl Std                 0.09868
evaluation/env_infos/reward_ctrl Max                -0.0311213
evaluation/env_infos/reward_ctrl Min                -0.59083
time/data storing (s)                                0.00690351
time/evaluation sampling (s)                         4.50679
time/exploration sampling (s)                        1.06009
time/logging (s)                                     0.0420698
time/saving (s)                                      0.0165716
time/training (s)                                   47.0715
time/epoch (s)                                      52.7039
time/total (s)                                    7955.62
Epoch                                              189
----------------------------------------------  ---------------
2020-07-08 23:19:39.984410 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 190 finished
----------------------------------------------  ---------------
replay_buffer/size                              192000
trainer/QF1 Loss                                     7.24445
trainer/QF2 Loss                                     7.51613
trainer/Policy Loss                               -248.108
trainer/Q1 Predictions Mean                        254.127
trainer/Q1 Predictions Std                          88.9028
trainer/Q1 Predictions Max                         322.931
trainer/Q1 Predictions Min                          16.6252
trainer/Q2 Predictions Mean                        254.143
trainer/Q2 Predictions Std                          89.0924
trainer/Q2 Predictions Max                         324.275
trainer/Q2 Predictions Min                          17.241
trainer/Q Targets Mean                             253.924
trainer/Q Targets Std                               89.0848
trainer/Q Targets Max                              324.185
trainer/Q Targets Min                               16.7009
trainer/Log Pis Mean                                 6.18651
trainer/Log Pis Std                                  5.19527
trainer/Log Pis Max                                 16.6666
trainer/Log Pis Min                                 -5.86685
trainer/Policy mu Mean                               0.0673547
trainer/Policy mu Std                                1.52009
trainer/Policy mu Max                                4.04238
trainer/Policy mu Min                               -4.41197
trainer/Policy log std Mean                         -0.819643
trainer/Policy log std Std                           0.345638
trainer/Policy log std Max                           0.0899326
trainer/Policy log std Min                          -2.55468
trainer/Alpha                                        0.106378
trainer/Alpha Loss                                   0.417898
exploration/num steps total                     192000
exploration/num paths total                        192
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.53516
exploration/Rewards Std                              1.05621
exploration/Rewards Max                              6.46686
exploration/Rewards Min                             -0.616425
exploration/Returns Mean                          4535.16
exploration/Returns Std                              0
exploration/Returns Max                           4535.16
exploration/Returns Min                           4535.16
exploration/Actions Mean                             0.00023362
exploration/Actions Std                              0.818499
exploration/Actions Max                              0.999672
exploration/Actions Min                             -0.999369
exploration/Num Paths                                1
exploration/Average Returns                       4535.16
exploration/env_infos/final/reward_run Mean          4.90802
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.90802
exploration/env_infos/final/reward_run Min           4.90802
exploration/env_infos/initial/reward_run Mean        0.0875539
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0875539
exploration/env_infos/initial/reward_run Min         0.0875539
exploration/env_infos/reward_run Mean                4.93712
exploration/env_infos/reward_run Std                 1.04336
exploration/env_infos/reward_run Max                 6.77261
exploration/env_infos/reward_run Min                -0.217744
exploration/env_infos/final/reward_ctrl Mean        -0.276432
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.276432
exploration/env_infos/final/reward_ctrl Min         -0.276432
exploration/env_infos/initial/reward_ctrl Mean      -0.0352323
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0352323
exploration/env_infos/initial/reward_ctrl Min       -0.0352323
exploration/env_infos/reward_ctrl Mean              -0.401965
exploration/env_infos/reward_ctrl Std                0.0933992
exploration/env_infos/reward_ctrl Max               -0.0352323
exploration/env_infos/reward_ctrl Min               -0.577324
evaluation/num steps total                      955000
evaluation/num paths total                         955
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.52628
evaluation/Rewards Std                               1.07903
evaluation/Rewards Max                               6.82112
evaluation/Rewards Min                              -0.943436
evaluation/Returns Mean                           4526.28
evaluation/Returns Std                              52.8839
evaluation/Returns Max                            4572.08
evaluation/Returns Min                            4432.88
evaluation/Actions Mean                             -0.006781
evaluation/Actions Std                               0.833971
evaluation/Actions Max                               0.997819
evaluation/Actions Min                              -0.996506
evaluation/Num Paths                                 5
evaluation/Average Returns                        4526.28
evaluation/env_infos/final/reward_run Mean           5.01896
evaluation/env_infos/final/reward_run Std            0.711372
evaluation/env_infos/final/reward_run Max            6.37973
evaluation/env_infos/final/reward_run Min            4.30783
evaluation/env_infos/initial/reward_run Mean         0.0683662
evaluation/env_infos/initial/reward_run Std          0.116535
evaluation/env_infos/initial/reward_run Max          0.269972
evaluation/env_infos/initial/reward_run Min         -0.0750308
evaluation/env_infos/reward_run Mean                 4.94361
evaluation/env_infos/reward_run Std                  1.06589
evaluation/env_infos/reward_run Max                  7.21204
evaluation/env_infos/reward_run Min                 -0.393611
evaluation/env_infos/final/reward_ctrl Mean         -0.368003
evaluation/env_infos/final/reward_ctrl Std           0.0907136
evaluation/env_infos/final/reward_ctrl Max          -0.240882
evaluation/env_infos/final/reward_ctrl Min          -0.514984
evaluation/env_infos/initial/reward_ctrl Mean       -0.0685286
evaluation/env_infos/initial/reward_ctrl Std         0.0220776
evaluation/env_infos/initial/reward_ctrl Max        -0.0296648
evaluation/env_infos/initial/reward_ctrl Min        -0.0937488
evaluation/env_infos/reward_ctrl Mean               -0.417332
evaluation/env_infos/reward_ctrl Std                 0.0961201
evaluation/env_infos/reward_ctrl Max                -0.0296648
evaluation/env_infos/reward_ctrl Min                -0.579179
time/data storing (s)                                0.00744595
time/evaluation sampling (s)                         5.03379
time/exploration sampling (s)                        0.769388
time/logging (s)                                     0.0450374
time/saving (s)                                      0.0214571
time/training (s)                                   46.5346
time/epoch (s)                                      52.4118
time/total (s)                                    8008.07
Epoch                                              190
----------------------------------------------  ---------------
2020-07-08 23:20:26.969898 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 191 finished
----------------------------------------------  ---------------
replay_buffer/size                              193000
trainer/QF1 Loss                                     6.52494
trainer/QF2 Loss                                    11.3312
trainer/Policy Loss                               -240.136
trainer/Q1 Predictions Mean                        246.221
trainer/Q1 Predictions Std                          97.2695
trainer/Q1 Predictions Max                         324.419
trainer/Q1 Predictions Min                          15.9945
trainer/Q2 Predictions Mean                        245.563
trainer/Q2 Predictions Std                          97.032
trainer/Q2 Predictions Max                         323.983
trainer/Q2 Predictions Min                          16.0617
trainer/Q Targets Mean                             246.742
trainer/Q Targets Std                               97.4139
trainer/Q Targets Max                              326.43
trainer/Q Targets Min                               16.3536
trainer/Log Pis Mean                                 5.70943
trainer/Log Pis Std                                  5.16215
trainer/Log Pis Max                                 18.9106
trainer/Log Pis Min                                 -7.3453
trainer/Policy mu Mean                               0.0219991
trainer/Policy mu Std                                1.48211
trainer/Policy mu Max                                3.6629
trainer/Policy mu Min                               -3.38686
trainer/Policy log std Mean                         -0.794455
trainer/Policy log std Std                           0.326458
trainer/Policy log std Max                           0.00788732
trainer/Policy log std Min                          -2.27771
trainer/Alpha                                        0.108299
trainer/Alpha Loss                                  -0.645861
exploration/num steps total                     193000
exploration/num paths total                        193
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.27447
exploration/Rewards Std                              1.02685
exploration/Rewards Max                              6.4273
exploration/Rewards Min                             -0.370037
exploration/Returns Mean                          4274.47
exploration/Returns Std                              0
exploration/Returns Max                           4274.47
exploration/Returns Min                           4274.47
exploration/Actions Mean                            -0.0206371
exploration/Actions Std                              0.818923
exploration/Actions Max                              0.999598
exploration/Actions Min                             -0.999788
exploration/Num Paths                                1
exploration/Average Returns                       4274.47
exploration/env_infos/final/reward_run Mean          3.79799
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.79799
exploration/env_infos/final/reward_run Min           3.79799
exploration/env_infos/initial/reward_run Mean        0.330637
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.330637
exploration/env_infos/initial/reward_run Min         0.330637
exploration/env_infos/reward_run Mean                4.67711
exploration/env_infos/reward_run Std                 1.01254
exploration/env_infos/reward_run Max                 6.77816
exploration/env_infos/reward_run Min                 0.0949095
exploration/env_infos/final/reward_ctrl Mean        -0.465688
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.465688
exploration/env_infos/final/reward_ctrl Min         -0.465688
exploration/env_infos/initial/reward_ctrl Mean      -0.0555156
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0555156
exploration/env_infos/initial/reward_ctrl Min       -0.0555156
exploration/env_infos/reward_ctrl Mean              -0.402636
exploration/env_infos/reward_ctrl Std                0.0996247
exploration/env_infos/reward_ctrl Max               -0.0555156
exploration/env_infos/reward_ctrl Min               -0.579041
evaluation/num steps total                      960000
evaluation/num paths total                         960
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.52682
evaluation/Rewards Std                               1.07381
evaluation/Rewards Max                               6.5394
evaluation/Rewards Min                              -0.771559
evaluation/Returns Mean                           4526.82
evaluation/Returns Std                              21.9763
evaluation/Returns Max                            4568.85
evaluation/Returns Min                            4510.91
evaluation/Actions Mean                             -0.0297614
evaluation/Actions Std                               0.83342
evaluation/Actions Max                               0.999065
evaluation/Actions Min                              -0.999339
evaluation/Num Paths                                 5
evaluation/Average Returns                        4526.82
evaluation/env_infos/final/reward_run Mean           5.0782
evaluation/env_infos/final/reward_run Std            0.483379
evaluation/env_infos/final/reward_run Max            5.42369
evaluation/env_infos/final/reward_run Min            4.12869
evaluation/env_infos/initial/reward_run Mean        -0.0400194
evaluation/env_infos/initial/reward_run Std          0.183223
evaluation/env_infos/initial/reward_run Max          0.272534
evaluation/env_infos/initial/reward_run Min         -0.299114
evaluation/env_infos/reward_run Mean                 4.9441
evaluation/env_infos/reward_run Std                  1.06294
evaluation/env_infos/reward_run Max                  6.95779
evaluation/env_infos/reward_run Min                 -0.299114
evaluation/env_infos/final/reward_ctrl Mean         -0.472628
evaluation/env_infos/final/reward_ctrl Std           0.0685919
evaluation/env_infos/final/reward_ctrl Max          -0.338068
evaluation/env_infos/final/reward_ctrl Min          -0.522695
evaluation/env_infos/initial/reward_ctrl Mean       -0.0635822
evaluation/env_infos/initial/reward_ctrl Std         0.0275161
evaluation/env_infos/initial/reward_ctrl Max        -0.0165422
evaluation/env_infos/initial/reward_ctrl Min        -0.0898277
evaluation/env_infos/reward_ctrl Mean               -0.417285
evaluation/env_infos/reward_ctrl Std                 0.0981432
evaluation/env_infos/reward_ctrl Max                -0.0165422
evaluation/env_infos/reward_ctrl Min                -0.586038
time/data storing (s)                                0.00719741
time/evaluation sampling (s)                         4.20815
time/exploration sampling (s)                        0.862596
time/logging (s)                                     0.0474177
time/saving (s)                                      0.0174884
time/training (s)                                   41.8081
time/epoch (s)                                      46.951
time/total (s)                                    8055.05
Epoch                                              191
----------------------------------------------  ---------------
2020-07-08 23:21:12.358461 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 192 finished
----------------------------------------------  ---------------
replay_buffer/size                              194000
trainer/QF1 Loss                                     6.53186
trainer/QF2 Loss                                     8.30793
trainer/Policy Loss                               -240.243
trainer/Q1 Predictions Mean                        245.888
trainer/Q1 Predictions Std                          97.4681
trainer/Q1 Predictions Max                         325.651
trainer/Q1 Predictions Min                          17.348
trainer/Q2 Predictions Mean                        246.415
trainer/Q2 Predictions Std                          97.6432
trainer/Q2 Predictions Max                         325.586
trainer/Q2 Predictions Min                          17.5293
trainer/Q Targets Mean                             246.202
trainer/Q Targets Std                               97.7397
trainer/Q Targets Max                              327.335
trainer/Q Targets Min                               16.0884
trainer/Log Pis Mean                                 6.104
trainer/Log Pis Std                                  5.56726
trainer/Log Pis Max                                 19.3923
trainer/Log Pis Min                                 -4.73025
trainer/Policy mu Mean                              -0.0246521
trainer/Policy mu Std                                1.54912
trainer/Policy mu Max                                3.93173
trainer/Policy mu Min                               -4.39078
trainer/Policy log std Mean                         -0.779626
trainer/Policy log std Std                           0.323804
trainer/Policy log std Max                          -0.023431
trainer/Policy log std Min                          -2.519
trainer/Alpha                                        0.107891
trainer/Alpha Loss                                   0.23157
exploration/num steps total                     194000
exploration/num paths total                        194
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.3593
exploration/Rewards Std                              1.10732
exploration/Rewards Max                              6.57869
exploration/Rewards Min                             -0.722708
exploration/Returns Mean                          4359.3
exploration/Returns Std                              0
exploration/Returns Max                           4359.3
exploration/Returns Min                           4359.3
exploration/Actions Mean                            -0.0197958
exploration/Actions Std                              0.831288
exploration/Actions Max                              0.999482
exploration/Actions Min                             -0.999576
exploration/Num Paths                                1
exploration/Average Returns                       4359.3
exploration/env_infos/final/reward_run Mean          6.28849
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.28849
exploration/env_infos/final/reward_run Min           6.28849
exploration/env_infos/initial/reward_run Mean        0.171302
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.171302
exploration/env_infos/initial/reward_run Min         0.171302
exploration/env_infos/reward_run Mean                4.77416
exploration/env_infos/reward_run Std                 1.10101
exploration/env_infos/reward_run Max                 6.89084
exploration/env_infos/reward_run Min                -0.198558
exploration/env_infos/final/reward_ctrl Mean        -0.517672
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.517672
exploration/env_infos/final/reward_ctrl Min         -0.517672
exploration/env_infos/initial/reward_ctrl Mean      -0.0503688
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0503688
exploration/env_infos/initial/reward_ctrl Min       -0.0503688
exploration/env_infos/reward_ctrl Mean              -0.414859
exploration/env_infos/reward_ctrl Std                0.0992291
exploration/env_infos/reward_ctrl Max               -0.0503688
exploration/env_infos/reward_ctrl Min               -0.57688
evaluation/num steps total                      965000
evaluation/num paths total                         965
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.77984
evaluation/Rewards Std                               1.78544
evaluation/Rewards Max                               6.69948
evaluation/Rewards Min                              -1.56106
evaluation/Returns Mean                           3779.84
evaluation/Returns Std                            1339.51
evaluation/Returns Max                            4527.82
evaluation/Returns Min                            1102.07
evaluation/Actions Mean                             -0.0468472
evaluation/Actions Std                               0.815364
evaluation/Actions Max                               0.998678
evaluation/Actions Min                              -0.999542
evaluation/Num Paths                                 5
evaluation/Average Returns                        3779.84
evaluation/env_infos/final/reward_run Mean           4.28202
evaluation/env_infos/final/reward_run Std            1.89126
evaluation/env_infos/final/reward_run Max            6.3848
evaluation/env_infos/final/reward_run Min            1.37023
evaluation/env_infos/initial/reward_run Mean        -0.0538128
evaluation/env_infos/initial/reward_run Std          0.132481
evaluation/env_infos/initial/reward_run Max          0.0970054
evaluation/env_infos/initial/reward_run Min         -0.284933
evaluation/env_infos/reward_run Mean                 4.18005
evaluation/env_infos/reward_run Std                  1.83121
evaluation/env_infos/reward_run Max                  7.02456
evaluation/env_infos/reward_run Min                 -1.24307
evaluation/env_infos/final/reward_ctrl Mean         -0.437308
evaluation/env_infos/final/reward_ctrl Std           0.067178
evaluation/env_infos/final/reward_ctrl Max          -0.377637
evaluation/env_infos/final/reward_ctrl Min          -0.555338
evaluation/env_infos/initial/reward_ctrl Mean       -0.104078
evaluation/env_infos/initial/reward_ctrl Std         0.0256124
evaluation/env_infos/initial/reward_ctrl Max        -0.0607578
evaluation/env_infos/initial/reward_ctrl Min        -0.130751
evaluation/env_infos/reward_ctrl Mean               -0.400208
evaluation/env_infos/reward_ctrl Std                 0.116265
evaluation/env_infos/reward_ctrl Max                -0.0580393
evaluation/env_infos/reward_ctrl Min                -0.587726
time/data storing (s)                                0.00983985
time/evaluation sampling (s)                         4.3539
time/exploration sampling (s)                        0.815766
time/logging (s)                                     0.0539381
time/saving (s)                                      0.0179668
time/training (s)                                   40.121
time/epoch (s)                                      45.3724
time/total (s)                                    8100.44
Epoch                                              192
----------------------------------------------  ---------------
2020-07-08 23:21:57.589074 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 193 finished
----------------------------------------------  ---------------
replay_buffer/size                              195000
trainer/QF1 Loss                                     8.1594
trainer/QF2 Loss                                     7.78293
trainer/Policy Loss                               -259.811
trainer/Q1 Predictions Mean                        265.6
trainer/Q1 Predictions Std                          77.5607
trainer/Q1 Predictions Max                         327.651
trainer/Q1 Predictions Min                          17.0655
trainer/Q2 Predictions Mean                        265.765
trainer/Q2 Predictions Std                          77.5719
trainer/Q2 Predictions Max                         328.069
trainer/Q2 Predictions Min                          18.0892
trainer/Q Targets Mean                             266.398
trainer/Q Targets Std                               77.8134
trainer/Q Targets Max                              329.349
trainer/Q Targets Min                               17.981
trainer/Log Pis Mean                                 5.83151
trainer/Log Pis Std                                  4.4895
trainer/Log Pis Max                                 15.8099
trainer/Log Pis Min                                 -3.8251
trainer/Policy mu Mean                               0.0546191
trainer/Policy mu Std                                1.49136
trainer/Policy mu Max                                3.06559
trainer/Policy mu Min                               -3.61306
trainer/Policy log std Mean                         -0.838073
trainer/Policy log std Std                           0.325215
trainer/Policy log std Max                          -0.0855357
trainer/Policy log std Min                          -2.47352
trainer/Alpha                                        0.108781
trainer/Alpha Loss                                  -0.373804
exploration/num steps total                     195000
exploration/num paths total                        195
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.38149
exploration/Rewards Std                              1.08372
exploration/Rewards Max                              6.56989
exploration/Rewards Min                             -0.177438
exploration/Returns Mean                          4381.49
exploration/Returns Std                              0
exploration/Returns Max                           4381.49
exploration/Returns Min                           4381.49
exploration/Actions Mean                             0.00538913
exploration/Actions Std                              0.806003
exploration/Actions Max                              0.999726
exploration/Actions Min                             -0.999552
exploration/Num Paths                                1
exploration/Average Returns                       4381.49
exploration/env_infos/final/reward_run Mean          5.04121
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.04121
exploration/env_infos/final/reward_run Min           5.04121
exploration/env_infos/initial/reward_run Mean        0.0860411
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0860411
exploration/env_infos/initial/reward_run Min         0.0860411
exploration/env_infos/reward_run Mean                4.77129
exploration/env_infos/reward_run Std                 1.0768
exploration/env_infos/reward_run Max                 7.04002
exploration/env_infos/reward_run Min                 0.0860411
exploration/env_infos/final/reward_ctrl Mean        -0.463444
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.463444
exploration/env_infos/final/reward_ctrl Min         -0.463444
exploration/env_infos/initial/reward_ctrl Mean      -0.0694483
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0694483
exploration/env_infos/initial/reward_ctrl Min       -0.0694483
exploration/env_infos/reward_ctrl Mean              -0.389802
exploration/env_infos/reward_ctrl Std                0.101666
exploration/env_infos/reward_ctrl Max               -0.0251468
exploration/env_infos/reward_ctrl Min               -0.581756
evaluation/num steps total                      970000
evaluation/num paths total                         970
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.46395
evaluation/Rewards Std                               1.11444
evaluation/Rewards Max                               6.71841
evaluation/Rewards Min                              -0.819878
evaluation/Returns Mean                           4463.95
evaluation/Returns Std                              91.6403
evaluation/Returns Max                            4613.38
evaluation/Returns Min                            4363.79
evaluation/Actions Mean                              0.00224825
evaluation/Actions Std                               0.821615
evaluation/Actions Max                               0.998298
evaluation/Actions Min                              -0.998007
evaluation/Num Paths                                 5
evaluation/Average Returns                        4463.95
evaluation/env_infos/final/reward_run Mean           4.64108
evaluation/env_infos/final/reward_run Std            0.690977
evaluation/env_infos/final/reward_run Max            5.37451
evaluation/env_infos/final/reward_run Min            3.76366
evaluation/env_infos/initial/reward_run Mean         0.0273112
evaluation/env_infos/initial/reward_run Std          0.247501
evaluation/env_infos/initial/reward_run Max          0.518411
evaluation/env_infos/initial/reward_run Min         -0.138373
evaluation/env_infos/reward_run Mean                 4.86898
evaluation/env_infos/reward_run Std                  1.09971
evaluation/env_infos/reward_run Max                  7.10594
evaluation/env_infos/reward_run Min                 -0.348995
evaluation/env_infos/final/reward_ctrl Mean         -0.373107
evaluation/env_infos/final/reward_ctrl Std           0.10866
evaluation/env_infos/final/reward_ctrl Max          -0.244405
evaluation/env_infos/final/reward_ctrl Min          -0.519558
evaluation/env_infos/initial/reward_ctrl Mean       -0.06724
evaluation/env_infos/initial/reward_ctrl Std         0.0367317
evaluation/env_infos/initial/reward_ctrl Max        -0.0269854
evaluation/env_infos/initial/reward_ctrl Min        -0.135911
evaluation/env_infos/reward_ctrl Mean               -0.405033
evaluation/env_infos/reward_ctrl Std                 0.103055
evaluation/env_infos/reward_ctrl Max                -0.0269854
evaluation/env_infos/reward_ctrl Min                -0.571021
time/data storing (s)                                0.00769885
time/evaluation sampling (s)                         3.23915
time/exploration sampling (s)                        0.806077
time/logging (s)                                     0.0415587
time/saving (s)                                      0.0159348
time/training (s)                                   41.0682
time/epoch (s)                                      45.1787
time/total (s)                                    8145.65
Epoch                                              193
----------------------------------------------  ---------------
2020-07-08 23:22:38.119607 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 194 finished
----------------------------------------------  ---------------
replay_buffer/size                              196000
trainer/QF1 Loss                                     9.65547
trainer/QF2 Loss                                    10.7346
trainer/Policy Loss                               -256.979
trainer/Q1 Predictions Mean                        263.226
trainer/Q1 Predictions Std                          79.1897
trainer/Q1 Predictions Max                         324.368
trainer/Q1 Predictions Min                          17.6752
trainer/Q2 Predictions Mean                        263.385
trainer/Q2 Predictions Std                          79.2672
trainer/Q2 Predictions Max                         323.292
trainer/Q2 Predictions Min                          16.0398
trainer/Q Targets Mean                             263.378
trainer/Q Targets Std                               79.5432
trainer/Q Targets Max                              323.855
trainer/Q Targets Min                               16.1754
trainer/Log Pis Mean                                 6.24703
trainer/Log Pis Std                                  4.68255
trainer/Log Pis Max                                 16.1147
trainer/Log Pis Min                                -13.9823
trainer/Policy mu Mean                               0.00805891
trainer/Policy mu Std                                1.50679
trainer/Policy mu Max                                3.46166
trainer/Policy mu Min                               -3.65776
trainer/Policy log std Mean                         -0.843175
trainer/Policy log std Std                           0.334096
trainer/Policy log std Max                          -0.018037
trainer/Policy log std Min                          -2.46596
trainer/Alpha                                        0.10774
trainer/Alpha Loss                                   0.550378
exploration/num steps total                     196000
exploration/num paths total                        196
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.50308
exploration/Rewards Std                              1.1296
exploration/Rewards Max                              6.58992
exploration/Rewards Min                             -0.798304
exploration/Returns Mean                          4503.08
exploration/Returns Std                              0
exploration/Returns Max                           4503.08
exploration/Returns Min                           4503.08
exploration/Actions Mean                            -0.00374426
exploration/Actions Std                              0.817816
exploration/Actions Max                              0.999913
exploration/Actions Min                             -0.999626
exploration/Num Paths                                1
exploration/Average Returns                       4503.08
exploration/env_infos/final/reward_run Mean          5.75412
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.75412
exploration/env_infos/final/reward_run Min           5.75412
exploration/env_infos/initial/reward_run Mean        0.0107097
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0107097
exploration/env_infos/initial/reward_run Min         0.0107097
exploration/env_infos/reward_run Mean                4.90438
exploration/env_infos/reward_run Std                 1.12332
exploration/env_infos/reward_run Max                 6.97896
exploration/env_infos/reward_run Min                -0.433832
exploration/env_infos/final/reward_ctrl Mean        -0.221246
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.221246
exploration/env_infos/final/reward_ctrl Min         -0.221246
exploration/env_infos/initial/reward_ctrl Mean      -0.0218244
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0218244
exploration/env_infos/initial/reward_ctrl Min       -0.0218244
exploration/env_infos/reward_ctrl Mean              -0.401303
exploration/env_infos/reward_ctrl Std                0.097184
exploration/env_infos/reward_ctrl Max               -0.0218244
exploration/env_infos/reward_ctrl Min               -0.579238
evaluation/num steps total                      975000
evaluation/num paths total                         975
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.49696
evaluation/Rewards Std                               1.09958
evaluation/Rewards Max                               6.69952
evaluation/Rewards Min                              -0.686819
evaluation/Returns Mean                           4496.96
evaluation/Returns Std                              44.4292
evaluation/Returns Max                            4576.44
evaluation/Returns Min                            4441.24
evaluation/Actions Mean                             -0.00796241
evaluation/Actions Std                               0.830056
evaluation/Actions Max                               0.999484
evaluation/Actions Min                              -0.997271
evaluation/Num Paths                                 5
evaluation/Average Returns                        4496.96
evaluation/env_infos/final/reward_run Mean           5.39814
evaluation/env_infos/final/reward_run Std            0.43432
evaluation/env_infos/final/reward_run Max            6.00486
evaluation/env_infos/final/reward_run Min            4.86415
evaluation/env_infos/initial/reward_run Mean        -0.00474176
evaluation/env_infos/initial/reward_run Std          0.0605852
evaluation/env_infos/initial/reward_run Max          0.0357031
evaluation/env_infos/initial/reward_run Min         -0.123689
evaluation/env_infos/reward_run Mean                 4.9104
evaluation/env_infos/reward_run Std                  1.08877
evaluation/env_infos/reward_run Max                  7.04527
evaluation/env_infos/reward_run Min                 -0.218709
evaluation/env_infos/final/reward_ctrl Mean         -0.355599
evaluation/env_infos/final/reward_ctrl Std           0.076522
evaluation/env_infos/final/reward_ctrl Max          -0.278553
evaluation/env_infos/final/reward_ctrl Min          -0.497169
evaluation/env_infos/initial/reward_ctrl Mean       -0.0713587
evaluation/env_infos/initial/reward_ctrl Std         0.0364239
evaluation/env_infos/initial/reward_ctrl Max        -0.0266142
evaluation/env_infos/initial/reward_ctrl Min        -0.132527
evaluation/env_infos/reward_ctrl Mean               -0.413434
evaluation/env_infos/reward_ctrl Std                 0.0969584
evaluation/env_infos/reward_ctrl Max                -0.0251421
evaluation/env_infos/reward_ctrl Min                -0.567206
time/data storing (s)                                0.00695141
time/evaluation sampling (s)                         2.53053
time/exploration sampling (s)                        0.662547
time/logging (s)                                     0.0431225
time/saving (s)                                      0.0163509
time/training (s)                                   37.2344
time/epoch (s)                                      40.4939
time/total (s)                                    8186.18
Epoch                                              194
----------------------------------------------  ---------------
2020-07-08 23:23:18.711733 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 195 finished
----------------------------------------------  ---------------
replay_buffer/size                              197000
trainer/QF1 Loss                                     6.64563
trainer/QF2 Loss                                     8.84997
trainer/Policy Loss                               -248.891
trainer/Q1 Predictions Mean                        254.854
trainer/Q1 Predictions Std                          91.9264
trainer/Q1 Predictions Max                         326.153
trainer/Q1 Predictions Min                          15.7725
trainer/Q2 Predictions Mean                        254.582
trainer/Q2 Predictions Std                          91.8254
trainer/Q2 Predictions Max                         323.481
trainer/Q2 Predictions Min                          16.6537
trainer/Q Targets Mean                             254.987
trainer/Q Targets Std                               91.9622
trainer/Q Targets Max                              328.335
trainer/Q Targets Min                               16.2979
trainer/Log Pis Mean                                 5.88817
trainer/Log Pis Std                                  5.47569
trainer/Log Pis Max                                 24.9613
trainer/Log Pis Min                                 -5.79941
trainer/Policy mu Mean                               0.0151192
trainer/Policy mu Std                                1.52228
trainer/Policy mu Max                                4.5936
trainer/Policy mu Min                               -4.0611
trainer/Policy log std Mean                         -0.824603
trainer/Policy log std Std                           0.340927
trainer/Policy log std Max                           0.0144972
trainer/Policy log std Min                          -2.7103
trainer/Alpha                                        0.105469
trainer/Alpha Loss                                  -0.251539
exploration/num steps total                     197000
exploration/num paths total                        197
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.30955
exploration/Rewards Std                              1.08406
exploration/Rewards Max                              6.34121
exploration/Rewards Min                             -0.894031
exploration/Returns Mean                          4309.55
exploration/Returns Std                              0
exploration/Returns Max                           4309.55
exploration/Returns Min                           4309.55
exploration/Actions Mean                            -0.0139306
exploration/Actions Std                              0.8132
exploration/Actions Max                              0.99947
exploration/Actions Min                             -0.999477
exploration/Num Paths                                1
exploration/Average Returns                       4309.55
exploration/env_infos/final/reward_run Mean          4.27123
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.27123
exploration/env_infos/final/reward_run Min           4.27123
exploration/env_infos/initial/reward_run Mean       -0.205825
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.205825
exploration/env_infos/initial/reward_run Min        -0.205825
exploration/env_infos/reward_run Mean                4.70644
exploration/env_infos/reward_run Std                 1.07482
exploration/env_infos/reward_run Max                 6.66469
exploration/env_infos/reward_run Min                -0.502351
exploration/env_infos/final/reward_ctrl Mean        -0.454112
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.454112
exploration/env_infos/final/reward_ctrl Min         -0.454112
exploration/env_infos/initial/reward_ctrl Mean      -0.0307352
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0307352
exploration/env_infos/initial/reward_ctrl Min       -0.0307352
exploration/env_infos/reward_ctrl Mean              -0.396893
exploration/env_infos/reward_ctrl Std                0.106303
exploration/env_infos/reward_ctrl Max               -0.0307352
exploration/env_infos/reward_ctrl Min               -0.582363
evaluation/num steps total                      980000
evaluation/num paths total                         980
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.52609
evaluation/Rewards Std                               1.08977
evaluation/Rewards Max                               6.79202
evaluation/Rewards Min                              -0.726927
evaluation/Returns Mean                           4526.09
evaluation/Returns Std                              71.9177
evaluation/Returns Max                            4654.37
evaluation/Returns Min                            4434.11
evaluation/Actions Mean                             -0.0204405
evaluation/Actions Std                               0.826284
evaluation/Actions Max                               0.999787
evaluation/Actions Min                              -0.999165
evaluation/Num Paths                                 5
evaluation/Average Returns                        4526.09
evaluation/env_infos/final/reward_run Mean           5.54067
evaluation/env_infos/final/reward_run Std            0.830454
evaluation/env_infos/final/reward_run Max            7.07995
evaluation/env_infos/final/reward_run Min            4.67372
evaluation/env_infos/initial/reward_run Mean        -0.1056
evaluation/env_infos/initial/reward_run Std          0.11809
evaluation/env_infos/initial/reward_run Max          0.121068
evaluation/env_infos/initial/reward_run Min         -0.199262
evaluation/env_infos/reward_run Mean                 4.93598
evaluation/env_infos/reward_run Std                  1.07642
evaluation/env_infos/reward_run Max                  7.18957
evaluation/env_infos/reward_run Min                 -0.600998
evaluation/env_infos/final/reward_ctrl Mean         -0.420055
evaluation/env_infos/final/reward_ctrl Std           0.0892938
evaluation/env_infos/final/reward_ctrl Max          -0.297912
evaluation/env_infos/final/reward_ctrl Min          -0.538502
evaluation/env_infos/initial/reward_ctrl Mean       -0.0995027
evaluation/env_infos/initial/reward_ctrl Std         0.0356268
evaluation/env_infos/initial/reward_ctrl Max        -0.0581161
evaluation/env_infos/initial/reward_ctrl Min        -0.16077
evaluation/env_infos/reward_ctrl Mean               -0.409898
evaluation/env_infos/reward_ctrl Std                 0.107532
evaluation/env_infos/reward_ctrl Max                -0.038258
evaluation/env_infos/reward_ctrl Min                -0.580329
time/data storing (s)                                0.00694647
time/evaluation sampling (s)                         2.60426
time/exploration sampling (s)                        0.668393
time/logging (s)                                     0.0456673
time/saving (s)                                      0.0160444
time/training (s)                                   37.2335
time/epoch (s)                                      40.5748
time/total (s)                                    8226.76
Epoch                                              195
----------------------------------------------  ---------------
2020-07-08 23:23:55.664999 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 196 finished
----------------------------------------------  ---------------
replay_buffer/size                              198000
trainer/QF1 Loss                                     6.73622
trainer/QF2 Loss                                     6.72127
trainer/Policy Loss                               -252.005
trainer/Q1 Predictions Mean                        257.519
trainer/Q1 Predictions Std                          91.5106
trainer/Q1 Predictions Max                         324.901
trainer/Q1 Predictions Min                          15.4284
trainer/Q2 Predictions Mean                        257.699
trainer/Q2 Predictions Std                          91.5601
trainer/Q2 Predictions Max                         323.326
trainer/Q2 Predictions Min                          15.581
trainer/Q Targets Mean                             257.33
trainer/Q Targets Std                               91.3637
trainer/Q Targets Max                              322.665
trainer/Q Targets Min                               15.9518
trainer/Log Pis Mean                                 5.53373
trainer/Log Pis Std                                  4.91758
trainer/Log Pis Max                                 18.6963
trainer/Log Pis Min                                 -5.99323
trainer/Policy mu Mean                               0.0389537
trainer/Policy mu Std                                1.44186
trainer/Policy mu Max                                3.61154
trainer/Policy mu Min                               -3.52967
trainer/Policy log std Mean                         -0.814339
trainer/Policy log std Std                           0.319204
trainer/Policy log std Max                          -0.00833963
trainer/Policy log std Min                          -2.45695
trainer/Alpha                                        0.109837
trainer/Alpha Loss                                  -1.02989
exploration/num steps total                     198000
exploration/num paths total                        198
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.37765
exploration/Rewards Std                              1.07311
exploration/Rewards Max                              6.75549
exploration/Rewards Min                             -0.553629
exploration/Returns Mean                          4377.65
exploration/Returns Std                              0
exploration/Returns Max                           4377.65
exploration/Returns Min                           4377.65
exploration/Actions Mean                            -0.0148939
exploration/Actions Std                              0.818937
exploration/Actions Max                              0.999418
exploration/Actions Min                             -0.9994
exploration/Num Paths                                1
exploration/Average Returns                       4377.65
exploration/env_infos/final/reward_run Mean          4.81752
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.81752
exploration/env_infos/final/reward_run Min           4.81752
exploration/env_infos/initial/reward_run Mean        0.383936
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.383936
exploration/env_infos/initial/reward_run Min         0.383936
exploration/env_infos/reward_run Mean                4.78018
exploration/env_infos/reward_run Std                 1.0637
exploration/env_infos/reward_run Max                 7.12534
exploration/env_infos/reward_run Min                -0.215269
exploration/env_infos/final/reward_ctrl Mean        -0.331058
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.331058
exploration/env_infos/final/reward_ctrl Min         -0.331058
exploration/env_infos/initial/reward_ctrl Mean      -0.088472
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.088472
exploration/env_infos/initial/reward_ctrl Min       -0.088472
exploration/env_infos/reward_ctrl Mean              -0.402528
exploration/env_infos/reward_ctrl Std                0.095614
exploration/env_infos/reward_ctrl Max               -0.088472
exploration/env_infos/reward_ctrl Min               -0.573976
evaluation/num steps total                      985000
evaluation/num paths total                         985
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.59466
evaluation/Rewards Std                               1.11362
evaluation/Rewards Max                               6.87759
evaluation/Rewards Min                              -0.824808
evaluation/Returns Mean                           4594.66
evaluation/Returns Std                              23.2297
evaluation/Returns Max                            4632.05
evaluation/Returns Min                            4565.45
evaluation/Actions Mean                             -0.0222847
evaluation/Actions Std                               0.832923
evaluation/Actions Max                               0.996874
evaluation/Actions Min                              -0.998232
evaluation/Num Paths                                 5
evaluation/Average Returns                        4594.66
evaluation/env_infos/final/reward_run Mean           6.18005
evaluation/env_infos/final/reward_run Std            0.223063
evaluation/env_infos/final/reward_run Max            6.48537
evaluation/env_infos/final/reward_run Min            5.92907
evaluation/env_infos/initial/reward_run Mean        -0.0904509
evaluation/env_infos/initial/reward_run Std          0.250986
evaluation/env_infos/initial/reward_run Max          0.375908
evaluation/env_infos/initial/reward_run Min         -0.348726
evaluation/env_infos/reward_run Mean                 5.01122
evaluation/env_infos/reward_run Std                  1.10383
evaluation/env_infos/reward_run Max                  7.21463
evaluation/env_infos/reward_run Min                 -0.348726
evaluation/env_infos/final/reward_ctrl Mean         -0.394718
evaluation/env_infos/final/reward_ctrl Std           0.0883454
evaluation/env_infos/final/reward_ctrl Max          -0.296355
evaluation/env_infos/final/reward_ctrl Min          -0.502915
evaluation/env_infos/initial/reward_ctrl Mean       -0.0563359
evaluation/env_infos/initial/reward_ctrl Std         0.0178583
evaluation/env_infos/initial/reward_ctrl Max        -0.0223022
evaluation/env_infos/initial/reward_ctrl Min        -0.0749083
evaluation/env_infos/reward_ctrl Mean               -0.416554
evaluation/env_infos/reward_ctrl Std                 0.0960509
evaluation/env_infos/reward_ctrl Max                -0.0206919
evaluation/env_infos/reward_ctrl Min                -0.577646
time/data storing (s)                                0.00708411
time/evaluation sampling (s)                         2.53695
time/exploration sampling (s)                        0.66922
time/logging (s)                                     0.0410276
time/saving (s)                                      0.0164444
time/training (s)                                   33.659
time/epoch (s)                                      36.9297
time/total (s)                                    8263.71
Epoch                                              196
----------------------------------------------  ---------------
2020-07-08 23:24:39.988886 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 197 finished
----------------------------------------------  ----------------
replay_buffer/size                              199000
trainer/QF1 Loss                                     7.54355
trainer/QF2 Loss                                     9.18926
trainer/Policy Loss                               -247.534
trainer/Q1 Predictions Mean                        252.995
trainer/Q1 Predictions Std                          93.4506
trainer/Q1 Predictions Max                         326.357
trainer/Q1 Predictions Min                          16.6255
trainer/Q2 Predictions Mean                        253.114
trainer/Q2 Predictions Std                          93.3768
trainer/Q2 Predictions Max                         326.214
trainer/Q2 Predictions Min                          16.9373
trainer/Q Targets Mean                             252.772
trainer/Q Targets Std                               93.4135
trainer/Q Targets Max                              326.905
trainer/Q Targets Min                               16.1571
trainer/Log Pis Mean                                 5.44931
trainer/Log Pis Std                                  5.23684
trainer/Log Pis Max                                 19.2825
trainer/Log Pis Min                                 -4.75843
trainer/Policy mu Mean                               0.0667176
trainer/Policy mu Std                                1.477
trainer/Policy mu Max                                3.55483
trainer/Policy mu Min                               -3.50233
trainer/Policy log std Mean                         -0.80348
trainer/Policy log std Std                           0.335129
trainer/Policy log std Max                           0.0825434
trainer/Policy log std Min                          -2.4777
trainer/Alpha                                        0.1103
trainer/Alpha Loss                                  -1.21399
exploration/num steps total                     199000
exploration/num paths total                        199
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.4191
exploration/Rewards Std                              1.10877
exploration/Rewards Max                              6.43707
exploration/Rewards Min                             -0.675369
exploration/Returns Mean                          4419.1
exploration/Returns Std                              0
exploration/Returns Max                           4419.1
exploration/Returns Min                           4419.1
exploration/Actions Mean                             0.000409827
exploration/Actions Std                              0.815348
exploration/Actions Max                              0.999677
exploration/Actions Min                             -0.999525
exploration/Num Paths                                1
exploration/Average Returns                       4419.1
exploration/env_infos/final/reward_run Mean          6.43652
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.43652
exploration/env_infos/final/reward_run Min           6.43652
exploration/env_infos/initial/reward_run Mean        0.099821
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.099821
exploration/env_infos/initial/reward_run Min         0.099821
exploration/env_infos/reward_run Mean                4.81797
exploration/env_infos/reward_run Std                 1.10027
exploration/env_infos/reward_run Max                 6.91754
exploration/env_infos/reward_run Min                -0.357672
exploration/env_infos/final/reward_ctrl Mean        -0.350616
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.350616
exploration/env_infos/final/reward_ctrl Min         -0.350616
exploration/env_infos/initial/reward_ctrl Mean      -0.0596143
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0596143
exploration/env_infos/initial/reward_ctrl Min       -0.0596143
exploration/env_infos/reward_ctrl Mean              -0.398875
exploration/env_infos/reward_ctrl Std                0.101429
exploration/env_infos/reward_ctrl Max               -0.0596143
exploration/env_infos/reward_ctrl Min               -0.585946
evaluation/num steps total                      990000
evaluation/num paths total                         990
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.50735
evaluation/Rewards Std                               1.10313
evaluation/Rewards Max                               6.62707
evaluation/Rewards Min                              -0.823165
evaluation/Returns Mean                           4507.35
evaluation/Returns Std                              64.3308
evaluation/Returns Max                            4634.28
evaluation/Returns Min                            4462.22
evaluation/Actions Mean                             -0.00505569
evaluation/Actions Std                               0.830955
evaluation/Actions Max                               0.999473
evaluation/Actions Min                              -0.998087
evaluation/Num Paths                                 5
evaluation/Average Returns                        4507.35
evaluation/env_infos/final/reward_run Mean           4.95874
evaluation/env_infos/final/reward_run Std            0.567232
evaluation/env_infos/final/reward_run Max            6.0921
evaluation/env_infos/final/reward_run Min            4.64262
evaluation/env_infos/initial/reward_run Mean        -0.0733919
evaluation/env_infos/initial/reward_run Std          0.13414
evaluation/env_infos/initial/reward_run Max          0.0839002
evaluation/env_infos/initial/reward_run Min         -0.285908
evaluation/env_infos/reward_run Mean                 4.92166
evaluation/env_infos/reward_run Std                  1.09061
evaluation/env_infos/reward_run Max                  7.01208
evaluation/env_infos/reward_run Min                 -0.325637
evaluation/env_infos/final/reward_ctrl Mean         -0.37076
evaluation/env_infos/final/reward_ctrl Std           0.106493
evaluation/env_infos/final/reward_ctrl Max          -0.236144
evaluation/env_infos/final/reward_ctrl Min          -0.50166
evaluation/env_infos/initial/reward_ctrl Mean       -0.0871467
evaluation/env_infos/initial/reward_ctrl Std         0.0309392
evaluation/env_infos/initial/reward_ctrl Max        -0.0467152
evaluation/env_infos/initial/reward_ctrl Min        -0.123925
evaluation/env_infos/reward_ctrl Mean               -0.414307
evaluation/env_infos/reward_ctrl Std                 0.102612
evaluation/env_infos/reward_ctrl Max                -0.0467152
evaluation/env_infos/reward_ctrl Min                -0.57506
time/data storing (s)                                0.00719558
time/evaluation sampling (s)                         3.13902
time/exploration sampling (s)                        0.849307
time/logging (s)                                     0.0410107
time/saving (s)                                      0.0162779
time/training (s)                                   40.2063
time/epoch (s)                                      44.2591
time/total (s)                                    8308.02
Epoch                                              197
----------------------------------------------  ----------------
2020-07-08 23:25:29.424850 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 198 finished
----------------------------------------------  ---------------
replay_buffer/size                              200000
trainer/QF1 Loss                                    13.9038
trainer/QF2 Loss                                    10.63
trainer/Policy Loss                               -264.539
trainer/Q1 Predictions Mean                        271.467
trainer/Q1 Predictions Std                          73.299
trainer/Q1 Predictions Max                         328.046
trainer/Q1 Predictions Min                          17.4897
trainer/Q2 Predictions Mean                        271.567
trainer/Q2 Predictions Std                          73.4774
trainer/Q2 Predictions Max                         329.808
trainer/Q2 Predictions Min                          17.6343
trainer/Q Targets Mean                             271.482
trainer/Q Targets Std                               74.0012
trainer/Q Targets Max                              329.255
trainer/Q Targets Min                               14.8993
trainer/Log Pis Mean                                 6.96796
trainer/Log Pis Std                                  4.84958
trainer/Log Pis Max                                 24.7185
trainer/Log Pis Min                                 -6.38598
trainer/Policy mu Mean                               0.0442005
trainer/Policy mu Std                                1.57159
trainer/Policy mu Max                                4.17573
trainer/Policy mu Min                               -4.85818
trainer/Policy log std Mean                         -0.847829
trainer/Policy log std Std                           0.331327
trainer/Policy log std Max                           0.23495
trainer/Policy log std Min                          -2.40735
trainer/Alpha                                        0.109083
trainer/Alpha Loss                                   2.14478
exploration/num steps total                     200000
exploration/num paths total                        200
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.39459
exploration/Rewards Std                              1.07668
exploration/Rewards Max                              6.53517
exploration/Rewards Min                             -0.427131
exploration/Returns Mean                          4394.59
exploration/Returns Std                              0
exploration/Returns Max                           4394.59
exploration/Returns Min                           4394.59
exploration/Actions Mean                            -0.00684425
exploration/Actions Std                              0.819196
exploration/Actions Max                              0.999647
exploration/Actions Min                             -0.999474
exploration/Num Paths                                1
exploration/Average Returns                       4394.59
exploration/env_infos/final/reward_run Mean          5.9909
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.9909
exploration/env_infos/final/reward_run Min           5.9909
exploration/env_infos/initial/reward_run Mean       -0.0849713
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0849713
exploration/env_infos/initial/reward_run Min        -0.0849713
exploration/env_infos/reward_run Mean                4.79727
exploration/env_infos/reward_run Std                 1.06028
exploration/env_infos/reward_run Max                 6.91491
exploration/env_infos/reward_run Min                -0.24242
exploration/env_infos/final/reward_ctrl Mean        -0.455239
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.455239
exploration/env_infos/final/reward_ctrl Min         -0.455239
exploration/env_infos/initial/reward_ctrl Mean      -0.0778503
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0778503
exploration/env_infos/initial/reward_ctrl Min       -0.0778503
exploration/env_infos/reward_ctrl Mean              -0.402678
exploration/env_infos/reward_ctrl Std                0.0965963
exploration/env_infos/reward_ctrl Max               -0.0778503
exploration/env_infos/reward_ctrl Min               -0.576462
evaluation/num steps total                      995000
evaluation/num paths total                         995
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.48762
evaluation/Rewards Std                               1.08162
evaluation/Rewards Max                               6.72861
evaluation/Rewards Min                              -0.546103
evaluation/Returns Mean                           4487.62
evaluation/Returns Std                              47.902
evaluation/Returns Max                            4565.83
evaluation/Returns Min                            4434.64
evaluation/Actions Mean                             -0.0146846
evaluation/Actions Std                               0.828541
evaluation/Actions Max                               0.998637
evaluation/Actions Min                              -0.99849
evaluation/Num Paths                                 5
evaluation/Average Returns                        4487.62
evaluation/env_infos/final/reward_run Mean           4.88485
evaluation/env_infos/final/reward_run Std            0.380611
evaluation/env_infos/final/reward_run Max            5.29991
evaluation/env_infos/final/reward_run Min            4.36026
evaluation/env_infos/initial/reward_run Mean        -0.12195
evaluation/env_infos/initial/reward_run Std          0.126706
evaluation/env_infos/initial/reward_run Max          0.0246786
evaluation/env_infos/initial/reward_run Min         -0.289307
evaluation/env_infos/reward_run Mean                 4.89964
evaluation/env_infos/reward_run Std                  1.06273
evaluation/env_infos/reward_run Max                  7.02277
evaluation/env_infos/reward_run Min                 -0.314419
evaluation/env_infos/final/reward_ctrl Mean         -0.386174
evaluation/env_infos/final/reward_ctrl Std           0.103853
evaluation/env_infos/final/reward_ctrl Max          -0.288597
evaluation/env_infos/final/reward_ctrl Min          -0.542143
evaluation/env_infos/initial/reward_ctrl Mean       -0.0793838
evaluation/env_infos/initial/reward_ctrl Std         0.0318085
evaluation/env_infos/initial/reward_ctrl Max        -0.034869
evaluation/env_infos/initial/reward_ctrl Min        -0.134326
evaluation/env_infos/reward_ctrl Mean               -0.412017
evaluation/env_infos/reward_ctrl Std                 0.0976911
evaluation/env_infos/reward_ctrl Max                -0.034869
evaluation/env_infos/reward_ctrl Min                -0.579732
time/data storing (s)                                0.00690677
time/evaluation sampling (s)                         2.82215
time/exploration sampling (s)                        0.872189
time/logging (s)                                     0.0494565
time/saving (s)                                      0.0184503
time/training (s)                                   45.6311
time/epoch (s)                                      49.4003
time/total (s)                                    8357.46
Epoch                                              198
----------------------------------------------  ---------------
2020-07-08 23:26:16.824473 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 199 finished
----------------------------------------------  ---------------
replay_buffer/size                              201000
trainer/QF1 Loss                                     6.46294
trainer/QF2 Loss                                     7.1989
trainer/Policy Loss                               -243.518
trainer/Q1 Predictions Mean                        249.13
trainer/Q1 Predictions Std                          99.5579
trainer/Q1 Predictions Max                         325.101
trainer/Q1 Predictions Min                          17.2134
trainer/Q2 Predictions Mean                        249.206
trainer/Q2 Predictions Std                          99.6866
trainer/Q2 Predictions Max                         324.844
trainer/Q2 Predictions Min                          17.415
trainer/Q Targets Mean                             249.402
trainer/Q Targets Std                               99.4375
trainer/Q Targets Max                              325.413
trainer/Q Targets Min                               17.7601
trainer/Log Pis Mean                                 5.6835
trainer/Log Pis Std                                  5.19451
trainer/Log Pis Max                                 21.9768
trainer/Log Pis Min                                 -5.90369
trainer/Policy mu Mean                               0.0302018
trainer/Policy mu Std                                1.48169
trainer/Policy mu Max                                3.84457
trainer/Policy mu Min                               -3.92676
trainer/Policy log std Mean                         -0.83026
trainer/Policy log std Std                           0.364608
trainer/Policy log std Max                           0.173517
trainer/Policy log std Min                          -2.57849
trainer/Alpha                                        0.105605
trainer/Alpha Loss                                  -0.711521
exploration/num steps total                     201000
exploration/num paths total                        201
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.24954
exploration/Rewards Std                              1.00871
exploration/Rewards Max                              6.30049
exploration/Rewards Min                             -0.191419
exploration/Returns Mean                          4249.54
exploration/Returns Std                              0
exploration/Returns Max                           4249.54
exploration/Returns Min                           4249.54
exploration/Actions Mean                            -0.011779
exploration/Actions Std                              0.81284
exploration/Actions Max                              0.999702
exploration/Actions Min                             -0.999571
exploration/Num Paths                                1
exploration/Average Returns                       4249.54
exploration/env_infos/final/reward_run Mean          4.1407
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.1407
exploration/env_infos/final/reward_run Min           4.1407
exploration/env_infos/initial/reward_run Mean        0.213497
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.213497
exploration/env_infos/initial/reward_run Min         0.213497
exploration/env_infos/reward_run Mean                4.64605
exploration/env_infos/reward_run Std                 0.994009
exploration/env_infos/reward_run Max                 6.66303
exploration/env_infos/reward_run Min                -0.0200862
exploration/env_infos/final/reward_ctrl Mean        -0.276865
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.276865
exploration/env_infos/final/reward_ctrl Min         -0.276865
exploration/env_infos/initial/reward_ctrl Mean      -0.0621744
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0621744
exploration/env_infos/initial/reward_ctrl Min       -0.0621744
exploration/env_infos/reward_ctrl Mean              -0.396509
exploration/env_infos/reward_ctrl Std                0.105175
exploration/env_infos/reward_ctrl Max               -0.0621744
exploration/env_infos/reward_ctrl Min               -0.581264
evaluation/num steps total                           1e+06
evaluation/num paths total                        1000
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.52158
evaluation/Rewards Std                               1.10731
evaluation/Rewards Max                               6.65279
evaluation/Rewards Min                              -1.09546
evaluation/Returns Mean                           4521.58
evaluation/Returns Std                              46.9269
evaluation/Returns Max                            4590.36
evaluation/Returns Min                            4449.55
evaluation/Actions Mean                             -0.0177239
evaluation/Actions Std                               0.828289
evaluation/Actions Max                               0.998821
evaluation/Actions Min                              -0.997377
evaluation/Num Paths                                 5
evaluation/Average Returns                        4521.58
evaluation/env_infos/final/reward_run Mean           5.82367
evaluation/env_infos/final/reward_run Std            0.773829
evaluation/env_infos/final/reward_run Max            6.35135
evaluation/env_infos/final/reward_run Min            4.29042
evaluation/env_infos/initial/reward_run Mean         0.0060163
evaluation/env_infos/initial/reward_run Std          0.266211
evaluation/env_infos/initial/reward_run Max          0.314985
evaluation/env_infos/initial/reward_run Min         -0.341814
evaluation/env_infos/reward_run Mean                 4.93341
evaluation/env_infos/reward_run Std                  1.09438
evaluation/env_infos/reward_run Max                  7.117
evaluation/env_infos/reward_run Min                 -0.831416
evaluation/env_infos/final/reward_ctrl Mean         -0.469904
evaluation/env_infos/final/reward_ctrl Std           0.0770935
evaluation/env_infos/final/reward_ctrl Max          -0.322779
evaluation/env_infos/final/reward_ctrl Min          -0.529931
evaluation/env_infos/initial/reward_ctrl Mean       -0.0481806
evaluation/env_infos/initial/reward_ctrl Std         0.0295408
evaluation/env_infos/initial/reward_ctrl Max        -0.0165348
evaluation/env_infos/initial/reward_ctrl Min        -0.0965132
evaluation/env_infos/reward_ctrl Mean               -0.411826
evaluation/env_infos/reward_ctrl Std                 0.108334
evaluation/env_infos/reward_ctrl Max                -0.0165348
evaluation/env_infos/reward_ctrl Min                -0.581785
time/data storing (s)                                0.00945077
time/evaluation sampling (s)                         4.37604
time/exploration sampling (s)                        0.752395
time/logging (s)                                     0.0421536
time/saving (s)                                      0.0163935
time/training (s)                                   42.1725
time/epoch (s)                                      47.3689
time/total (s)                                    8404.84
Epoch                                              199
----------------------------------------------  ---------------
2020-07-08 23:26:51.555931 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 200 finished
----------------------------------------------  ---------------
replay_buffer/size                              202000
trainer/QF1 Loss                                     5.93021
trainer/QF2 Loss                                     7.06397
trainer/Policy Loss                               -263.274
trainer/Q1 Predictions Mean                        269.56
trainer/Q1 Predictions Std                          76.7646
trainer/Q1 Predictions Max                         328.05
trainer/Q1 Predictions Min                          17.3128
trainer/Q2 Predictions Mean                        269.535
trainer/Q2 Predictions Std                          76.8132
trainer/Q2 Predictions Max                         328.08
trainer/Q2 Predictions Min                          17.7136
trainer/Q Targets Mean                             269.483
trainer/Q Targets Std                               76.793
trainer/Q Targets Max                              327.166
trainer/Q Targets Min                               17.449
trainer/Log Pis Mean                                 6.38815
trainer/Log Pis Std                                  4.89603
trainer/Log Pis Max                                 16.7069
trainer/Log Pis Min                                 -4.90932
trainer/Policy mu Mean                               0.0960813
trainer/Policy mu Std                                1.52466
trainer/Policy mu Max                                3.53903
trainer/Policy mu Min                               -3.51091
trainer/Policy log std Mean                         -0.834551
trainer/Policy log std Std                           0.33416
trainer/Policy log std Max                          -0.0239271
trainer/Policy log std Min                          -2.6158
trainer/Alpha                                        0.10843
trainer/Alpha Loss                                   0.862361
exploration/num steps total                     202000
exploration/num paths total                        202
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.25356
exploration/Rewards Std                              1.11888
exploration/Rewards Max                              6.3727
exploration/Rewards Min                             -0.569624
exploration/Returns Mean                          4253.56
exploration/Returns Std                              0
exploration/Returns Max                           4253.56
exploration/Returns Min                           4253.56
exploration/Actions Mean                             0.0186956
exploration/Actions Std                              0.803012
exploration/Actions Max                              0.9997
exploration/Actions Min                             -0.999032
exploration/Num Paths                                1
exploration/Average Returns                       4253.56
exploration/env_infos/final/reward_run Mean          4.43515
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.43515
exploration/env_infos/final/reward_run Min           4.43515
exploration/env_infos/initial/reward_run Mean       -0.0106212
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0106212
exploration/env_infos/initial/reward_run Min        -0.0106212
exploration/env_infos/reward_run Mean                4.64067
exploration/env_infos/reward_run Std                 1.10346
exploration/env_infos/reward_run Max                 6.83151
exploration/env_infos/reward_run Min                -0.0106212
exploration/env_infos/final/reward_ctrl Mean        -0.485028
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.485028
exploration/env_infos/final/reward_ctrl Min         -0.485028
exploration/env_infos/initial/reward_ctrl Mean      -0.0424321
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0424321
exploration/env_infos/initial/reward_ctrl Min       -0.0424321
exploration/env_infos/reward_ctrl Mean              -0.387107
exploration/env_infos/reward_ctrl Std                0.106307
exploration/env_infos/reward_ctrl Max               -0.0424321
exploration/env_infos/reward_ctrl Min               -0.57672
evaluation/num steps total                           1.005e+06
evaluation/num paths total                        1005
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.5334
evaluation/Rewards Std                               1.14986
evaluation/Rewards Max                               6.82148
evaluation/Rewards Min                              -1.03265
evaluation/Returns Mean                           4533.4
evaluation/Returns Std                              69.565
evaluation/Returns Max                            4598.34
evaluation/Returns Min                            4398.35
evaluation/Actions Mean                              0.00954567
evaluation/Actions Std                               0.810619
evaluation/Actions Max                               0.99816
evaluation/Actions Min                              -0.998969
evaluation/Num Paths                                 5
evaluation/Average Returns                        4533.4
evaluation/env_infos/final/reward_run Mean           4.64451
evaluation/env_infos/final/reward_run Std            1.1879
evaluation/env_infos/final/reward_run Max            6.9367
evaluation/env_infos/final/reward_run Min            3.72218
evaluation/env_infos/initial/reward_run Mean         0.136478
evaluation/env_infos/initial/reward_run Std          0.177329
evaluation/env_infos/initial/reward_run Max          0.367289
evaluation/env_infos/initial/reward_run Min         -0.134935
evaluation/env_infos/reward_run Mean                 4.92772
evaluation/env_infos/reward_run Std                  1.13069
evaluation/env_infos/reward_run Max                  7.16116
evaluation/env_infos/reward_run Min                 -0.700174
evaluation/env_infos/final/reward_ctrl Mean         -0.410865
evaluation/env_infos/final/reward_ctrl Std           0.0970972
evaluation/env_infos/final/reward_ctrl Max          -0.305605
evaluation/env_infos/final/reward_ctrl Min          -0.526324
evaluation/env_infos/initial/reward_ctrl Mean       -0.0604708
evaluation/env_infos/initial/reward_ctrl Std         0.0257752
evaluation/env_infos/initial/reward_ctrl Max        -0.0287625
evaluation/env_infos/initial/reward_ctrl Min        -0.0931207
evaluation/env_infos/reward_ctrl Mean               -0.394316
evaluation/env_infos/reward_ctrl Std                 0.108874
evaluation/env_infos/reward_ctrl Max                -0.0287625
evaluation/env_infos/reward_ctrl Min                -0.576884
time/data storing (s)                                0.00669663
time/evaluation sampling (s)                         2.71146
time/exploration sampling (s)                        0.716002
time/logging (s)                                     0.0399043
time/saving (s)                                      0.015878
time/training (s)                                   31.2207
time/epoch (s)                                      34.7106
time/total (s)                                    8439.57
Epoch                                              200
----------------------------------------------  ---------------
2020-07-08 23:27:25.907756 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 201 finished
----------------------------------------------  ---------------
replay_buffer/size                              203000
trainer/QF1 Loss                                     5.13612
trainer/QF2 Loss                                     7.22824
trainer/Policy Loss                               -262.405
trainer/Q1 Predictions Mean                        268.466
trainer/Q1 Predictions Std                          78.9934
trainer/Q1 Predictions Max                         327.318
trainer/Q1 Predictions Min                          15.815
trainer/Q2 Predictions Mean                        268.065
trainer/Q2 Predictions Std                          78.748
trainer/Q2 Predictions Max                         325.607
trainer/Q2 Predictions Min                          15.4557
trainer/Q Targets Mean                             268.087
trainer/Q Targets Std                               79.0875
trainer/Q Targets Max                              325.68
trainer/Q Targets Min                               14.3151
trainer/Log Pis Mean                                 5.82754
trainer/Log Pis Std                                  4.58175
trainer/Log Pis Max                                 18.6754
trainer/Log Pis Min                                 -5.11613
trainer/Policy mu Mean                               0.0813574
trainer/Policy mu Std                                1.47136
trainer/Policy mu Max                                3.06697
trainer/Policy mu Min                               -3.38734
trainer/Policy log std Mean                         -0.849493
trainer/Policy log std Std                           0.34695
trainer/Policy log std Max                           0.0932889
trainer/Policy log std Min                          -2.54333
trainer/Alpha                                        0.108745
trainer/Alpha Loss                                  -0.382639
exploration/num steps total                     203000
exploration/num paths total                        203
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.45372
exploration/Rewards Std                              1.0684
exploration/Rewards Max                              6.634
exploration/Rewards Min                             -0.327493
exploration/Returns Mean                          4453.72
exploration/Returns Std                              0
exploration/Returns Max                           4453.72
exploration/Returns Min                           4453.72
exploration/Actions Mean                             0.00386719
exploration/Actions Std                              0.820928
exploration/Actions Max                              0.999812
exploration/Actions Min                             -0.999674
exploration/Num Paths                                1
exploration/Average Returns                       4453.72
exploration/env_infos/final/reward_run Mean          4.81123
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.81123
exploration/env_infos/final/reward_run Min           4.81123
exploration/env_infos/initial/reward_run Mean        0.122942
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.122942
exploration/env_infos/initial/reward_run Min         0.122942
exploration/env_infos/reward_run Mean                4.85808
exploration/env_infos/reward_run Std                 1.05269
exploration/env_infos/reward_run Max                 6.97197
exploration/env_infos/reward_run Min                 0.0575977
exploration/env_infos/final/reward_ctrl Mean        -0.500065
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.500065
exploration/env_infos/final/reward_ctrl Min         -0.500065
exploration/env_infos/initial/reward_ctrl Mean      -0.0834497
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0834497
exploration/env_infos/initial/reward_ctrl Min       -0.0834497
exploration/env_infos/reward_ctrl Mean              -0.404363
exploration/env_infos/reward_ctrl Std                0.0961318
exploration/env_infos/reward_ctrl Max               -0.0834497
exploration/env_infos/reward_ctrl Min               -0.573096
evaluation/num steps total                           1.01e+06
evaluation/num paths total                        1010
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.55852
evaluation/Rewards Std                               1.1299
evaluation/Rewards Max                               6.72199
evaluation/Rewards Min                              -0.891256
evaluation/Returns Mean                           4558.52
evaluation/Returns Std                              48.8697
evaluation/Returns Max                            4628.83
evaluation/Returns Min                            4508.24
evaluation/Actions Mean                              0.00436182
evaluation/Actions Std                               0.827291
evaluation/Actions Max                               0.998998
evaluation/Actions Min                              -0.998605
evaluation/Num Paths                                 5
evaluation/Average Returns                        4558.52
evaluation/env_infos/final/reward_run Mean           5.19606
evaluation/env_infos/final/reward_run Std            0.688593
evaluation/env_infos/final/reward_run Max            5.98933
evaluation/env_infos/final/reward_run Min            4.20629
evaluation/env_infos/initial/reward_run Mean        -0.0519978
evaluation/env_infos/initial/reward_run Std          0.116695
evaluation/env_infos/initial/reward_run Max          0.117361
evaluation/env_infos/initial/reward_run Min         -0.20671
evaluation/env_infos/reward_run Mean                 4.96918
evaluation/env_infos/reward_run Std                  1.11305
evaluation/env_infos/reward_run Max                  7.02716
evaluation/env_infos/reward_run Min                 -0.43786
evaluation/env_infos/final/reward_ctrl Mean         -0.37771
evaluation/env_infos/final/reward_ctrl Std           0.149058
evaluation/env_infos/final/reward_ctrl Max          -0.134914
evaluation/env_infos/final/reward_ctrl Min          -0.528907
evaluation/env_infos/initial/reward_ctrl Mean       -0.0686973
evaluation/env_infos/initial/reward_ctrl Std         0.0158684
evaluation/env_infos/initial/reward_ctrl Max        -0.0459353
evaluation/env_infos/initial/reward_ctrl Min        -0.0914136
evaluation/env_infos/reward_ctrl Mean               -0.410657
evaluation/env_infos/reward_ctrl Std                 0.10051
evaluation/env_infos/reward_ctrl Max                -0.0342084
evaluation/env_infos/reward_ctrl Min                -0.581642
time/data storing (s)                                0.00690022
time/evaluation sampling (s)                         2.33447
time/exploration sampling (s)                        0.63706
time/logging (s)                                     0.0403306
time/saving (s)                                      0.0171284
time/training (s)                                   31.2296
time/epoch (s)                                      34.2654
time/total (s)                                    8473.91
Epoch                                              201
----------------------------------------------  ---------------
2020-07-08 23:28:01.473893 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 202 finished
----------------------------------------------  ----------------
replay_buffer/size                              204000
trainer/QF1 Loss                                     6.60693
trainer/QF2 Loss                                     7.85721
trainer/Policy Loss                               -249.089
trainer/Q1 Predictions Mean                        255.176
trainer/Q1 Predictions Std                          96.082
trainer/Q1 Predictions Max                         331.901
trainer/Q1 Predictions Min                          16.6141
trainer/Q2 Predictions Mean                        254.988
trainer/Q2 Predictions Std                          96.0683
trainer/Q2 Predictions Max                         333.516
trainer/Q2 Predictions Min                          17.2171
trainer/Q Targets Mean                             255.387
trainer/Q Targets Std                               95.9913
trainer/Q Targets Max                              332.118
trainer/Q Targets Min                               17.1341
trainer/Log Pis Mean                                 5.9739
trainer/Log Pis Std                                  4.88341
trainer/Log Pis Max                                 21.912
trainer/Log Pis Min                                 -5.9349
trainer/Policy mu Mean                               0.000463886
trainer/Policy mu Std                                1.49471
trainer/Policy mu Max                                4.26126
trainer/Policy mu Min                               -3.41224
trainer/Policy log std Mean                         -0.814251
trainer/Policy log std Std                           0.345543
trainer/Policy log std Max                           0.652609
trainer/Policy log std Min                          -2.40187
trainer/Alpha                                        0.107971
trainer/Alpha Loss                                  -0.0581022
exploration/num steps total                     204000
exploration/num paths total                        204
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.26784
exploration/Rewards Std                              1.10979
exploration/Rewards Max                              6.59929
exploration/Rewards Min                             -0.719604
exploration/Returns Mean                          4267.84
exploration/Returns Std                              0
exploration/Returns Max                           4267.84
exploration/Returns Min                           4267.84
exploration/Actions Mean                            -0.0228834
exploration/Actions Std                              0.81024
exploration/Actions Max                              0.999131
exploration/Actions Min                             -0.99945
exploration/Num Paths                                1
exploration/Average Returns                       4267.84
exploration/env_infos/final/reward_run Mean          3.83346
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.83346
exploration/env_infos/final/reward_run Min           3.83346
exploration/env_infos/initial/reward_run Mean        0.194825
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.194825
exploration/env_infos/initial/reward_run Min         0.194825
exploration/env_infos/reward_run Mean                4.66205
exploration/env_infos/reward_run Std                 1.0984
exploration/env_infos/reward_run Max                 6.95157
exploration/env_infos/reward_run Min                -0.277876
exploration/env_infos/final/reward_ctrl Mean        -0.373705
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.373705
exploration/env_infos/final/reward_ctrl Min         -0.373705
exploration/env_infos/initial/reward_ctrl Mean      -0.0379385
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0379385
exploration/env_infos/initial/reward_ctrl Min       -0.0379385
exploration/env_infos/reward_ctrl Mean              -0.394208
exploration/env_infos/reward_ctrl Std                0.0973325
exploration/env_infos/reward_ctrl Max               -0.0379385
exploration/env_infos/reward_ctrl Min               -0.57407
evaluation/num steps total                           1.015e+06
evaluation/num paths total                        1015
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.63541
evaluation/Rewards Std                               1.09244
evaluation/Rewards Max                               6.75985
evaluation/Rewards Min                              -0.989267
evaluation/Returns Mean                           4635.41
evaluation/Returns Std                              31.6153
evaluation/Returns Max                            4681.03
evaluation/Returns Min                            4586.74
evaluation/Actions Mean                             -0.0333659
evaluation/Actions Std                               0.831808
evaluation/Actions Max                               0.999339
evaluation/Actions Min                              -0.997214
evaluation/Num Paths                                 5
evaluation/Average Returns                        4635.41
evaluation/env_infos/final/reward_run Mean           5.62662
evaluation/env_infos/final/reward_run Std            0.600277
evaluation/env_infos/final/reward_run Max            6.33312
evaluation/env_infos/final/reward_run Min            4.81063
evaluation/env_infos/initial/reward_run Mean        -0.0682355
evaluation/env_infos/initial/reward_run Std          0.100646
evaluation/env_infos/initial/reward_run Max          0.0566862
evaluation/env_infos/initial/reward_run Min         -0.239138
evaluation/env_infos/reward_run Mean                 5.05122
evaluation/env_infos/reward_run Std                  1.07633
evaluation/env_infos/reward_run Max                  7.17314
evaluation/env_infos/reward_run Min                 -0.64423
evaluation/env_infos/final/reward_ctrl Mean         -0.436602
evaluation/env_infos/final/reward_ctrl Std           0.0482864
evaluation/env_infos/final/reward_ctrl Max          -0.377852
evaluation/env_infos/final/reward_ctrl Min          -0.494333
evaluation/env_infos/initial/reward_ctrl Mean       -0.0804437
evaluation/env_infos/initial/reward_ctrl Std         0.0196678
evaluation/env_infos/initial/reward_ctrl Max        -0.0590638
evaluation/env_infos/initial/reward_ctrl Min        -0.110439
evaluation/env_infos/reward_ctrl Mean               -0.415811
evaluation/env_infos/reward_ctrl Std                 0.0968686
evaluation/env_infos/reward_ctrl Max                -0.0462902
evaluation/env_infos/reward_ctrl Min                -0.589256
time/data storing (s)                                0.00807596
time/evaluation sampling (s)                         2.57884
time/exploration sampling (s)                        0.634395
time/logging (s)                                     0.0419449
time/saving (s)                                      0.0208379
time/training (s)                                   32.253
time/epoch (s)                                      35.5371
time/total (s)                                    8509.47
Epoch                                              202
----------------------------------------------  ----------------
2020-07-08 23:28:40.039100 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 203 finished
----------------------------------------------  ---------------
replay_buffer/size                              205000
trainer/QF1 Loss                                     5.95619
trainer/QF2 Loss                                     7.17303
trainer/Policy Loss                               -258.285
trainer/Q1 Predictions Mean                        264.53
trainer/Q1 Predictions Std                          82.9633
trainer/Q1 Predictions Max                         326.998
trainer/Q1 Predictions Min                          17.6843
trainer/Q2 Predictions Mean                        264.091
trainer/Q2 Predictions Std                          82.9488
trainer/Q2 Predictions Max                         328.528
trainer/Q2 Predictions Min                          14.2667
trainer/Q Targets Mean                             264.854
trainer/Q Targets Std                               83.1924
trainer/Q Targets Max                              327.382
trainer/Q Targets Min                               17.451
trainer/Log Pis Mean                                 6.004
trainer/Log Pis Std                                  4.52861
trainer/Log Pis Max                                 17.9299
trainer/Log Pis Min                                 -6.23125
trainer/Policy mu Mean                               0.100084
trainer/Policy mu Std                                1.47277
trainer/Policy mu Max                                3.33498
trainer/Policy mu Min                               -3.74509
trainer/Policy log std Mean                         -0.833563
trainer/Policy log std Std                           0.338329
trainer/Policy log std Max                           0.0192387
trainer/Policy log std Min                          -2.7766
trainer/Alpha                                        0.109662
trainer/Alpha Loss                                   0.0088352
exploration/num steps total                     205000
exploration/num paths total                        205
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.30772
exploration/Rewards Std                              1.07492
exploration/Rewards Max                              6.38565
exploration/Rewards Min                             -0.704731
exploration/Returns Mean                          4307.72
exploration/Returns Std                              0
exploration/Returns Max                           4307.72
exploration/Returns Min                           4307.72
exploration/Actions Mean                             0.00853379
exploration/Actions Std                              0.814989
exploration/Actions Max                              0.999807
exploration/Actions Min                             -0.999461
exploration/Num Paths                                1
exploration/Average Returns                       4307.72
exploration/env_infos/final/reward_run Mean          4.7265
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.7265
exploration/env_infos/final/reward_run Min           4.7265
exploration/env_infos/initial/reward_run Mean        0.0254469
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0254469
exploration/env_infos/initial/reward_run Min         0.0254469
exploration/env_infos/reward_run Mean                4.70629
exploration/env_infos/reward_run Std                 1.06208
exploration/env_infos/reward_run Max                 6.7143
exploration/env_infos/reward_run Min                -0.166779
exploration/env_infos/final/reward_ctrl Mean        -0.462451
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.462451
exploration/env_infos/final/reward_ctrl Min         -0.462451
exploration/env_infos/initial/reward_ctrl Mean      -0.0784779
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0784779
exploration/env_infos/initial/reward_ctrl Min       -0.0784779
exploration/env_infos/reward_ctrl Mean              -0.398568
exploration/env_infos/reward_ctrl Std                0.103544
exploration/env_infos/reward_ctrl Max               -0.0784779
exploration/env_infos/reward_ctrl Min               -0.583107
evaluation/num steps total                           1.02e+06
evaluation/num paths total                        1020
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.56735
evaluation/Rewards Std                               1.0949
evaluation/Rewards Max                               6.80548
evaluation/Rewards Min                              -1.15295
evaluation/Returns Mean                           4567.35
evaluation/Returns Std                              86.1924
evaluation/Returns Max                            4663.72
evaluation/Returns Min                            4416.3
evaluation/Actions Mean                             -0.00407603
evaluation/Actions Std                               0.835408
evaluation/Actions Max                               0.999781
evaluation/Actions Min                              -0.999393
evaluation/Num Paths                                 5
evaluation/Average Returns                        4567.35
evaluation/env_infos/final/reward_run Mean           5.95488
evaluation/env_infos/final/reward_run Std            0.611267
evaluation/env_infos/final/reward_run Max            6.70578
evaluation/env_infos/final/reward_run Min            5.01435
evaluation/env_infos/initial/reward_run Mean         0.174563
evaluation/env_infos/initial/reward_run Std          0.170731
evaluation/env_infos/initial/reward_run Max          0.400855
evaluation/env_infos/initial/reward_run Min         -0.0368091
evaluation/env_infos/reward_run Mean                 4.98611
evaluation/env_infos/reward_run Std                  1.07779
evaluation/env_infos/reward_run Max                  7.11086
evaluation/env_infos/reward_run Min                 -0.608013
evaluation/env_infos/final/reward_ctrl Mean         -0.476905
evaluation/env_infos/final/reward_ctrl Std           0.0496361
evaluation/env_infos/final/reward_ctrl Max          -0.394019
evaluation/env_infos/final/reward_ctrl Min          -0.53564
evaluation/env_infos/initial/reward_ctrl Mean       -0.0670701
evaluation/env_infos/initial/reward_ctrl Std         0.0232371
evaluation/env_infos/initial/reward_ctrl Max        -0.0473952
evaluation/env_infos/initial/reward_ctrl Min        -0.110453
evaluation/env_infos/reward_ctrl Mean               -0.418754
evaluation/env_infos/reward_ctrl Std                 0.101163
evaluation/env_infos/reward_ctrl Max                -0.0473952
evaluation/env_infos/reward_ctrl Min                -0.585521
time/data storing (s)                                0.00860223
time/evaluation sampling (s)                         3.35226
time/exploration sampling (s)                        0.806502
time/logging (s)                                     0.0442905
time/saving (s)                                      0.0166987
time/training (s)                                   34.3187
time/epoch (s)                                      38.547
time/total (s)                                    8548.03
Epoch                                              203
----------------------------------------------  ---------------
2020-07-08 23:29:23.495912 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 204 finished
----------------------------------------------  ---------------
replay_buffer/size                              206000
trainer/QF1 Loss                                     5.39675
trainer/QF2 Loss                                     7.05065
trainer/Policy Loss                               -253.993
trainer/Q1 Predictions Mean                        260.104
trainer/Q1 Predictions Std                          91.237
trainer/Q1 Predictions Max                         329.001
trainer/Q1 Predictions Min                          17.4555
trainer/Q2 Predictions Mean                        260.117
trainer/Q2 Predictions Std                          91.2749
trainer/Q2 Predictions Max                         328.063
trainer/Q2 Predictions Min                          17.2959
trainer/Q Targets Mean                             259.928
trainer/Q Targets Std                               91.1246
trainer/Q Targets Max                              328.484
trainer/Q Targets Min                               16.8714
trainer/Log Pis Mean                                 6.21207
trainer/Log Pis Std                                  5.14092
trainer/Log Pis Max                                 17.8066
trainer/Log Pis Min                                 -5.72014
trainer/Policy mu Mean                               0.00761049
trainer/Policy mu Std                                1.49706
trainer/Policy mu Max                                3.18305
trainer/Policy mu Min                               -3.34228
trainer/Policy log std Mean                         -0.80682
trainer/Policy log std Std                           0.328554
trainer/Policy log std Max                          -0.0321984
trainer/Policy log std Min                          -2.46688
trainer/Alpha                                        0.107643
trainer/Alpha Loss                                   0.472679
exploration/num steps total                     206000
exploration/num paths total                        206
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.12319
exploration/Rewards Std                              1.04912
exploration/Rewards Max                              6.12966
exploration/Rewards Min                             -0.488253
exploration/Returns Mean                          4123.19
exploration/Returns Std                              0
exploration/Returns Max                           4123.19
exploration/Returns Min                           4123.19
exploration/Actions Mean                            -0.0337604
exploration/Actions Std                              0.816565
exploration/Actions Max                              0.99938
exploration/Actions Min                             -0.999085
exploration/Num Paths                                1
exploration/Average Returns                       4123.19
exploration/env_infos/final/reward_run Mean          6.05508
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.05508
exploration/env_infos/final/reward_run Min           6.05508
exploration/env_infos/initial/reward_run Mean       -0.0715121
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0715121
exploration/env_infos/initial/reward_run Min        -0.0715121
exploration/env_infos/reward_run Mean                4.52394
exploration/env_infos/reward_run Std                 1.02994
exploration/env_infos/reward_run Max                 6.52821
exploration/env_infos/reward_run Min                -0.0715121
exploration/env_infos/final/reward_ctrl Mean        -0.434717
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.434717
exploration/env_infos/final/reward_ctrl Min         -0.434717
exploration/env_infos/initial/reward_ctrl Mean      -0.107941
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.107941
exploration/env_infos/initial/reward_ctrl Min       -0.107941
exploration/env_infos/reward_ctrl Mean              -0.40075
exploration/env_infos/reward_ctrl Std                0.100646
exploration/env_infos/reward_ctrl Max               -0.107941
exploration/env_infos/reward_ctrl Min               -0.577372
evaluation/num steps total                           1.025e+06
evaluation/num paths total                        1025
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.25195
evaluation/Rewards Std                               1.07005
evaluation/Rewards Max                               6.64967
evaluation/Rewards Min                              -0.85816
evaluation/Returns Mean                           4251.95
evaluation/Returns Std                              61.9167
evaluation/Returns Max                            4357.22
evaluation/Returns Min                            4165.22
evaluation/Actions Mean                             -0.0580555
evaluation/Actions Std                               0.834598
evaluation/Actions Max                               0.999192
evaluation/Actions Min                              -0.998808
evaluation/Num Paths                                 5
evaluation/Average Returns                        4251.95
evaluation/env_infos/final/reward_run Mean           4.13461
evaluation/env_infos/final/reward_run Std            0.738424
evaluation/env_infos/final/reward_run Max            5.22232
evaluation/env_infos/final/reward_run Min            3.07896
evaluation/env_infos/initial/reward_run Mean         0.114158
evaluation/env_infos/initial/reward_run Std          0.310556
evaluation/env_infos/initial/reward_run Max          0.54416
evaluation/env_infos/initial/reward_run Min         -0.368107
evaluation/env_infos/reward_run Mean                 4.6719
evaluation/env_infos/reward_run Std                  1.05534
evaluation/env_infos/reward_run Max                  6.96141
evaluation/env_infos/reward_run Min                 -0.368107
evaluation/env_infos/final/reward_ctrl Mean         -0.450335
evaluation/env_infos/final/reward_ctrl Std           0.10385
evaluation/env_infos/final/reward_ctrl Max          -0.295409
evaluation/env_infos/final/reward_ctrl Min          -0.558883
evaluation/env_infos/initial/reward_ctrl Mean       -0.0779145
evaluation/env_infos/initial/reward_ctrl Std         0.0401128
evaluation/env_infos/initial/reward_ctrl Max        -0.0304517
evaluation/env_infos/initial/reward_ctrl Min        -0.148381
evaluation/env_infos/reward_ctrl Mean               -0.419955
evaluation/env_infos/reward_ctrl Std                 0.103242
evaluation/env_infos/reward_ctrl Max                -0.0304517
evaluation/env_infos/reward_ctrl Min                -0.578252
time/data storing (s)                                0.00881657
time/evaluation sampling (s)                         3.11832
time/exploration sampling (s)                        0.802808
time/logging (s)                                     0.0448828
time/saving (s)                                      0.0172734
time/training (s)                                   39.3154
time/epoch (s)                                      43.3075
time/total (s)                                    8591.49
Epoch                                              204
----------------------------------------------  ---------------
2020-07-08 23:30:04.695339 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 205 finished
----------------------------------------------  ---------------
replay_buffer/size                              207000
trainer/QF1 Loss                                    10.0329
trainer/QF2 Loss                                    12.8926
trainer/Policy Loss                               -256.527
trainer/Q1 Predictions Mean                        263.108
trainer/Q1 Predictions Std                          85.9875
trainer/Q1 Predictions Max                         328.106
trainer/Q1 Predictions Min                          17.3919
trainer/Q2 Predictions Mean                        262.548
trainer/Q2 Predictions Std                          85.7484
trainer/Q2 Predictions Max                         327.521
trainer/Q2 Predictions Min                          17.1028
trainer/Q Targets Mean                             263.172
trainer/Q Targets Std                               85.8463
trainer/Q Targets Max                              327.503
trainer/Q Targets Min                               17.7373
trainer/Log Pis Mean                                 6.36211
trainer/Log Pis Std                                  4.75865
trainer/Log Pis Max                                 23.3712
trainer/Log Pis Min                                 -5.13348
trainer/Policy mu Mean                               0.0684934
trainer/Policy mu Std                                1.52597
trainer/Policy mu Max                                5.01072
trainer/Policy mu Min                               -4.03945
trainer/Policy log std Mean                         -0.837275
trainer/Policy log std Std                           0.339725
trainer/Policy log std Max                          -0.0380107
trainer/Policy log std Min                          -2.6651
trainer/Alpha                                        0.111231
trainer/Alpha Loss                                   0.795304
exploration/num steps total                     207000
exploration/num paths total                        207
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.35225
exploration/Rewards Std                              1.03937
exploration/Rewards Max                              6.41636
exploration/Rewards Min                             -0.216947
exploration/Returns Mean                          4352.25
exploration/Returns Std                              0
exploration/Returns Max                           4352.25
exploration/Returns Min                           4352.25
exploration/Actions Mean                            -0.00285292
exploration/Actions Std                              0.820792
exploration/Actions Max                              0.999307
exploration/Actions Min                             -0.999505
exploration/Num Paths                                1
exploration/Average Returns                       4352.25
exploration/env_infos/final/reward_run Mean          4.80033
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.80033
exploration/env_infos/final/reward_run Min           4.80033
exploration/env_infos/initial/reward_run Mean        0.178477
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.178477
exploration/env_infos/initial/reward_run Min         0.178477
exploration/env_infos/reward_run Mean                4.75648
exploration/env_infos/reward_run Std                 1.02555
exploration/env_infos/reward_run Max                 6.74428
exploration/env_infos/reward_run Min                -0.077828
exploration/env_infos/final/reward_ctrl Mean        -0.536276
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.536276
exploration/env_infos/final/reward_ctrl Min         -0.536276
exploration/env_infos/initial/reward_ctrl Mean      -0.116233
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.116233
exploration/env_infos/initial/reward_ctrl Min       -0.116233
exploration/env_infos/reward_ctrl Mean              -0.404225
exploration/env_infos/reward_ctrl Std                0.0956697
exploration/env_infos/reward_ctrl Max               -0.10637
exploration/env_infos/reward_ctrl Min               -0.577356
evaluation/num steps total                           1.03e+06
evaluation/num paths total                        1030
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.608
evaluation/Rewards Std                               1.0939
evaluation/Rewards Max                               6.84682
evaluation/Rewards Min                              -0.828309
evaluation/Returns Mean                           4608
evaluation/Returns Std                              26.5532
evaluation/Returns Max                            4643.18
evaluation/Returns Min                            4564.03
evaluation/Actions Mean                             -0.0130921
evaluation/Actions Std                               0.832724
evaluation/Actions Max                               0.996895
evaluation/Actions Min                              -0.997958
evaluation/Num Paths                                 5
evaluation/Average Returns                        4608
evaluation/env_infos/final/reward_run Mean           5.38866
evaluation/env_infos/final/reward_run Std            0.943456
evaluation/env_infos/final/reward_run Max            6.38946
evaluation/env_infos/final/reward_run Min            4.23645
evaluation/env_infos/initial/reward_run Mean         0.0543533
evaluation/env_infos/initial/reward_run Std          0.161968
evaluation/env_infos/initial/reward_run Max          0.31275
evaluation/env_infos/initial/reward_run Min         -0.18581
evaluation/env_infos/reward_run Mean                 5.02416
evaluation/env_infos/reward_run Std                  1.07881
evaluation/env_infos/reward_run Max                  7.1905
evaluation/env_infos/reward_run Min                 -0.307172
evaluation/env_infos/final/reward_ctrl Mean         -0.419782
evaluation/env_infos/final/reward_ctrl Std           0.0631541
evaluation/env_infos/final/reward_ctrl Max          -0.303301
evaluation/env_infos/final/reward_ctrl Min          -0.495889
evaluation/env_infos/initial/reward_ctrl Mean       -0.0753195
evaluation/env_infos/initial/reward_ctrl Std         0.0532736
evaluation/env_infos/initial/reward_ctrl Max        -0.0273355
evaluation/env_infos/initial/reward_ctrl Min        -0.150106
evaluation/env_infos/reward_ctrl Mean               -0.41616
evaluation/env_infos/reward_ctrl Std                 0.0958739
evaluation/env_infos/reward_ctrl Max                -0.0273355
evaluation/env_infos/reward_ctrl Min                -0.579223
time/data storing (s)                                0.0163683
time/evaluation sampling (s)                         3.53748
time/exploration sampling (s)                        1.16878
time/logging (s)                                     0.0420487
time/saving (s)                                      0.0166742
time/training (s)                                   36.3737
time/epoch (s)                                      41.1551
time/total (s)                                    8632.67
Epoch                                              205
----------------------------------------------  ---------------
2020-07-08 23:30:53.569500 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 206 finished
----------------------------------------------  ---------------
replay_buffer/size                              208000
trainer/QF1 Loss                                    11.4708
trainer/QF2 Loss                                    10.9745
trainer/Policy Loss                               -249.068
trainer/Q1 Predictions Mean                        255.143
trainer/Q1 Predictions Std                          96.332
trainer/Q1 Predictions Max                         332.281
trainer/Q1 Predictions Min                          17.2219
trainer/Q2 Predictions Mean                        255.17
trainer/Q2 Predictions Std                          96.2405
trainer/Q2 Predictions Max                         331.66
trainer/Q2 Predictions Min                          16.9067
trainer/Q Targets Mean                             254.357
trainer/Q Targets Std                               96.2549
trainer/Q Targets Max                              329.125
trainer/Q Targets Min                               16.3672
trainer/Log Pis Mean                                 6.18276
trainer/Log Pis Std                                  5.0797
trainer/Log Pis Max                                 18.0046
trainer/Log Pis Min                                 -6.63679
trainer/Policy mu Mean                               0.0622614
trainer/Policy mu Std                                1.5366
trainer/Policy mu Max                                4.81845
trainer/Policy mu Min                               -3.51976
trainer/Policy log std Mean                         -0.80713
trainer/Policy log std Std                           0.342488
trainer/Policy log std Max                          -0.0685434
trainer/Policy log std Min                          -2.54511
trainer/Alpha                                        0.109689
trainer/Alpha Loss                                   0.403925
exploration/num steps total                     208000
exploration/num paths total                        208
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.47256
exploration/Rewards Std                              1.0175
exploration/Rewards Max                              6.43308
exploration/Rewards Min                             -0.192241
exploration/Returns Mean                          4472.56
exploration/Returns Std                              0
exploration/Returns Max                           4472.56
exploration/Returns Min                           4472.56
exploration/Actions Mean                             0.00828887
exploration/Actions Std                              0.800463
exploration/Actions Max                              0.999593
exploration/Actions Min                             -0.999242
exploration/Num Paths                                1
exploration/Average Returns                       4472.56
exploration/env_infos/final/reward_run Mean          5.57653
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.57653
exploration/env_infos/final/reward_run Min           5.57653
exploration/env_infos/initial/reward_run Mean       -0.0830375
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0830375
exploration/env_infos/initial/reward_run Min        -0.0830375
exploration/env_infos/reward_run Mean                4.85704
exploration/env_infos/reward_run Std                 0.996634
exploration/env_infos/reward_run Max                 6.87506
exploration/env_infos/reward_run Min                -0.0830375
exploration/env_infos/final/reward_ctrl Mean        -0.312015
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.312015
exploration/env_infos/final/reward_ctrl Min         -0.312015
exploration/env_infos/initial/reward_ctrl Mean      -0.0304479
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0304479
exploration/env_infos/initial/reward_ctrl Min       -0.0304479
exploration/env_infos/reward_ctrl Mean              -0.384486
exploration/env_infos/reward_ctrl Std                0.101801
exploration/env_infos/reward_ctrl Max               -0.0304479
exploration/env_infos/reward_ctrl Min               -0.575713
evaluation/num steps total                           1.035e+06
evaluation/num paths total                        1035
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.59472
evaluation/Rewards Std                               1.12347
evaluation/Rewards Max                               6.68903
evaluation/Rewards Min                              -0.967999
evaluation/Returns Mean                           4594.72
evaluation/Returns Std                              65.9354
evaluation/Returns Max                            4714.85
evaluation/Returns Min                            4530.46
evaluation/Actions Mean                              0.00456408
evaluation/Actions Std                               0.820414
evaluation/Actions Max                               0.998936
evaluation/Actions Min                              -0.999249
evaluation/Num Paths                                 5
evaluation/Average Returns                        4594.72
evaluation/env_infos/final/reward_run Mean           5.81758
evaluation/env_infos/final/reward_run Std            0.7748
evaluation/env_infos/final/reward_run Max            6.49837
evaluation/env_infos/final/reward_run Min            4.3205
evaluation/env_infos/initial/reward_run Mean         0.0432347
evaluation/env_infos/initial/reward_run Std          0.156619
evaluation/env_infos/initial/reward_run Max          0.309976
evaluation/env_infos/initial/reward_run Min         -0.15549
evaluation/env_infos/reward_run Mean                 4.99858
evaluation/env_infos/reward_run Std                  1.10298
evaluation/env_infos/reward_run Max                  7.06895
evaluation/env_infos/reward_run Min                 -0.665063
evaluation/env_infos/final/reward_ctrl Mean         -0.341629
evaluation/env_infos/final/reward_ctrl Std           0.0843629
evaluation/env_infos/final/reward_ctrl Max          -0.226147
evaluation/env_infos/final/reward_ctrl Min          -0.463488
evaluation/env_infos/initial/reward_ctrl Mean       -0.0593372
evaluation/env_infos/initial/reward_ctrl Std         0.02203
evaluation/env_infos/initial/reward_ctrl Max        -0.0334108
evaluation/env_infos/initial/reward_ctrl Min        -0.0991308
evaluation/env_infos/reward_ctrl Mean               -0.40386
evaluation/env_infos/reward_ctrl Std                 0.0980109
evaluation/env_infos/reward_ctrl Max                -0.0334108
evaluation/env_infos/reward_ctrl Min                -0.583003
time/data storing (s)                                0.00720428
time/evaluation sampling (s)                         2.7318
time/exploration sampling (s)                        0.778485
time/logging (s)                                     0.0463829
time/saving (s)                                      0.0168668
time/training (s)                                   45.1525
time/epoch (s)                                      48.7333
time/total (s)                                    8681.55
Epoch                                              206
----------------------------------------------  ---------------
2020-07-08 23:31:39.804974 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 207 finished
----------------------------------------------  ----------------
replay_buffer/size                              209000
trainer/QF1 Loss                                     6.59137
trainer/QF2 Loss                                     6.73199
trainer/Policy Loss                               -267.714
trainer/Q1 Predictions Mean                        273.904
trainer/Q1 Predictions Std                          73.6795
trainer/Q1 Predictions Max                         329.914
trainer/Q1 Predictions Min                          18.1933
trainer/Q2 Predictions Mean                        274.197
trainer/Q2 Predictions Std                          73.6215
trainer/Q2 Predictions Max                         327.337
trainer/Q2 Predictions Min                          18.0972
trainer/Q Targets Mean                             273.842
trainer/Q Targets Std                               73.6126
trainer/Q Targets Max                              329.063
trainer/Q Targets Min                               18.2426
trainer/Log Pis Mean                                 6.53751
trainer/Log Pis Std                                  4.95692
trainer/Log Pis Max                                 18.0624
trainer/Log Pis Min                                 -6.94744
trainer/Policy mu Mean                              -0.0162186
trainer/Policy mu Std                                1.54762
trainer/Policy mu Max                                4.69836
trainer/Policy mu Min                               -3.41139
trainer/Policy log std Mean                         -0.861807
trainer/Policy log std Std                           0.329937
trainer/Policy log std Max                           0.00894481
trainer/Policy log std Min                          -2.59933
trainer/Alpha                                        0.108657
trainer/Alpha Loss                                   1.19305
exploration/num steps total                     209000
exploration/num paths total                        209
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.33689
exploration/Rewards Std                              1.08563
exploration/Rewards Max                              6.38214
exploration/Rewards Min                             -0.499686
exploration/Returns Mean                          4336.89
exploration/Returns Std                              0
exploration/Returns Max                           4336.89
exploration/Returns Min                           4336.89
exploration/Actions Mean                             0.000224192
exploration/Actions Std                              0.802461
exploration/Actions Max                              0.999537
exploration/Actions Min                             -0.999585
exploration/Num Paths                                1
exploration/Average Returns                       4336.89
exploration/env_infos/final/reward_run Mean          5.06922
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.06922
exploration/env_infos/final/reward_run Min           5.06922
exploration/env_infos/initial/reward_run Mean        0.0697363
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0697363
exploration/env_infos/initial/reward_run Min         0.0697363
exploration/env_infos/reward_run Mean                4.72326
exploration/env_infos/reward_run Std                 1.0656
exploration/env_infos/reward_run Max                 6.61325
exploration/env_infos/reward_run Min                -0.0760251
exploration/env_infos/final/reward_ctrl Mean        -0.453532
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.453532
exploration/env_infos/final/reward_ctrl Min         -0.453532
exploration/env_infos/initial/reward_ctrl Mean      -0.11675
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.11675
exploration/env_infos/initial/reward_ctrl Min       -0.11675
exploration/env_infos/reward_ctrl Mean              -0.386366
exploration/env_infos/reward_ctrl Std                0.0986046
exploration/env_infos/reward_ctrl Max               -0.0751766
exploration/env_infos/reward_ctrl Min               -0.582593
evaluation/num steps total                           1.04e+06
evaluation/num paths total                        1040
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.61587
evaluation/Rewards Std                               1.11054
evaluation/Rewards Max                               6.94885
evaluation/Rewards Min                              -1.46231
evaluation/Returns Mean                           4615.87
evaluation/Returns Std                              73.5454
evaluation/Returns Max                            4718.71
evaluation/Returns Min                            4503.11
evaluation/Actions Mean                             -0.0140161
evaluation/Actions Std                               0.81508
evaluation/Actions Max                               0.998802
evaluation/Actions Min                              -0.99781
evaluation/Num Paths                                 5
evaluation/Average Returns                        4615.87
evaluation/env_infos/final/reward_run Mean           4.64602
evaluation/env_infos/final/reward_run Std            0.973806
evaluation/env_infos/final/reward_run Max            5.7485
evaluation/env_infos/final/reward_run Min            3.35996
evaluation/env_infos/initial/reward_run Mean        -0.0197495
evaluation/env_infos/initial/reward_run Std          0.156825
evaluation/env_infos/initial/reward_run Max          0.234428
evaluation/env_infos/initial/reward_run Min         -0.260114
evaluation/env_infos/reward_run Mean                 5.0146
evaluation/env_infos/reward_run Std                  1.08841
evaluation/env_infos/reward_run Max                  7.21255
evaluation/env_infos/reward_run Min                 -1.01628
evaluation/env_infos/final/reward_ctrl Mean         -0.466457
evaluation/env_infos/final/reward_ctrl Std           0.0785209
evaluation/env_infos/final/reward_ctrl Max          -0.31528
evaluation/env_infos/final/reward_ctrl Min          -0.537652
evaluation/env_infos/initial/reward_ctrl Mean       -0.126132
evaluation/env_infos/initial/reward_ctrl Std         0.0700702
evaluation/env_infos/initial/reward_ctrl Max        -0.0758982
evaluation/env_infos/initial/reward_ctrl Min        -0.263373
evaluation/env_infos/reward_ctrl Mean               -0.398731
evaluation/env_infos/reward_ctrl Std                 0.105039
evaluation/env_infos/reward_ctrl Max                -0.0384656
evaluation/env_infos/reward_ctrl Min                -0.576556
time/data storing (s)                                0.0089297
time/evaluation sampling (s)                         3.27042
time/exploration sampling (s)                        0.688764
time/logging (s)                                     0.0424255
time/saving (s)                                      0.0164308
time/training (s)                                   42.1838
time/epoch (s)                                      46.2108
time/total (s)                                    8727.77
Epoch                                              207
----------------------------------------------  ----------------
2020-07-08 23:32:20.276017 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 208 finished
----------------------------------------------  ---------------
replay_buffer/size                              210000
trainer/QF1 Loss                                     7.45195
trainer/QF2 Loss                                     8.51461
trainer/Policy Loss                               -262.989
trainer/Q1 Predictions Mean                        269.618
trainer/Q1 Predictions Std                          81.3979
trainer/Q1 Predictions Max                         331.898
trainer/Q1 Predictions Min                          17.5335
trainer/Q2 Predictions Mean                        269.571
trainer/Q2 Predictions Std                          81.3189
trainer/Q2 Predictions Max                         332.188
trainer/Q2 Predictions Min                          17.5865
trainer/Q Targets Mean                             269.595
trainer/Q Targets Std                               81.4681
trainer/Q Targets Max                              333.616
trainer/Q Targets Min                               16.2943
trainer/Log Pis Mean                                 6.6781
trainer/Log Pis Std                                  4.85489
trainer/Log Pis Max                                 17.6002
trainer/Log Pis Min                                 -5.4261
trainer/Policy mu Mean                               0.131072
trainer/Policy mu Std                                1.56285
trainer/Policy mu Max                                3.78796
trainer/Policy mu Min                               -3.76942
trainer/Policy log std Mean                         -0.821908
trainer/Policy log std Std                           0.347243
trainer/Policy log std Max                           0.218829
trainer/Policy log std Min                          -2.78715
trainer/Alpha                                        0.109727
trainer/Alpha Loss                                   1.49841
exploration/num steps total                     210000
exploration/num paths total                        210
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.32667
exploration/Rewards Std                              1.06787
exploration/Rewards Max                              6.77988
exploration/Rewards Min                             -0.792628
exploration/Returns Mean                          4326.67
exploration/Returns Std                              0
exploration/Returns Max                           4326.67
exploration/Returns Min                           4326.67
exploration/Actions Mean                            -0.00852742
exploration/Actions Std                              0.818417
exploration/Actions Max                              0.999817
exploration/Actions Min                             -0.999872
exploration/Num Paths                                1
exploration/Average Returns                       4326.67
exploration/env_infos/final/reward_run Mean          4.77205
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.77205
exploration/env_infos/final/reward_run Min           4.77205
exploration/env_infos/initial/reward_run Mean        0.12013
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.12013
exploration/env_infos/initial/reward_run Min         0.12013
exploration/env_infos/reward_run Mean                4.7286
exploration/env_infos/reward_run Std                 1.04648
exploration/env_infos/reward_run Max                 7.11926
exploration/env_infos/reward_run Min                -0.30409
exploration/env_infos/final/reward_ctrl Mean        -0.370222
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.370222
exploration/env_infos/final/reward_ctrl Min         -0.370222
exploration/env_infos/initial/reward_ctrl Mean      -0.0821291
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0821291
exploration/env_infos/initial/reward_ctrl Min       -0.0821291
exploration/env_infos/reward_ctrl Mean              -0.401928
exploration/env_infos/reward_ctrl Std                0.0996474
exploration/env_infos/reward_ctrl Max               -0.0821291
exploration/env_infos/reward_ctrl Min               -0.580118
evaluation/num steps total                           1.045e+06
evaluation/num paths total                        1045
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.45597
evaluation/Rewards Std                               1.11809
evaluation/Rewards Max                               6.85939
evaluation/Rewards Min                              -1.02532
evaluation/Returns Mean                           4455.97
evaluation/Returns Std                              53.2327
evaluation/Returns Max                            4552.15
evaluation/Returns Min                            4397.8
evaluation/Actions Mean                             -0.0251406
evaluation/Actions Std                               0.83578
evaluation/Actions Max                               0.996364
evaluation/Actions Min                              -0.998337
evaluation/Num Paths                                 5
evaluation/Average Returns                        4455.97
evaluation/env_infos/final/reward_run Mean           5.52049
evaluation/env_infos/final/reward_run Std            0.88706
evaluation/env_infos/final/reward_run Max            6.6233
evaluation/env_infos/final/reward_run Min            4.04488
evaluation/env_infos/initial/reward_run Mean        -0.0815017
evaluation/env_infos/initial/reward_run Std          0.217161
evaluation/env_infos/initial/reward_run Max          0.205423
evaluation/env_infos/initial/reward_run Min         -0.442364
evaluation/env_infos/reward_run Mean                 4.87546
evaluation/env_infos/reward_run Std                  1.10132
evaluation/env_infos/reward_run Max                  7.13941
evaluation/env_infos/reward_run Min                 -0.535178
evaluation/env_infos/final/reward_ctrl Mean         -0.429725
evaluation/env_infos/final/reward_ctrl Std           0.104981
evaluation/env_infos/final/reward_ctrl Max          -0.28057
evaluation/env_infos/final/reward_ctrl Min          -0.55171
evaluation/env_infos/initial/reward_ctrl Mean       -0.0824635
evaluation/env_infos/initial/reward_ctrl Std         0.0526646
evaluation/env_infos/initial/reward_ctrl Max        -0.0449638
evaluation/env_infos/initial/reward_ctrl Min        -0.18638
evaluation/env_infos/reward_ctrl Mean               -0.419496
evaluation/env_infos/reward_ctrl Std                 0.0986106
evaluation/env_infos/reward_ctrl Max                -0.0449638
evaluation/env_infos/reward_ctrl Min                -0.581304
time/data storing (s)                                0.00739656
time/evaluation sampling (s)                         2.92775
time/exploration sampling (s)                        0.916909
time/logging (s)                                     0.0469733
time/saving (s)                                      0.0184603
time/training (s)                                   36.5187
time/epoch (s)                                      40.4362
time/total (s)                                    8768.24
Epoch                                              208
----------------------------------------------  ---------------
2020-07-08 23:33:03.985236 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 209 finished
----------------------------------------------  ---------------
replay_buffer/size                              211000
trainer/QF1 Loss                                     6.93945
trainer/QF2 Loss                                     6.73901
trainer/Policy Loss                               -258.355
trainer/Q1 Predictions Mean                        265.169
trainer/Q1 Predictions Std                          86.6024
trainer/Q1 Predictions Max                         328.135
trainer/Q1 Predictions Min                          14.8534
trainer/Q2 Predictions Mean                        264.723
trainer/Q2 Predictions Std                          86.4596
trainer/Q2 Predictions Max                         327.071
trainer/Q2 Predictions Min                          15.6694
trainer/Q Targets Mean                             265.253
trainer/Q Targets Std                               86.6323
trainer/Q Targets Max                              329.883
trainer/Q Targets Min                               15.0852
trainer/Log Pis Mean                                 6.38083
trainer/Log Pis Std                                  4.78534
trainer/Log Pis Max                                 15.9909
trainer/Log Pis Min                                 -4.21336
trainer/Policy mu Mean                              -0.0129135
trainer/Policy mu Std                                1.538
trainer/Policy mu Max                                3.43982
trainer/Policy mu Min                               -3.86117
trainer/Policy log std Mean                         -0.806135
trainer/Policy log std Std                           0.345553
trainer/Policy log std Max                          -0.0335793
trainer/Policy log std Min                          -2.61616
trainer/Alpha                                        0.11142
trainer/Alpha Loss                                   0.835752
exploration/num steps total                     211000
exploration/num paths total                        211
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.37386
exploration/Rewards Std                              1.08127
exploration/Rewards Max                              6.8226
exploration/Rewards Min                             -0.781737
exploration/Returns Mean                          4373.86
exploration/Returns Std                              0
exploration/Returns Max                           4373.86
exploration/Returns Min                           4373.86
exploration/Actions Mean                            -0.00461402
exploration/Actions Std                              0.804317
exploration/Actions Max                              0.999653
exploration/Actions Min                             -0.998874
exploration/Num Paths                                1
exploration/Average Returns                       4373.86
exploration/env_infos/final/reward_run Mean          6.38084
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.38084
exploration/env_infos/final/reward_run Min           6.38084
exploration/env_infos/initial/reward_run Mean       -0.285479
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.285479
exploration/env_infos/initial/reward_run Min        -0.285479
exploration/env_infos/reward_run Mean                4.76202
exploration/env_infos/reward_run Std                 1.06442
exploration/env_infos/reward_run Max                 7.04383
exploration/env_infos/reward_run Min                -0.317283
exploration/env_infos/final/reward_ctrl Mean        -0.447069
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.447069
exploration/env_infos/final/reward_ctrl Min         -0.447069
exploration/env_infos/initial/reward_ctrl Mean      -0.217659
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.217659
exploration/env_infos/initial/reward_ctrl Min       -0.217659
exploration/env_infos/reward_ctrl Mean              -0.388168
exploration/env_infos/reward_ctrl Std                0.105247
exploration/env_infos/reward_ctrl Max               -0.0878625
exploration/env_infos/reward_ctrl Min               -0.584421
evaluation/num steps total                           1.05e+06
evaluation/num paths total                        1050
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.62524
evaluation/Rewards Std                               1.09283
evaluation/Rewards Max                               6.78751
evaluation/Rewards Min                              -0.94854
evaluation/Returns Mean                           4625.24
evaluation/Returns Std                              57.0435
evaluation/Returns Max                            4726.67
evaluation/Returns Min                            4555.2
evaluation/Actions Mean                             -0.0210846
evaluation/Actions Std                               0.821437
evaluation/Actions Max                               0.997722
evaluation/Actions Min                              -0.998212
evaluation/Num Paths                                 5
evaluation/Average Returns                        4625.24
evaluation/env_infos/final/reward_run Mean           5.61031
evaluation/env_infos/final/reward_run Std            0.326114
evaluation/env_infos/final/reward_run Max            6.08452
evaluation/env_infos/final/reward_run Min            5.11444
evaluation/env_infos/initial/reward_run Mean        -0.0289579
evaluation/env_infos/initial/reward_run Std          0.167687
evaluation/env_infos/initial/reward_run Max          0.185152
evaluation/env_infos/initial/reward_run Min         -0.270947
evaluation/env_infos/reward_run Mean                 5.03036
evaluation/env_infos/reward_run Std                  1.07594
evaluation/env_infos/reward_run Max                  7.09114
evaluation/env_infos/reward_run Min                 -0.456905
evaluation/env_infos/final/reward_ctrl Mean         -0.397203
evaluation/env_infos/final/reward_ctrl Std           0.0679844
evaluation/env_infos/final/reward_ctrl Max          -0.33077
evaluation/env_infos/final/reward_ctrl Min          -0.508478
evaluation/env_infos/initial/reward_ctrl Mean       -0.0789512
evaluation/env_infos/initial/reward_ctrl Std         0.0727598
evaluation/env_infos/initial/reward_ctrl Max        -0.0220681
evaluation/env_infos/initial/reward_ctrl Min        -0.20943
evaluation/env_infos/reward_ctrl Mean               -0.405122
evaluation/env_infos/reward_ctrl Std                 0.104624
evaluation/env_infos/reward_ctrl Max                -0.0220681
evaluation/env_infos/reward_ctrl Min                -0.577871
time/data storing (s)                                0.00680429
time/evaluation sampling (s)                         3.02462
time/exploration sampling (s)                        0.851639
time/logging (s)                                     0.0422092
time/saving (s)                                      0.0167511
time/training (s)                                   39.7393
time/epoch (s)                                      43.6813
time/total (s)                                    8811.94
Epoch                                              209
----------------------------------------------  ---------------
2020-07-08 23:33:53.519514 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 210 finished
----------------------------------------------  ---------------
replay_buffer/size                              212000
trainer/QF1 Loss                                     6.37002
trainer/QF2 Loss                                     7.28086
trainer/Policy Loss                               -256.305
trainer/Q1 Predictions Mean                        262.313
trainer/Q1 Predictions Std                          91.8987
trainer/Q1 Predictions Max                         333.879
trainer/Q1 Predictions Min                          17.627
trainer/Q2 Predictions Mean                        262.337
trainer/Q2 Predictions Std                          91.8476
trainer/Q2 Predictions Max                         331.256
trainer/Q2 Predictions Min                          18.0711
trainer/Q Targets Mean                             262.814
trainer/Q Targets Std                               91.9798
trainer/Q Targets Max                              333.621
trainer/Q Targets Min                               16.1645
trainer/Log Pis Mean                                 6.01022
trainer/Log Pis Std                                  5.02329
trainer/Log Pis Max                                 19.2621
trainer/Log Pis Min                                 -5.13231
trainer/Policy mu Mean                               0.0694367
trainer/Policy mu Std                                1.47344
trainer/Policy mu Max                                3.61336
trainer/Policy mu Min                               -3.4482
trainer/Policy log std Mean                         -0.83591
trainer/Policy log std Std                           0.365007
trainer/Policy log std Max                          -0.0326743
trainer/Policy log std Min                          -2.87662
trainer/Alpha                                        0.10876
trainer/Alpha Loss                                   0.0226836
exploration/num steps total                     212000
exploration/num paths total                        212
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.58042
exploration/Rewards Std                              1.07533
exploration/Rewards Max                              6.6221
exploration/Rewards Min                             -0.67658
exploration/Returns Mean                          4580.42
exploration/Returns Std                              0
exploration/Returns Max                           4580.42
exploration/Returns Min                           4580.42
exploration/Actions Mean                             0.0264747
exploration/Actions Std                              0.799453
exploration/Actions Max                              0.999209
exploration/Actions Min                             -0.999036
exploration/Num Paths                                1
exploration/Average Returns                       4580.42
exploration/env_infos/final/reward_run Mean          4.88654
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.88654
exploration/env_infos/final/reward_run Min           4.88654
exploration/env_infos/initial/reward_run Mean        0.0504491
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0504491
exploration/env_infos/initial/reward_run Min         0.0504491
exploration/env_infos/reward_run Mean                4.96432
exploration/env_infos/reward_run Std                 1.05544
exploration/env_infos/reward_run Max                 6.95953
exploration/env_infos/reward_run Min                -0.206487
exploration/env_infos/final/reward_ctrl Mean        -0.206235
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.206235
exploration/env_infos/final/reward_ctrl Min         -0.206235
exploration/env_infos/initial/reward_ctrl Mean      -0.08763
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.08763
exploration/env_infos/initial/reward_ctrl Min       -0.08763
exploration/env_infos/reward_ctrl Mean              -0.383896
exploration/env_infos/reward_ctrl Std                0.100509
exploration/env_infos/reward_ctrl Max               -0.0842038
exploration/env_infos/reward_ctrl Min               -0.576696
evaluation/num steps total                           1.055e+06
evaluation/num paths total                        1055
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.62054
evaluation/Rewards Std                               1.10587
evaluation/Rewards Max                               6.94535
evaluation/Rewards Min                              -0.932094
evaluation/Returns Mean                           4620.54
evaluation/Returns Std                              50.9249
evaluation/Returns Max                            4667.21
evaluation/Returns Min                            4524.32
evaluation/Actions Mean                              0.009885
evaluation/Actions Std                               0.810451
evaluation/Actions Max                               0.99833
evaluation/Actions Min                              -0.998735
evaluation/Num Paths                                 5
evaluation/Average Returns                        4620.54
evaluation/env_infos/final/reward_run Mean           5.24572
evaluation/env_infos/final/reward_run Std            0.73689
evaluation/env_infos/final/reward_run Max            6.32133
evaluation/env_infos/final/reward_run Min            4.56502
evaluation/env_infos/initial/reward_run Mean         0.184558
evaluation/env_infos/initial/reward_run Std          0.199337
evaluation/env_infos/initial/reward_run Max          0.400762
evaluation/env_infos/initial/reward_run Min         -0.096843
evaluation/env_infos/reward_run Mean                 5.01469
evaluation/env_infos/reward_run Std                  1.08376
evaluation/env_infos/reward_run Max                  7.33599
evaluation/env_infos/reward_run Min                 -0.454589
evaluation/env_infos/final/reward_ctrl Mean         -0.406785
evaluation/env_infos/final/reward_ctrl Std           0.0865613
evaluation/env_infos/final/reward_ctrl Max          -0.271042
evaluation/env_infos/final/reward_ctrl Min          -0.527991
evaluation/env_infos/initial/reward_ctrl Mean       -0.103241
evaluation/env_infos/initial/reward_ctrl Std         0.00878305
evaluation/env_infos/initial/reward_ctrl Max        -0.0891852
evaluation/env_infos/initial/reward_ctrl Min        -0.112908
evaluation/env_infos/reward_ctrl Mean               -0.394157
evaluation/env_infos/reward_ctrl Std                 0.106747
evaluation/env_infos/reward_ctrl Max                -0.0417205
evaluation/env_infos/reward_ctrl Min                -0.578381
time/data storing (s)                                0.00785285
time/evaluation sampling (s)                         5.07321
time/exploration sampling (s)                        0.87602
time/logging (s)                                     0.0422021
time/saving (s)                                      0.0165696
time/training (s)                                   43.4989
time/epoch (s)                                      49.5147
time/total (s)                                    8861.46
Epoch                                              210
----------------------------------------------  ---------------
2020-07-08 23:34:41.355976 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 211 finished
----------------------------------------------  ---------------
replay_buffer/size                              213000
trainer/QF1 Loss                                     6.7854
trainer/QF2 Loss                                     6.53388
trainer/Policy Loss                               -239.294
trainer/Q1 Predictions Mean                        244.193
trainer/Q1 Predictions Std                         107.412
trainer/Q1 Predictions Max                         329.149
trainer/Q1 Predictions Min                          17.3011
trainer/Q2 Predictions Mean                        244.277
trainer/Q2 Predictions Std                         107.237
trainer/Q2 Predictions Max                         331.956
trainer/Q2 Predictions Min                          17.6337
trainer/Q Targets Mean                             244.585
trainer/Q Targets Std                              107.405
trainer/Q Targets Max                              331.575
trainer/Q Targets Min                               16.9758
trainer/Log Pis Mean                                 5.01106
trainer/Log Pis Std                                  4.63536
trainer/Log Pis Max                                 14.8376
trainer/Log Pis Min                                 -7.22998
trainer/Policy mu Mean                               0.0441137
trainer/Policy mu Std                                1.42058
trainer/Policy mu Max                                3.64686
trainer/Policy mu Min                               -3.18596
trainer/Policy log std Mean                         -0.795546
trainer/Policy log std Std                           0.35592
trainer/Policy log std Max                           0.00537546
trainer/Policy log std Min                          -2.56986
trainer/Alpha                                        0.109429
trainer/Alpha Loss                                  -2.18779
exploration/num steps total                     213000
exploration/num paths total                        213
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.33731
exploration/Rewards Std                              1.08405
exploration/Rewards Max                              6.41537
exploration/Rewards Min                             -0.749025
exploration/Returns Mean                          4337.31
exploration/Returns Std                              0
exploration/Returns Max                           4337.31
exploration/Returns Min                           4337.31
exploration/Actions Mean                             0.0217331
exploration/Actions Std                              0.795732
exploration/Actions Max                              0.999688
exploration/Actions Min                             -0.999268
exploration/Num Paths                                1
exploration/Average Returns                       4337.31
exploration/env_infos/final/reward_run Mean          3.8017
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.8017
exploration/env_infos/final/reward_run Min           3.8017
exploration/env_infos/initial/reward_run Mean        0.13323
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.13323
exploration/env_infos/initial/reward_run Min         0.13323
exploration/env_infos/reward_run Mean                4.7175
exploration/env_infos/reward_run Std                 1.06603
exploration/env_infos/reward_run Max                 6.73393
exploration/env_infos/reward_run Min                -0.236837
exploration/env_infos/final/reward_ctrl Mean        -0.429925
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.429925
exploration/env_infos/final/reward_ctrl Min         -0.429925
exploration/env_infos/initial/reward_ctrl Mean      -0.0454215
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0454215
exploration/env_infos/initial/reward_ctrl Min       -0.0454215
exploration/env_infos/reward_ctrl Mean              -0.380197
exploration/env_infos/reward_ctrl Std                0.0989205
exploration/env_infos/reward_ctrl Max               -0.0454215
exploration/env_infos/reward_ctrl Min               -0.573426
evaluation/num steps total                           1.06e+06
evaluation/num paths total                        1060
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.58841
evaluation/Rewards Std                               1.12733
evaluation/Rewards Max                               6.84157
evaluation/Rewards Min                              -1.07371
evaluation/Returns Mean                           4588.41
evaluation/Returns Std                              61.0456
evaluation/Returns Max                            4676.96
evaluation/Returns Min                            4487.26
evaluation/Actions Mean                              0.023037
evaluation/Actions Std                               0.808479
evaluation/Actions Max                               0.999073
evaluation/Actions Min                              -0.996516
evaluation/Num Paths                                 5
evaluation/Average Returns                        4588.41
evaluation/env_infos/final/reward_run Mean           5.24594
evaluation/env_infos/final/reward_run Std            1.10333
evaluation/env_infos/final/reward_run Max            6.93096
evaluation/env_infos/final/reward_run Min            3.79052
evaluation/env_infos/initial/reward_run Mean         0.142592
evaluation/env_infos/initial/reward_run Std          0.284806
evaluation/env_infos/initial/reward_run Max          0.643797
evaluation/env_infos/initial/reward_run Min         -0.137704
evaluation/env_infos/reward_run Mean                 4.98091
evaluation/env_infos/reward_run Std                  1.1043
evaluation/env_infos/reward_run Max                  7.11303
evaluation/env_infos/reward_run Min                 -0.78682
evaluation/env_infos/final/reward_ctrl Mean         -0.368532
evaluation/env_infos/final/reward_ctrl Std           0.0919833
evaluation/env_infos/final/reward_ctrl Max          -0.243592
evaluation/env_infos/final/reward_ctrl Min          -0.507112
evaluation/env_infos/initial/reward_ctrl Mean       -0.136368
evaluation/env_infos/initial/reward_ctrl Std         0.0739059
evaluation/env_infos/initial/reward_ctrl Max        -0.0678903
evaluation/env_infos/initial/reward_ctrl Min        -0.23017
evaluation/env_infos/reward_ctrl Mean               -0.392502
evaluation/env_infos/reward_ctrl Std                 0.101263
evaluation/env_infos/reward_ctrl Max                -0.0678903
evaluation/env_infos/reward_ctrl Min                -0.568263
time/data storing (s)                                0.00691112
time/evaluation sampling (s)                         2.80536
time/exploration sampling (s)                        0.72978
time/logging (s)                                     0.159073
time/saving (s)                                      0.0163721
time/training (s)                                   44.1422
time/epoch (s)                                      47.8597
time/total (s)                                    8909.41
Epoch                                              211
----------------------------------------------  ---------------
2020-07-08 23:35:23.396751 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 212 finished
----------------------------------------------  ---------------
replay_buffer/size                              214000
trainer/QF1 Loss                                     6.0523
trainer/QF2 Loss                                     7.65535
trainer/Policy Loss                               -257.027
trainer/Q1 Predictions Mean                        263.729
trainer/Q1 Predictions Std                          88.3988
trainer/Q1 Predictions Max                         336.546
trainer/Q1 Predictions Min                          17.2267
trainer/Q2 Predictions Mean                        263.293
trainer/Q2 Predictions Std                          88.2784
trainer/Q2 Predictions Max                         337.48
trainer/Q2 Predictions Min                          10.5959
trainer/Q Targets Mean                             263.262
trainer/Q Targets Std                               88.3119
trainer/Q Targets Max                              335.981
trainer/Q Targets Min                               17.7595
trainer/Log Pis Mean                                 6.43099
trainer/Log Pis Std                                  5.38337
trainer/Log Pis Max                                 23.1272
trainer/Log Pis Min                                 -6.15365
trainer/Policy mu Mean                              -0.0531096
trainer/Policy mu Std                                1.55662
trainer/Policy mu Max                                5.81783
trainer/Policy mu Min                               -4.596
trainer/Policy log std Mean                         -0.816825
trainer/Policy log std Std                           0.335929
trainer/Policy log std Max                           0.39319
trainer/Policy log std Min                          -2.53387
trainer/Alpha                                        0.112919
trainer/Alpha Loss                                   0.940051
exploration/num steps total                     214000
exploration/num paths total                        214
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.41263
exploration/Rewards Std                              1.086
exploration/Rewards Max                              6.69085
exploration/Rewards Min                             -0.848833
exploration/Returns Mean                          4412.63
exploration/Returns Std                              0
exploration/Returns Max                           4412.63
exploration/Returns Min                           4412.63
exploration/Actions Mean                            -0.0220068
exploration/Actions Std                              0.808511
exploration/Actions Max                              0.999708
exploration/Actions Min                             -0.999598
exploration/Num Paths                                1
exploration/Average Returns                       4412.63
exploration/env_infos/final/reward_run Mean          4.26042
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.26042
exploration/env_infos/final/reward_run Min           4.26042
exploration/env_infos/initial/reward_run Mean       -0.293392
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.293392
exploration/env_infos/initial/reward_run Min        -0.293392
exploration/env_infos/reward_run Mean                4.80514
exploration/env_infos/reward_run Std                 1.06495
exploration/env_infos/reward_run Max                 7.15024
exploration/env_infos/reward_run Min                -0.578331
exploration/env_infos/final/reward_ctrl Mean        -0.454762
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.454762
exploration/env_infos/final/reward_ctrl Min         -0.454762
exploration/env_infos/initial/reward_ctrl Mean      -0.188099
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.188099
exploration/env_infos/initial/reward_ctrl Min       -0.188099
exploration/env_infos/reward_ctrl Mean              -0.392504
exploration/env_infos/reward_ctrl Std                0.102801
exploration/env_infos/reward_ctrl Max               -0.0988587
exploration/env_infos/reward_ctrl Min               -0.585883
evaluation/num steps total                           1.065e+06
evaluation/num paths total                        1065
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.57742
evaluation/Rewards Std                               1.11856
evaluation/Rewards Max                               6.8191
evaluation/Rewards Min                              -0.937253
evaluation/Returns Mean                           4577.42
evaluation/Returns Std                              56.8284
evaluation/Returns Max                            4641.92
evaluation/Returns Min                            4482.72
evaluation/Actions Mean                             -0.0374489
evaluation/Actions Std                               0.825967
evaluation/Actions Max                               0.996563
evaluation/Actions Min                              -0.997669
evaluation/Num Paths                                 5
evaluation/Average Returns                        4577.42
evaluation/env_infos/final/reward_run Mean           4.11544
evaluation/env_infos/final/reward_run Std            0.408914
evaluation/env_infos/final/reward_run Max            4.66965
evaluation/env_infos/final/reward_run Min            3.43177
evaluation/env_infos/initial/reward_run Mean        -0.0978103
evaluation/env_infos/initial/reward_run Std          0.115011
evaluation/env_infos/initial/reward_run Max          0.0923801
evaluation/env_infos/initial/reward_run Min         -0.238335
evaluation/env_infos/reward_run Mean                 4.98759
evaluation/env_infos/reward_run Std                  1.09856
evaluation/env_infos/reward_run Max                  7.13954
evaluation/env_infos/reward_run Min                 -0.372928
evaluation/env_infos/final/reward_ctrl Mean         -0.482169
evaluation/env_infos/final/reward_ctrl Std           0.0632274
evaluation/env_infos/final/reward_ctrl Max          -0.371558
evaluation/env_infos/final/reward_ctrl Min          -0.561459
evaluation/env_infos/initial/reward_ctrl Mean       -0.0713858
evaluation/env_infos/initial/reward_ctrl Std         0.0393458
evaluation/env_infos/initial/reward_ctrl Max        -0.0351522
evaluation/env_infos/initial/reward_ctrl Min        -0.133136
evaluation/env_infos/reward_ctrl Mean               -0.410174
evaluation/env_infos/reward_ctrl Std                 0.101586
evaluation/env_infos/reward_ctrl Max                -0.00876301
evaluation/env_infos/reward_ctrl Min                -0.57913
time/data storing (s)                                0.00831254
time/evaluation sampling (s)                         3.81961
time/exploration sampling (s)                        1.03334
time/logging (s)                                     0.0479307
time/saving (s)                                      0.0173132
time/training (s)                                   36.6659
time/epoch (s)                                      41.5924
time/total (s)                                    8951.33
Epoch                                              212
----------------------------------------------  ---------------
2020-07-08 23:36:01.040533 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 213 finished
----------------------------------------------  ---------------
replay_buffer/size                              215000
trainer/QF1 Loss                                     6.94632
trainer/QF2 Loss                                     6.33323
trainer/Policy Loss                               -263.215
trainer/Q1 Predictions Mean                        269.591
trainer/Q1 Predictions Std                          83.2178
trainer/Q1 Predictions Max                         332.656
trainer/Q1 Predictions Min                          17.5075
trainer/Q2 Predictions Mean                        269.229
trainer/Q2 Predictions Std                          83.066
trainer/Q2 Predictions Max                         333.386
trainer/Q2 Predictions Min                          18.3395
trainer/Q Targets Mean                             269.117
trainer/Q Targets Std                               83.1243
trainer/Q Targets Max                              332.105
trainer/Q Targets Min                               17.7044
trainer/Log Pis Mean                                 6.1902
trainer/Log Pis Std                                  5.04165
trainer/Log Pis Max                                 18.7564
trainer/Log Pis Min                                 -8.2654
trainer/Policy mu Mean                               0.0270509
trainer/Policy mu Std                                1.50291
trainer/Policy mu Max                                3.55524
trainer/Policy mu Min                               -3.42471
trainer/Policy log std Mean                         -0.825025
trainer/Policy log std Std                           0.341016
trainer/Policy log std Max                           0.0202116
trainer/Policy log std Min                          -2.63047
trainer/Alpha                                        0.115031
trainer/Alpha Loss                                   0.411297
exploration/num steps total                     215000
exploration/num paths total                        215
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.42735
exploration/Rewards Std                              1.05559
exploration/Rewards Max                              6.73547
exploration/Rewards Min                             -0.563509
exploration/Returns Mean                          4427.35
exploration/Returns Std                              0
exploration/Returns Max                           4427.35
exploration/Returns Min                           4427.35
exploration/Actions Mean                            -0.00135965
exploration/Actions Std                              0.79543
exploration/Actions Max                              0.999516
exploration/Actions Min                             -0.999786
exploration/Num Paths                                1
exploration/Average Returns                       4427.35
exploration/env_infos/final/reward_run Mean          6.30659
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.30659
exploration/env_infos/final/reward_run Min           6.30659
exploration/env_infos/initial/reward_run Mean       -0.0929353
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0929353
exploration/env_infos/initial/reward_run Min        -0.0929353
exploration/env_infos/reward_run Mean                4.80697
exploration/env_infos/reward_run Std                 1.03468
exploration/env_infos/reward_run Max                 6.98152
exploration/env_infos/reward_run Min                -0.121121
exploration/env_infos/final/reward_ctrl Mean        -0.26586
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.26586
exploration/env_infos/final/reward_ctrl Min         -0.26586
exploration/env_infos/initial/reward_ctrl Mean      -0.128724
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.128724
exploration/env_infos/initial/reward_ctrl Min       -0.128724
exploration/env_infos/reward_ctrl Mean              -0.379627
exploration/env_infos/reward_ctrl Std                0.0994931
exploration/env_infos/reward_ctrl Max               -0.09333
exploration/env_infos/reward_ctrl Min               -0.574567
evaluation/num steps total                           1.07e+06
evaluation/num paths total                        1070
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.52238
evaluation/Rewards Std                               1.09613
evaluation/Rewards Max                               6.82379
evaluation/Rewards Min                              -1.16557
evaluation/Returns Mean                           4522.38
evaluation/Returns Std                              72.7887
evaluation/Returns Max                            4598.33
evaluation/Returns Min                            4422.82
evaluation/Actions Mean                             -0.00854607
evaluation/Actions Std                               0.803366
evaluation/Actions Max                               0.998665
evaluation/Actions Min                              -0.998183
evaluation/Num Paths                                 5
evaluation/Average Returns                        4522.38
evaluation/env_infos/final/reward_run Mean           4.70506
evaluation/env_infos/final/reward_run Std            0.642006
evaluation/env_infos/final/reward_run Max            5.61035
evaluation/env_infos/final/reward_run Min            3.9694
evaluation/env_infos/initial/reward_run Mean        -0.011643
evaluation/env_infos/initial/reward_run Std          0.196592
evaluation/env_infos/initial/reward_run Max          0.271466
evaluation/env_infos/initial/reward_run Min         -0.23785
evaluation/env_infos/reward_run Mean                 4.90966
evaluation/env_infos/reward_run Std                  1.07359
evaluation/env_infos/reward_run Max                  7.1639
evaluation/env_infos/reward_run Min                 -0.636417
evaluation/env_infos/final/reward_ctrl Mean         -0.386117
evaluation/env_infos/final/reward_ctrl Std           0.104829
evaluation/env_infos/final/reward_ctrl Max          -0.250219
evaluation/env_infos/final/reward_ctrl Min          -0.510368
evaluation/env_infos/initial/reward_ctrl Mean       -0.0775889
evaluation/env_infos/initial/reward_ctrl Std         0.0260233
evaluation/env_infos/initial/reward_ctrl Max        -0.0392294
evaluation/env_infos/initial/reward_ctrl Min        -0.109784
evaluation/env_infos/reward_ctrl Mean               -0.387282
evaluation/env_infos/reward_ctrl Std                 0.103907
evaluation/env_infos/reward_ctrl Max                -0.0392294
evaluation/env_infos/reward_ctrl Min                -0.574008
time/data storing (s)                                0.0086288
time/evaluation sampling (s)                         2.83558
time/exploration sampling (s)                        0.668223
time/logging (s)                                     0.0407324
time/saving (s)                                      0.0202138
time/training (s)                                   33.9659
time/epoch (s)                                      37.5392
time/total (s)                                    8988.96
Epoch                                              213
----------------------------------------------  ---------------
2020-07-08 23:36:42.474503 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 214 finished
----------------------------------------------  ----------------
replay_buffer/size                              216000
trainer/QF1 Loss                                     5.48473
trainer/QF2 Loss                                     7.49549
trainer/Policy Loss                               -262.361
trainer/Q1 Predictions Mean                        268.215
trainer/Q1 Predictions Std                          85.9493
trainer/Q1 Predictions Max                         330.326
trainer/Q1 Predictions Min                          18.7568
trainer/Q2 Predictions Mean                        268.164
trainer/Q2 Predictions Std                          85.7705
trainer/Q2 Predictions Max                         328.202
trainer/Q2 Predictions Min                          18.5009
trainer/Q Targets Mean                             268.529
trainer/Q Targets Std                               86.0408
trainer/Q Targets Max                              330.637
trainer/Q Targets Min                               18.6452
trainer/Log Pis Mean                                 6.15237
trainer/Log Pis Std                                  4.66239
trainer/Log Pis Max                                 17.7489
trainer/Log Pis Min                                 -5.48668
trainer/Policy mu Mean                               0.128002
trainer/Policy mu Std                                1.5128
trainer/Policy mu Max                                4.35181
trainer/Policy mu Min                               -3.59988
trainer/Policy log std Mean                         -0.829167
trainer/Policy log std Std                           0.348573
trainer/Policy log std Max                           0.085357
trainer/Policy log std Min                          -2.70839
trainer/Alpha                                        0.112636
trainer/Alpha Loss                                   0.332726
exploration/num steps total                     216000
exploration/num paths total                        216
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.30045
exploration/Rewards Std                              1.10497
exploration/Rewards Max                              6.4466
exploration/Rewards Min                             -0.691918
exploration/Returns Mean                          4300.45
exploration/Returns Std                              0
exploration/Returns Max                           4300.45
exploration/Returns Min                           4300.45
exploration/Actions Mean                             0.00724867
exploration/Actions Std                              0.819809
exploration/Actions Max                              0.999619
exploration/Actions Min                             -0.999328
exploration/Num Paths                                1
exploration/Average Returns                       4300.45
exploration/env_infos/final/reward_run Mean          6.32928
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.32928
exploration/env_infos/final/reward_run Min           6.32928
exploration/env_infos/initial/reward_run Mean       -0.00807229
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.00807229
exploration/env_infos/initial/reward_run Min        -0.00807229
exploration/env_infos/reward_run Mean                4.70373
exploration/env_infos/reward_run Std                 1.09183
exploration/env_infos/reward_run Max                 6.77755
exploration/env_infos/reward_run Min                -0.181464
exploration/env_infos/final/reward_ctrl Mean        -0.431014
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.431014
exploration/env_infos/final/reward_ctrl Min         -0.431014
exploration/env_infos/initial/reward_ctrl Mean      -0.113481
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.113481
exploration/env_infos/initial/reward_ctrl Min       -0.113481
exploration/env_infos/reward_ctrl Mean              -0.403284
exploration/env_infos/reward_ctrl Std                0.0969346
exploration/env_infos/reward_ctrl Max               -0.113481
exploration/env_infos/reward_ctrl Min               -0.570887
evaluation/num steps total                           1.075e+06
evaluation/num paths total                        1075
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.651
evaluation/Rewards Std                               1.13455
evaluation/Rewards Max                               6.96941
evaluation/Rewards Min                              -0.765211
evaluation/Returns Mean                           4651
evaluation/Returns Std                              66.2402
evaluation/Returns Max                            4740.91
evaluation/Returns Min                            4553.04
evaluation/Actions Mean                             -0.000971111
evaluation/Actions Std                               0.833959
evaluation/Actions Max                               0.998858
evaluation/Actions Min                              -0.997751
evaluation/Num Paths                                 5
evaluation/Average Returns                        4651
evaluation/env_infos/final/reward_run Mean           4.96666
evaluation/env_infos/final/reward_run Std            1.0129
evaluation/env_infos/final/reward_run Max            6.51753
evaluation/env_infos/final/reward_run Min            3.90299
evaluation/env_infos/initial/reward_run Mean        -0.047532
evaluation/env_infos/initial/reward_run Std          0.114484
evaluation/env_infos/initial/reward_run Max          0.123837
evaluation/env_infos/initial/reward_run Min         -0.198456
evaluation/env_infos/reward_run Mean                 5.06829
evaluation/env_infos/reward_run Std                  1.11521
evaluation/env_infos/reward_run Max                  7.2716
evaluation/env_infos/reward_run Min                 -0.277364
evaluation/env_infos/final/reward_ctrl Mean         -0.443269
evaluation/env_infos/final/reward_ctrl Std           0.0960601
evaluation/env_infos/final/reward_ctrl Max          -0.324339
evaluation/env_infos/final/reward_ctrl Min          -0.559453
evaluation/env_infos/initial/reward_ctrl Mean       -0.0857939
evaluation/env_infos/initial/reward_ctrl Std         0.0255936
evaluation/env_infos/initial/reward_ctrl Max        -0.037332
evaluation/env_infos/initial/reward_ctrl Min        -0.113531
evaluation/env_infos/reward_ctrl Mean               -0.417294
evaluation/env_infos/reward_ctrl Std                 0.0993069
evaluation/env_infos/reward_ctrl Max                -0.037332
evaluation/env_infos/reward_ctrl Min                -0.575001
time/data storing (s)                                0.00670645
time/evaluation sampling (s)                         2.50625
time/exploration sampling (s)                        0.633857
time/logging (s)                                     0.0447477
time/saving (s)                                      0.0160764
time/training (s)                                   38.2112
time/epoch (s)                                      41.4188
time/total (s)                                    9030.39
Epoch                                              214
----------------------------------------------  ----------------
2020-07-08 23:37:28.285712 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 215 finished
----------------------------------------------  ---------------
replay_buffer/size                              217000
trainer/QF1 Loss                                     9.04572
trainer/QF2 Loss                                     6.95618
trainer/Policy Loss                               -260.474
trainer/Q1 Predictions Mean                        266.372
trainer/Q1 Predictions Std                          91.5304
trainer/Q1 Predictions Max                         329.167
trainer/Q1 Predictions Min                          17.8854
trainer/Q2 Predictions Mean                        266.653
trainer/Q2 Predictions Std                          91.4022
trainer/Q2 Predictions Max                         329.419
trainer/Q2 Predictions Min                          18.359
trainer/Q Targets Mean                             266.563
trainer/Q Targets Std                               91.4451
trainer/Q Targets Max                              329.816
trainer/Q Targets Min                               18.131
trainer/Log Pis Mean                                 6.19328
trainer/Log Pis Std                                  5.03552
trainer/Log Pis Max                                 17.8939
trainer/Log Pis Min                                 -8.01204
trainer/Policy mu Mean                               0.0070529
trainer/Policy mu Std                                1.52059
trainer/Policy mu Max                                3.43612
trainer/Policy mu Min                               -3.41768
trainer/Policy log std Mean                         -0.813252
trainer/Policy log std Std                           0.33826
trainer/Policy log std Max                           0.00358796
trainer/Policy log std Min                          -2.9123
trainer/Alpha                                        0.112203
trainer/Alpha Loss                                   0.422801
exploration/num steps total                     217000
exploration/num paths total                        217
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.48191
exploration/Rewards Std                              1.08069
exploration/Rewards Max                              6.72183
exploration/Rewards Min                             -0.598373
exploration/Returns Mean                          4481.91
exploration/Returns Std                              0
exploration/Returns Max                           4481.91
exploration/Returns Min                           4481.91
exploration/Actions Mean                            -0.0164356
exploration/Actions Std                              0.818412
exploration/Actions Max                              0.999564
exploration/Actions Min                             -0.999745
exploration/Num Paths                                1
exploration/Average Returns                       4481.91
exploration/env_infos/final/reward_run Mean          5.36841
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.36841
exploration/env_infos/final/reward_run Min           5.36841
exploration/env_infos/initial/reward_run Mean       -0.161462
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.161462
exploration/env_infos/initial/reward_run Min        -0.161462
exploration/env_infos/reward_run Mean                4.88395
exploration/env_infos/reward_run Std                 1.06319
exploration/env_infos/reward_run Max                 7.11877
exploration/env_infos/reward_run Min                -0.161462
exploration/env_infos/final/reward_ctrl Mean        -0.410113
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.410113
exploration/env_infos/final/reward_ctrl Min         -0.410113
exploration/env_infos/initial/reward_ctrl Mean      -0.229618
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.229618
exploration/env_infos/initial/reward_ctrl Min       -0.229618
exploration/env_infos/reward_ctrl Mean              -0.402041
exploration/env_infos/reward_ctrl Std                0.0999462
exploration/env_infos/reward_ctrl Max               -0.109197
exploration/env_infos/reward_ctrl Min               -0.578474
evaluation/num steps total                           1.08e+06
evaluation/num paths total                        1080
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.60716
evaluation/Rewards Std                               1.09353
evaluation/Rewards Max                               6.85303
evaluation/Rewards Min                              -0.711192
evaluation/Returns Mean                           4607.16
evaluation/Returns Std                              72.2105
evaluation/Returns Max                            4712.55
evaluation/Returns Min                            4502.88
evaluation/Actions Mean                             -0.0107254
evaluation/Actions Std                               0.824793
evaluation/Actions Max                               0.997575
evaluation/Actions Min                              -0.999014
evaluation/Num Paths                                 5
evaluation/Average Returns                        4607.16
evaluation/env_infos/final/reward_run Mean           4.95019
evaluation/env_infos/final/reward_run Std            0.916309
evaluation/env_infos/final/reward_run Max            6.39211
evaluation/env_infos/final/reward_run Min            3.94292
evaluation/env_infos/initial/reward_run Mean        -0.0622639
evaluation/env_infos/initial/reward_run Std          0.207777
evaluation/env_infos/initial/reward_run Max          0.271329
evaluation/env_infos/initial/reward_run Min         -0.382806
evaluation/env_infos/reward_run Mean                 5.0154
evaluation/env_infos/reward_run Std                  1.06969
evaluation/env_infos/reward_run Max                  7.23748
evaluation/env_infos/reward_run Min                 -0.382806
evaluation/env_infos/final/reward_ctrl Mean         -0.430118
evaluation/env_infos/final/reward_ctrl Std           0.0816733
evaluation/env_infos/final/reward_ctrl Max          -0.305314
evaluation/env_infos/final/reward_ctrl Min          -0.531344
evaluation/env_infos/initial/reward_ctrl Mean       -0.157434
evaluation/env_infos/initial/reward_ctrl Std         0.0608296
evaluation/env_infos/initial/reward_ctrl Max        -0.0508443
evaluation/env_infos/initial/reward_ctrl Min        -0.223045
evaluation/env_infos/reward_ctrl Mean               -0.408239
evaluation/env_infos/reward_ctrl Std                 0.102171
evaluation/env_infos/reward_ctrl Max                -0.0508443
evaluation/env_infos/reward_ctrl Min                -0.579907
time/data storing (s)                                0.0069882
time/evaluation sampling (s)                         3.06057
time/exploration sampling (s)                        0.78091
time/logging (s)                                     0.0581777
time/saving (s)                                      0.106221
time/training (s)                                   41.5753
time/epoch (s)                                      45.5881
time/total (s)                                    9076.21
Epoch                                              215
----------------------------------------------  ---------------
2020-07-08 23:38:15.594665 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 216 finished
----------------------------------------------  ---------------
replay_buffer/size                              218000
trainer/QF1 Loss                                     6.59599
trainer/QF2 Loss                                     5.99918
trainer/Policy Loss                               -261.366
trainer/Q1 Predictions Mean                        267.313
trainer/Q1 Predictions Std                          88.4015
trainer/Q1 Predictions Max                         333.973
trainer/Q1 Predictions Min                          16.4708
trainer/Q2 Predictions Mean                        267.245
trainer/Q2 Predictions Std                          88.4424
trainer/Q2 Predictions Max                         331.981
trainer/Q2 Predictions Min                          16.9934
trainer/Q Targets Mean                             267.771
trainer/Q Targets Std                               88.5254
trainer/Q Targets Max                              335.028
trainer/Q Targets Min                               16.2465
trainer/Log Pis Mean                                 5.98684
trainer/Log Pis Std                                  4.82226
trainer/Log Pis Max                                 16.9536
trainer/Log Pis Min                                 -6.7828
trainer/Policy mu Mean                              -0.00739915
trainer/Policy mu Std                                1.4968
trainer/Policy mu Max                                3.91596
trainer/Policy mu Min                               -3.56133
trainer/Policy log std Mean                         -0.824576
trainer/Policy log std Std                           0.359959
trainer/Policy log std Max                          -0.036322
trainer/Policy log std Min                          -2.64274
trainer/Alpha                                        0.11085
trainer/Alpha Loss                                  -0.0289432
exploration/num steps total                     218000
exploration/num paths total                        218
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.53392
exploration/Rewards Std                              1.08001
exploration/Rewards Max                              6.84248
exploration/Rewards Min                             -0.373131
exploration/Returns Mean                          4533.92
exploration/Returns Std                              0
exploration/Returns Max                           4533.92
exploration/Returns Min                           4533.92
exploration/Actions Mean                             0.017327
exploration/Actions Std                              0.797617
exploration/Actions Max                              0.999577
exploration/Actions Min                             -0.999622
exploration/Num Paths                                1
exploration/Average Returns                       4533.92
exploration/env_infos/final/reward_run Mean          5.14235
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.14235
exploration/env_infos/final/reward_run Min           5.14235
exploration/env_infos/initial/reward_run Mean        0.0689187
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0689187
exploration/env_infos/initial/reward_run Min         0.0689187
exploration/env_infos/reward_run Mean                4.91581
exploration/env_infos/reward_run Std                 1.05716
exploration/env_infos/reward_run Max                 7.14131
exploration/env_infos/reward_run Min                -0.170534
exploration/env_infos/final/reward_ctrl Mean        -0.318406
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.318406
exploration/env_infos/final/reward_ctrl Min         -0.318406
exploration/env_infos/initial/reward_ctrl Mean      -0.0465504
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0465504
exploration/env_infos/initial/reward_ctrl Min       -0.0465504
exploration/env_infos/reward_ctrl Mean              -0.381896
exploration/env_infos/reward_ctrl Std                0.0996749
exploration/env_infos/reward_ctrl Max               -0.0465504
exploration/env_infos/reward_ctrl Min               -0.581002
evaluation/num steps total                           1.085e+06
evaluation/num paths total                        1085
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.62351
evaluation/Rewards Std                               1.12971
evaluation/Rewards Max                               6.83306
evaluation/Rewards Min                              -0.846607
evaluation/Returns Mean                           4623.51
evaluation/Returns Std                              45.3376
evaluation/Returns Max                            4695.66
evaluation/Returns Min                            4576.23
evaluation/Actions Mean                              0.0119811
evaluation/Actions Std                               0.803369
evaluation/Actions Max                               0.997392
evaluation/Actions Min                              -0.996537
evaluation/Num Paths                                 5
evaluation/Average Returns                        4623.51
evaluation/env_infos/final/reward_run Mean           4.97536
evaluation/env_infos/final/reward_run Std            0.853492
evaluation/env_infos/final/reward_run Max            6.27925
evaluation/env_infos/final/reward_run Min            4.05001
evaluation/env_infos/initial/reward_run Mean         0.0490947
evaluation/env_infos/initial/reward_run Std          0.130867
evaluation/env_infos/initial/reward_run Max          0.279784
evaluation/env_infos/initial/reward_run Min         -0.0884119
evaluation/env_infos/reward_run Mean                 5.01084
evaluation/env_infos/reward_run Std                  1.10345
evaluation/env_infos/reward_run Max                  7.22402
evaluation/env_infos/reward_run Min                 -0.347729
evaluation/env_infos/final/reward_ctrl Mean         -0.407957
evaluation/env_infos/final/reward_ctrl Std           0.0827088
evaluation/env_infos/final/reward_ctrl Max          -0.310071
evaluation/env_infos/final/reward_ctrl Min          -0.524713
evaluation/env_infos/initial/reward_ctrl Mean       -0.0765345
evaluation/env_infos/initial/reward_ctrl Std         0.0344871
evaluation/env_infos/initial/reward_ctrl Max        -0.0193189
evaluation/env_infos/initial/reward_ctrl Min        -0.115401
evaluation/env_infos/reward_ctrl Mean               -0.387327
evaluation/env_infos/reward_ctrl Std                 0.103434
evaluation/env_infos/reward_ctrl Max                -0.0193189
evaluation/env_infos/reward_ctrl Min                -0.573321
time/data storing (s)                                0.00685258
time/evaluation sampling (s)                         4.7001
time/exploration sampling (s)                        0.706964
time/logging (s)                                     0.0566298
time/saving (s)                                      0.0173875
time/training (s)                                   41.7946
time/epoch (s)                                      47.2825
time/total (s)                                    9123.5
Epoch                                              216
----------------------------------------------  ---------------
2020-07-08 23:39:05.691994 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 217 finished
----------------------------------------------  ----------------
replay_buffer/size                              219000
trainer/QF1 Loss                                     6.28686
trainer/QF2 Loss                                     6.3302
trainer/Policy Loss                               -265.7
trainer/Q1 Predictions Mean                        271.257
trainer/Q1 Predictions Std                          80.7842
trainer/Q1 Predictions Max                         332.848
trainer/Q1 Predictions Min                          17.332
trainer/Q2 Predictions Mean                        271.543
trainer/Q2 Predictions Std                          80.8624
trainer/Q2 Predictions Max                         333.659
trainer/Q2 Predictions Min                          17.911
trainer/Q Targets Mean                             271.448
trainer/Q Targets Std                               80.8312
trainer/Q Targets Max                              331.574
trainer/Q Targets Min                               16.8369
trainer/Log Pis Mean                                 5.86969
trainer/Log Pis Std                                  4.62367
trainer/Log Pis Max                                 16.0572
trainer/Log Pis Min                                 -7.63723
trainer/Policy mu Mean                               0.0557076
trainer/Policy mu Std                                1.4737
trainer/Policy mu Max                                3.53796
trainer/Policy mu Min                               -3.41998
trainer/Policy log std Mean                         -0.849807
trainer/Policy log std Std                           0.354935
trainer/Policy log std Max                           0.0356349
trainer/Policy log std Min                          -2.6156
trainer/Alpha                                        0.11108
trainer/Alpha Loss                                  -0.286362
exploration/num steps total                     219000
exploration/num paths total                        219
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.48924
exploration/Rewards Std                              1.07368
exploration/Rewards Max                              6.70035
exploration/Rewards Min                             -0.416463
exploration/Returns Mean                          4489.24
exploration/Returns Std                              0
exploration/Returns Max                           4489.24
exploration/Returns Min                           4489.24
exploration/Actions Mean                            -0.000315212
exploration/Actions Std                              0.804527
exploration/Actions Max                              0.999618
exploration/Actions Min                             -0.998835
exploration/Num Paths                                1
exploration/Average Returns                       4489.24
exploration/env_infos/final/reward_run Mean          5.80462
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.80462
exploration/env_infos/final/reward_run Min           5.80462
exploration/env_infos/initial/reward_run Mean       -0.198768
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.198768
exploration/env_infos/initial/reward_run Min        -0.198768
exploration/env_infos/reward_run Mean                4.8776
exploration/env_infos/reward_run Std                 1.05298
exploration/env_infos/reward_run Max                 7.04888
exploration/env_infos/reward_run Min                -0.198768
exploration/env_infos/final/reward_ctrl Mean        -0.277155
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.277155
exploration/env_infos/final/reward_ctrl Min         -0.277155
exploration/env_infos/initial/reward_ctrl Mean      -0.217695
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.217695
exploration/env_infos/initial/reward_ctrl Min       -0.217695
exploration/env_infos/reward_ctrl Mean              -0.388358
exploration/env_infos/reward_ctrl Std                0.102303
exploration/env_infos/reward_ctrl Max               -0.115564
exploration/env_infos/reward_ctrl Min               -0.579035
evaluation/num steps total                           1.09e+06
evaluation/num paths total                        1090
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.64516
evaluation/Rewards Std                               1.11005
evaluation/Rewards Max                               6.82156
evaluation/Rewards Min                              -1.33673
evaluation/Returns Mean                           4645.16
evaluation/Returns Std                              30.4049
evaluation/Returns Max                            4679.9
evaluation/Returns Min                            4598.6
evaluation/Actions Mean                             -0.0134605
evaluation/Actions Std                               0.820033
evaluation/Actions Max                               0.998686
evaluation/Actions Min                              -0.996797
evaluation/Num Paths                                 5
evaluation/Average Returns                        4645.16
evaluation/env_infos/final/reward_run Mean           4.83781
evaluation/env_infos/final/reward_run Std            0.881361
evaluation/env_infos/final/reward_run Max            5.7777
evaluation/env_infos/final/reward_run Min            3.15923
evaluation/env_infos/initial/reward_run Mean        -0.0344713
evaluation/env_infos/initial/reward_run Std          0.122055
evaluation/env_infos/initial/reward_run Max          0.0876074
evaluation/env_infos/initial/reward_run Min         -0.239045
evaluation/env_infos/reward_run Mean                 5.04874
evaluation/env_infos/reward_run Std                  1.0851
evaluation/env_infos/reward_run Max                  7.23765
evaluation/env_infos/reward_run Min                 -0.914805
evaluation/env_infos/final/reward_ctrl Mean         -0.317026
evaluation/env_infos/final/reward_ctrl Std           0.109554
evaluation/env_infos/final/reward_ctrl Max          -0.219173
evaluation/env_infos/final/reward_ctrl Min          -0.522088
evaluation/env_infos/initial/reward_ctrl Mean       -0.119219
evaluation/env_infos/initial/reward_ctrl Std         0.0709586
evaluation/env_infos/initial/reward_ctrl Max        -0.0177999
evaluation/env_infos/initial/reward_ctrl Min        -0.230237
evaluation/env_infos/reward_ctrl Mean               -0.403581
evaluation/env_infos/reward_ctrl Std                 0.105013
evaluation/env_infos/reward_ctrl Max                -0.0177999
evaluation/env_infos/reward_ctrl Min                -0.572556
time/data storing (s)                                0.0067787
time/evaluation sampling (s)                         3.4298
time/exploration sampling (s)                        1.05238
time/logging (s)                                     0.0460348
time/saving (s)                                      0.0181986
time/training (s)                                   45.48
time/epoch (s)                                      50.0332
time/total (s)                                    9173.58
Epoch                                              217
----------------------------------------------  ----------------
2020-07-08 23:39:47.049324 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 218 finished
----------------------------------------------  ----------------
replay_buffer/size                              220000
trainer/QF1 Loss                                     8.31924
trainer/QF2 Loss                                     8.1837
trainer/Policy Loss                               -265.387
trainer/Q1 Predictions Mean                        271.625
trainer/Q1 Predictions Std                          82.766
trainer/Q1 Predictions Max                         332.292
trainer/Q1 Predictions Min                          18.4824
trainer/Q2 Predictions Mean                        272.507
trainer/Q2 Predictions Std                          82.9369
trainer/Q2 Predictions Max                         335.041
trainer/Q2 Predictions Min                          18.7368
trainer/Q Targets Mean                             272.492
trainer/Q Targets Std                               82.9525
trainer/Q Targets Max                              335.09
trainer/Q Targets Min                               18.47
trainer/Log Pis Mean                                 6.47793
trainer/Log Pis Std                                  4.82992
trainer/Log Pis Max                                 18.6957
trainer/Log Pis Min                                 -5.99704
trainer/Policy mu Mean                               0.0419663
trainer/Policy mu Std                                1.54286
trainer/Policy mu Max                                3.86672
trainer/Policy mu Min                               -3.29339
trainer/Policy log std Mean                         -0.843695
trainer/Policy log std Std                           0.344359
trainer/Policy log std Max                           0.0544842
trainer/Policy log std Min                          -2.58098
trainer/Alpha                                        0.110787
trainer/Alpha Loss                                   1.0516
exploration/num steps total                     220000
exploration/num paths total                        220
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.32443
exploration/Rewards Std                              1.04766
exploration/Rewards Max                              6.65708
exploration/Rewards Min                             -0.523238
exploration/Returns Mean                          4324.43
exploration/Returns Std                              0
exploration/Returns Max                           4324.43
exploration/Returns Min                           4324.43
exploration/Actions Mean                             0.000148045
exploration/Actions Std                              0.807819
exploration/Actions Max                              0.999529
exploration/Actions Min                             -0.999416
exploration/Num Paths                                1
exploration/Average Returns                       4324.43
exploration/env_infos/final/reward_run Mean          4.04564
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.04564
exploration/env_infos/final/reward_run Min           4.04564
exploration/env_infos/initial/reward_run Mean        0.158724
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.158724
exploration/env_infos/initial/reward_run Min         0.158724
exploration/env_infos/reward_run Mean                4.71597
exploration/env_infos/reward_run Std                 1.02207
exploration/env_infos/reward_run Max                 6.90881
exploration/env_infos/reward_run Min                -0.0412281
exploration/env_infos/final/reward_ctrl Mean        -0.506111
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.506111
exploration/env_infos/final/reward_ctrl Min         -0.506111
exploration/env_infos/initial/reward_ctrl Mean      -0.0972044
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0972044
exploration/env_infos/initial/reward_ctrl Min       -0.0972044
exploration/env_infos/reward_ctrl Mean              -0.391543
exploration/env_infos/reward_ctrl Std                0.105354
exploration/env_infos/reward_ctrl Max               -0.0779858
exploration/env_infos/reward_ctrl Min               -0.582864
evaluation/num steps total                           1.095e+06
evaluation/num paths total                        1095
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.55088
evaluation/Rewards Std                               1.13731
evaluation/Rewards Max                               6.98491
evaluation/Rewards Min                              -0.694798
evaluation/Returns Mean                           4550.88
evaluation/Returns Std                              45.6455
evaluation/Returns Max                            4638.31
evaluation/Returns Min                            4514.42
evaluation/Actions Mean                             -0.0107469
evaluation/Actions Std                               0.821455
evaluation/Actions Max                               0.999047
evaluation/Actions Min                              -0.999782
evaluation/Num Paths                                 5
evaluation/Average Returns                        4550.88
evaluation/env_infos/final/reward_run Mean           5.98965
evaluation/env_infos/final/reward_run Std            0.563666
evaluation/env_infos/final/reward_run Max            6.55426
evaluation/env_infos/final/reward_run Min            5.01728
evaluation/env_infos/initial/reward_run Mean         0.0134549
evaluation/env_infos/initial/reward_run Std          0.13932
evaluation/env_infos/initial/reward_run Max          0.161096
evaluation/env_infos/initial/reward_run Min         -0.177404
evaluation/env_infos/reward_run Mean                 4.95582
evaluation/env_infos/reward_run Std                  1.11523
evaluation/env_infos/reward_run Max                  7.46625
evaluation/env_infos/reward_run Min                 -0.430378
evaluation/env_infos/final/reward_ctrl Mean         -0.362008
evaluation/env_infos/final/reward_ctrl Std           0.0832663
evaluation/env_infos/final/reward_ctrl Max          -0.279567
evaluation/env_infos/final/reward_ctrl Min          -0.467784
evaluation/env_infos/initial/reward_ctrl Mean       -0.100692
evaluation/env_infos/initial/reward_ctrl Std         0.0584091
evaluation/env_infos/initial/reward_ctrl Max        -0.0191284
evaluation/env_infos/initial/reward_ctrl Min        -0.184516
evaluation/env_infos/reward_ctrl Mean               -0.404943
evaluation/env_infos/reward_ctrl Std                 0.11064
evaluation/env_infos/reward_ctrl Max                -0.0191284
evaluation/env_infos/reward_ctrl Min                -0.581616
time/data storing (s)                                0.00678321
time/evaluation sampling (s)                         2.67967
time/exploration sampling (s)                        0.636858
time/logging (s)                                     0.0455313
time/saving (s)                                      0.0163407
time/training (s)                                   37.8996
time/epoch (s)                                      41.2848
time/total (s)                                    9214.93
Epoch                                              218
----------------------------------------------  ----------------
2020-07-08 23:40:27.483408 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 219 finished
----------------------------------------------  ---------------
replay_buffer/size                              221000
trainer/QF1 Loss                                     8.44585
trainer/QF2 Loss                                     6.45906
trainer/Policy Loss                               -252.142
trainer/Q1 Predictions Mean                        257.499
trainer/Q1 Predictions Std                          97.0885
trainer/Q1 Predictions Max                         335.078
trainer/Q1 Predictions Min                          18.8198
trainer/Q2 Predictions Mean                        258.383
trainer/Q2 Predictions Std                          97.1531
trainer/Q2 Predictions Max                         334.54
trainer/Q2 Predictions Min                          18.842
trainer/Q Targets Mean                             258.755
trainer/Q Targets Std                               97.2856
trainer/Q Targets Max                              336.928
trainer/Q Targets Min                               18.5086
trainer/Log Pis Mean                                 6.04103
trainer/Log Pis Std                                  4.79932
trainer/Log Pis Max                                 21.6142
trainer/Log Pis Min                                 -4.55113
trainer/Policy mu Mean                              -0.0469682
trainer/Policy mu Std                                1.50411
trainer/Policy mu Max                                5.49123
trainer/Policy mu Min                               -4.31794
trainer/Policy log std Mean                         -0.814134
trainer/Policy log std Std                           0.356303
trainer/Policy log std Max                           0.449245
trainer/Policy log std Min                          -2.59458
trainer/Alpha                                        0.111053
trainer/Alpha Loss                                   0.0901742
exploration/num steps total                     221000
exploration/num paths total                        221
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.53112
exploration/Rewards Std                              1.13362
exploration/Rewards Max                              6.58752
exploration/Rewards Min                             -0.5081
exploration/Returns Mean                          4531.12
exploration/Returns Std                              0
exploration/Returns Max                           4531.12
exploration/Returns Min                           4531.12
exploration/Actions Mean                             0.0104383
exploration/Actions Std                              0.821537
exploration/Actions Max                              0.999574
exploration/Actions Min                             -0.998841
exploration/Num Paths                                1
exploration/Average Returns                       4531.12
exploration/env_infos/final/reward_run Mean          6.41041
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.41041
exploration/env_infos/final/reward_run Min           6.41041
exploration/env_infos/initial/reward_run Mean       -0.0713353
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0713353
exploration/env_infos/initial/reward_run Min        -0.0713353
exploration/env_infos/reward_run Mean                4.93614
exploration/env_infos/reward_run Std                 1.11724
exploration/env_infos/reward_run Max                 6.93599
exploration/env_infos/reward_run Min                -0.223542
exploration/env_infos/final/reward_ctrl Mean        -0.359148
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.359148
exploration/env_infos/final/reward_ctrl Min         -0.359148
exploration/env_infos/initial/reward_ctrl Mean      -0.160413
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.160413
exploration/env_infos/initial/reward_ctrl Min       -0.160413
exploration/env_infos/reward_ctrl Mean              -0.40502
exploration/env_infos/reward_ctrl Std                0.0963809
exploration/env_infos/reward_ctrl Max               -0.12394
exploration/env_infos/reward_ctrl Min               -0.573223
evaluation/num steps total                           1.1e+06
evaluation/num paths total                        1100
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              3.88872
evaluation/Rewards Std                               1.8623
evaluation/Rewards Max                               6.83385
evaluation/Rewards Min                              -1.48183
evaluation/Returns Mean                           3888.72
evaluation/Returns Std                            1414.71
evaluation/Returns Max                            4657.91
evaluation/Returns Min                            1061.2
evaluation/Actions Mean                             -0.0225784
evaluation/Actions Std                               0.795113
evaluation/Actions Max                               0.99972
evaluation/Actions Min                              -0.999751
evaluation/Num Paths                                 5
evaluation/Average Returns                        3888.72
evaluation/env_infos/final/reward_run Mean           3.85061
evaluation/env_infos/final/reward_run Std            1.95806
evaluation/env_infos/final/reward_run Max            5.64447
evaluation/env_infos/final/reward_run Min            0.0454614
evaluation/env_infos/initial/reward_run Mean        -0.0429188
evaluation/env_infos/initial/reward_run Std          0.0838605
evaluation/env_infos/initial/reward_run Max          0.110998
evaluation/env_infos/initial/reward_run Min         -0.130507
evaluation/env_infos/reward_run Mean                 4.26835
evaluation/env_infos/reward_run Std                  1.90249
evaluation/env_infos/reward_run Max                  7.23197
evaluation/env_infos/reward_run Min                 -1.2245
evaluation/env_infos/final/reward_ctrl Mean         -0.326301
evaluation/env_infos/final/reward_ctrl Std           0.0602987
evaluation/env_infos/final/reward_ctrl Max          -0.274665
evaluation/env_infos/final/reward_ctrl Min          -0.441746
evaluation/env_infos/initial/reward_ctrl Mean       -0.109879
evaluation/env_infos/initial/reward_ctrl Std         0.0492151
evaluation/env_infos/initial/reward_ctrl Max        -0.0327751
evaluation/env_infos/initial/reward_ctrl Min        -0.185115
evaluation/env_infos/reward_ctrl Mean               -0.379628
evaluation/env_infos/reward_ctrl Std                 0.11637
evaluation/env_infos/reward_ctrl Max                -0.0327751
evaluation/env_infos/reward_ctrl Min                -0.573095
time/data storing (s)                                0.00690711
time/evaluation sampling (s)                         4.41563
time/exploration sampling (s)                        0.818681
time/logging (s)                                     0.0407372
time/saving (s)                                      0.0160095
time/training (s)                                   34.9192
time/epoch (s)                                      40.2172
time/total (s)                                    9255.35
Epoch                                              219
----------------------------------------------  ---------------
2020-07-08 23:41:21.872221 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 220 finished
----------------------------------------------  --------------
replay_buffer/size                              222000
trainer/QF1 Loss                                     9.8749
trainer/QF2 Loss                                    11.5312
trainer/Policy Loss                               -255.455
trainer/Q1 Predictions Mean                        260.689
trainer/Q1 Predictions Std                          96.8531
trainer/Q1 Predictions Max                         331.867
trainer/Q1 Predictions Min                          17.8744
trainer/Q2 Predictions Mean                        261.019
trainer/Q2 Predictions Std                          96.822
trainer/Q2 Predictions Max                         331.797
trainer/Q2 Predictions Min                          17.4965
trainer/Q Targets Mean                             260.846
trainer/Q Targets Std                               96.8496
trainer/Q Targets Max                              331.939
trainer/Q Targets Min                               16.0645
trainer/Log Pis Mean                                 5.49609
trainer/Log Pis Std                                  4.63692
trainer/Log Pis Max                                 17.5866
trainer/Log Pis Min                                 -5.25723
trainer/Policy mu Mean                               0.100029
trainer/Policy mu Std                                1.44917
trainer/Policy mu Max                                3.28658
trainer/Policy mu Min                               -3.08274
trainer/Policy log std Mean                         -0.811518
trainer/Policy log std Std                           0.346611
trainer/Policy log std Max                          -0.0552912
trainer/Policy log std Min                          -2.51224
trainer/Alpha                                        0.111563
trainer/Alpha Loss                                  -1.10512
exploration/num steps total                     222000
exploration/num paths total                        222
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.47077
exploration/Rewards Std                              1.0933
exploration/Rewards Max                              6.49842
exploration/Rewards Min                             -0.216277
exploration/Returns Mean                          4470.77
exploration/Returns Std                              0
exploration/Returns Max                           4470.77
exploration/Returns Min                           4470.77
exploration/Actions Mean                             0.0270549
exploration/Actions Std                              0.79869
exploration/Actions Max                              0.999745
exploration/Actions Min                             -0.999439
exploration/Num Paths                                1
exploration/Average Returns                       4470.77
exploration/env_infos/final/reward_run Mean          4.7126
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.7126
exploration/env_infos/final/reward_run Min           4.7126
exploration/env_infos/initial/reward_run Mean       -0.0851152
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0851152
exploration/env_infos/initial/reward_run Min        -0.0851152
exploration/env_infos/reward_run Mean                4.85396
exploration/env_infos/reward_run Std                 1.0761
exploration/env_infos/reward_run Max                 6.84167
exploration/env_infos/reward_run Min                -0.0851152
exploration/env_infos/final/reward_ctrl Mean        -0.489737
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.489737
exploration/env_infos/final/reward_ctrl Min         -0.489737
exploration/env_infos/initial/reward_ctrl Mean      -0.060652
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.060652
exploration/env_infos/initial/reward_ctrl Min       -0.060652
exploration/env_infos/reward_ctrl Mean              -0.383183
exploration/env_infos/reward_ctrl Std                0.101431
exploration/env_infos/reward_ctrl Max               -0.060652
exploration/env_infos/reward_ctrl Min               -0.565937
evaluation/num steps total                           1.105e+06
evaluation/num paths total                        1105
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.66823
evaluation/Rewards Std                               1.14018
evaluation/Rewards Max                               7.00615
evaluation/Rewards Min                              -0.890696
evaluation/Returns Mean                           4668.23
evaluation/Returns Std                              56.9521
evaluation/Returns Max                            4774.76
evaluation/Returns Min                            4604.03
evaluation/Actions Mean                              0.0220426
evaluation/Actions Std                               0.807862
evaluation/Actions Max                               0.999525
evaluation/Actions Min                              -0.996708
evaluation/Num Paths                                 5
evaluation/Average Returns                        4668.23
evaluation/env_infos/final/reward_run Mean           5.51823
evaluation/env_infos/final/reward_run Std            0.99783
evaluation/env_infos/final/reward_run Max            6.87427
evaluation/env_infos/final/reward_run Min            3.79716
evaluation/env_infos/initial/reward_run Mean         0.100117
evaluation/env_infos/initial/reward_run Std          0.272455
evaluation/env_infos/initial/reward_run Max          0.545606
evaluation/env_infos/initial/reward_run Min         -0.142563
evaluation/env_infos/reward_run Mean                 5.06011
evaluation/env_infos/reward_run Std                  1.11494
evaluation/env_infos/reward_run Max                  7.32664
evaluation/env_infos/reward_run Min                 -0.371109
evaluation/env_infos/final/reward_ctrl Mean         -0.398465
evaluation/env_infos/final/reward_ctrl Std           0.100582
evaluation/env_infos/final/reward_ctrl Max          -0.230013
evaluation/env_infos/final/reward_ctrl Min          -0.498068
evaluation/env_infos/initial/reward_ctrl Mean       -0.0895753
evaluation/env_infos/initial/reward_ctrl Std         0.0384033
evaluation/env_infos/initial/reward_ctrl Max        -0.0330599
evaluation/env_infos/initial/reward_ctrl Min        -0.138203
evaluation/env_infos/reward_ctrl Mean               -0.391877
evaluation/env_infos/reward_ctrl Std                 0.104325
evaluation/env_infos/reward_ctrl Max                -0.0323877
evaluation/env_infos/reward_ctrl Min                -0.569877
time/data storing (s)                                0.0184729
time/evaluation sampling (s)                         3.74458
time/exploration sampling (s)                        1.66149
time/logging (s)                                     0.0578858
time/saving (s)                                      0.0219658
time/training (s)                                   48.873
time/epoch (s)                                      54.3774
time/total (s)                                    9309.75
Epoch                                              220
----------------------------------------------  --------------
2020-07-08 23:42:07.058283 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 221 finished
----------------------------------------------  ---------------
replay_buffer/size                              223000
trainer/QF1 Loss                                     9.70101
trainer/QF2 Loss                                     6.85604
trainer/Policy Loss                               -266.078
trainer/Q1 Predictions Mean                        272.492
trainer/Q1 Predictions Std                          83.5808
trainer/Q1 Predictions Max                         336.57
trainer/Q1 Predictions Min                          18.919
trainer/Q2 Predictions Mean                        272.328
trainer/Q2 Predictions Std                          83.4992
trainer/Q2 Predictions Max                         334.889
trainer/Q2 Predictions Min                          19.0321
trainer/Q Targets Mean                             272.061
trainer/Q Targets Std                               83.498
trainer/Q Targets Max                              335.1
trainer/Q Targets Min                               19.7721
trainer/Log Pis Mean                                 6.41895
trainer/Log Pis Std                                  4.69177
trainer/Log Pis Max                                 18.7896
trainer/Log Pis Min                                 -4.87463
trainer/Policy mu Mean                              -0.0080201
trainer/Policy mu Std                                1.51305
trainer/Policy mu Max                                3.56278
trainer/Policy mu Min                               -3.74756
trainer/Policy log std Mean                         -0.828329
trainer/Policy log std Std                           0.336965
trainer/Policy log std Max                           0.0279841
trainer/Policy log std Min                          -2.68391
trainer/Alpha                                        0.113088
trainer/Alpha Loss                                   0.9132
exploration/num steps total                     223000
exploration/num paths total                        223
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.34718
exploration/Rewards Std                              1.17485
exploration/Rewards Max                              6.61181
exploration/Rewards Min                             -0.927779
exploration/Returns Mean                          4347.18
exploration/Returns Std                              0
exploration/Returns Max                           4347.18
exploration/Returns Min                           4347.18
exploration/Actions Mean                            -0.00635078
exploration/Actions Std                              0.800611
exploration/Actions Max                              0.999462
exploration/Actions Min                             -0.999681
exploration/Num Paths                                1
exploration/Average Returns                       4347.18
exploration/env_infos/final/reward_run Mean          5.04971
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.04971
exploration/env_infos/final/reward_run Min           5.04971
exploration/env_infos/initial/reward_run Mean        0.163725
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.163725
exploration/env_infos/initial/reward_run Min         0.163725
exploration/env_infos/reward_run Mean                4.73179
exploration/env_infos/reward_run Std                 1.15609
exploration/env_infos/reward_run Max                 6.8945
exploration/env_infos/reward_run Min                -0.444451
exploration/env_infos/final/reward_ctrl Mean        -0.47088
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.47088
exploration/env_infos/final/reward_ctrl Min         -0.47088
exploration/env_infos/initial/reward_ctrl Mean      -0.0460206
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0460206
exploration/env_infos/initial/reward_ctrl Min       -0.0460206
exploration/env_infos/reward_ctrl Mean              -0.384611
exploration/env_infos/reward_ctrl Std                0.104958
exploration/env_infos/reward_ctrl Max               -0.0460206
exploration/env_infos/reward_ctrl Min               -0.581537
evaluation/num steps total                           1.11e+06
evaluation/num paths total                        1110
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.69399
evaluation/Rewards Std                               1.14397
evaluation/Rewards Max                               6.90211
evaluation/Rewards Min                              -0.973169
evaluation/Returns Mean                           4693.99
evaluation/Returns Std                              61.1782
evaluation/Returns Max                            4773.26
evaluation/Returns Min                            4612.57
evaluation/Actions Mean                             -0.020272
evaluation/Actions Std                               0.813369
evaluation/Actions Max                               0.998033
evaluation/Actions Min                              -0.997778
evaluation/Num Paths                                 5
evaluation/Average Returns                        4693.99
evaluation/env_infos/final/reward_run Mean           4.56103
evaluation/env_infos/final/reward_run Std            0.709501
evaluation/env_infos/final/reward_run Max            5.40336
evaluation/env_infos/final/reward_run Min            3.40544
evaluation/env_infos/initial/reward_run Mean        -0.0222199
evaluation/env_infos/initial/reward_run Std          0.0942333
evaluation/env_infos/initial/reward_run Max          0.133242
evaluation/env_infos/initial/reward_run Min         -0.130302
evaluation/env_infos/reward_run Mean                 5.09118
evaluation/env_infos/reward_run Std                  1.11724
evaluation/env_infos/reward_run Max                  7.26777
evaluation/env_infos/reward_run Min                 -0.508258
evaluation/env_infos/final/reward_ctrl Mean         -0.451612
evaluation/env_infos/final/reward_ctrl Std           0.095992
evaluation/env_infos/final/reward_ctrl Max          -0.260645
evaluation/env_infos/final/reward_ctrl Min          -0.515668
evaluation/env_infos/initial/reward_ctrl Mean       -0.0722198
evaluation/env_infos/initial/reward_ctrl Std         0.0368604
evaluation/env_infos/initial/reward_ctrl Max        -0.0315027
evaluation/env_infos/initial/reward_ctrl Min        -0.121839
evaluation/env_infos/reward_ctrl Mean               -0.397188
evaluation/env_infos/reward_ctrl Std                 0.107891
evaluation/env_infos/reward_ctrl Max                -0.0314133
evaluation/env_infos/reward_ctrl Min                -0.572852
time/data storing (s)                                0.00799918
time/evaluation sampling (s)                         4.06437
time/exploration sampling (s)                        0.767873
time/logging (s)                                     0.0446307
time/saving (s)                                      0.016169
time/training (s)                                   40.1472
time/epoch (s)                                      45.0482
time/total (s)                                    9354.92
Epoch                                              221
----------------------------------------------  ---------------
2020-07-08 23:42:43.298206 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 222 finished
----------------------------------------------  ---------------
replay_buffer/size                              224000
trainer/QF1 Loss                                     6.30707
trainer/QF2 Loss                                     7.80912
trainer/Policy Loss                               -271.689
trainer/Q1 Predictions Mean                        277.721
trainer/Q1 Predictions Std                          79.375
trainer/Q1 Predictions Max                         338.066
trainer/Q1 Predictions Min                          17.4634
trainer/Q2 Predictions Mean                        277.816
trainer/Q2 Predictions Std                          79.525
trainer/Q2 Predictions Max                         338.884
trainer/Q2 Predictions Min                          18.0736
trainer/Q Targets Mean                             277.695
trainer/Q Targets Std                               79.6111
trainer/Q Targets Max                              338.017
trainer/Q Targets Min                               17.7869
trainer/Log Pis Mean                                 6.20051
trainer/Log Pis Std                                  5.34805
trainer/Log Pis Max                                 35.6483
trainer/Log Pis Min                                 -5.06915
trainer/Policy mu Mean                               0.088329
trainer/Policy mu Std                                1.52727
trainer/Policy mu Max                                6.50783
trainer/Policy mu Min                               -4.35574
trainer/Policy log std Mean                         -0.834296
trainer/Policy log std Std                           0.336972
trainer/Policy log std Max                          -0.063935
trainer/Policy log std Min                          -2.52513
trainer/Alpha                                        0.112016
trainer/Alpha Loss                                   0.438957
exploration/num steps total                     224000
exploration/num paths total                        224
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.51479
exploration/Rewards Std                              1.13217
exploration/Rewards Max                              7.06304
exploration/Rewards Min                             -0.243735
exploration/Returns Mean                          4514.79
exploration/Returns Std                              0
exploration/Returns Max                           4514.79
exploration/Returns Min                           4514.79
exploration/Actions Mean                             0.00216473
exploration/Actions Std                              0.808954
exploration/Actions Max                              0.999402
exploration/Actions Min                             -0.999669
exploration/Num Paths                                1
exploration/Average Returns                       4514.79
exploration/env_infos/final/reward_run Mean          4.77442
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.77442
exploration/env_infos/final/reward_run Min           4.77442
exploration/env_infos/initial/reward_run Mean       -0.0924326
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0924326
exploration/env_infos/initial/reward_run Min        -0.0924326
exploration/env_infos/reward_run Mean                4.90744
exploration/env_infos/reward_run Std                 1.11463
exploration/env_infos/reward_run Max                 7.28099
exploration/env_infos/reward_run Min                -0.104011
exploration/env_infos/final/reward_ctrl Mean        -0.392051
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.392051
exploration/env_infos/final/reward_ctrl Min         -0.392051
exploration/env_infos/initial/reward_ctrl Mean      -0.0346489
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0346489
exploration/env_infos/initial/reward_ctrl Min       -0.0346489
exploration/env_infos/reward_ctrl Mean              -0.392646
exploration/env_infos/reward_ctrl Std                0.0995967
exploration/env_infos/reward_ctrl Max               -0.0346489
exploration/env_infos/reward_ctrl Min               -0.570308
evaluation/num steps total                           1.115e+06
evaluation/num paths total                        1115
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.61241
evaluation/Rewards Std                               1.15043
evaluation/Rewards Max                               6.911
evaluation/Rewards Min                              -1.26712
evaluation/Returns Mean                           4612.41
evaluation/Returns Std                              33.6613
evaluation/Returns Max                            4664.47
evaluation/Returns Min                            4562.51
evaluation/Actions Mean                             -0.00566063
evaluation/Actions Std                               0.819069
evaluation/Actions Max                               0.998673
evaluation/Actions Min                              -0.996952
evaluation/Num Paths                                 5
evaluation/Average Returns                        4612.41
evaluation/env_infos/final/reward_run Mean           5.28201
evaluation/env_infos/final/reward_run Std            0.807705
evaluation/env_infos/final/reward_run Max            6.08926
evaluation/env_infos/final/reward_run Min            4.0037
evaluation/env_infos/initial/reward_run Mean         0.0569886
evaluation/env_infos/initial/reward_run Std          0.123798
evaluation/env_infos/initial/reward_run Max          0.18449
evaluation/env_infos/initial/reward_run Min         -0.123693
evaluation/env_infos/reward_run Mean                 5.01495
evaluation/env_infos/reward_run Std                  1.1295
evaluation/env_infos/reward_run Max                  7.3917
evaluation/env_infos/reward_run Min                 -0.808426
evaluation/env_infos/final/reward_ctrl Mean         -0.346292
evaluation/env_infos/final/reward_ctrl Std           0.0724018
evaluation/env_infos/final/reward_ctrl Max          -0.255451
evaluation/env_infos/final/reward_ctrl Min          -0.473923
evaluation/env_infos/initial/reward_ctrl Mean       -0.0341622
evaluation/env_infos/initial/reward_ctrl Std         0.0287359
evaluation/env_infos/initial/reward_ctrl Max        -0.0142804
evaluation/env_infos/initial/reward_ctrl Min        -0.090037
evaluation/env_infos/reward_ctrl Mean               -0.402543
evaluation/env_infos/reward_ctrl Std                 0.102129
evaluation/env_infos/reward_ctrl Max                -0.0142804
evaluation/env_infos/reward_ctrl Min                -0.571234
time/data storing (s)                                0.00674425
time/evaluation sampling (s)                         2.76868
time/exploration sampling (s)                        0.79161
time/logging (s)                                     0.0399519
time/saving (s)                                      0.0162387
time/training (s)                                   32.5595
time/epoch (s)                                      36.1827
time/total (s)                                    9391.14
Epoch                                              222
----------------------------------------------  ---------------
2020-07-08 23:43:18.147606 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 223 finished
----------------------------------------------  ---------------
replay_buffer/size                              225000
trainer/QF1 Loss                                     6.24532
trainer/QF2 Loss                                     8.02508
trainer/Policy Loss                               -267.38
trainer/Q1 Predictions Mean                        273.935
trainer/Q1 Predictions Std                          84.3345
trainer/Q1 Predictions Max                         339.226
trainer/Q1 Predictions Min                          18.2977
trainer/Q2 Predictions Mean                        273.537
trainer/Q2 Predictions Std                          84.2344
trainer/Q2 Predictions Max                         339.057
trainer/Q2 Predictions Min                          16.3005
trainer/Q Targets Mean                             274.191
trainer/Q Targets Std                               84.4411
trainer/Q Targets Max                              340.166
trainer/Q Targets Min                               16.5698
trainer/Log Pis Mean                                 6.51861
trainer/Log Pis Std                                  4.75916
trainer/Log Pis Max                                 31.2953
trainer/Log Pis Min                                 -4.61257
trainer/Policy mu Mean                               0.0284919
trainer/Policy mu Std                                1.54101
trainer/Policy mu Max                                6.81845
trainer/Policy mu Min                               -5.67946
trainer/Policy log std Mean                         -0.829713
trainer/Policy log std Std                           0.348927
trainer/Policy log std Max                           0.382863
trainer/Policy log std Min                          -2.46663
trainer/Alpha                                        0.113974
trainer/Alpha Loss                                   1.12629
exploration/num steps total                     225000
exploration/num paths total                        225
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.61967
exploration/Rewards Std                              1.08752
exploration/Rewards Max                              6.87833
exploration/Rewards Min                             -0.146739
exploration/Returns Mean                          4619.67
exploration/Returns Std                              0
exploration/Returns Max                           4619.67
exploration/Returns Min                           4619.67
exploration/Actions Mean                            -0.0102049
exploration/Actions Std                              0.822845
exploration/Actions Max                              0.999636
exploration/Actions Min                             -0.999859
exploration/Num Paths                                1
exploration/Average Returns                       4619.67
exploration/env_infos/final/reward_run Mean          5.49517
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.49517
exploration/env_infos/final/reward_run Min           5.49517
exploration/env_infos/initial/reward_run Mean        0.0565711
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0565711
exploration/env_infos/initial/reward_run Min         0.0565711
exploration/env_infos/reward_run Mean                5.02598
exploration/env_infos/reward_run Std                 1.0613
exploration/env_infos/reward_run Max                 7.20637
exploration/env_infos/reward_run Min                 0.0565711
exploration/env_infos/final/reward_ctrl Mean        -0.522974
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.522974
exploration/env_infos/final/reward_ctrl Min         -0.522974
exploration/env_infos/initial/reward_ctrl Mean      -0.20331
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.20331
exploration/env_infos/initial/reward_ctrl Min       -0.20331
exploration/env_infos/reward_ctrl Mean              -0.406307
exploration/env_infos/reward_ctrl Std                0.0965435
exploration/env_infos/reward_ctrl Max               -0.0604772
exploration/env_infos/reward_ctrl Min               -0.580018
evaluation/num steps total                           1.12e+06
evaluation/num paths total                        1120
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.61091
evaluation/Rewards Std                               1.13821
evaluation/Rewards Max                               7.08194
evaluation/Rewards Min                              -0.920435
evaluation/Returns Mean                           4610.91
evaluation/Returns Std                              44.5953
evaluation/Returns Max                            4661.75
evaluation/Returns Min                            4544.13
evaluation/Actions Mean                             -0.00974821
evaluation/Actions Std                               0.82812
evaluation/Actions Max                               0.999789
evaluation/Actions Min                              -0.99951
evaluation/Num Paths                                 5
evaluation/Average Returns                        4610.91
evaluation/env_infos/final/reward_run Mean           5.52738
evaluation/env_infos/final/reward_run Std            0.566123
evaluation/env_infos/final/reward_run Max            6.27971
evaluation/env_infos/final/reward_run Min            4.85337
evaluation/env_infos/initial/reward_run Mean        -0.0442194
evaluation/env_infos/initial/reward_run Std          0.10701
evaluation/env_infos/initial/reward_run Max          0.130744
evaluation/env_infos/initial/reward_run Min         -0.206868
evaluation/env_infos/reward_run Mean                 5.02243
evaluation/env_infos/reward_run Std                  1.11471
evaluation/env_infos/reward_run Max                  7.46039
evaluation/env_infos/reward_run Min                 -0.492208
evaluation/env_infos/final/reward_ctrl Mean         -0.459128
evaluation/env_infos/final/reward_ctrl Std           0.107291
evaluation/env_infos/final/reward_ctrl Max          -0.24823
evaluation/env_infos/final/reward_ctrl Min          -0.537563
evaluation/env_infos/initial/reward_ctrl Mean       -0.0750516
evaluation/env_infos/initial/reward_ctrl Std         0.0578132
evaluation/env_infos/initial/reward_ctrl Max        -0.0081924
evaluation/env_infos/initial/reward_ctrl Min        -0.162307
evaluation/env_infos/reward_ctrl Mean               -0.411526
evaluation/env_infos/reward_ctrl Std                 0.100962
evaluation/env_infos/reward_ctrl Max                -0.0081924
evaluation/env_infos/reward_ctrl Min                -0.581565
time/data storing (s)                                0.00675242
time/evaluation sampling (s)                         2.60255
time/exploration sampling (s)                        0.644065
time/logging (s)                                     0.0409115
time/saving (s)                                      0.016572
time/training (s)                                   31.5203
time/epoch (s)                                      34.8311
time/total (s)                                    9425.99
Epoch                                              223
----------------------------------------------  ---------------
2020-07-08 23:43:53.618404 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 224 finished
----------------------------------------------  ---------------
replay_buffer/size                              226000
trainer/QF1 Loss                                     7.0521
trainer/QF2 Loss                                     7.35183
trainer/Policy Loss                               -269.595
trainer/Q1 Predictions Mean                        276.002
trainer/Q1 Predictions Std                          81.5394
trainer/Q1 Predictions Max                         335.107
trainer/Q1 Predictions Min                          18.5311
trainer/Q2 Predictions Mean                        275.727
trainer/Q2 Predictions Std                          81.3654
trainer/Q2 Predictions Max                         333.956
trainer/Q2 Predictions Min                          18.7763
trainer/Q Targets Mean                             276.136
trainer/Q Targets Std                               81.5922
trainer/Q Targets Max                              334.003
trainer/Q Targets Min                               19.0548
trainer/Log Pis Mean                                 6.45739
trainer/Log Pis Std                                  4.99853
trainer/Log Pis Max                                 18.1297
trainer/Log Pis Min                                 -7.13273
trainer/Policy mu Mean                               0.0593115
trainer/Policy mu Std                                1.52851
trainer/Policy mu Max                                3.70129
trainer/Policy mu Min                               -4.86876
trainer/Policy log std Mean                         -0.824826
trainer/Policy log std Std                           0.344817
trainer/Policy log std Max                           0.514705
trainer/Policy log std Min                          -2.58612
trainer/Alpha                                        0.112858
trainer/Alpha Loss                                   0.99783
exploration/num steps total                     226000
exploration/num paths total                        226
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.56889
exploration/Rewards Std                              1.099
exploration/Rewards Max                              6.87454
exploration/Rewards Min                             -0.67862
exploration/Returns Mean                          4568.89
exploration/Returns Std                              0
exploration/Returns Max                           4568.89
exploration/Returns Min                           4568.89
exploration/Actions Mean                             0.00965008
exploration/Actions Std                              0.801155
exploration/Actions Max                              0.999488
exploration/Actions Min                             -0.999816
exploration/Num Paths                                1
exploration/Average Returns                       4568.89
exploration/env_infos/final/reward_run Mean          5.47216
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.47216
exploration/env_infos/final/reward_run Min           5.47216
exploration/env_infos/initial/reward_run Mean        0.14772
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.14772
exploration/env_infos/initial/reward_run Min         0.14772
exploration/env_infos/reward_run Mean                4.95406
exploration/env_infos/reward_run Std                 1.07419
exploration/env_infos/reward_run Max                 7.19111
exploration/env_infos/reward_run Min                -0.139118
exploration/env_infos/final/reward_ctrl Mean        -0.205038
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.205038
exploration/env_infos/final/reward_ctrl Min         -0.205038
exploration/env_infos/initial/reward_ctrl Mean      -0.069663
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.069663
exploration/env_infos/initial/reward_ctrl Min       -0.069663
exploration/env_infos/reward_ctrl Mean              -0.385165
exploration/env_infos/reward_ctrl Std                0.102944
exploration/env_infos/reward_ctrl Max               -0.069663
exploration/env_infos/reward_ctrl Min               -0.59037
evaluation/num steps total                           1.125e+06
evaluation/num paths total                        1125
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.71069
evaluation/Rewards Std                               1.11894
evaluation/Rewards Max                               6.97198
evaluation/Rewards Min                              -0.691771
evaluation/Returns Mean                           4710.69
evaluation/Returns Std                              73.1237
evaluation/Returns Max                            4852.35
evaluation/Returns Min                            4651.29
evaluation/Actions Mean                              0.00299112
evaluation/Actions Std                               0.814097
evaluation/Actions Max                               0.999652
evaluation/Actions Min                              -0.998661
evaluation/Num Paths                                 5
evaluation/Average Returns                        4710.69
evaluation/env_infos/final/reward_run Mean           4.63
evaluation/env_infos/final/reward_run Std            0.482146
evaluation/env_infos/final/reward_run Max            5.13779
evaluation/env_infos/final/reward_run Min            3.97177
evaluation/env_infos/initial/reward_run Mean        -0.0523334
evaluation/env_infos/initial/reward_run Std          0.109064
evaluation/env_infos/initial/reward_run Max          0.0970203
evaluation/env_infos/initial/reward_run Min         -0.201039
evaluation/env_infos/reward_run Mean                 5.10834
evaluation/env_infos/reward_run Std                  1.09362
evaluation/env_infos/reward_run Max                  7.36226
evaluation/env_infos/reward_run Min                 -0.23721
evaluation/env_infos/final/reward_ctrl Mean         -0.461291
evaluation/env_infos/final/reward_ctrl Std           0.0950689
evaluation/env_infos/final/reward_ctrl Max          -0.277831
evaluation/env_infos/final/reward_ctrl Min          -0.527695
evaluation/env_infos/initial/reward_ctrl Mean       -0.121834
evaluation/env_infos/initial/reward_ctrl Std         0.0551179
evaluation/env_infos/initial/reward_ctrl Max        -0.0253992
evaluation/env_infos/initial/reward_ctrl Min        -0.174323
evaluation/env_infos/reward_ctrl Mean               -0.397657
evaluation/env_infos/reward_ctrl Std                 0.105419
evaluation/env_infos/reward_ctrl Max                -0.0253992
evaluation/env_infos/reward_ctrl Min                -0.580873
time/data storing (s)                                0.00693883
time/evaluation sampling (s)                         2.67965
time/exploration sampling (s)                        0.637731
time/logging (s)                                     0.0417489
time/saving (s)                                      0.0170493
time/training (s)                                   32.0685
time/epoch (s)                                      35.4516
time/total (s)                                    9461.45
Epoch                                              224
----------------------------------------------  ---------------
2020-07-08 23:44:31.855869 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 225 finished
----------------------------------------------  ---------------
replay_buffer/size                              227000
trainer/QF1 Loss                                     4.8891
trainer/QF2 Loss                                     5.2201
trainer/Policy Loss                               -271.016
trainer/Q1 Predictions Mean                        277.636
trainer/Q1 Predictions Std                          77.384
trainer/Q1 Predictions Max                         338.316
trainer/Q1 Predictions Min                          18.3745
trainer/Q2 Predictions Mean                        277.208
trainer/Q2 Predictions Std                          77.1747
trainer/Q2 Predictions Max                         337.026
trainer/Q2 Predictions Min                          18.5725
trainer/Q Targets Mean                             277.496
trainer/Q Targets Std                               77.3858
trainer/Q Targets Max                              335.853
trainer/Q Targets Min                               16.279
trainer/Log Pis Mean                                 6.28532
trainer/Log Pis Std                                  4.94253
trainer/Log Pis Max                                 19.4047
trainer/Log Pis Min                                 -7.50397
trainer/Policy mu Mean                               0.0242835
trainer/Policy mu Std                                1.52007
trainer/Policy mu Max                                3.83695
trainer/Policy mu Min                               -3.96483
trainer/Policy log std Mean                         -0.842102
trainer/Policy log std Std                           0.331076
trainer/Policy log std Max                           0.0135033
trainer/Policy log std Min                          -2.776
trainer/Alpha                                        0.110686
trainer/Alpha Loss                                   0.62801
exploration/num steps total                     227000
exploration/num paths total                        227
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.46219
exploration/Rewards Std                              1.06503
exploration/Rewards Max                              6.65096
exploration/Rewards Min                             -0.682592
exploration/Returns Mean                          4462.19
exploration/Returns Std                              0
exploration/Returns Max                           4462.19
exploration/Returns Min                           4462.19
exploration/Actions Mean                             0.0174514
exploration/Actions Std                              0.798221
exploration/Actions Max                              0.999509
exploration/Actions Min                             -0.999166
exploration/Num Paths                                1
exploration/Average Returns                       4462.19
exploration/env_infos/final/reward_run Mean          5.22463
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.22463
exploration/env_infos/final/reward_run Min           5.22463
exploration/env_infos/initial/reward_run Mean        0.302518
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.302518
exploration/env_infos/initial/reward_run Min         0.302518
exploration/env_infos/reward_run Mean                4.84467
exploration/env_infos/reward_run Std                 1.03363
exploration/env_infos/reward_run Max                 6.98168
exploration/env_infos/reward_run Min                -0.179697
exploration/env_infos/final/reward_ctrl Mean        -0.335541
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.335541
exploration/env_infos/final/reward_ctrl Min         -0.335541
exploration/env_infos/initial/reward_ctrl Mean      -0.0700079
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0700079
exploration/env_infos/initial/reward_ctrl Min       -0.0700079
exploration/env_infos/reward_ctrl Mean              -0.382477
exploration/env_infos/reward_ctrl Std                0.105121
exploration/env_infos/reward_ctrl Max               -0.0431762
exploration/env_infos/reward_ctrl Min               -0.574423
evaluation/num steps total                           1.13e+06
evaluation/num paths total                        1130
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.61708
evaluation/Rewards Std                               1.13351
evaluation/Rewards Max                               7.13561
evaluation/Rewards Min                              -0.598886
evaluation/Returns Mean                           4617.08
evaluation/Returns Std                              87.3519
evaluation/Returns Max                            4760.99
evaluation/Returns Min                            4516.9
evaluation/Actions Mean                              0.00548482
evaluation/Actions Std                               0.810884
evaluation/Actions Max                               0.999324
evaluation/Actions Min                              -0.998435
evaluation/Num Paths                                 5
evaluation/Average Returns                        4617.08
evaluation/env_infos/final/reward_run Mean           5.1694
evaluation/env_infos/final/reward_run Std            0.809994
evaluation/env_infos/final/reward_run Max            6.49945
evaluation/env_infos/final/reward_run Min            4.21042
evaluation/env_infos/initial/reward_run Mean         0.0906016
evaluation/env_infos/initial/reward_run Std          0.101846
evaluation/env_infos/initial/reward_run Max          0.242743
evaluation/env_infos/initial/reward_run Min         -0.0135734
evaluation/env_infos/reward_run Mean                 5.01162
evaluation/env_infos/reward_run Std                  1.10637
evaluation/env_infos/reward_run Max                  7.53847
evaluation/env_infos/reward_run Min                 -0.151942
evaluation/env_infos/final/reward_ctrl Mean         -0.46203
evaluation/env_infos/final/reward_ctrl Std           0.0712177
evaluation/env_infos/final/reward_ctrl Max          -0.327076
evaluation/env_infos/final/reward_ctrl Min          -0.528236
evaluation/env_infos/initial/reward_ctrl Mean       -0.0531592
evaluation/env_infos/initial/reward_ctrl Std         0.0437915
evaluation/env_infos/initial/reward_ctrl Max        -0.0145544
evaluation/env_infos/initial/reward_ctrl Min        -0.13702
evaluation/env_infos/reward_ctrl Mean               -0.394538
evaluation/env_infos/reward_ctrl Std                 0.10498
evaluation/env_infos/reward_ctrl Max                -0.0145544
evaluation/env_infos/reward_ctrl Min                -0.586457
time/data storing (s)                                0.00686598
time/evaluation sampling (s)                         2.4667
time/exploration sampling (s)                        0.657141
time/logging (s)                                     0.0411897
time/saving (s)                                      0.0189963
time/training (s)                                   34.8998
time/epoch (s)                                      38.0907
time/total (s)                                    9499.68
Epoch                                              225
----------------------------------------------  ---------------
2020-07-08 23:45:13.233781 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 226 finished
----------------------------------------------  ---------------
replay_buffer/size                              228000
trainer/QF1 Loss                                     6.5298
trainer/QF2 Loss                                     6.28954
trainer/Policy Loss                               -261.701
trainer/Q1 Predictions Mean                        267.32
trainer/Q1 Predictions Std                          90.9894
trainer/Q1 Predictions Max                         340.113
trainer/Q1 Predictions Min                          19.001
trainer/Q2 Predictions Mean                        267.229
trainer/Q2 Predictions Std                          90.8296
trainer/Q2 Predictions Max                         339.881
trainer/Q2 Predictions Min                          19.6271
trainer/Q Targets Mean                             267.305
trainer/Q Targets Std                               90.8928
trainer/Q Targets Max                              339.59
trainer/Q Targets Min                               17.2049
trainer/Log Pis Mean                                 5.80873
trainer/Log Pis Std                                  4.87571
trainer/Log Pis Max                                 19.0333
trainer/Log Pis Min                                 -5.28671
trainer/Policy mu Mean                               0.0453327
trainer/Policy mu Std                                1.49272
trainer/Policy mu Max                                5.38632
trainer/Policy mu Min                               -3.96596
trainer/Policy log std Mean                         -0.81504
trainer/Policy log std Std                           0.342598
trainer/Policy log std Max                           0.295538
trainer/Policy log std Min                          -2.49198
trainer/Alpha                                        0.113093
trainer/Alpha Loss                                  -0.416862
exploration/num steps total                     228000
exploration/num paths total                        228
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.57295
exploration/Rewards Std                              1.07419
exploration/Rewards Max                              6.6379
exploration/Rewards Min                             -0.403342
exploration/Returns Mean                          4572.95
exploration/Returns Std                              0
exploration/Returns Max                           4572.95
exploration/Returns Min                           4572.95
exploration/Actions Mean                             0.00896669
exploration/Actions Std                              0.792252
exploration/Actions Max                              0.999975
exploration/Actions Min                             -0.999853
exploration/Num Paths                                1
exploration/Average Returns                       4572.95
exploration/env_infos/final/reward_run Mean          4.34508
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.34508
exploration/env_infos/final/reward_run Min           4.34508
exploration/env_infos/initial/reward_run Mean       -0.0526871
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0526871
exploration/env_infos/initial/reward_run Min        -0.0526871
exploration/env_infos/reward_run Mean                4.94959
exploration/env_infos/reward_run Std                 1.04975
exploration/env_infos/reward_run Max                 6.85655
exploration/env_infos/reward_run Min                -0.0526871
exploration/env_infos/final/reward_ctrl Mean        -0.25654
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.25654
exploration/env_infos/final/reward_ctrl Min         -0.25654
exploration/env_infos/initial/reward_ctrl Mean      -0.0507207
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0507207
exploration/env_infos/initial/reward_ctrl Min       -0.0507207
exploration/env_infos/reward_ctrl Mean              -0.376646
exploration/env_infos/reward_ctrl Std                0.103769
exploration/env_infos/reward_ctrl Max               -0.0507207
exploration/env_infos/reward_ctrl Min               -0.581154
evaluation/num steps total                           1.135e+06
evaluation/num paths total                        1135
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.62537
evaluation/Rewards Std                               1.12933
evaluation/Rewards Max                               6.79223
evaluation/Rewards Min                              -0.868571
evaluation/Returns Mean                           4625.37
evaluation/Returns Std                              51.2613
evaluation/Returns Max                            4682.38
evaluation/Returns Min                            4560.39
evaluation/Actions Mean                              0.00281922
evaluation/Actions Std                               0.801617
evaluation/Actions Max                               0.999229
evaluation/Actions Min                              -0.998397
evaluation/Num Paths                                 5
evaluation/Average Returns                        4625.37
evaluation/env_infos/final/reward_run Mean           5.18689
evaluation/env_infos/final/reward_run Std            0.9845
evaluation/env_infos/final/reward_run Max            6.42201
evaluation/env_infos/final/reward_run Min            4.23398
evaluation/env_infos/initial/reward_run Mean         0.0750938
evaluation/env_infos/initial/reward_run Std          0.205978
evaluation/env_infos/initial/reward_run Max          0.466158
evaluation/env_infos/initial/reward_run Min         -0.142956
evaluation/env_infos/reward_run Mean                 5.01093
evaluation/env_infos/reward_run Std                  1.1025
evaluation/env_infos/reward_run Max                  7.08907
evaluation/env_infos/reward_run Min                 -0.407074
evaluation/env_infos/final/reward_ctrl Mean         -0.387675
evaluation/env_infos/final/reward_ctrl Std           0.119647
evaluation/env_infos/final/reward_ctrl Max          -0.224682
evaluation/env_infos/final/reward_ctrl Min          -0.536988
evaluation/env_infos/initial/reward_ctrl Mean       -0.034706
evaluation/env_infos/initial/reward_ctrl Std         0.0150185
evaluation/env_infos/initial/reward_ctrl Max        -0.0207281
evaluation/env_infos/initial/reward_ctrl Min        -0.0544015
evaluation/env_infos/reward_ctrl Mean               -0.385559
evaluation/env_infos/reward_ctrl Std                 0.108316
evaluation/env_infos/reward_ctrl Max                -0.0207281
evaluation/env_infos/reward_ctrl Min                -0.57901
time/data storing (s)                                0.00689002
time/evaluation sampling (s)                         3.1457
time/exploration sampling (s)                        1.26998
time/logging (s)                                     0.0406553
time/saving (s)                                      0.0163298
time/training (s)                                   36.6306
time/epoch (s)                                      41.1102
time/total (s)                                    9541.05
Epoch                                              226
----------------------------------------------  ---------------
2020-07-08 23:45:58.712007 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 227 finished
----------------------------------------------  ---------------
replay_buffer/size                              229000
trainer/QF1 Loss                                     6.63526
trainer/QF2 Loss                                     7.59798
trainer/Policy Loss                               -273.174
trainer/Q1 Predictions Mean                        279.215
trainer/Q1 Predictions Std                          75.142
trainer/Q1 Predictions Max                         343.419
trainer/Q1 Predictions Min                          20.3353
trainer/Q2 Predictions Mean                        279.701
trainer/Q2 Predictions Std                          75.0801
trainer/Q2 Predictions Max                         341.314
trainer/Q2 Predictions Min                          20.5196
trainer/Q Targets Mean                             279.433
trainer/Q Targets Std                               75.2637
trainer/Q Targets Max                              344.555
trainer/Q Targets Min                               19.2442
trainer/Log Pis Mean                                 6.27078
trainer/Log Pis Std                                  5.09362
trainer/Log Pis Max                                 19.4044
trainer/Log Pis Min                                 -6.40235
trainer/Policy mu Mean                              -0.0132108
trainer/Policy mu Std                                1.53699
trainer/Policy mu Max                                3.43309
trainer/Policy mu Min                               -3.63307
trainer/Policy log std Mean                         -0.827815
trainer/Policy log std Std                           0.320802
trainer/Policy log std Max                          -0.0802391
trainer/Policy log std Min                          -2.40839
trainer/Alpha                                        0.11425
trainer/Alpha Loss                                   0.587459
exploration/num steps total                     229000
exploration/num paths total                        229
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.40947
exploration/Rewards Std                              1.13336
exploration/Rewards Max                              6.57416
exploration/Rewards Min                             -1.04104
exploration/Returns Mean                          4409.47
exploration/Returns Std                              0
exploration/Returns Max                           4409.47
exploration/Returns Min                           4409.47
exploration/Actions Mean                             0.0160201
exploration/Actions Std                              0.780793
exploration/Actions Max                              0.999532
exploration/Actions Min                             -0.999273
exploration/Num Paths                                1
exploration/Average Returns                       4409.47
exploration/env_infos/final/reward_run Mean          5.06995
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.06995
exploration/env_infos/final/reward_run Min           5.06995
exploration/env_infos/initial/reward_run Mean       -0.0521507
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0521507
exploration/env_infos/initial/reward_run Min        -0.0521507
exploration/env_infos/reward_run Mean                4.77541
exploration/env_infos/reward_run Std                 1.11267
exploration/env_infos/reward_run Max                 7.00232
exploration/env_infos/reward_run Min                -0.543664
exploration/env_infos/final/reward_ctrl Mean        -0.321644
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.321644
exploration/env_infos/final/reward_ctrl Min         -0.321644
exploration/env_infos/initial/reward_ctrl Mean      -0.0366443
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0366443
exploration/env_infos/initial/reward_ctrl Min       -0.0366443
exploration/env_infos/reward_ctrl Mean              -0.365936
exploration/env_infos/reward_ctrl Std                0.10912
exploration/env_infos/reward_ctrl Max               -0.0366443
exploration/env_infos/reward_ctrl Min               -0.580149
evaluation/num steps total                           1.14e+06
evaluation/num paths total                        1140
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.59786
evaluation/Rewards Std                               1.15171
evaluation/Rewards Max                               6.75076
evaluation/Rewards Min                              -0.999514
evaluation/Returns Mean                           4597.86
evaluation/Returns Std                              80.3795
evaluation/Returns Max                            4728.67
evaluation/Returns Min                            4519.15
evaluation/Actions Mean                              0.0102015
evaluation/Actions Std                               0.800998
evaluation/Actions Max                               0.999467
evaluation/Actions Min                              -0.999727
evaluation/Num Paths                                 5
evaluation/Average Returns                        4597.86
evaluation/env_infos/final/reward_run Mean           5.27374
evaluation/env_infos/final/reward_run Std            0.631966
evaluation/env_infos/final/reward_run Max            5.73387
evaluation/env_infos/final/reward_run Min            4.06325
evaluation/env_infos/initial/reward_run Mean         0.0315958
evaluation/env_infos/initial/reward_run Std          0.143757
evaluation/env_infos/initial/reward_run Max          0.195085
evaluation/env_infos/initial/reward_run Min         -0.138858
evaluation/env_infos/reward_run Mean                 4.98288
evaluation/env_infos/reward_run Std                  1.1231
evaluation/env_infos/reward_run Max                  7.16629
evaluation/env_infos/reward_run Min                 -0.522065
evaluation/env_infos/final/reward_ctrl Mean         -0.406172
evaluation/env_infos/final/reward_ctrl Std           0.127671
evaluation/env_infos/final/reward_ctrl Max          -0.197681
evaluation/env_infos/final/reward_ctrl Min          -0.545657
evaluation/env_infos/initial/reward_ctrl Mean       -0.059307
evaluation/env_infos/initial/reward_ctrl Std         0.0375722
evaluation/env_infos/initial/reward_ctrl Max        -0.0114542
evaluation/env_infos/initial/reward_ctrl Min        -0.114905
evaluation/env_infos/reward_ctrl Mean               -0.385021
evaluation/env_infos/reward_ctrl Std                 0.109272
evaluation/env_infos/reward_ctrl Max                -0.0114542
evaluation/env_infos/reward_ctrl Min                -0.573442
time/data storing (s)                                0.00676539
time/evaluation sampling (s)                         2.62219
time/exploration sampling (s)                        0.641281
time/logging (s)                                     0.046211
time/saving (s)                                      0.0193636
time/training (s)                                   42.1112
time/epoch (s)                                      45.447
time/total (s)                                    9586.53
Epoch                                              227
----------------------------------------------  ---------------
2020-07-08 23:46:49.993873 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 228 finished
----------------------------------------------  ---------------
replay_buffer/size                              230000
trainer/QF1 Loss                                     6.4742
trainer/QF2 Loss                                     6.19397
trainer/Policy Loss                               -265.36
trainer/Q1 Predictions Mean                        271.048
trainer/Q1 Predictions Std                          91.0315
trainer/Q1 Predictions Max                         336.628
trainer/Q1 Predictions Min                          18.473
trainer/Q2 Predictions Mean                        270.811
trainer/Q2 Predictions Std                          90.9005
trainer/Q2 Predictions Max                         335.242
trainer/Q2 Predictions Min                          18.2406
trainer/Q Targets Mean                             270.561
trainer/Q Targets Std                               91.0679
trainer/Q Targets Max                              336.177
trainer/Q Targets Min                               16.3072
trainer/Log Pis Mean                                 5.88726
trainer/Log Pis Std                                  5.05571
trainer/Log Pis Max                                 18.5961
trainer/Log Pis Min                                 -5.95514
trainer/Policy mu Mean                               0.137053
trainer/Policy mu Std                                1.47483
trainer/Policy mu Max                                3.19084
trainer/Policy mu Min                               -3.77004
trainer/Policy log std Mean                         -0.820581
trainer/Policy log std Std                           0.353356
trainer/Policy log std Max                           0.16023
trainer/Policy log std Min                          -2.49349
trainer/Alpha                                        0.115016
trainer/Alpha Loss                                  -0.243811
exploration/num steps total                     230000
exploration/num paths total                        230
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.50772
exploration/Rewards Std                              1.11083
exploration/Rewards Max                              6.80894
exploration/Rewards Min                             -0.679465
exploration/Returns Mean                          4507.72
exploration/Returns Std                              0
exploration/Returns Max                           4507.72
exploration/Returns Min                           4507.72
exploration/Actions Mean                             0.0287708
exploration/Actions Std                              0.793642
exploration/Actions Max                              0.999763
exploration/Actions Min                             -0.999571
exploration/Num Paths                                1
exploration/Average Returns                       4507.72
exploration/env_infos/final/reward_run Mean          5.56131
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.56131
exploration/env_infos/final/reward_run Min           5.56131
exploration/env_infos/initial/reward_run Mean        0.374624
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.374624
exploration/env_infos/initial/reward_run Min         0.374624
exploration/env_infos/reward_run Mean                4.88614
exploration/env_infos/reward_run Std                 1.08573
exploration/env_infos/reward_run Max                 7.06911
exploration/env_infos/reward_run Min                -0.127326
exploration/env_infos/final/reward_ctrl Mean        -0.48403
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.48403
exploration/env_infos/final/reward_ctrl Min         -0.48403
exploration/env_infos/initial/reward_ctrl Mean      -0.0684479
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0684479
exploration/env_infos/initial/reward_ctrl Min       -0.0684479
exploration/env_infos/reward_ctrl Mean              -0.378418
exploration/env_infos/reward_ctrl Std                0.106043
exploration/env_infos/reward_ctrl Max               -0.0684479
exploration/env_infos/reward_ctrl Min               -0.584492
evaluation/num steps total                           1.145e+06
evaluation/num paths total                        1145
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.75189
evaluation/Rewards Std                               1.16976
evaluation/Rewards Max                               6.89502
evaluation/Rewards Min                              -0.986833
evaluation/Returns Mean                           4751.89
evaluation/Returns Std                              67.103
evaluation/Returns Max                            4842.85
evaluation/Returns Min                            4653
evaluation/Actions Mean                              0.0158179
evaluation/Actions Std                               0.81415
evaluation/Actions Max                               0.997692
evaluation/Actions Min                              -0.998075
evaluation/Num Paths                                 5
evaluation/Average Returns                        4751.89
evaluation/env_infos/final/reward_run Mean           4.57889
evaluation/env_infos/final/reward_run Std            0.966486
evaluation/env_infos/final/reward_run Max            6.35471
evaluation/env_infos/final/reward_run Min            3.55976
evaluation/env_infos/initial/reward_run Mean         0.00343936
evaluation/env_infos/initial/reward_run Std          0.199148
evaluation/env_infos/initial/reward_run Max          0.342577
evaluation/env_infos/initial/reward_run Min         -0.179698
evaluation/env_infos/reward_run Mean                 5.14975
evaluation/env_infos/reward_run Std                  1.14116
evaluation/env_infos/reward_run Max                  7.26982
evaluation/env_infos/reward_run Min                 -0.511704
evaluation/env_infos/final/reward_ctrl Mean         -0.347711
evaluation/env_infos/final/reward_ctrl Std           0.0745085
evaluation/env_infos/final/reward_ctrl Max          -0.273636
evaluation/env_infos/final/reward_ctrl Min          -0.491288
evaluation/env_infos/initial/reward_ctrl Mean       -0.0718962
evaluation/env_infos/initial/reward_ctrl Std         0.0291087
evaluation/env_infos/initial/reward_ctrl Max        -0.0240101
evaluation/env_infos/initial/reward_ctrl Min        -0.115331
evaluation/env_infos/reward_ctrl Mean               -0.397854
evaluation/env_infos/reward_ctrl Std                 0.106334
evaluation/env_infos/reward_ctrl Max                -0.0240101
evaluation/env_infos/reward_ctrl Min                -0.586795
time/data storing (s)                                0.00752774
time/evaluation sampling (s)                         4.04633
time/exploration sampling (s)                        0.993795
time/logging (s)                                     0.045836
time/saving (s)                                      0.0131858
time/training (s)                                   46.1538
time/epoch (s)                                      51.2604
time/total (s)                                    9637.8
Epoch                                              228
----------------------------------------------  ---------------
2020-07-08 23:47:36.323258 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 229 finished
----------------------------------------------  ----------------
replay_buffer/size                              231000
trainer/QF1 Loss                                     5.03104
trainer/QF2 Loss                                     7.20125
trainer/Policy Loss                               -256.389
trainer/Q1 Predictions Mean                        261.935
trainer/Q1 Predictions Std                         100.243
trainer/Q1 Predictions Max                         339.478
trainer/Q1 Predictions Min                          18.9711
trainer/Q2 Predictions Mean                        262.128
trainer/Q2 Predictions Std                         100.325
trainer/Q2 Predictions Max                         338.68
trainer/Q2 Predictions Min                          18.564
trainer/Q Targets Mean                             261.847
trainer/Q Targets Std                              100.202
trainer/Q Targets Max                              338.202
trainer/Q Targets Min                               14.4594
trainer/Log Pis Mean                                 5.80264
trainer/Log Pis Std                                  4.91486
trainer/Log Pis Max                                 18.4723
trainer/Log Pis Min                                 -6.40931
trainer/Policy mu Mean                               0.0393744
trainer/Policy mu Std                                1.49709
trainer/Policy mu Max                                4.15682
trainer/Policy mu Min                               -4.12006
trainer/Policy log std Mean                         -0.796536
trainer/Policy log std Std                           0.347905
trainer/Policy log std Max                          -0.00993743
trainer/Policy log std Min                          -2.54399
trainer/Alpha                                        0.113254
trainer/Alpha Loss                                  -0.429874
exploration/num steps total                     231000
exploration/num paths total                        231
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.62521
exploration/Rewards Std                              1.13558
exploration/Rewards Max                              6.79515
exploration/Rewards Min                             -0.74321
exploration/Returns Mean                          4625.21
exploration/Returns Std                              0
exploration/Returns Max                           4625.21
exploration/Returns Min                           4625.21
exploration/Actions Mean                             0.00315304
exploration/Actions Std                              0.800596
exploration/Actions Max                              0.999702
exploration/Actions Min                             -0.998896
exploration/Num Paths                                1
exploration/Average Returns                       4625.21
exploration/env_infos/final/reward_run Mean          5.27499
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.27499
exploration/env_infos/final/reward_run Min           5.27499
exploration/env_infos/initial/reward_run Mean        0.0152945
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0152945
exploration/env_infos/initial/reward_run Min         0.0152945
exploration/env_infos/reward_run Mean                5.00979
exploration/env_infos/reward_run Std                 1.11305
exploration/env_infos/reward_run Max                 7.15398
exploration/env_infos/reward_run Min                -0.250999
exploration/env_infos/final/reward_ctrl Mean        -0.305827
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.305827
exploration/env_infos/final/reward_ctrl Min         -0.305827
exploration/env_infos/initial/reward_ctrl Mean      -0.0705869
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0705869
exploration/env_infos/initial/reward_ctrl Min       -0.0705869
exploration/env_infos/reward_ctrl Mean              -0.384579
exploration/env_infos/reward_ctrl Std                0.101656
exploration/env_infos/reward_ctrl Max               -0.0705869
exploration/env_infos/reward_ctrl Min               -0.578004
evaluation/num steps total                           1.15e+06
evaluation/num paths total                        1150
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.65983
evaluation/Rewards Std                               1.17065
evaluation/Rewards Max                               6.79386
evaluation/Rewards Min                              -0.81671
evaluation/Returns Mean                           4659.83
evaluation/Returns Std                              75.0398
evaluation/Returns Max                            4730.3
evaluation/Returns Min                            4544.48
evaluation/Actions Mean                              0.000906425
evaluation/Actions Std                               0.80952
evaluation/Actions Max                               0.997999
evaluation/Actions Min                              -0.997256
evaluation/Num Paths                                 5
evaluation/Average Returns                        4659.83
evaluation/env_infos/final/reward_run Mean           4.6564
evaluation/env_infos/final/reward_run Std            1.1074
evaluation/env_infos/final/reward_run Max            6.6986
evaluation/env_infos/final/reward_run Min            3.80615
evaluation/env_infos/initial/reward_run Mean        -0.181514
evaluation/env_infos/initial/reward_run Std          0.149442
evaluation/env_infos/initial/reward_run Max         -0.0128787
evaluation/env_infos/initial/reward_run Min         -0.453221
evaluation/env_infos/reward_run Mean                 5.05303
evaluation/env_infos/reward_run Std                  1.14177
evaluation/env_infos/reward_run Max                  7.14729
evaluation/env_infos/reward_run Min                 -0.487657
evaluation/env_infos/final/reward_ctrl Mean         -0.444018
evaluation/env_infos/final/reward_ctrl Std           0.122395
evaluation/env_infos/final/reward_ctrl Max          -0.204673
evaluation/env_infos/final/reward_ctrl Min          -0.533331
evaluation/env_infos/initial/reward_ctrl Mean       -0.073958
evaluation/env_infos/initial/reward_ctrl Std         0.067639
evaluation/env_infos/initial/reward_ctrl Max        -0.0198542
evaluation/env_infos/initial/reward_ctrl Min        -0.195376
evaluation/env_infos/reward_ctrl Mean               -0.393194
evaluation/env_infos/reward_ctrl Std                 0.106714
evaluation/env_infos/reward_ctrl Max                -0.0198542
evaluation/env_infos/reward_ctrl Min                -0.571525
time/data storing (s)                                0.00676833
time/evaluation sampling (s)                         4.29434
time/exploration sampling (s)                        0.934591
time/logging (s)                                     0.0463829
time/saving (s)                                      0.0169095
time/training (s)                                   41.008
time/epoch (s)                                      46.307
time/total (s)                                    9684.12
Epoch                                              229
----------------------------------------------  ----------------
2020-07-08 23:48:12.723156 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 230 finished
----------------------------------------------  ---------------
replay_buffer/size                              232000
trainer/QF1 Loss                                     7.31749
trainer/QF2 Loss                                     7.40399
trainer/Policy Loss                               -279.418
trainer/Q1 Predictions Mean                        286.048
trainer/Q1 Predictions Std                          70.8138
trainer/Q1 Predictions Max                         335.93
trainer/Q1 Predictions Min                          20.6881
trainer/Q2 Predictions Mean                        286.296
trainer/Q2 Predictions Std                          70.9065
trainer/Q2 Predictions Max                         334.879
trainer/Q2 Predictions Min                          20.6575
trainer/Q Targets Mean                             286.457
trainer/Q Targets Std                               70.869
trainer/Q Targets Max                              338.968
trainer/Q Targets Min                               20.948
trainer/Log Pis Mean                                 6.83616
trainer/Log Pis Std                                  4.59732
trainer/Log Pis Max                                 20.4246
trainer/Log Pis Min                                 -6.57798
trainer/Policy mu Mean                               0.0316369
trainer/Policy mu Std                                1.56538
trainer/Policy mu Max                                3.31331
trainer/Policy mu Min                               -3.35607
trainer/Policy log std Mean                         -0.841624
trainer/Policy log std Std                           0.347716
trainer/Policy log std Max                           0.192432
trainer/Policy log std Min                          -2.55034
trainer/Alpha                                        0.1136
trainer/Alpha Loss                                   1.81885
exploration/num steps total                     232000
exploration/num paths total                        232
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.55522
exploration/Rewards Std                              1.07498
exploration/Rewards Max                              6.56105
exploration/Rewards Min                             -0.493119
exploration/Returns Mean                          4555.22
exploration/Returns Std                              0
exploration/Returns Max                           4555.22
exploration/Returns Min                           4555.22
exploration/Actions Mean                             0.0132784
exploration/Actions Std                              0.789668
exploration/Actions Max                              0.999676
exploration/Actions Min                             -0.999661
exploration/Num Paths                                1
exploration/Average Returns                       4555.22
exploration/env_infos/final/reward_run Mean          6.24421
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.24421
exploration/env_infos/final/reward_run Min           6.24421
exploration/env_infos/initial/reward_run Mean       -0.0579141
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0579141
exploration/env_infos/initial/reward_run Min        -0.0579141
exploration/env_infos/reward_run Mean                4.92947
exploration/env_infos/reward_run Std                 1.04301
exploration/env_infos/reward_run Max                 6.96246
exploration/env_infos/reward_run Min                -0.0579141
exploration/env_infos/final/reward_ctrl Mean        -0.291642
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.291642
exploration/env_infos/final/reward_ctrl Min         -0.291642
exploration/env_infos/initial/reward_ctrl Mean      -0.0887912
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0887912
exploration/env_infos/initial/reward_ctrl Min       -0.0887912
exploration/env_infos/reward_ctrl Mean              -0.374252
exploration/env_infos/reward_ctrl Std                0.10524
exploration/env_infos/reward_ctrl Max               -0.0715363
exploration/env_infos/reward_ctrl Min               -0.575962
evaluation/num steps total                           1.155e+06
evaluation/num paths total                        1155
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.70912
evaluation/Rewards Std                               1.1204
evaluation/Rewards Max                               7.06634
evaluation/Rewards Min                              -0.876949
evaluation/Returns Mean                           4709.12
evaluation/Returns Std                              18.1394
evaluation/Returns Max                            4736.14
evaluation/Returns Min                            4679.55
evaluation/Actions Mean                              0.00759491
evaluation/Actions Std                               0.800845
evaluation/Actions Max                               0.998095
evaluation/Actions Min                              -0.997485
evaluation/Num Paths                                 5
evaluation/Average Returns                        4709.12
evaluation/env_infos/final/reward_run Mean           5.41362
evaluation/env_infos/final/reward_run Std            0.676419
evaluation/env_infos/final/reward_run Max            6.23492
evaluation/env_infos/final/reward_run Min            4.62767
evaluation/env_infos/initial/reward_run Mean         0.0462302
evaluation/env_infos/initial/reward_run Std          0.217046
evaluation/env_infos/initial/reward_run Max          0.284071
evaluation/env_infos/initial/reward_run Min         -0.265101
evaluation/env_infos/reward_run Mean                 5.09397
evaluation/env_infos/reward_run Std                  1.09127
evaluation/env_infos/reward_run Max                  7.34534
evaluation/env_infos/reward_run Min                 -0.406944
evaluation/env_infos/final/reward_ctrl Mean         -0.381939
evaluation/env_infos/final/reward_ctrl Std           0.118835
evaluation/env_infos/final/reward_ctrl Max          -0.240407
evaluation/env_infos/final/reward_ctrl Min          -0.564082
evaluation/env_infos/initial/reward_ctrl Mean       -0.0404868
evaluation/env_infos/initial/reward_ctrl Std         0.0266893
evaluation/env_infos/initial/reward_ctrl Max        -0.00889478
evaluation/env_infos/initial/reward_ctrl Min        -0.0854097
evaluation/env_infos/reward_ctrl Mean               -0.384846
evaluation/env_infos/reward_ctrl Std                 0.109567
evaluation/env_infos/reward_ctrl Max                -0.00889478
evaluation/env_infos/reward_ctrl Min                -0.580764
time/data storing (s)                                0.00683419
time/evaluation sampling (s)                         2.61929
time/exploration sampling (s)                        0.659212
time/logging (s)                                     0.040404
time/saving (s)                                      0.0194131
time/training (s)                                   32.7936
time/epoch (s)                                      36.1388
time/total (s)                                    9720.51
Epoch                                              230
----------------------------------------------  ---------------
2020-07-08 23:48:58.732937 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 231 finished
----------------------------------------------  ----------------
replay_buffer/size                              233000
trainer/QF1 Loss                                     6.04795
trainer/QF2 Loss                                     7.37371
trainer/Policy Loss                               -278.119
trainer/Q1 Predictions Mean                        284.444
trainer/Q1 Predictions Std                          73.3743
trainer/Q1 Predictions Max                         338.83
trainer/Q1 Predictions Min                          18.279
trainer/Q2 Predictions Mean                        284.275
trainer/Q2 Predictions Std                          73.344
trainer/Q2 Predictions Max                         337.831
trainer/Q2 Predictions Min                          17.894
trainer/Q Targets Mean                             284.169
trainer/Q Targets Std                               73.3543
trainer/Q Targets Max                              337.608
trainer/Q Targets Min                               17.8452
trainer/Log Pis Mean                                 6.45461
trainer/Log Pis Std                                  4.51945
trainer/Log Pis Max                                 17.0571
trainer/Log Pis Min                                 -6.27199
trainer/Policy mu Mean                               0.029591
trainer/Policy mu Std                                1.52625
trainer/Policy mu Max                                3.19922
trainer/Policy mu Min                               -3.16123
trainer/Policy log std Mean                         -0.84869
trainer/Policy log std Std                           0.336717
trainer/Policy log std Max                          -0.0236487
trainer/Policy log std Min                          -2.56581
trainer/Alpha                                        0.113252
trainer/Alpha Loss                                   0.990232
exploration/num steps total                     233000
exploration/num paths total                        233
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.51985
exploration/Rewards Std                              1.10585
exploration/Rewards Max                              6.45466
exploration/Rewards Min                             -0.399455
exploration/Returns Mean                          4519.85
exploration/Returns Std                              0
exploration/Returns Max                           4519.85
exploration/Returns Min                           4519.85
exploration/Actions Mean                             0.00898101
exploration/Actions Std                              0.801146
exploration/Actions Max                              0.999491
exploration/Actions Min                             -0.999778
exploration/Num Paths                                1
exploration/Average Returns                       4519.85
exploration/env_infos/final/reward_run Mean          5.00423
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.00423
exploration/env_infos/final/reward_run Min           5.00423
exploration/env_infos/initial/reward_run Mean        0.366665
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.366665
exploration/env_infos/initial/reward_run Min         0.366665
exploration/env_infos/reward_run Mean                4.905
exploration/env_infos/reward_run Std                 1.07763
exploration/env_infos/reward_run Max                 6.90619
exploration/env_infos/reward_run Min                 0.0872825
exploration/env_infos/final/reward_ctrl Mean        -0.502636
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.502636
exploration/env_infos/final/reward_ctrl Min         -0.502636
exploration/env_infos/initial/reward_ctrl Mean      -0.129494
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.129494
exploration/env_infos/initial/reward_ctrl Min       -0.129494
exploration/env_infos/reward_ctrl Mean              -0.385149
exploration/env_infos/reward_ctrl Std                0.10269
exploration/env_infos/reward_ctrl Max               -0.0928941
exploration/env_infos/reward_ctrl Min               -0.579302
evaluation/num steps total                           1.16e+06
evaluation/num paths total                        1160
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.70136
evaluation/Rewards Std                               1.15704
evaluation/Rewards Max                               7.05721
evaluation/Rewards Min                              -0.925706
evaluation/Returns Mean                           4701.36
evaluation/Returns Std                              45.3711
evaluation/Returns Max                            4774.1
evaluation/Returns Min                            4654.45
evaluation/Actions Mean                              0.00519629
evaluation/Actions Std                               0.804843
evaluation/Actions Max                               0.999066
evaluation/Actions Min                              -0.9975
evaluation/Num Paths                                 5
evaluation/Average Returns                        4701.36
evaluation/env_infos/final/reward_run Mean           4.81838
evaluation/env_infos/final/reward_run Std            0.803954
evaluation/env_infos/final/reward_run Max            6.25137
evaluation/env_infos/final/reward_run Min            3.8426
evaluation/env_infos/initial/reward_run Mean         0.000655935
evaluation/env_infos/initial/reward_run Std          0.121304
evaluation/env_infos/initial/reward_run Max          0.220465
evaluation/env_infos/initial/reward_run Min         -0.146304
evaluation/env_infos/reward_run Mean                 5.09004
evaluation/env_infos/reward_run Std                  1.12866
evaluation/env_infos/reward_run Max                  7.32242
evaluation/env_infos/reward_run Min                 -0.388002
evaluation/env_infos/final/reward_ctrl Mean         -0.34631
evaluation/env_infos/final/reward_ctrl Std           0.132326
evaluation/env_infos/final/reward_ctrl Max          -0.156933
evaluation/env_infos/final/reward_ctrl Min          -0.509258
evaluation/env_infos/initial/reward_ctrl Mean       -0.0422173
evaluation/env_infos/initial/reward_ctrl Std         0.0341419
evaluation/env_infos/initial/reward_ctrl Max        -0.0126794
evaluation/env_infos/initial/reward_ctrl Min        -0.108773
evaluation/env_infos/reward_ctrl Mean               -0.38868
evaluation/env_infos/reward_ctrl Std                 0.108656
evaluation/env_infos/reward_ctrl Max                -0.0126794
evaluation/env_infos/reward_ctrl Min                -0.576952
time/data storing (s)                                0.00830682
time/evaluation sampling (s)                         3.71497
time/exploration sampling (s)                        0.753532
time/logging (s)                                     0.0416749
time/saving (s)                                      0.0200211
time/training (s)                                   41.3064
time/epoch (s)                                      45.8449
time/total (s)                                    9766.51
Epoch                                              231
----------------------------------------------  ----------------
2020-07-08 23:49:46.227790 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 232 finished
----------------------------------------------  ---------------
replay_buffer/size                              234000
trainer/QF1 Loss                                     4.5495
trainer/QF2 Loss                                     5.38077
trainer/Policy Loss                               -277.807
trainer/Q1 Predictions Mean                        283.754
trainer/Q1 Predictions Std                          75.9238
trainer/Q1 Predictions Max                         334.406
trainer/Q1 Predictions Min                          19.4466
trainer/Q2 Predictions Mean                        284.038
trainer/Q2 Predictions Std                          75.9449
trainer/Q2 Predictions Max                         334.442
trainer/Q2 Predictions Min                          19.7495
trainer/Q Targets Mean                             284.032
trainer/Q Targets Std                               76.2068
trainer/Q Targets Max                              338.332
trainer/Q Targets Min                               19.0336
trainer/Log Pis Mean                                 6.09975
trainer/Log Pis Std                                  4.94206
trainer/Log Pis Max                                 20.348
trainer/Log Pis Min                                 -6.70169
trainer/Policy mu Mean                               0.0397832
trainer/Policy mu Std                                1.51282
trainer/Policy mu Max                                4.32425
trainer/Policy mu Min                               -4.02802
trainer/Policy log std Mean                         -0.841837
trainer/Policy log std Std                           0.342661
trainer/Policy log std Max                          -0.0487512
trainer/Policy log std Min                          -2.77343
trainer/Alpha                                        0.112836
trainer/Alpha Loss                                   0.217658
exploration/num steps total                     234000
exploration/num paths total                        234
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.58546
exploration/Rewards Std                              1.11636
exploration/Rewards Max                              6.93226
exploration/Rewards Min                             -0.471615
exploration/Returns Mean                          4585.46
exploration/Returns Std                              0
exploration/Returns Max                           4585.46
exploration/Returns Min                           4585.46
exploration/Actions Mean                             0.00244152
exploration/Actions Std                              0.821423
exploration/Actions Max                              0.999381
exploration/Actions Min                             -0.999559
exploration/Num Paths                                1
exploration/Average Returns                       4585.46
exploration/env_infos/final/reward_run Mean          6.03318
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.03318
exploration/env_infos/final/reward_run Min           6.03318
exploration/env_infos/initial/reward_run Mean        0.0709859
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0709859
exploration/env_infos/initial/reward_run Min         0.0709859
exploration/env_infos/reward_run Mean                4.99031
exploration/env_infos/reward_run Std                 1.09527
exploration/env_infos/reward_run Max                 7.197
exploration/env_infos/reward_run Min                 0.0351029
exploration/env_infos/final/reward_ctrl Mean        -0.427831
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.427831
exploration/env_infos/final/reward_ctrl Min         -0.427831
exploration/env_infos/initial/reward_ctrl Mean      -0.0522722
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0522722
exploration/env_infos/initial/reward_ctrl Min       -0.0522722
exploration/env_infos/reward_ctrl Mean              -0.404845
exploration/env_infos/reward_ctrl Std                0.0988767
exploration/env_infos/reward_ctrl Max               -0.0522722
exploration/env_infos/reward_ctrl Min               -0.580412
evaluation/num steps total                           1.165e+06
evaluation/num paths total                        1165
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.67187
evaluation/Rewards Std                               1.163
evaluation/Rewards Max                               7.07468
evaluation/Rewards Min                              -0.797844
evaluation/Returns Mean                           4671.87
evaluation/Returns Std                              94.2951
evaluation/Returns Max                            4779.75
evaluation/Returns Min                            4502.14
evaluation/Actions Mean                             -0.00879929
evaluation/Actions Std                               0.829086
evaluation/Actions Max                               0.998576
evaluation/Actions Min                              -0.999126
evaluation/Num Paths                                 5
evaluation/Average Returns                        4671.87
evaluation/env_infos/final/reward_run Mean           5.08707
evaluation/env_infos/final/reward_run Std            0.683512
evaluation/env_infos/final/reward_run Max            6.38211
evaluation/env_infos/final/reward_run Min            4.5428
evaluation/env_infos/initial/reward_run Mean         0.140388
evaluation/env_infos/initial/reward_run Std          0.200704
evaluation/env_infos/initial/reward_run Max          0.502859
evaluation/env_infos/initial/reward_run Min         -0.0546203
evaluation/env_infos/reward_run Mean                 5.08434
evaluation/env_infos/reward_run Std                  1.14051
evaluation/env_infos/reward_run Max                  7.30183
evaluation/env_infos/reward_run Min                 -0.486316
evaluation/env_infos/final/reward_ctrl Mean         -0.389255
evaluation/env_infos/final/reward_ctrl Std           0.0696928
evaluation/env_infos/final/reward_ctrl Max          -0.320899
evaluation/env_infos/final/reward_ctrl Min          -0.47622
evaluation/env_infos/initial/reward_ctrl Mean       -0.0426419
evaluation/env_infos/initial/reward_ctrl Std         0.0251472
evaluation/env_infos/initial/reward_ctrl Max        -0.0171089
evaluation/env_infos/initial/reward_ctrl Min        -0.0757654
evaluation/env_infos/reward_ctrl Mean               -0.412476
evaluation/env_infos/reward_ctrl Std                 0.102798
evaluation/env_infos/reward_ctrl Max                -0.0121451
evaluation/env_infos/reward_ctrl Min                -0.587998
time/data storing (s)                                0.0139621
time/evaluation sampling (s)                         3.77237
time/exploration sampling (s)                        1.1802
time/logging (s)                                     0.044006
time/saving (s)                                      0.0173305
time/training (s)                                   42.4245
time/epoch (s)                                      47.4523
time/total (s)                                    9814
Epoch                                              232
----------------------------------------------  ---------------
2020-07-08 23:50:32.793900 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 233 finished
----------------------------------------------  ---------------
replay_buffer/size                              235000
trainer/QF1 Loss                                     7.07845
trainer/QF2 Loss                                     7.84974
trainer/Policy Loss                               -265.296
trainer/Q1 Predictions Mean                        271.141
trainer/Q1 Predictions Std                          92.0085
trainer/Q1 Predictions Max                         342.101
trainer/Q1 Predictions Min                          18.3938
trainer/Q2 Predictions Mean                        271.093
trainer/Q2 Predictions Std                          91.9995
trainer/Q2 Predictions Max                         341.887
trainer/Q2 Predictions Min                          19.032
trainer/Q Targets Mean                             270.804
trainer/Q Targets Std                               91.6976
trainer/Q Targets Max                              340.526
trainer/Q Targets Min                               17.8295
trainer/Log Pis Mean                                 5.91698
trainer/Log Pis Std                                  4.76642
trainer/Log Pis Max                                 16.7141
trainer/Log Pis Min                                 -6.15105
trainer/Policy mu Mean                               0.0598925
trainer/Policy mu Std                                1.50195
trainer/Policy mu Max                                2.91035
trainer/Policy mu Min                               -4.16206
trainer/Policy log std Mean                         -0.80868
trainer/Policy log std Std                           0.334101
trainer/Policy log std Max                           0.0450914
trainer/Policy log std Min                          -2.70113
trainer/Alpha                                        0.114831
trainer/Alpha Loss                                  -0.179676
exploration/num steps total                     235000
exploration/num paths total                        235
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.56402
exploration/Rewards Std                              1.18875
exploration/Rewards Max                              6.7642
exploration/Rewards Min                             -0.890614
exploration/Returns Mean                          4564.02
exploration/Returns Std                              0
exploration/Returns Max                           4564.02
exploration/Returns Min                           4564.02
exploration/Actions Mean                             0.0181654
exploration/Actions Std                              0.795574
exploration/Actions Max                              0.999733
exploration/Actions Min                             -0.999621
exploration/Num Paths                                1
exploration/Average Returns                       4564.02
exploration/env_infos/final/reward_run Mean          4.67737
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.67737
exploration/env_infos/final/reward_run Min           4.67737
exploration/env_infos/initial/reward_run Mean       -0.0378442
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0378442
exploration/env_infos/initial/reward_run Min        -0.0378442
exploration/env_infos/reward_run Mean                4.94398
exploration/env_infos/reward_run Std                 1.17424
exploration/env_infos/reward_run Max                 7.18271
exploration/env_infos/reward_run Min                -0.59562
exploration/env_infos/final/reward_ctrl Mean        -0.30417
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.30417
exploration/env_infos/final/reward_ctrl Min         -0.30417
exploration/env_infos/initial/reward_ctrl Mean      -0.0732933
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0732933
exploration/env_infos/initial/reward_ctrl Min       -0.0732933
exploration/env_infos/reward_ctrl Mean              -0.379961
exploration/env_infos/reward_ctrl Std                0.109222
exploration/env_infos/reward_ctrl Max               -0.057179
exploration/env_infos/reward_ctrl Min               -0.585663
evaluation/num steps total                           1.17e+06
evaluation/num paths total                        1170
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.65514
evaluation/Rewards Std                               1.14334
evaluation/Rewards Max                               6.88794
evaluation/Rewards Min                              -0.744998
evaluation/Returns Mean                           4655.14
evaluation/Returns Std                              77.5444
evaluation/Returns Max                            4775.5
evaluation/Returns Min                            4550.55
evaluation/Actions Mean                             -0.00636931
evaluation/Actions Std                               0.81563
evaluation/Actions Max                               0.99876
evaluation/Actions Min                              -0.997824
evaluation/Num Paths                                 5
evaluation/Average Returns                        4655.14
evaluation/env_infos/final/reward_run Mean           5.4048
evaluation/env_infos/final/reward_run Std            0.934371
evaluation/env_infos/final/reward_run Max            6.48291
evaluation/env_infos/final/reward_run Min            3.70004
evaluation/env_infos/initial/reward_run Mean        -0.0711459
evaluation/env_infos/initial/reward_run Std          0.0913279
evaluation/env_infos/initial/reward_run Max          0.0326337
evaluation/env_infos/initial/reward_run Min         -0.18096
evaluation/env_infos/reward_run Mean                 5.05432
evaluation/env_infos/reward_run Std                  1.11709
evaluation/env_infos/reward_run Max                  7.19596
evaluation/env_infos/reward_run Min                 -0.417431
evaluation/env_infos/final/reward_ctrl Mean         -0.404346
evaluation/env_infos/final/reward_ctrl Std           0.0568565
evaluation/env_infos/final/reward_ctrl Max          -0.332697
evaluation/env_infos/final/reward_ctrl Min          -0.496785
evaluation/env_infos/initial/reward_ctrl Mean       -0.096631
evaluation/env_infos/initial/reward_ctrl Std         0.0494459
evaluation/env_infos/initial/reward_ctrl Max        -0.0192116
evaluation/env_infos/initial/reward_ctrl Min        -0.150042
evaluation/env_infos/reward_ctrl Mean               -0.399175
evaluation/env_infos/reward_ctrl Std                 0.109399
evaluation/env_infos/reward_ctrl Max                -0.0192116
evaluation/env_infos/reward_ctrl Min                -0.581282
time/data storing (s)                                0.00795943
time/evaluation sampling (s)                         4.03913
time/exploration sampling (s)                        1.14049
time/logging (s)                                     0.0424082
time/saving (s)                                      0.0200841
time/training (s)                                   41.1121
time/epoch (s)                                      46.3621
time/total (s)                                    9860.56
Epoch                                              233
----------------------------------------------  ---------------
2020-07-08 23:51:18.938440 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 234 finished
----------------------------------------------  ---------------
replay_buffer/size                              236000
trainer/QF1 Loss                                     7.0436
trainer/QF2 Loss                                     7.29414
trainer/Policy Loss                               -269.045
trainer/Q1 Predictions Mean                        274.926
trainer/Q1 Predictions Std                          86.8481
trainer/Q1 Predictions Max                         341.807
trainer/Q1 Predictions Min                          19.6935
trainer/Q2 Predictions Mean                        275.669
trainer/Q2 Predictions Std                          87.002
trainer/Q2 Predictions Max                         342.17
trainer/Q2 Predictions Min                          19.6509
trainer/Q Targets Mean                             275.478
trainer/Q Targets Std                               86.8382
trainer/Q Targets Max                              338.116
trainer/Q Targets Min                               19.7976
trainer/Log Pis Mean                                 6.22726
trainer/Log Pis Std                                  4.9065
trainer/Log Pis Max                                 21.1931
trainer/Log Pis Min                                 -6.21772
trainer/Policy mu Mean                               0.0311738
trainer/Policy mu Std                                1.53429
trainer/Policy mu Max                                4.02488
trainer/Policy mu Min                               -4.07645
trainer/Policy log std Mean                         -0.831011
trainer/Policy log std Std                           0.360276
trainer/Policy log std Max                           0.264345
trainer/Policy log std Min                          -2.88641
trainer/Alpha                                        0.112948
trainer/Alpha Loss                                   0.495632
exploration/num steps total                     236000
exploration/num paths total                        236
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.3538
exploration/Rewards Std                              1.14151
exploration/Rewards Max                              6.76496
exploration/Rewards Min                             -0.567123
exploration/Returns Mean                          4353.8
exploration/Returns Std                              0
exploration/Returns Max                           4353.8
exploration/Returns Min                           4353.8
exploration/Actions Mean                             0.0123962
exploration/Actions Std                              0.798977
exploration/Actions Max                              0.999801
exploration/Actions Min                             -0.999006
exploration/Num Paths                                1
exploration/Average Returns                       4353.8
exploration/env_infos/final/reward_run Mean          6.25259
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.25259
exploration/env_infos/final/reward_run Min           6.25259
exploration/env_infos/initial/reward_run Mean       -0.357043
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.357043
exploration/env_infos/initial/reward_run Min        -0.357043
exploration/env_infos/reward_run Mean                4.73691
exploration/env_infos/reward_run Std                 1.12292
exploration/env_infos/reward_run Max                 7.09433
exploration/env_infos/reward_run Min                -0.357043
exploration/env_infos/final/reward_ctrl Mean        -0.374824
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.374824
exploration/env_infos/final/reward_ctrl Min         -0.374824
exploration/env_infos/initial/reward_ctrl Mean      -0.21008
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.21008
exploration/env_infos/initial/reward_ctrl Min       -0.21008
exploration/env_infos/reward_ctrl Mean              -0.383111
exploration/env_infos/reward_ctrl Std                0.100652
exploration/env_infos/reward_ctrl Max               -0.0882225
exploration/env_infos/reward_ctrl Min               -0.577946
evaluation/num steps total                           1.175e+06
evaluation/num paths total                        1175
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.68719
evaluation/Rewards Std                               1.19425
evaluation/Rewards Max                               7.07381
evaluation/Rewards Min                              -1.01296
evaluation/Returns Mean                           4687.19
evaluation/Returns Std                              77.3574
evaluation/Returns Max                            4777.66
evaluation/Returns Min                            4548.56
evaluation/Actions Mean                             -0.00675887
evaluation/Actions Std                               0.820548
evaluation/Actions Max                               0.999588
evaluation/Actions Min                              -0.996467
evaluation/Num Paths                                 5
evaluation/Average Returns                        4687.19
evaluation/env_infos/final/reward_run Mean           5.54523
evaluation/env_infos/final/reward_run Std            0.602103
evaluation/env_infos/final/reward_run Max            6.31097
evaluation/env_infos/final/reward_run Min            4.62798
evaluation/env_infos/initial/reward_run Mean        -0.0711617
evaluation/env_infos/initial/reward_run Std          0.0798666
evaluation/env_infos/initial/reward_run Max          0.0265093
evaluation/env_infos/initial/reward_run Min         -0.166476
evaluation/env_infos/reward_run Mean                 5.0912
evaluation/env_infos/reward_run Std                  1.17174
evaluation/env_infos/reward_run Max                  7.3534
evaluation/env_infos/reward_run Min                 -0.516593
evaluation/env_infos/final/reward_ctrl Mean         -0.438675
evaluation/env_infos/final/reward_ctrl Std           0.103178
evaluation/env_infos/final/reward_ctrl Max          -0.242486
evaluation/env_infos/final/reward_ctrl Min          -0.548384
evaluation/env_infos/initial/reward_ctrl Mean       -0.0329065
evaluation/env_infos/initial/reward_ctrl Std         0.0215816
evaluation/env_infos/initial/reward_ctrl Max        -0.009309
evaluation/env_infos/initial/reward_ctrl Min        -0.0720984
evaluation/env_infos/reward_ctrl Mean               -0.404006
evaluation/env_infos/reward_ctrl Std                 0.103635
evaluation/env_infos/reward_ctrl Max                -0.009309
evaluation/env_infos/reward_ctrl Min                -0.577343
time/data storing (s)                                0.0135395
time/evaluation sampling (s)                         3.43623
time/exploration sampling (s)                        1.0111
time/logging (s)                                     0.0483328
time/saving (s)                                      0.0186892
time/training (s)                                   41.5795
time/epoch (s)                                      46.1074
time/total (s)                                    9906.7
Epoch                                              234
----------------------------------------------  ---------------
2020-07-08 23:52:04.462811 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 235 finished
----------------------------------------------  ---------------
replay_buffer/size                              237000
trainer/QF1 Loss                                     9.93929
trainer/QF2 Loss                                     8.21122
trainer/Policy Loss                               -267.908
trainer/Q1 Predictions Mean                        273.673
trainer/Q1 Predictions Std                          91.1526
trainer/Q1 Predictions Max                         344.019
trainer/Q1 Predictions Min                          19.4973
trainer/Q2 Predictions Mean                        274.219
trainer/Q2 Predictions Std                          91.1877
trainer/Q2 Predictions Max                         342.77
trainer/Q2 Predictions Min                          19.5936
trainer/Q Targets Mean                             274.133
trainer/Q Targets Std                               90.9995
trainer/Q Targets Max                              341.942
trainer/Q Targets Min                               20.1744
trainer/Log Pis Mean                                 5.88229
trainer/Log Pis Std                                  4.62465
trainer/Log Pis Max                                 20.2391
trainer/Log Pis Min                                 -4.79487
trainer/Policy mu Mean                               0.0710058
trainer/Policy mu Std                                1.44815
trainer/Policy mu Max                                3.62093
trainer/Policy mu Min                               -3.88219
trainer/Policy log std Mean                         -0.825052
trainer/Policy log std Std                           0.346683
trainer/Policy log std Max                          -0.0661472
trainer/Policy log std Min                          -2.40117
trainer/Alpha                                        0.113811
trainer/Alpha Loss                                  -0.255786
exploration/num steps total                     237000
exploration/num paths total                        237
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.47406
exploration/Rewards Std                              1.13424
exploration/Rewards Max                              6.62376
exploration/Rewards Min                             -0.824857
exploration/Returns Mean                          4474.06
exploration/Returns Std                              0
exploration/Returns Max                           4474.06
exploration/Returns Min                           4474.06
exploration/Actions Mean                             0.0118987
exploration/Actions Std                              0.800335
exploration/Actions Max                              0.999046
exploration/Actions Min                             -0.999727
exploration/Num Paths                                1
exploration/Average Returns                       4474.06
exploration/env_infos/final/reward_run Mean          4.93843
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.93843
exploration/env_infos/final/reward_run Min           4.93843
exploration/env_infos/initial/reward_run Mean        0.172185
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.172185
exploration/env_infos/initial/reward_run Min         0.172185
exploration/env_infos/reward_run Mean                4.85847
exploration/env_infos/reward_run Std                 1.12229
exploration/env_infos/reward_run Max                 7.00339
exploration/env_infos/reward_run Min                -0.315816
exploration/env_infos/final/reward_ctrl Mean        -0.449761
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.449761
exploration/env_infos/final/reward_ctrl Min         -0.449761
exploration/env_infos/initial/reward_ctrl Mean      -0.0656636
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0656636
exploration/env_infos/initial/reward_ctrl Min       -0.0656636
exploration/env_infos/reward_ctrl Mean              -0.384407
exploration/env_infos/reward_ctrl Std                0.101482
exploration/env_infos/reward_ctrl Max               -0.0656636
exploration/env_infos/reward_ctrl Min               -0.577828
evaluation/num steps total                           1.18e+06
evaluation/num paths total                        1180
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.70856
evaluation/Rewards Std                               1.16178
evaluation/Rewards Max                               7.17059
evaluation/Rewards Min                              -0.91625
evaluation/Returns Mean                           4708.56
evaluation/Returns Std                              92.1055
evaluation/Returns Max                            4811.6
evaluation/Returns Min                            4585.05
evaluation/Actions Mean                              0.00828991
evaluation/Actions Std                               0.819702
evaluation/Actions Max                               0.998971
evaluation/Actions Min                              -0.997582
evaluation/Num Paths                                 5
evaluation/Average Returns                        4708.56
evaluation/env_infos/final/reward_run Mean           5.2947
evaluation/env_infos/final/reward_run Std            0.994359
evaluation/env_infos/final/reward_run Max            6.42403
evaluation/env_infos/final/reward_run Min            3.65746
evaluation/env_infos/initial/reward_run Mean        -0.0127157
evaluation/env_infos/initial/reward_run Std          0.183251
evaluation/env_infos/initial/reward_run Max          0.275784
evaluation/env_infos/initial/reward_run Min         -0.204513
evaluation/env_infos/reward_run Mean                 5.11175
evaluation/env_infos/reward_run Std                  1.13517
evaluation/env_infos/reward_run Max                  7.44528
evaluation/env_infos/reward_run Min                 -0.596608
evaluation/env_infos/final/reward_ctrl Mean         -0.330914
evaluation/env_infos/final/reward_ctrl Std           0.128118
evaluation/env_infos/final/reward_ctrl Max          -0.155373
evaluation/env_infos/final/reward_ctrl Min          -0.497406
evaluation/env_infos/initial/reward_ctrl Mean       -0.0883532
evaluation/env_infos/initial/reward_ctrl Std         0.069127
evaluation/env_infos/initial/reward_ctrl Max        -0.0193931
evaluation/env_infos/initial/reward_ctrl Min        -0.217686
evaluation/env_infos/reward_ctrl Mean               -0.403188
evaluation/env_infos/reward_ctrl Std                 0.102382
evaluation/env_infos/reward_ctrl Max                -0.0193931
evaluation/env_infos/reward_ctrl Min                -0.57627
time/data storing (s)                                0.013487
time/evaluation sampling (s)                         3.47883
time/exploration sampling (s)                        1.13321
time/logging (s)                                     0.0449347
time/saving (s)                                      0.0184023
time/training (s)                                   40.7417
time/epoch (s)                                      45.4305
time/total (s)                                    9952.21
Epoch                                              235
----------------------------------------------  ---------------
2020-07-08 23:52:48.149029 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 236 finished
----------------------------------------------  ---------------
replay_buffer/size                              238000
trainer/QF1 Loss                                     5.55215
trainer/QF2 Loss                                     6.38148
trainer/Policy Loss                               -271.76
trainer/Q1 Predictions Mean                        277.896
trainer/Q1 Predictions Std                          87.3046
trainer/Q1 Predictions Max                         339.235
trainer/Q1 Predictions Min                          17.1947
trainer/Q2 Predictions Mean                        277.648
trainer/Q2 Predictions Std                          87.3709
trainer/Q2 Predictions Max                         338.759
trainer/Q2 Predictions Min                          18.2264
trainer/Q Targets Mean                             277.971
trainer/Q Targets Std                               87.3207
trainer/Q Targets Max                              340.023
trainer/Q Targets Min                               16.6545
trainer/Log Pis Mean                                 6.01323
trainer/Log Pis Std                                  4.6541
trainer/Log Pis Max                                 17.3704
trainer/Log Pis Min                                 -6.6241
trainer/Policy mu Mean                               0.123963
trainer/Policy mu Std                                1.50811
trainer/Policy mu Max                                3.71461
trainer/Policy mu Min                               -3.23441
trainer/Policy log std Mean                         -0.817371
trainer/Policy log std Std                           0.351711
trainer/Policy log std Max                           0.0297807
trainer/Policy log std Min                          -2.66206
trainer/Alpha                                        0.11352
trainer/Alpha Loss                                   0.0287851
exploration/num steps total                     238000
exploration/num paths total                        238
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.46256
exploration/Rewards Std                              1.13743
exploration/Rewards Max                              6.69542
exploration/Rewards Min                             -0.517634
exploration/Returns Mean                          4462.56
exploration/Returns Std                              0
exploration/Returns Max                           4462.56
exploration/Returns Min                           4462.56
exploration/Actions Mean                             0.0395331
exploration/Actions Std                              0.80381
exploration/Actions Max                              0.999822
exploration/Actions Min                             -0.999512
exploration/Num Paths                                1
exploration/Average Returns                       4462.56
exploration/env_infos/final/reward_run Mean          6.07045
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.07045
exploration/env_infos/final/reward_run Min           6.07045
exploration/env_infos/initial/reward_run Mean       -0.00785519
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.00785519
exploration/env_infos/initial/reward_run Min        -0.00785519
exploration/env_infos/reward_run Mean                4.85117
exploration/env_infos/reward_run Std                 1.10817
exploration/env_infos/reward_run Max                 7.12944
exploration/env_infos/reward_run Min                -0.00865845
exploration/env_infos/final/reward_ctrl Mean        -0.197935
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.197935
exploration/env_infos/final/reward_ctrl Min         -0.197935
exploration/env_infos/initial/reward_ctrl Mean      -0.200799
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.200799
exploration/env_infos/initial/reward_ctrl Min       -0.200799
exploration/env_infos/reward_ctrl Mean              -0.388604
exploration/env_infos/reward_ctrl Std                0.101658
exploration/env_infos/reward_ctrl Max               -0.116984
exploration/env_infos/reward_ctrl Min               -0.586604
evaluation/num steps total                           1.185e+06
evaluation/num paths total                        1185
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.65885
evaluation/Rewards Std                               1.16005
evaluation/Rewards Max                               6.96048
evaluation/Rewards Min                              -0.847536
evaluation/Returns Mean                           4658.85
evaluation/Returns Std                              39.4384
evaluation/Returns Max                            4694.79
evaluation/Returns Min                            4585.47
evaluation/Actions Mean                              0.0274303
evaluation/Actions Std                               0.81251
evaluation/Actions Max                               0.999179
evaluation/Actions Min                              -0.998539
evaluation/Num Paths                                 5
evaluation/Average Returns                        4658.85
evaluation/env_infos/final/reward_run Mean           4.57968
evaluation/env_infos/final/reward_run Std            0.208026
evaluation/env_infos/final/reward_run Max            4.91106
evaluation/env_infos/final/reward_run Min            4.32896
evaluation/env_infos/initial/reward_run Mean         0.10423
evaluation/env_infos/initial/reward_run Std          0.218693
evaluation/env_infos/initial/reward_run Max          0.483863
evaluation/env_infos/initial/reward_run Min         -0.129217
evaluation/env_infos/reward_run Mean                 5.05541
evaluation/env_infos/reward_run Std                  1.12663
evaluation/env_infos/reward_run Max                  7.25275
evaluation/env_infos/reward_run Min                 -0.444056
evaluation/env_infos/final/reward_ctrl Mean         -0.332457
evaluation/env_infos/final/reward_ctrl Std           0.0600637
evaluation/env_infos/final/reward_ctrl Max          -0.275017
evaluation/env_infos/final/reward_ctrl Min          -0.448477
evaluation/env_infos/initial/reward_ctrl Mean       -0.108396
evaluation/env_infos/initial/reward_ctrl Std         0.0723284
evaluation/env_infos/initial/reward_ctrl Max        -0.00976839
evaluation/env_infos/initial/reward_ctrl Min        -0.211059
evaluation/env_infos/reward_ctrl Mean               -0.396555
evaluation/env_infos/reward_ctrl Std                 0.106409
evaluation/env_infos/reward_ctrl Max                -0.00976839
evaluation/env_infos/reward_ctrl Min                -0.576232
time/data storing (s)                                0.00829077
time/evaluation sampling (s)                         4.44866
time/exploration sampling (s)                        0.905959
time/logging (s)                                     0.0427926
time/saving (s)                                      0.016038
time/training (s)                                   38.1903
time/epoch (s)                                      43.6121
time/total (s)                                    9995.89
Epoch                                              236
----------------------------------------------  ---------------
2020-07-08 23:53:25.864028 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 237 finished
----------------------------------------------  ---------------
replay_buffer/size                              239000
trainer/QF1 Loss                                     6.26872
trainer/QF2 Loss                                     7.88153
trainer/Policy Loss                               -261.472
trainer/Q1 Predictions Mean                        267.55
trainer/Q1 Predictions Std                         100.108
trainer/Q1 Predictions Max                         345.354
trainer/Q1 Predictions Min                          19.3121
trainer/Q2 Predictions Mean                        267.35
trainer/Q2 Predictions Std                          99.9108
trainer/Q2 Predictions Max                         346.354
trainer/Q2 Predictions Min                          19.5681
trainer/Q Targets Mean                             268.434
trainer/Q Targets Std                              100.518
trainer/Q Targets Max                              345.645
trainer/Q Targets Min                               18.4936
trainer/Log Pis Mean                                 6.07234
trainer/Log Pis Std                                  5.24884
trainer/Log Pis Max                                 30.6745
trainer/Log Pis Min                                 -6.86
trainer/Policy mu Mean                               0.0193029
trainer/Policy mu Std                                1.52176
trainer/Policy mu Max                                5.31155
trainer/Policy mu Min                               -3.97125
trainer/Policy log std Mean                         -0.808669
trainer/Policy log std Std                           0.369716
trainer/Policy log std Max                           0.0262216
trainer/Policy log std Min                          -2.66072
trainer/Alpha                                        0.114824
trainer/Alpha Loss                                   0.156573
exploration/num steps total                     239000
exploration/num paths total                        239
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.55927
exploration/Rewards Std                              1.08783
exploration/Rewards Max                              6.58687
exploration/Rewards Min                             -0.456383
exploration/Returns Mean                          4559.27
exploration/Returns Std                              0
exploration/Returns Max                           4559.27
exploration/Returns Min                           4559.27
exploration/Actions Mean                             0.0121046
exploration/Actions Std                              0.798219
exploration/Actions Max                              0.999915
exploration/Actions Min                             -0.999602
exploration/Num Paths                                1
exploration/Average Returns                       4559.27
exploration/env_infos/final/reward_run Mean          4.50448
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.50448
exploration/env_infos/final/reward_run Min           4.50448
exploration/env_infos/initial/reward_run Mean       -0.216882
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.216882
exploration/env_infos/initial/reward_run Min        -0.216882
exploration/env_infos/reward_run Mean                4.94165
exploration/env_infos/reward_run Std                 1.06067
exploration/env_infos/reward_run Max                 7.00245
exploration/env_infos/reward_run Min                -0.216882
exploration/env_infos/final/reward_ctrl Mean        -0.531659
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.531659
exploration/env_infos/final/reward_ctrl Min         -0.531659
exploration/env_infos/initial/reward_ctrl Mean      -0.0889336
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0889336
exploration/env_infos/initial/reward_ctrl Min       -0.0889336
exploration/env_infos/reward_ctrl Mean              -0.38238
exploration/env_infos/reward_ctrl Std                0.103344
exploration/env_infos/reward_ctrl Max               -0.0889336
exploration/env_infos/reward_ctrl Min               -0.574674
evaluation/num steps total                           1.19e+06
evaluation/num paths total                        1190
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.6704
evaluation/Rewards Std                               1.13932
evaluation/Rewards Max                               6.94509
evaluation/Rewards Min                              -0.79745
evaluation/Returns Mean                           4670.4
evaluation/Returns Std                              35.4859
evaluation/Returns Max                            4720.46
evaluation/Returns Min                            4625.12
evaluation/Actions Mean                              0.00889992
evaluation/Actions Std                               0.807002
evaluation/Actions Max                               0.997848
evaluation/Actions Min                              -0.996891
evaluation/Num Paths                                 5
evaluation/Average Returns                        4670.4
evaluation/env_infos/final/reward_run Mean           4.76379
evaluation/env_infos/final/reward_run Std            1.07111
evaluation/env_infos/final/reward_run Max            6.40423
evaluation/env_infos/final/reward_run Min            3.43594
evaluation/env_infos/initial/reward_run Mean        -0.126081
evaluation/env_infos/initial/reward_run Std          0.0950382
evaluation/env_infos/initial/reward_run Max         -0.0298182
evaluation/env_infos/initial/reward_run Min         -0.24694
evaluation/env_infos/reward_run Mean                 5.0612
evaluation/env_infos/reward_run Std                  1.10784
evaluation/env_infos/reward_run Max                  7.33215
evaluation/env_infos/reward_run Min                 -0.269242
evaluation/env_infos/final/reward_ctrl Mean         -0.392482
evaluation/env_infos/final/reward_ctrl Std           0.098367
evaluation/env_infos/final/reward_ctrl Max          -0.282944
evaluation/env_infos/final/reward_ctrl Min          -0.531223
evaluation/env_infos/initial/reward_ctrl Mean       -0.0903236
evaluation/env_infos/initial/reward_ctrl Std         0.0307275
evaluation/env_infos/initial/reward_ctrl Max        -0.0354108
evaluation/env_infos/initial/reward_ctrl Min        -0.128597
evaluation/env_infos/reward_ctrl Mean               -0.390799
evaluation/env_infos/reward_ctrl Std                 0.110226
evaluation/env_infos/reward_ctrl Max                -0.0120683
evaluation/env_infos/reward_ctrl Min                -0.579525
time/data storing (s)                                0.0067726
time/evaluation sampling (s)                         2.45709
time/exploration sampling (s)                        0.632202
time/logging (s)                                     0.0419088
time/saving (s)                                      0.0185723
time/training (s)                                   34.5253
time/epoch (s)                                      37.6819
time/total (s)                                   10033.6
Epoch                                              237
----------------------------------------------  ---------------
2020-07-08 23:54:08.213472 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 238 finished
----------------------------------------------  ---------------
replay_buffer/size                              240000
trainer/QF1 Loss                                     5.9847
trainer/QF2 Loss                                     6.53377
trainer/Policy Loss                               -271.632
trainer/Q1 Predictions Mean                        278.036
trainer/Q1 Predictions Std                          92.0094
trainer/Q1 Predictions Max                         342.37
trainer/Q1 Predictions Min                          19.5295
trainer/Q2 Predictions Mean                        277.975
trainer/Q2 Predictions Std                          91.7826
trainer/Q2 Predictions Max                         342.619
trainer/Q2 Predictions Min                          20.0508
trainer/Q Targets Mean                             277.733
trainer/Q Targets Std                               91.9102
trainer/Q Targets Max                              340.529
trainer/Q Targets Min                               20.2002
trainer/Log Pis Mean                                 5.97559
trainer/Log Pis Std                                  4.99747
trainer/Log Pis Max                                 17.8984
trainer/Log Pis Min                                 -7.92205
trainer/Policy mu Mean                               0.0348869
trainer/Policy mu Std                                1.49057
trainer/Policy mu Max                                3.61559
trainer/Policy mu Min                               -3.3416
trainer/Policy log std Mean                         -0.822542
trainer/Policy log std Std                           0.3498
trainer/Policy log std Max                          -0.070844
trainer/Policy log std Min                          -2.67751
trainer/Alpha                                        0.1175
trainer/Alpha Loss                                  -0.0522635
exploration/num steps total                     240000
exploration/num paths total                        240
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             1.49096
exploration/Rewards Std                              2.05339
exploration/Rewards Max                              6.59319
exploration/Rewards Min                             -1.61877
exploration/Returns Mean                          1490.96
exploration/Returns Std                              0
exploration/Returns Max                           1490.96
exploration/Returns Min                           1490.96
exploration/Actions Mean                            -0.0280005
exploration/Actions Std                              0.715371
exploration/Actions Max                              0.999979
exploration/Actions Min                             -0.999762
exploration/Num Paths                                1
exploration/Average Returns                       1490.96
exploration/env_infos/final/reward_run Mean          1.47925
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           1.47925
exploration/env_infos/final/reward_run Min           1.47925
exploration/env_infos/initial/reward_run Mean       -0.120025
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.120025
exploration/env_infos/initial/reward_run Min        -0.120025
exploration/env_infos/reward_run Mean                1.79848
exploration/env_infos/reward_run Std                 2.08139
exploration/env_infos/reward_run Max                 6.93042
exploration/env_infos/reward_run Min                -1.33864
exploration/env_infos/final/reward_ctrl Mean        -0.219268
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.219268
exploration/env_infos/final/reward_ctrl Min         -0.219268
exploration/env_infos/initial/reward_ctrl Mean      -0.0910516
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0910516
exploration/env_infos/initial/reward_ctrl Min       -0.0910516
exploration/env_infos/reward_ctrl Mean              -0.307523
exploration/env_infos/reward_ctrl Std                0.102254
exploration/env_infos/reward_ctrl Max               -0.0365503
exploration/env_infos/reward_ctrl Min               -0.562294
evaluation/num steps total                           1.195e+06
evaluation/num paths total                        1195
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.64379
evaluation/Rewards Std                               1.13769
evaluation/Rewards Max                               7.12723
evaluation/Rewards Min                              -0.812749
evaluation/Returns Mean                           4643.79
evaluation/Returns Std                              73.5438
evaluation/Returns Max                            4782.82
evaluation/Returns Min                            4572.52
evaluation/Actions Mean                              0.0246169
evaluation/Actions Std                               0.799034
evaluation/Actions Max                               0.998927
evaluation/Actions Min                              -0.998509
evaluation/Num Paths                                 5
evaluation/Average Returns                        4643.79
evaluation/env_infos/final/reward_run Mean           5.12369
evaluation/env_infos/final/reward_run Std            0.784316
evaluation/env_infos/final/reward_run Max            6.2959
evaluation/env_infos/final/reward_run Min            4.14632
evaluation/env_infos/initial/reward_run Mean         0.293649
evaluation/env_infos/initial/reward_run Std          0.447678
evaluation/env_infos/initial/reward_run Max          0.88731
evaluation/env_infos/initial/reward_run Min         -0.189722
evaluation/env_infos/reward_run Mean                 5.02723
evaluation/env_infos/reward_run Std                  1.10424
evaluation/env_infos/reward_run Max                  7.35677
evaluation/env_infos/reward_run Min                 -0.439869
evaluation/env_infos/final/reward_ctrl Mean         -0.344386
evaluation/env_infos/final/reward_ctrl Std           0.111227
evaluation/env_infos/final/reward_ctrl Max          -0.199621
evaluation/env_infos/final/reward_ctrl Min          -0.509642
evaluation/env_infos/initial/reward_ctrl Mean       -0.0909102
evaluation/env_infos/initial/reward_ctrl Std         0.04244
evaluation/env_infos/initial/reward_ctrl Max        -0.0383795
evaluation/env_infos/initial/reward_ctrl Min        -0.153809
evaluation/env_infos/reward_ctrl Mean               -0.383437
evaluation/env_infos/reward_ctrl Std                 0.112127
evaluation/env_infos/reward_ctrl Max                -0.0383795
evaluation/env_infos/reward_ctrl Min                -0.574741
time/data storing (s)                                0.00802316
time/evaluation sampling (s)                         4.01064
time/exploration sampling (s)                        1.10064
time/logging (s)                                     0.0464519
time/saving (s)                                      0.018623
time/training (s)                                   37.1421
time/epoch (s)                                      42.3265
time/total (s)                                   10075.9
Epoch                                              238
----------------------------------------------  ---------------
2020-07-08 23:54:58.614727 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 239 finished
----------------------------------------------  ---------------
replay_buffer/size                              241000
trainer/QF1 Loss                                     5.69271
trainer/QF2 Loss                                     5.46296
trainer/Policy Loss                               -275.176
trainer/Q1 Predictions Mean                        281.486
trainer/Q1 Predictions Std                          87.8223
trainer/Q1 Predictions Max                         342.827
trainer/Q1 Predictions Min                          16.8194
trainer/Q2 Predictions Mean                        281.687
trainer/Q2 Predictions Std                          87.7188
trainer/Q2 Predictions Max                         341.66
trainer/Q2 Predictions Min                          17.146
trainer/Q Targets Mean                             281.029
trainer/Q Targets Std                               87.6479
trainer/Q Targets Max                              341.802
trainer/Q Targets Min                               17.1964
trainer/Log Pis Mean                                 6.30818
trainer/Log Pis Std                                  5.0875
trainer/Log Pis Max                                 18.7579
trainer/Log Pis Min                                 -6.74437
trainer/Policy mu Mean                               0.00079664
trainer/Policy mu Std                                1.53039
trainer/Policy mu Max                                3.24826
trainer/Policy mu Min                               -3.81967
trainer/Policy log std Mean                         -0.822384
trainer/Policy log std Std                           0.347172
trainer/Policy log std Max                          -0.0184802
trainer/Policy log std Min                          -2.58788
trainer/Alpha                                        0.114652
trainer/Alpha Loss                                   0.66744
exploration/num steps total                     241000
exploration/num paths total                        241
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.50856
exploration/Rewards Std                              1.0675
exploration/Rewards Max                              6.63427
exploration/Rewards Min                             -0.322612
exploration/Returns Mean                          4508.56
exploration/Returns Std                              0
exploration/Returns Max                           4508.56
exploration/Returns Min                           4508.56
exploration/Actions Mean                             0.0264641
exploration/Actions Std                              0.786656
exploration/Actions Max                              0.999814
exploration/Actions Min                             -0.999438
exploration/Num Paths                                1
exploration/Average Returns                       4508.56
exploration/env_infos/final/reward_run Mean          6.33486
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.33486
exploration/env_infos/final/reward_run Min           6.33486
exploration/env_infos/initial/reward_run Mean        0.576249
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.576249
exploration/env_infos/initial/reward_run Min         0.576249
exploration/env_infos/reward_run Mean                4.88027
exploration/env_infos/reward_run Std                 1.03462
exploration/env_infos/reward_run Max                 6.92027
exploration/env_infos/reward_run Min                 0.045212
exploration/env_infos/final/reward_ctrl Mean        -0.383896
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.383896
exploration/env_infos/final/reward_ctrl Min         -0.383896
exploration/env_infos/initial/reward_ctrl Mean      -0.10379
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.10379
exploration/env_infos/initial/reward_ctrl Min       -0.10379
exploration/env_infos/reward_ctrl Mean              -0.371717
exploration/env_infos/reward_ctrl Std                0.110325
exploration/env_infos/reward_ctrl Max               -0.0241073
exploration/env_infos/reward_ctrl Min               -0.573993
evaluation/num steps total                           1.2e+06
evaluation/num paths total                        1200
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.6833
evaluation/Rewards Std                               1.15926
evaluation/Rewards Max                               7.06273
evaluation/Rewards Min                              -0.682253
evaluation/Returns Mean                           4683.3
evaluation/Returns Std                              85.9565
evaluation/Returns Max                            4785.07
evaluation/Returns Min                            4589.04
evaluation/Actions Mean                              0.0207871
evaluation/Actions Std                               0.805422
evaluation/Actions Max                               0.998802
evaluation/Actions Min                              -0.998395
evaluation/Num Paths                                 5
evaluation/Average Returns                        4683.3
evaluation/env_infos/final/reward_run Mean           5.87573
evaluation/env_infos/final/reward_run Std            0.740674
evaluation/env_infos/final/reward_run Max            6.4729
evaluation/env_infos/final/reward_run Min            4.43494
evaluation/env_infos/initial/reward_run Mean        -0.161175
evaluation/env_infos/initial/reward_run Std          0.179202
evaluation/env_infos/initial/reward_run Max          0.143221
evaluation/env_infos/initial/reward_run Min         -0.330089
evaluation/env_infos/reward_run Mean                 5.07278
evaluation/env_infos/reward_run Std                  1.12726
evaluation/env_infos/reward_run Max                  7.47312
evaluation/env_infos/reward_run Min                 -0.330089
evaluation/env_infos/final/reward_ctrl Mean         -0.345393
evaluation/env_infos/final/reward_ctrl Std           0.10708
evaluation/env_infos/final/reward_ctrl Max          -0.216068
evaluation/env_infos/final/reward_ctrl Min          -0.514392
evaluation/env_infos/initial/reward_ctrl Mean       -0.139147
evaluation/env_infos/initial/reward_ctrl Std         0.0577045
evaluation/env_infos/initial/reward_ctrl Max        -0.0616728
evaluation/env_infos/initial/reward_ctrl Min        -0.201591
evaluation/env_infos/reward_ctrl Mean               -0.389482
evaluation/env_infos/reward_ctrl Std                 0.1145
evaluation/env_infos/reward_ctrl Max                -0.0512644
evaluation/env_infos/reward_ctrl Min                -0.578322
time/data storing (s)                                0.0405941
time/evaluation sampling (s)                         4.40758
time/exploration sampling (s)                        1.68485
time/logging (s)                                     0.0424817
time/saving (s)                                      0.0176605
time/training (s)                                   44.1828
time/epoch (s)                                      50.3759
time/total (s)                                   10126.3
Epoch                                              239
----------------------------------------------  ---------------
2020-07-08 23:55:33.445904 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 240 finished
----------------------------------------------  ---------------
replay_buffer/size                              242000
trainer/QF1 Loss                                     6.40044
trainer/QF2 Loss                                     7.85064
trainer/Policy Loss                               -278.555
trainer/Q1 Predictions Mean                        285.687
trainer/Q1 Predictions Std                          78.9957
trainer/Q1 Predictions Max                         346.694
trainer/Q1 Predictions Min                          20.3323
trainer/Q2 Predictions Mean                        284.789
trainer/Q2 Predictions Std                          78.6682
trainer/Q2 Predictions Max                         346.93
trainer/Q2 Predictions Min                          19.2358
trainer/Q Targets Mean                             285.031
trainer/Q Targets Std                               79.0177
trainer/Q Targets Max                              347.322
trainer/Q Targets Min                               18.0621
trainer/Log Pis Mean                                 6.5209
trainer/Log Pis Std                                  5.07672
trainer/Log Pis Max                                 17.7251
trainer/Log Pis Min                                 -5.22589
trainer/Policy mu Mean                               0.0366889
trainer/Policy mu Std                                1.53901
trainer/Policy mu Max                                3.33896
trainer/Policy mu Min                               -3.41218
trainer/Policy log std Mean                         -0.84682
trainer/Policy log std Std                           0.353144
trainer/Policy log std Max                           0.0167255
trainer/Policy log std Min                          -2.75383
trainer/Alpha                                        0.113807
trainer/Alpha Loss                                   1.13208
exploration/num steps total                     242000
exploration/num paths total                        242
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.50177
exploration/Rewards Std                              1.12684
exploration/Rewards Max                              6.47697
exploration/Rewards Min                             -1.0479
exploration/Returns Mean                          4501.77
exploration/Returns Std                              0
exploration/Returns Max                           4501.77
exploration/Returns Min                           4501.77
exploration/Actions Mean                             0.0307714
exploration/Actions Std                              0.787731
exploration/Actions Max                              0.999469
exploration/Actions Min                             -0.999459
exploration/Num Paths                                1
exploration/Average Returns                       4501.77
exploration/env_infos/final/reward_run Mean          4.03554
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.03554
exploration/env_infos/final/reward_run Min           4.03554
exploration/env_infos/initial/reward_run Mean       -0.250257
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.250257
exploration/env_infos/initial/reward_run Min        -0.250257
exploration/env_infos/reward_run Mean                4.87465
exploration/env_infos/reward_run Std                 1.101
exploration/env_infos/reward_run Max                 6.8142
exploration/env_infos/reward_run Min                -0.624522
exploration/env_infos/final/reward_ctrl Mean        -0.526914
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.526914
exploration/env_infos/final/reward_ctrl Min         -0.526914
exploration/env_infos/initial/reward_ctrl Mean      -0.0488392
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0488392
exploration/env_infos/initial/reward_ctrl Min       -0.0488392
exploration/env_infos/reward_ctrl Mean              -0.37288
exploration/env_infos/reward_ctrl Std                0.105936
exploration/env_infos/reward_ctrl Max               -0.0488392
exploration/env_infos/reward_ctrl Min               -0.581764
evaluation/num steps total                           1.205e+06
evaluation/num paths total                        1205
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.58256
evaluation/Rewards Std                               1.13249
evaluation/Rewards Max                               6.82875
evaluation/Rewards Min                              -1.00587
evaluation/Returns Mean                           4582.56
evaluation/Returns Std                              40.5968
evaluation/Returns Max                            4633.73
evaluation/Returns Min                            4511.62
evaluation/Actions Mean                              0.020782
evaluation/Actions Std                               0.788428
evaluation/Actions Max                               0.998496
evaluation/Actions Min                              -0.998481
evaluation/Num Paths                                 5
evaluation/Average Returns                        4582.56
evaluation/env_infos/final/reward_run Mean           5.96503
evaluation/env_infos/final/reward_run Std            0.716823
evaluation/env_infos/final/reward_run Max            6.69963
evaluation/env_infos/final/reward_run Min            4.78401
evaluation/env_infos/initial/reward_run Mean         0.0290252
evaluation/env_infos/initial/reward_run Std          0.193341
evaluation/env_infos/initial/reward_run Max          0.228464
evaluation/env_infos/initial/reward_run Min         -0.272332
evaluation/env_infos/reward_run Mean                 4.95579
evaluation/env_infos/reward_run Std                  1.10247
evaluation/env_infos/reward_run Max                  7.27754
evaluation/env_infos/reward_run Min                 -0.497242
evaluation/env_infos/final/reward_ctrl Mean         -0.3485
evaluation/env_infos/final/reward_ctrl Std           0.0631432
evaluation/env_infos/final/reward_ctrl Max          -0.287187
evaluation/env_infos/final/reward_ctrl Min          -0.45591
evaluation/env_infos/initial/reward_ctrl Mean       -0.0741139
evaluation/env_infos/initial/reward_ctrl Std         0.0559352
evaluation/env_infos/initial/reward_ctrl Max        -0.0227501
evaluation/env_infos/initial/reward_ctrl Min        -0.178553
evaluation/env_infos/reward_ctrl Mean               -0.37323
evaluation/env_infos/reward_ctrl Std                 0.10994
evaluation/env_infos/reward_ctrl Max                -0.0227501
evaluation/env_infos/reward_ctrl Min                -0.577017
time/data storing (s)                                0.00664368
time/evaluation sampling (s)                         2.54277
time/exploration sampling (s)                        0.644049
time/logging (s)                                     0.0401456
time/saving (s)                                      0.0159916
time/training (s)                                   31.5583
time/epoch (s)                                      34.8079
time/total (s)                                   10161.2
Epoch                                              240
----------------------------------------------  ---------------
2020-07-08 23:56:09.364301 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 241 finished
----------------------------------------------  ----------------
replay_buffer/size                              243000
trainer/QF1 Loss                                     5.95203
trainer/QF2 Loss                                     6.91558
trainer/Policy Loss                               -278.049
trainer/Q1 Predictions Mean                        283.998
trainer/Q1 Predictions Std                          79.9373
trainer/Q1 Predictions Max                         345.462
trainer/Q1 Predictions Min                          19.1418
trainer/Q2 Predictions Mean                        284.378
trainer/Q2 Predictions Std                          79.9127
trainer/Q2 Predictions Max                         343.769
trainer/Q2 Predictions Min                          19.6866
trainer/Q Targets Mean                             284.498
trainer/Q Targets Std                               80.0703
trainer/Q Targets Max                              346.047
trainer/Q Targets Min                               19.6375
trainer/Log Pis Mean                                 6.15998
trainer/Log Pis Std                                  4.65181
trainer/Log Pis Max                                 16.9998
trainer/Log Pis Min                                 -5.46622
trainer/Policy mu Mean                               0.0468864
trainer/Policy mu Std                                1.51124
trainer/Policy mu Max                                3.45194
trainer/Policy mu Min                               -3.13633
trainer/Policy log std Mean                         -0.83035
trainer/Policy log std Std                           0.337707
trainer/Policy log std Max                          -0.0767061
trainer/Policy log std Min                          -2.57171
trainer/Alpha                                        0.114752
trainer/Alpha Loss                                   0.346352
exploration/num steps total                     243000
exploration/num paths total                        243
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.53957
exploration/Rewards Std                              1.11235
exploration/Rewards Max                              6.47212
exploration/Rewards Min                             -0.544219
exploration/Returns Mean                          4539.57
exploration/Returns Std                              0
exploration/Returns Max                           4539.57
exploration/Returns Min                           4539.57
exploration/Actions Mean                            -0.00102322
exploration/Actions Std                              0.801392
exploration/Actions Max                              0.999728
exploration/Actions Min                             -0.999332
exploration/Num Paths                                1
exploration/Average Returns                       4539.57
exploration/env_infos/final/reward_run Mean          3.84893
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           3.84893
exploration/env_infos/final/reward_run Min           3.84893
exploration/env_infos/initial/reward_run Mean       -0.124393
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.124393
exploration/env_infos/initial/reward_run Min        -0.124393
exploration/env_infos/reward_run Mean                4.9249
exploration/env_infos/reward_run Std                 1.08692
exploration/env_infos/reward_run Max                 6.76096
exploration/env_infos/reward_run Min                -0.124393
exploration/env_infos/final/reward_ctrl Mean        -0.534071
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.534071
exploration/env_infos/final/reward_ctrl Min         -0.534071
exploration/env_infos/initial/reward_ctrl Mean      -0.0852748
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0852748
exploration/env_infos/initial/reward_ctrl Min       -0.0852748
exploration/env_infos/reward_ctrl Mean              -0.385338
exploration/env_infos/reward_ctrl Std                0.102093
exploration/env_infos/reward_ctrl Max               -0.0530462
exploration/env_infos/reward_ctrl Min               -0.570994
evaluation/num steps total                           1.21e+06
evaluation/num paths total                        1210
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.68571
evaluation/Rewards Std                               1.1517
evaluation/Rewards Max                               7.19389
evaluation/Rewards Min                              -0.886133
evaluation/Returns Mean                           4685.71
evaluation/Returns Std                              76.9958
evaluation/Returns Max                            4803.63
evaluation/Returns Min                            4581.88
evaluation/Actions Mean                             -0.000409496
evaluation/Actions Std                               0.811767
evaluation/Actions Max                               0.997761
evaluation/Actions Min                              -0.997359
evaluation/Num Paths                                 5
evaluation/Average Returns                        4685.71
evaluation/env_infos/final/reward_run Mean           5.3268
evaluation/env_infos/final/reward_run Std            0.415
evaluation/env_infos/final/reward_run Max            6.1305
evaluation/env_infos/final/reward_run Min            4.99819
evaluation/env_infos/initial/reward_run Mean        -0.286359
evaluation/env_infos/initial/reward_run Std          0.212079
evaluation/env_infos/initial/reward_run Max          0.0168539
evaluation/env_infos/initial/reward_run Min         -0.558306
evaluation/env_infos/reward_run Mean                 5.08109
evaluation/env_infos/reward_run Std                  1.12664
evaluation/env_infos/reward_run Max                  7.46034
evaluation/env_infos/reward_run Min                 -0.558306
evaluation/env_infos/final/reward_ctrl Mean         -0.323697
evaluation/env_infos/final/reward_ctrl Std           0.0976494
evaluation/env_infos/final/reward_ctrl Max          -0.242667
evaluation/env_infos/final/reward_ctrl Min          -0.513009
evaluation/env_infos/initial/reward_ctrl Mean       -0.114693
evaluation/env_infos/initial/reward_ctrl Std         0.0664321
evaluation/env_infos/initial/reward_ctrl Max        -0.0530316
evaluation/env_infos/initial/reward_ctrl Min        -0.213539
evaluation/env_infos/reward_ctrl Mean               -0.39538
evaluation/env_infos/reward_ctrl Std                 0.10679
evaluation/env_infos/reward_ctrl Max                -0.0342463
evaluation/env_infos/reward_ctrl Min                -0.579218
time/data storing (s)                                0.00667977
time/evaluation sampling (s)                         2.46391
time/exploration sampling (s)                        0.625267
time/logging (s)                                     0.0421734
time/saving (s)                                      0.0161321
time/training (s)                                   32.7228
time/epoch (s)                                      35.877
time/total (s)                                   10197.1
Epoch                                              241
----------------------------------------------  ----------------
2020-07-08 23:56:44.526780 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 242 finished
----------------------------------------------  ---------------
replay_buffer/size                              244000
trainer/QF1 Loss                                     5.66907
trainer/QF2 Loss                                     7.55884
trainer/Policy Loss                               -265.203
trainer/Q1 Predictions Mean                        270.688
trainer/Q1 Predictions Std                          95.7014
trainer/Q1 Predictions Max                         343.787
trainer/Q1 Predictions Min                          19.3962
trainer/Q2 Predictions Mean                        271.859
trainer/Q2 Predictions Std                          96.1683
trainer/Q2 Predictions Max                         343.404
trainer/Q2 Predictions Min                          19.548
trainer/Q Targets Mean                             271.242
trainer/Q Targets Std                               95.8303
trainer/Q Targets Max                              344.33
trainer/Q Targets Min                               18.9179
trainer/Log Pis Mean                                 5.90503
trainer/Log Pis Std                                  5.45691
trainer/Log Pis Max                                 17.3971
trainer/Log Pis Min                                 -5.99971
trainer/Policy mu Mean                               0.0333858
trainer/Policy mu Std                                1.49427
trainer/Policy mu Max                                3.48314
trainer/Policy mu Min                               -3.62788
trainer/Policy log std Mean                         -0.820531
trainer/Policy log std Std                           0.363678
trainer/Policy log std Max                           0.0788858
trainer/Policy log std Min                          -2.57366
trainer/Alpha                                        0.114195
trainer/Alpha Loss                                  -0.206058
exploration/num steps total                     244000
exploration/num paths total                        244
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.50642
exploration/Rewards Std                              1.12714
exploration/Rewards Max                              6.79682
exploration/Rewards Min                             -0.8355
exploration/Returns Mean                          4506.42
exploration/Returns Std                              0
exploration/Returns Max                           4506.42
exploration/Returns Min                           4506.42
exploration/Actions Mean                             0.0300259
exploration/Actions Std                              0.801365
exploration/Actions Max                              0.99979
exploration/Actions Min                             -0.999518
exploration/Num Paths                                1
exploration/Average Returns                       4506.42
exploration/env_infos/final/reward_run Mean          4.63765
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.63765
exploration/env_infos/final/reward_run Min           4.63765
exploration/env_infos/initial/reward_run Mean        0.272052
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.272052
exploration/env_infos/initial/reward_run Min         0.272052
exploration/env_infos/reward_run Mean                4.89227
exploration/env_infos/reward_run Std                 1.10289
exploration/env_infos/reward_run Max                 7.23342
exploration/env_infos/reward_run Min                -0.36065
exploration/env_infos/final/reward_ctrl Mean        -0.404216
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.404216
exploration/env_infos/final/reward_ctrl Min         -0.404216
exploration/env_infos/initial/reward_ctrl Mean      -0.0418313
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0418313
exploration/env_infos/initial/reward_ctrl Min       -0.0418313
exploration/env_infos/reward_ctrl Mean              -0.385853
exploration/env_infos/reward_ctrl Std                0.103675
exploration/env_infos/reward_ctrl Max               -0.0418313
exploration/env_infos/reward_ctrl Min               -0.582035
evaluation/num steps total                           1.215e+06
evaluation/num paths total                        1215
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.78049
evaluation/Rewards Std                               1.15988
evaluation/Rewards Max                               7.03652
evaluation/Rewards Min                              -0.767653
evaluation/Returns Mean                           4780.49
evaluation/Returns Std                              92.3661
evaluation/Returns Max                            4921.26
evaluation/Returns Min                            4670.23
evaluation/Actions Mean                              0.0165559
evaluation/Actions Std                               0.817966
evaluation/Actions Max                               0.998544
evaluation/Actions Min                              -0.998734
evaluation/Num Paths                                 5
evaluation/Average Returns                        4780.49
evaluation/env_infos/final/reward_run Mean           5.07994
evaluation/env_infos/final/reward_run Std            0.807946
evaluation/env_infos/final/reward_run Max            6.54767
evaluation/env_infos/final/reward_run Min            4.31023
evaluation/env_infos/initial/reward_run Mean        -0.154046
evaluation/env_infos/initial/reward_run Std          0.196146
evaluation/env_infos/initial/reward_run Max          0.185953
evaluation/env_infos/initial/reward_run Min         -0.413553
evaluation/env_infos/reward_run Mean                 5.1821
evaluation/env_infos/reward_run Std                  1.12812
evaluation/env_infos/reward_run Max                  7.28827
evaluation/env_infos/reward_run Min                 -0.413553
evaluation/env_infos/final/reward_ctrl Mean         -0.379975
evaluation/env_infos/final/reward_ctrl Std           0.107844
evaluation/env_infos/final/reward_ctrl Max          -0.2416
evaluation/env_infos/final/reward_ctrl Min          -0.521895
evaluation/env_infos/initial/reward_ctrl Mean       -0.121727
evaluation/env_infos/initial/reward_ctrl Std         0.0569422
evaluation/env_infos/initial/reward_ctrl Max        -0.0478745
evaluation/env_infos/initial/reward_ctrl Min        -0.186307
evaluation/env_infos/reward_ctrl Mean               -0.401605
evaluation/env_infos/reward_ctrl Std                 0.104349
evaluation/env_infos/reward_ctrl Max                -0.0303666
evaluation/env_infos/reward_ctrl Min                -0.578454
time/data storing (s)                                0.00669421
time/evaluation sampling (s)                         2.71574
time/exploration sampling (s)                        0.647843
time/logging (s)                                     0.169975
time/saving (s)                                      0.0169765
time/training (s)                                   31.7079
time/epoch (s)                                      35.2652
time/total (s)                                   10232.3
Epoch                                              242
----------------------------------------------  ---------------
2020-07-08 23:57:20.018549 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 243 finished
----------------------------------------------  ---------------
replay_buffer/size                              245000
trainer/QF1 Loss                                     5.95465
trainer/QF2 Loss                                     6.79502
trainer/Policy Loss                               -276.676
trainer/Q1 Predictions Mean                        282.917
trainer/Q1 Predictions Std                          82.7644
trainer/Q1 Predictions Max                         351.882
trainer/Q1 Predictions Min                          16.9217
trainer/Q2 Predictions Mean                        283.625
trainer/Q2 Predictions Std                          83.0396
trainer/Q2 Predictions Max                         353.392
trainer/Q2 Predictions Min                          17.7637
trainer/Q Targets Mean                             282.753
trainer/Q Targets Std                               82.6305
trainer/Q Targets Max                              351.041
trainer/Q Targets Min                               17.5665
trainer/Log Pis Mean                                 6.43295
trainer/Log Pis Std                                  4.72109
trainer/Log Pis Max                                 21.0376
trainer/Log Pis Min                                 -5.81319
trainer/Policy mu Mean                               0.125619
trainer/Policy mu Std                                1.53426
trainer/Policy mu Max                                4.0054
trainer/Policy mu Min                               -4.1131
trainer/Policy log std Mean                         -0.814703
trainer/Policy log std Std                           0.349685
trainer/Policy log std Max                           0.00120299
trainer/Policy log std Min                          -2.7934
trainer/Alpha                                        0.114505
trainer/Alpha Loss                                   0.938294
exploration/num steps total                     245000
exploration/num paths total                        245
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.59289
exploration/Rewards Std                              1.08904
exploration/Rewards Max                              6.74017
exploration/Rewards Min                             -0.468283
exploration/Returns Mean                          4592.89
exploration/Returns Std                              0
exploration/Returns Max                           4592.89
exploration/Returns Min                           4592.89
exploration/Actions Mean                             0.0229494
exploration/Actions Std                              0.799422
exploration/Actions Max                              0.999752
exploration/Actions Min                             -0.99949
exploration/Num Paths                                1
exploration/Average Returns                       4592.89
exploration/env_infos/final/reward_run Mean          5.03703
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.03703
exploration/env_infos/final/reward_run Min           5.03703
exploration/env_infos/initial/reward_run Mean       -0.335164
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.335164
exploration/env_infos/initial/reward_run Min        -0.335164
exploration/env_infos/reward_run Mean                4.97665
exploration/env_infos/reward_run Std                 1.06799
exploration/env_infos/reward_run Max                 7.022
exploration/env_infos/reward_run Min                -0.335164
exploration/env_infos/final/reward_ctrl Mean        -0.386036
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.386036
exploration/env_infos/final/reward_ctrl Min         -0.386036
exploration/env_infos/initial/reward_ctrl Mean      -0.133118
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.133118
exploration/env_infos/initial/reward_ctrl Min       -0.133118
exploration/env_infos/reward_ctrl Mean              -0.383762
exploration/env_infos/reward_ctrl Std                0.102141
exploration/env_infos/reward_ctrl Max               -0.0950617
exploration/env_infos/reward_ctrl Min               -0.581884
evaluation/num steps total                           1.22e+06
evaluation/num paths total                        1220
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.69442
evaluation/Rewards Std                               1.17279
evaluation/Rewards Max                               7.12882
evaluation/Rewards Min                              -0.948916
evaluation/Returns Mean                           4694.42
evaluation/Returns Std                              63.2463
evaluation/Returns Max                            4764.96
evaluation/Returns Min                            4575.71
evaluation/Actions Mean                              0.0135971
evaluation/Actions Std                               0.810763
evaluation/Actions Max                               0.999503
evaluation/Actions Min                              -0.998153
evaluation/Num Paths                                 5
evaluation/Average Returns                        4694.42
evaluation/env_infos/final/reward_run Mean           5.4236
evaluation/env_infos/final/reward_run Std            1.01407
evaluation/env_infos/final/reward_run Max            6.31909
evaluation/env_infos/final/reward_run Min            3.48203
evaluation/env_infos/initial/reward_run Mean        -0.125118
evaluation/env_infos/initial/reward_run Std          0.0998315
evaluation/env_infos/initial/reward_run Max          0.0148542
evaluation/env_infos/initial/reward_run Min         -0.270306
evaluation/env_infos/reward_run Mean                 5.08894
evaluation/env_infos/reward_run Std                  1.14413
evaluation/env_infos/reward_run Max                  7.50377
evaluation/env_infos/reward_run Min                 -0.495275
evaluation/env_infos/final/reward_ctrl Mean         -0.347885
evaluation/env_infos/final/reward_ctrl Std           0.130961
evaluation/env_infos/final/reward_ctrl Max          -0.184381
evaluation/env_infos/final/reward_ctrl Min          -0.528536
evaluation/env_infos/initial/reward_ctrl Mean       -0.0991584
evaluation/env_infos/initial/reward_ctrl Std         0.0771311
evaluation/env_infos/initial/reward_ctrl Max        -0.00671703
evaluation/env_infos/initial/reward_ctrl Min        -0.212007
evaluation/env_infos/reward_ctrl Mean               -0.394513
evaluation/env_infos/reward_ctrl Std                 0.104311
evaluation/env_infos/reward_ctrl Max                -0.00671703
evaluation/env_infos/reward_ctrl Min                -0.579724
time/data storing (s)                                0.009015
time/evaluation sampling (s)                         2.51919
time/exploration sampling (s)                        0.63263
time/logging (s)                                     0.0506495
time/saving (s)                                      0.0182179
time/training (s)                                   31.9589
time/epoch (s)                                      35.1886
time/total (s)                                   10267.7
Epoch                                              243
----------------------------------------------  ---------------
2020-07-08 23:57:56.018120 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 244 finished
----------------------------------------------  ---------------
replay_buffer/size                              246000
trainer/QF1 Loss                                     5.99403
trainer/QF2 Loss                                     6.01323
trainer/Policy Loss                               -282.052
trainer/Q1 Predictions Mean                        288.193
trainer/Q1 Predictions Std                          76.3382
trainer/Q1 Predictions Max                         341.373
trainer/Q1 Predictions Min                          18.2146
trainer/Q2 Predictions Mean                        288.453
trainer/Q2 Predictions Std                          76.3144
trainer/Q2 Predictions Max                         343.106
trainer/Q2 Predictions Min                          18.3301
trainer/Q Targets Mean                             288.844
trainer/Q Targets Std                               76.6434
trainer/Q Targets Max                              346.962
trainer/Q Targets Min                               18.1823
trainer/Log Pis Mean                                 6.26766
trainer/Log Pis Std                                  4.82166
trainer/Log Pis Max                                 17.6372
trainer/Log Pis Min                                 -5.69805
trainer/Policy mu Mean                               0.0528265
trainer/Policy mu Std                                1.4803
trainer/Policy mu Max                                3.20179
trainer/Policy mu Min                               -4.3666
trainer/Policy log std Mean                         -0.866743
trainer/Policy log std Std                           0.376069
trainer/Policy log std Max                           0.0210577
trainer/Policy log std Min                          -2.58172
trainer/Alpha                                        0.114252
trainer/Alpha Loss                                   0.580611
exploration/num steps total                     246000
exploration/num paths total                        246
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.47364
exploration/Rewards Std                              1.08978
exploration/Rewards Max                              6.58986
exploration/Rewards Min                             -0.530439
exploration/Returns Mean                          4473.64
exploration/Returns Std                              0
exploration/Returns Max                           4473.64
exploration/Returns Min                           4473.64
exploration/Actions Mean                             0.0154499
exploration/Actions Std                              0.798049
exploration/Actions Max                              0.999703
exploration/Actions Min                             -0.999514
exploration/Num Paths                                1
exploration/Average Returns                       4473.64
exploration/env_infos/final/reward_run Mean          4.06234
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.06234
exploration/env_infos/final/reward_run Min           4.06234
exploration/env_infos/initial/reward_run Mean        0.285453
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.285453
exploration/env_infos/initial/reward_run Min         0.285453
exploration/env_infos/reward_run Mean                4.85592
exploration/env_infos/reward_run Std                 1.06463
exploration/env_infos/reward_run Max                 6.89132
exploration/env_infos/reward_run Min                -0.0121523
exploration/env_infos/final/reward_ctrl Mean        -0.492379
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.492379
exploration/env_infos/final/reward_ctrl Min         -0.492379
exploration/env_infos/initial/reward_ctrl Mean      -0.139237
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.139237
exploration/env_infos/initial/reward_ctrl Min       -0.139237
exploration/env_infos/reward_ctrl Mean              -0.382272
exploration/env_infos/reward_ctrl Std                0.106115
exploration/env_infos/reward_ctrl Max               -0.0610304
exploration/env_infos/reward_ctrl Min               -0.575387
evaluation/num steps total                           1.225e+06
evaluation/num paths total                        1225
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.68128
evaluation/Rewards Std                               1.16115
evaluation/Rewards Max                               6.98455
evaluation/Rewards Min                              -0.889263
evaluation/Returns Mean                           4681.28
evaluation/Returns Std                              80.1609
evaluation/Returns Max                            4808.47
evaluation/Returns Min                            4556.76
evaluation/Actions Mean                              0.0235178
evaluation/Actions Std                               0.807875
evaluation/Actions Max                               0.998402
evaluation/Actions Min                              -0.99662
evaluation/Num Paths                                 5
evaluation/Average Returns                        4681.28
evaluation/env_infos/final/reward_run Mean           4.7869
evaluation/env_infos/final/reward_run Std            1.04529
evaluation/env_infos/final/reward_run Max            6.86134
evaluation/env_infos/final/reward_run Min            4.16184
evaluation/env_infos/initial/reward_run Mean         0.0121645
evaluation/env_infos/initial/reward_run Std          0.116961
evaluation/env_infos/initial/reward_run Max          0.231073
evaluation/env_infos/initial/reward_run Min         -0.102069
evaluation/env_infos/reward_run Mean                 5.07321
evaluation/env_infos/reward_run Std                  1.13057
evaluation/env_infos/reward_run Max                  7.2915
evaluation/env_infos/reward_run Min                 -0.422018
evaluation/env_infos/final/reward_ctrl Mean         -0.415126
evaluation/env_infos/final/reward_ctrl Std           0.0675313
evaluation/env_infos/final/reward_ctrl Max          -0.349125
evaluation/env_infos/final/reward_ctrl Min          -0.522295
evaluation/env_infos/initial/reward_ctrl Mean       -0.0775709
evaluation/env_infos/initial/reward_ctrl Std         0.0641849
evaluation/env_infos/initial/reward_ctrl Max        -0.0268311
evaluation/env_infos/initial/reward_ctrl Min        -0.202268
evaluation/env_infos/reward_ctrl Mean               -0.39193
evaluation/env_infos/reward_ctrl Std                 0.110061
evaluation/env_infos/reward_ctrl Max                -0.0268311
evaluation/env_infos/reward_ctrl Min                -0.576017
time/data storing (s)                                0.00668238
time/evaluation sampling (s)                         2.57112
time/exploration sampling (s)                        0.635632
time/logging (s)                                     0.0442816
time/saving (s)                                      0.0192482
time/training (s)                                   32.5026
time/epoch (s)                                      35.7796
time/total (s)                                   10303.7
Epoch                                              244
----------------------------------------------  ---------------
2020-07-08 23:58:35.158747 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 245 finished
----------------------------------------------  ---------------
replay_buffer/size                              247000
trainer/QF1 Loss                                     5.51859
trainer/QF2 Loss                                     6.53997
trainer/Policy Loss                               -277.732
trainer/Q1 Predictions Mean                        284.292
trainer/Q1 Predictions Std                          86.2802
trainer/Q1 Predictions Max                         348.69
trainer/Q1 Predictions Min                          18.7758
trainer/Q2 Predictions Mean                        284.54
trainer/Q2 Predictions Std                          86.2507
trainer/Q2 Predictions Max                         347.502
trainer/Q2 Predictions Min                          19.0932
trainer/Q Targets Mean                             284.952
trainer/Q Targets Std                               86.5276
trainer/Q Targets Max                              349.376
trainer/Q Targets Min                               18.3056
trainer/Log Pis Mean                                 6.5161
trainer/Log Pis Std                                  4.93877
trainer/Log Pis Max                                 21.4669
trainer/Log Pis Min                                 -6.3961
trainer/Policy mu Mean                               0.014338
trainer/Policy mu Std                                1.55462
trainer/Policy mu Max                                3.2121
trainer/Policy mu Min                               -3.20678
trainer/Policy log std Mean                         -0.839402
trainer/Policy log std Std                           0.367119
trainer/Policy log std Max                          -0.082748
trainer/Policy log std Min                          -2.65616
trainer/Alpha                                        0.115773
trainer/Alpha Loss                                   1.11279
exploration/num steps total                     247000
exploration/num paths total                        247
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.55074
exploration/Rewards Std                              1.10963
exploration/Rewards Max                              6.5085
exploration/Rewards Min                             -0.421978
exploration/Returns Mean                          4550.74
exploration/Returns Std                              0
exploration/Returns Max                           4550.74
exploration/Returns Min                           4550.74
exploration/Actions Mean                             0.0102334
exploration/Actions Std                              0.799343
exploration/Actions Max                              0.999447
exploration/Actions Min                             -0.999554
exploration/Num Paths                                1
exploration/Average Returns                       4550.74
exploration/env_infos/final/reward_run Mean          5.36223
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.36223
exploration/env_infos/final/reward_run Min           5.36223
exploration/env_infos/initial/reward_run Mean       -0.0215007
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.0215007
exploration/env_infos/initial/reward_run Min        -0.0215007
exploration/env_infos/reward_run Mean                4.93417
exploration/env_infos/reward_run Std                 1.08769
exploration/env_infos/reward_run Max                 6.89756
exploration/env_infos/reward_run Min                -0.136189
exploration/env_infos/final/reward_ctrl Mean        -0.339536
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.339536
exploration/env_infos/final/reward_ctrl Min         -0.339536
exploration/env_infos/initial/reward_ctrl Mean      -0.0673219
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0673219
exploration/env_infos/initial/reward_ctrl Min       -0.0673219
exploration/env_infos/reward_ctrl Mean              -0.383432
exploration/env_infos/reward_ctrl Std                0.109961
exploration/env_infos/reward_ctrl Max               -0.0673219
exploration/env_infos/reward_ctrl Min               -0.582197
evaluation/num steps total                           1.23e+06
evaluation/num paths total                        1230
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.62445
evaluation/Rewards Std                               1.1372
evaluation/Rewards Max                               6.96889
evaluation/Rewards Min                              -0.990305
evaluation/Returns Mean                           4624.45
evaluation/Returns Std                              87.9896
evaluation/Returns Max                            4699.18
evaluation/Returns Min                            4456.23
evaluation/Actions Mean                              0.00329292
evaluation/Actions Std                               0.811333
evaluation/Actions Max                               0.998002
evaluation/Actions Min                              -0.998707
evaluation/Num Paths                                 5
evaluation/Average Returns                        4624.45
evaluation/env_infos/final/reward_run Mean           4.75472
evaluation/env_infos/final/reward_run Std            0.787813
evaluation/env_infos/final/reward_run Max            6.27659
evaluation/env_infos/final/reward_run Min            4.13666
evaluation/env_infos/initial/reward_run Mean         0.165213
evaluation/env_infos/initial/reward_run Std          0.0697726
evaluation/env_infos/initial/reward_run Max          0.256829
evaluation/env_infos/initial/reward_run Min          0.0736239
evaluation/env_infos/reward_run Mean                 5.01942
evaluation/env_infos/reward_run Std                  1.10813
evaluation/env_infos/reward_run Max                  7.26983
evaluation/env_infos/reward_run Min                 -0.450252
evaluation/env_infos/final/reward_ctrl Mean         -0.43303
evaluation/env_infos/final/reward_ctrl Std           0.0987411
evaluation/env_infos/final/reward_ctrl Max          -0.242511
evaluation/env_infos/final/reward_ctrl Min          -0.530048
evaluation/env_infos/initial/reward_ctrl Mean       -0.038644
evaluation/env_infos/initial/reward_ctrl Std         0.00947027
evaluation/env_infos/initial/reward_ctrl Max        -0.0259313
evaluation/env_infos/initial/reward_ctrl Min        -0.0535165
evaluation/env_infos/reward_ctrl Mean               -0.394963
evaluation/env_infos/reward_ctrl Std                 0.111514
evaluation/env_infos/reward_ctrl Max                -0.0217512
evaluation/env_infos/reward_ctrl Min                -0.581439
time/data storing (s)                                0.00696035
time/evaluation sampling (s)                         2.6022
time/exploration sampling (s)                        0.651083
time/logging (s)                                     0.0426465
time/saving (s)                                      0.0167226
time/training (s)                                   35.6994
time/epoch (s)                                      39.019
time/total (s)                                   10342.8
Epoch                                              245
----------------------------------------------  ---------------
2020-07-08 23:59:17.912796 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 246 finished
----------------------------------------------  ---------------
replay_buffer/size                              248000
trainer/QF1 Loss                                     8.79512
trainer/QF2 Loss                                    11.3201
trainer/Policy Loss                               -280.938
trainer/Q1 Predictions Mean                        286.503
trainer/Q1 Predictions Std                          81.596
trainer/Q1 Predictions Max                         345.25
trainer/Q1 Predictions Min                          19.4099
trainer/Q2 Predictions Mean                        286.793
trainer/Q2 Predictions Std                          81.6533
trainer/Q2 Predictions Max                         345.437
trainer/Q2 Predictions Min                          19.7325
trainer/Q Targets Mean                             286.106
trainer/Q Targets Std                               81.6817
trainer/Q Targets Max                              344.653
trainer/Q Targets Min                               19.9017
trainer/Log Pis Mean                                 5.81918
trainer/Log Pis Std                                  4.53172
trainer/Log Pis Max                                 17.9067
trainer/Log Pis Min                                 -6.67014
trainer/Policy mu Mean                               0.0740881
trainer/Policy mu Std                                1.45736
trainer/Policy mu Max                                3.9592
trainer/Policy mu Min                               -2.92841
trainer/Policy log std Mean                         -0.863037
trainer/Policy log std Std                           0.356165
trainer/Policy log std Max                          -0.0988559
trainer/Policy log std Min                          -2.42792
trainer/Alpha                                        0.1138
trainer/Alpha Loss                                  -0.39298
exploration/num steps total                     248000
exploration/num paths total                        248
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.51894
exploration/Rewards Std                              1.09451
exploration/Rewards Max                              6.69288
exploration/Rewards Min                             -1.07957
exploration/Returns Mean                          4518.94
exploration/Returns Std                              0
exploration/Returns Max                           4518.94
exploration/Returns Min                           4518.94
exploration/Actions Mean                             0.0217932
exploration/Actions Std                              0.782147
exploration/Actions Max                              0.999613
exploration/Actions Min                             -0.999597
exploration/Num Paths                                1
exploration/Average Returns                       4518.94
exploration/env_infos/final/reward_run Mean          5.32315
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           5.32315
exploration/env_infos/final/reward_run Min           5.32315
exploration/env_infos/initial/reward_run Mean       -0.256385
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.256385
exploration/env_infos/initial/reward_run Min        -0.256385
exploration/env_infos/reward_run Mean                4.88628
exploration/env_infos/reward_run Std                 1.06491
exploration/env_infos/reward_run Max                 7.04594
exploration/env_infos/reward_run Min                -0.575664
exploration/env_infos/final/reward_ctrl Mean        -0.205254
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.205254
exploration/env_infos/final/reward_ctrl Min         -0.205254
exploration/env_infos/initial/reward_ctrl Mean      -0.107469
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.107469
exploration/env_infos/initial/reward_ctrl Min       -0.107469
exploration/env_infos/reward_ctrl Mean              -0.367337
exploration/env_infos/reward_ctrl Std                0.103691
exploration/env_infos/reward_ctrl Max               -0.0888736
exploration/env_infos/reward_ctrl Min               -0.584761
evaluation/num steps total                           1.235e+06
evaluation/num paths total                        1235
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.78533
evaluation/Rewards Std                               1.15672
evaluation/Rewards Max                               6.9487
evaluation/Rewards Min                              -1.10463
evaluation/Returns Mean                           4785.33
evaluation/Returns Std                              29.8131
evaluation/Returns Max                            4830.9
evaluation/Returns Min                            4740.44
evaluation/Actions Mean                              0.0055763
evaluation/Actions Std                               0.798587
evaluation/Actions Max                               0.997656
evaluation/Actions Min                              -0.996972
evaluation/Num Paths                                 5
evaluation/Average Returns                        4785.33
evaluation/env_infos/final/reward_run Mean           5.75122
evaluation/env_infos/final/reward_run Std            0.682317
evaluation/env_infos/final/reward_run Max            6.58666
evaluation/env_infos/final/reward_run Min            4.83584
evaluation/env_infos/initial/reward_run Mean         0.120412
evaluation/env_infos/initial/reward_run Std          0.225516
evaluation/env_infos/initial/reward_run Max          0.401681
evaluation/env_infos/initial/reward_run Min         -0.217304
evaluation/env_infos/reward_run Mean                 5.16799
evaluation/env_infos/reward_run Std                  1.12548
evaluation/env_infos/reward_run Max                  7.38118
evaluation/env_infos/reward_run Min                 -0.661482
evaluation/env_infos/final/reward_ctrl Mean         -0.263035
evaluation/env_infos/final/reward_ctrl Std           0.019705
evaluation/env_infos/final/reward_ctrl Max          -0.234894
evaluation/env_infos/final/reward_ctrl Min          -0.292554
evaluation/env_infos/initial/reward_ctrl Mean       -0.056848
evaluation/env_infos/initial/reward_ctrl Std         0.0439348
evaluation/env_infos/initial/reward_ctrl Max        -0.014532
evaluation/env_infos/initial/reward_ctrl Min        -0.134648
evaluation/env_infos/reward_ctrl Mean               -0.382664
evaluation/env_infos/reward_ctrl Std                 0.108751
evaluation/env_infos/reward_ctrl Max                -0.014532
evaluation/env_infos/reward_ctrl Min                -0.571374
time/data storing (s)                                0.00688462
time/evaluation sampling (s)                         2.66275
time/exploration sampling (s)                        0.630149
time/logging (s)                                     0.044123
time/saving (s)                                      0.0162491
time/training (s)                                   39.3703
time/epoch (s)                                      42.7304
time/total (s)                                   10385.6
Epoch                                              246
----------------------------------------------  ---------------
2020-07-08 23:59:53.766997 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 247 finished
----------------------------------------------  ----------------
replay_buffer/size                              249000
trainer/QF1 Loss                                     6.99367
trainer/QF2 Loss                                     7.17512
trainer/Policy Loss                               -268.339
trainer/Q1 Predictions Mean                        274.691
trainer/Q1 Predictions Std                          94.4224
trainer/Q1 Predictions Max                         342.389
trainer/Q1 Predictions Min                          18.3373
trainer/Q2 Predictions Mean                        274.086
trainer/Q2 Predictions Std                          94.155
trainer/Q2 Predictions Max                         340.758
trainer/Q2 Predictions Min                          18.2665
trainer/Q Targets Mean                             274.84
trainer/Q Targets Std                               94.2544
trainer/Q Targets Max                              341.584
trainer/Q Targets Min                               18.9223
trainer/Log Pis Mean                                 6.18578
trainer/Log Pis Std                                  4.75557
trainer/Log Pis Max                                 22.2024
trainer/Log Pis Min                                 -5.90592
trainer/Policy mu Mean                               0.0127945
trainer/Policy mu Std                                1.51066
trainer/Policy mu Max                                3.53983
trainer/Policy mu Min                               -3.80067
trainer/Policy log std Mean                         -0.834526
trainer/Policy log std Std                           0.359029
trainer/Policy log std Max                          -0.102508
trainer/Policy log std Min                          -2.51272
trainer/Alpha                                        0.114571
trainer/Alpha Loss                                   0.402508
exploration/num steps total                     249000
exploration/num paths total                        249
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.63512
exploration/Rewards Std                              1.13309
exploration/Rewards Max                              6.62959
exploration/Rewards Min                             -0.796039
exploration/Returns Mean                          4635.12
exploration/Returns Std                              0
exploration/Returns Max                           4635.12
exploration/Returns Min                           4635.12
exploration/Actions Mean                             0.0135208
exploration/Actions Std                              0.797187
exploration/Actions Max                              0.999637
exploration/Actions Min                             -0.999302
exploration/Num Paths                                1
exploration/Average Returns                       4635.12
exploration/env_infos/final/reward_run Mean          4.86225
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.86225
exploration/env_infos/final/reward_run Min           4.86225
exploration/env_infos/initial/reward_run Mean       -0.491383
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.491383
exploration/env_infos/initial/reward_run Min        -0.491383
exploration/env_infos/reward_run Mean                5.01653
exploration/env_infos/reward_run Std                 1.10338
exploration/env_infos/reward_run Max                 7.02724
exploration/env_infos/reward_run Min                -0.491383
exploration/env_infos/final/reward_ctrl Mean        -0.502788
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.502788
exploration/env_infos/final/reward_ctrl Min         -0.502788
exploration/env_infos/initial/reward_ctrl Mean      -0.175
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.175
exploration/env_infos/initial/reward_ctrl Min       -0.175
exploration/env_infos/reward_ctrl Mean              -0.381414
exploration/env_infos/reward_ctrl Std                0.106243
exploration/env_infos/reward_ctrl Max               -0.0612792
exploration/env_infos/reward_ctrl Min               -0.582858
evaluation/num steps total                           1.24e+06
evaluation/num paths total                        1240
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.70976
evaluation/Rewards Std                               1.15193
evaluation/Rewards Max                               7.25738
evaluation/Rewards Min                              -0.965407
evaluation/Returns Mean                           4709.76
evaluation/Returns Std                              44.3474
evaluation/Returns Max                            4767.42
evaluation/Returns Min                            4632.01
evaluation/Actions Mean                              0.00793762
evaluation/Actions Std                               0.795322
evaluation/Actions Max                               0.999048
evaluation/Actions Min                              -0.997792
evaluation/Num Paths                                 5
evaluation/Average Returns                        4709.76
evaluation/env_infos/final/reward_run Mean           5.63432
evaluation/env_infos/final/reward_run Std            0.65998
evaluation/env_infos/final/reward_run Max            6.40653
evaluation/env_infos/final/reward_run Min            4.74984
evaluation/env_infos/initial/reward_run Mean        -0.000650112
evaluation/env_infos/initial/reward_run Std          0.209093
evaluation/env_infos/initial/reward_run Max          0.266459
evaluation/env_infos/initial/reward_run Min         -0.332414
evaluation/env_infos/reward_run Mean                 5.08932
evaluation/env_infos/reward_run Std                  1.12026
evaluation/env_infos/reward_run Max                  7.53591
evaluation/env_infos/reward_run Min                 -0.405641
evaluation/env_infos/final/reward_ctrl Mean         -0.384944
evaluation/env_infos/final/reward_ctrl Std           0.081245
evaluation/env_infos/final/reward_ctrl Max          -0.276668
evaluation/env_infos/final/reward_ctrl Min          -0.525506
evaluation/env_infos/initial/reward_ctrl Mean       -0.0976461
evaluation/env_infos/initial/reward_ctrl Std         0.0471295
evaluation/env_infos/initial/reward_ctrl Max        -0.0310449
evaluation/env_infos/initial/reward_ctrl Min        -0.159154
evaluation/env_infos/reward_ctrl Mean               -0.37956
evaluation/env_infos/reward_ctrl Std                 0.116725
evaluation/env_infos/reward_ctrl Max                -0.0310449
evaluation/env_infos/reward_ctrl Min                -0.577046
time/data storing (s)                                0.00709066
time/evaluation sampling (s)                         2.67116
time/exploration sampling (s)                        0.636888
time/logging (s)                                     0.041693
time/saving (s)                                      0.0159005
time/training (s)                                   32.4582
time/epoch (s)                                      35.831
time/total (s)                                   10421.4
Epoch                                              247
----------------------------------------------  ----------------
2020-07-09 00:00:28.996301 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 248 finished
----------------------------------------------  ---------------
replay_buffer/size                              250000
trainer/QF1 Loss                                     8.42964
trainer/QF2 Loss                                    13.9936
trainer/Policy Loss                               -271.381
trainer/Q1 Predictions Mean                        277.448
trainer/Q1 Predictions Std                          92.8868
trainer/Q1 Predictions Max                         343.411
trainer/Q1 Predictions Min                          19.1064
trainer/Q2 Predictions Mean                        277.79
trainer/Q2 Predictions Std                          93.062
trainer/Q2 Predictions Max                         343.65
trainer/Q2 Predictions Min                          19.531
trainer/Q Targets Mean                             277.936
trainer/Q Targets Std                               92.7418
trainer/Q Targets Max                              344.924
trainer/Q Targets Min                               19.4712
trainer/Log Pis Mean                                 6.39698
trainer/Log Pis Std                                  4.80609
trainer/Log Pis Max                                 17.8174
trainer/Log Pis Min                                 -4.33159
trainer/Policy mu Mean                              -0.0122899
trainer/Policy mu Std                                1.51002
trainer/Policy mu Max                                3.62077
trainer/Policy mu Min                               -3.40885
trainer/Policy log std Mean                         -0.828617
trainer/Policy log std Std                           0.359617
trainer/Policy log std Max                          -0.037344
trainer/Policy log std Min                          -2.60915
trainer/Alpha                                        0.115175
trainer/Alpha Loss                                   0.858121
exploration/num steps total                     250000
exploration/num paths total                        250
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.51866
exploration/Rewards Std                              1.14675
exploration/Rewards Max                              6.72157
exploration/Rewards Min                             -1.055
exploration/Returns Mean                          4518.66
exploration/Returns Std                              0
exploration/Returns Max                           4518.66
exploration/Returns Min                           4518.66
exploration/Actions Mean                            -0.00353744
exploration/Actions Std                              0.797778
exploration/Actions Max                              0.999881
exploration/Actions Min                             -0.999521
exploration/Num Paths                                1
exploration/Average Returns                       4518.66
exploration/env_infos/final/reward_run Mean          4.37923
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.37923
exploration/env_infos/final/reward_run Min           4.37923
exploration/env_infos/initial/reward_run Mean        0.328097
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.328097
exploration/env_infos/initial/reward_run Min         0.328097
exploration/env_infos/reward_run Mean                4.90053
exploration/env_infos/reward_run Std                 1.12689
exploration/env_infos/reward_run Max                 7.13311
exploration/env_infos/reward_run Min                -0.520085
exploration/env_infos/final/reward_ctrl Mean        -0.511009
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.511009
exploration/env_infos/final/reward_ctrl Min         -0.511009
exploration/env_infos/initial/reward_ctrl Mean      -0.0435418
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0435418
exploration/env_infos/initial/reward_ctrl Min       -0.0435418
exploration/env_infos/reward_ctrl Mean              -0.381877
exploration/env_infos/reward_ctrl Std                0.104345
exploration/env_infos/reward_ctrl Max               -0.0435418
exploration/env_infos/reward_ctrl Min               -0.567917
evaluation/num steps total                           1.245e+06
evaluation/num paths total                        1245
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.75535
evaluation/Rewards Std                               1.16161
evaluation/Rewards Max                               7.04855
evaluation/Rewards Min                              -0.691926
evaluation/Returns Mean                           4755.35
evaluation/Returns Std                              39.3917
evaluation/Returns Max                            4826.8
evaluation/Returns Min                            4706.88
evaluation/Actions Mean                             -0.00652645
evaluation/Actions Std                               0.807968
evaluation/Actions Max                               0.998308
evaluation/Actions Min                              -0.996424
evaluation/Num Paths                                 5
evaluation/Average Returns                        4755.35
evaluation/env_infos/final/reward_run Mean           5.08478
evaluation/env_infos/final/reward_run Std            0.932709
evaluation/env_infos/final/reward_run Max            6.4619
evaluation/env_infos/final/reward_run Min            3.62852
evaluation/env_infos/initial/reward_run Mean        -0.127801
evaluation/env_infos/initial/reward_run Std          0.254616
evaluation/env_infos/initial/reward_run Max          0.279499
evaluation/env_infos/initial/reward_run Min         -0.428616
evaluation/env_infos/reward_run Mean                 5.14706
evaluation/env_infos/reward_run Std                  1.12627
evaluation/env_infos/reward_run Max                  7.35178
evaluation/env_infos/reward_run Min                 -0.428616
evaluation/env_infos/final/reward_ctrl Mean         -0.320645
evaluation/env_infos/final/reward_ctrl Std           0.0366179
evaluation/env_infos/final/reward_ctrl Max          -0.264821
evaluation/env_infos/final/reward_ctrl Min          -0.379718
evaluation/env_infos/initial/reward_ctrl Mean       -0.132444
evaluation/env_infos/initial/reward_ctrl Std         0.0674871
evaluation/env_infos/initial/reward_ctrl Max        -0.0793829
evaluation/env_infos/initial/reward_ctrl Min        -0.263309
evaluation/env_infos/reward_ctrl Mean               -0.391713
evaluation/env_infos/reward_ctrl Std                 0.108016
evaluation/env_infos/reward_ctrl Max                -0.0512686
evaluation/env_infos/reward_ctrl Min                -0.570872
time/data storing (s)                                0.00689836
time/evaluation sampling (s)                         2.40802
time/exploration sampling (s)                        0.64089
time/logging (s)                                     0.0419812
time/saving (s)                                      0.0177532
time/training (s)                                   32.0941
time/epoch (s)                                      35.2097
time/total (s)                                   10456.6
Epoch                                              248
----------------------------------------------  ---------------
2020-07-09 00:01:07.910284 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 249 finished
----------------------------------------------  ---------------
replay_buffer/size                              251000
trainer/QF1 Loss                                     6.21359
trainer/QF2 Loss                                     7.24723
trainer/Policy Loss                               -281.004
trainer/Q1 Predictions Mean                        287.319
trainer/Q1 Predictions Std                          83.5879
trainer/Q1 Predictions Max                         347.725
trainer/Q1 Predictions Min                          17.6267
trainer/Q2 Predictions Mean                        286.972
trainer/Q2 Predictions Std                          83.5564
trainer/Q2 Predictions Max                         345.653
trainer/Q2 Predictions Min                          17.1977
trainer/Q Targets Mean                             287.936
trainer/Q Targets Std                               83.8039
trainer/Q Targets Max                              351.535
trainer/Q Targets Min                               16.2617
trainer/Log Pis Mean                                 6.43237
trainer/Log Pis Std                                  4.98518
trainer/Log Pis Max                                 16.837
trainer/Log Pis Min                                 -8.31527
trainer/Policy mu Mean                               0.0251162
trainer/Policy mu Std                                1.52364
trainer/Policy mu Max                                3.34139
trainer/Policy mu Min                               -3.80519
trainer/Policy log std Mean                         -0.834741
trainer/Policy log std Std                           0.360784
trainer/Policy log std Max                           0.10906
trainer/Policy log std Min                          -2.67329
trainer/Alpha                                        0.113486
trainer/Alpha Loss                                   0.940843
exploration/num steps total                     251000
exploration/num paths total                        251
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.52554
exploration/Rewards Std                              1.10145
exploration/Rewards Max                              6.68381
exploration/Rewards Min                             -0.35402
exploration/Returns Mean                          4525.54
exploration/Returns Std                              0
exploration/Returns Max                           4525.54
exploration/Returns Min                           4525.54
exploration/Actions Mean                             0.00253091
exploration/Actions Std                              0.793388
exploration/Actions Max                              0.999855
exploration/Actions Min                             -0.998941
exploration/Num Paths                                1
exploration/Average Returns                       4525.54
exploration/env_infos/final/reward_run Mean          4.66075
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           4.66075
exploration/env_infos/final/reward_run Min           4.66075
exploration/env_infos/initial/reward_run Mean        0.0625823
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max         0.0625823
exploration/env_infos/initial/reward_run Min         0.0625823
exploration/env_infos/reward_run Mean                4.90322
exploration/env_infos/reward_run Std                 1.07848
exploration/env_infos/reward_run Max                 6.96455
exploration/env_infos/reward_run Min                -0.0275276
exploration/env_infos/final/reward_ctrl Mean        -0.274333
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.274333
exploration/env_infos/final/reward_ctrl Min         -0.274333
exploration/env_infos/initial/reward_ctrl Mean      -0.0740891
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0740891
exploration/env_infos/initial/reward_ctrl Min       -0.0740891
exploration/env_infos/reward_ctrl Mean              -0.377683
exploration/env_infos/reward_ctrl Std                0.108527
exploration/env_infos/reward_ctrl Max               -0.0531349
exploration/env_infos/reward_ctrl Min               -0.580328
evaluation/num steps total                           1.25e+06
evaluation/num paths total                        1250
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.79106
evaluation/Rewards Std                               1.13415
evaluation/Rewards Max                               7.06698
evaluation/Rewards Min                              -0.799874
evaluation/Returns Mean                           4791.06
evaluation/Returns Std                              12.0636
evaluation/Returns Max                            4809.29
evaluation/Returns Min                            4777.41
evaluation/Actions Mean                             -0.00189522
evaluation/Actions Std                               0.802652
evaluation/Actions Max                               0.997973
evaluation/Actions Min                              -0.99773
evaluation/Num Paths                                 5
evaluation/Average Returns                        4791.06
evaluation/env_infos/final/reward_run Mean           5.19257
evaluation/env_infos/final/reward_run Std            0.939341
evaluation/env_infos/final/reward_run Max            6.88558
evaluation/env_infos/final/reward_run Min            4.36563
evaluation/env_infos/initial/reward_run Mean        -0.0225063
evaluation/env_infos/initial/reward_run Std          0.169663
evaluation/env_infos/initial/reward_run Max          0.2945
evaluation/env_infos/initial/reward_run Min         -0.192566
evaluation/env_infos/reward_run Mean                 5.17762
evaluation/env_infos/reward_run Std                  1.10186
evaluation/env_infos/reward_run Max                  7.42488
evaluation/env_infos/reward_run Min                 -0.396263
evaluation/env_infos/final/reward_ctrl Mean         -0.433947
evaluation/env_infos/final/reward_ctrl Std           0.0668583
evaluation/env_infos/final/reward_ctrl Max          -0.302286
evaluation/env_infos/final/reward_ctrl Min          -0.489473
evaluation/env_infos/initial/reward_ctrl Mean       -0.0621059
evaluation/env_infos/initial/reward_ctrl Std         0.053643
evaluation/env_infos/initial/reward_ctrl Max        -0.0182299
evaluation/env_infos/initial/reward_ctrl Min        -0.164035
evaluation/env_infos/reward_ctrl Mean               -0.386552
evaluation/env_infos/reward_ctrl Std                 0.110164
evaluation/env_infos/reward_ctrl Max                -0.0182299
evaluation/env_infos/reward_ctrl Min                -0.579973
time/data storing (s)                                0.0069249
time/evaluation sampling (s)                         2.46684
time/exploration sampling (s)                        0.664792
time/logging (s)                                     0.0407141
time/saving (s)                                      0.0161891
time/training (s)                                   35.5133
time/epoch (s)                                      38.7087
time/total (s)                                   10495.5
Epoch                                              249
----------------------------------------------  ---------------
2020-07-09 00:01:47.302780 CST | [name-of-experiment_2020_07_08_19_58_11_0000--s-0] Epoch 250 finished
----------------------------------------------  ---------------
replay_buffer/size                              252000
trainer/QF1 Loss                                    11.4013
trainer/QF2 Loss                                    11.6692
trainer/Policy Loss                               -280.459
trainer/Q1 Predictions Mean                        286.653
trainer/Q1 Predictions Std                          82.2751
trainer/Q1 Predictions Max                         353.324
trainer/Q1 Predictions Min                          17.5202
trainer/Q2 Predictions Mean                        286.624
trainer/Q2 Predictions Std                          82.0658
trainer/Q2 Predictions Max                         351.488
trainer/Q2 Predictions Min                          18.7852
trainer/Q Targets Mean                             286.285
trainer/Q Targets Std                               82.2588
trainer/Q Targets Max                              354.092
trainer/Q Targets Min                               18.5354
trainer/Log Pis Mean                                 6.45426
trainer/Log Pis Std                                  5.09793
trainer/Log Pis Max                                 18.6966
trainer/Log Pis Min                                 -5.38712
trainer/Policy mu Mean                               0.086249
trainer/Policy mu Std                                1.51197
trainer/Policy mu Max                                3.6556
trainer/Policy mu Min                               -3.62133
trainer/Policy log std Mean                         -0.85613
trainer/Policy log std Std                           0.363845
trainer/Policy log std Max                           0.0284483
trainer/Policy log std Min                          -2.83445
trainer/Alpha                                        0.116958
trainer/Alpha Loss                                   0.974831
exploration/num steps total                     252000
exploration/num paths total                        252
exploration/path length Mean                      1000
exploration/path length Std                          0
exploration/path length Max                       1000
exploration/path length Min                       1000
exploration/Rewards Mean                             4.40455
exploration/Rewards Std                              1.09352
exploration/Rewards Max                              6.47828
exploration/Rewards Min                             -0.508974
exploration/Returns Mean                          4404.55
exploration/Returns Std                              0
exploration/Returns Max                           4404.55
exploration/Returns Min                           4404.55
exploration/Actions Mean                             0.0105514
exploration/Actions Std                              0.789214
exploration/Actions Max                              0.999919
exploration/Actions Min                             -0.999941
exploration/Num Paths                                1
exploration/Average Returns                       4404.55
exploration/env_infos/final/reward_run Mean          6.0551
exploration/env_infos/final/reward_run Std           0
exploration/env_infos/final/reward_run Max           6.0551
exploration/env_infos/final/reward_run Min           6.0551
exploration/env_infos/initial/reward_run Mean       -0.129192
exploration/env_infos/initial/reward_run Std         0
exploration/env_infos/initial/reward_run Max        -0.129192
exploration/env_infos/initial/reward_run Min        -0.129192
exploration/env_infos/reward_run Mean                4.77833
exploration/env_infos/reward_run Std                 1.06988
exploration/env_infos/reward_run Max                 6.82496
exploration/env_infos/reward_run Min                -0.160055
exploration/env_infos/final/reward_ctrl Mean        -0.410148
exploration/env_infos/final/reward_ctrl Std          0
exploration/env_infos/final/reward_ctrl Max         -0.410148
exploration/env_infos/final/reward_ctrl Min         -0.410148
exploration/env_infos/initial/reward_ctrl Mean      -0.0536753
exploration/env_infos/initial/reward_ctrl Std        0
exploration/env_infos/initial/reward_ctrl Max       -0.0536753
exploration/env_infos/initial/reward_ctrl Min       -0.0536753
exploration/env_infos/reward_ctrl Mean              -0.373782
exploration/env_infos/reward_ctrl Std                0.107565
exploration/env_infos/reward_ctrl Max               -0.0518784
exploration/env_infos/reward_ctrl Min               -0.574505
evaluation/num steps total                           1.255e+06
evaluation/num paths total                        1255
evaluation/path length Mean                       1000
evaluation/path length Std                           0
evaluation/path length Max                        1000
evaluation/path length Min                        1000
evaluation/Rewards Mean                              4.7472
evaluation/Rewards Std                               1.18069
evaluation/Rewards Max                               7.00773
evaluation/Rewards Min                              -1.10785
evaluation/Returns Mean                           4747.2
evaluation/Returns Std                              66.3682
evaluation/Returns Max                            4874.47
evaluation/Returns Min                            4679.78
evaluation/Actions Mean                              0.003691
evaluation/Actions Std                               0.804072
evaluation/Actions Max                               0.998747
evaluation/Actions Min                              -0.997824
evaluation/Num Paths                                 5
evaluation/Average Returns                        4747.2
evaluation/env_infos/final/reward_run Mean           5.29269
evaluation/env_infos/final/reward_run Std            0.984262
evaluation/env_infos/final/reward_run Max            7.0571
evaluation/env_infos/final/reward_run Min            4.10249
evaluation/env_infos/initial/reward_run Mean        -0.0696702
evaluation/env_infos/initial/reward_run Std          0.256569
evaluation/env_infos/initial/reward_run Max          0.396131
evaluation/env_infos/initial/reward_run Min         -0.281419
evaluation/env_infos/reward_run Mean                 5.13513
evaluation/env_infos/reward_run Std                  1.15339
evaluation/env_infos/reward_run Max                  7.43772
evaluation/env_infos/reward_run Min                 -0.649379
evaluation/env_infos/final/reward_ctrl Mean         -0.414613
evaluation/env_infos/final/reward_ctrl Std           0.0970885
evaluation/env_infos/final/reward_ctrl Max          -0.253256
evaluation/env_infos/final/reward_ctrl Min          -0.522664
evaluation/env_infos/initial/reward_ctrl Mean       -0.137878
evaluation/env_infos/initial/reward_ctrl Std         0.0532069
evaluation/env_infos/initial/reward_ctrl Max        -0.0698309
evaluation/env_infos/initial/reward_ctrl Min        -0.208873
evaluation/env_infos/reward_ctrl Mean               -0.387927
evaluation/env_infos/reward_ctrl Std                 0.106992
evaluation/env_infos/reward_ctrl Max                -0.0643959
evaluation/env_infos/reward_ctrl Min                -0.584312
time/data storing (s)                                0.00714878
time/evaluation sampling (s)                         2.45394
time/exploration sampling (s)                        0.675693
time/logging (s)                                     0.0437871
time/saving (s)                                      0.0161669
time/training (s)                                   36.1608
time/epoch (s)                                      39.3576
time/total (s)                                   10534.9
Epoch                                              250
----------------------------------------------  ---------------
